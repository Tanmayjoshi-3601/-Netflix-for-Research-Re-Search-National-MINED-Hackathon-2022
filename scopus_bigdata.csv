Authors,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,Link,Abstract,Author Keywords,Index Keywords,Document Type,Source,EID
"Lucero-Obusan C., Oda G., Mostaghimi A., Schirmer P., Holodniy M.","55646105500;8790286200;57449909700;26657639300;7004800293;","Public health surveillance in the U.S. Department of Veterans Affairs: evaluation of the Praedico surveillance system",2022,"BMC Public Health","22","1","272","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124500716&doi=10.1186%2fs12889-022-12578-2&partnerID=40&md5=e348ec5ee3dba358ed59a12a36dbc8b9","Background: Early threat detection and situational awareness are vital to achieving a comprehensive and accurate view of health-related events for federal, state, and local health agencies. Key to this are public health and syndromic surveillance systems that can analyze large data sets to discover patterns, trends, and correlations of public health significance. In 2020, Department of Veterans Affairs (VA) evaluated its public health surveillance system and identified areas for improvement. Methods: Using the Centers for Disease Control and Prevention (CDC) Guidelines for Evaluating Public Health Surveillance Systems, we assessed the ability of the Praedico Surveillance System to perform public health surveillance for a variety of health issues and evaluated its performance compared to an enterprise data solution (VA Corporate Data Warehouse), legacy surveillance system (VA ESSENCE) and a national, collaborative syndromic surveillance platform (CDC NSSP BioSense). Results: Review of system attributes found that the system was simple, flexible, and stable. Representativeness, timeliness, sensitivity, and Predictive Value Positive were acceptable but could be further improved. Data quality issues and acceptability present challenges that potentially affect the overall usefulness of the system. Conclusions: Praedico is a customizable surveillance and data analytics platform built on big data technologies. Functionality is straightforward, with rapid query generation and runtimes. Data can be graphed, mapped, analyzed, and shared with key decision makers and stakeholders. Evaluation findings suggest that future development and system enhancements should focus on addressing Praedico data quality issues and improving user acceptability. Because Praedico is designed to handle big data queries and work with data from a variety of sources, it could be enlisted as a tool for interdepartmental and interagency collaboration and public health data sharing. We suggest that future system evaluations include measurements of value and effectiveness along with additional organizations and functional assessments. © 2022, The Author(s).","Big data; Electronic health records; Health informatics; Public health; Surveillance; Syndromic surveillance; System evaluation; Veterans",,Article,Scopus,2-s2.0-85124500716
"Jilka S., Odoi C.M., van Bilsen J., Morris D., Erturk S., Cummins N., Cella M., Wykes T.","55245639900;57202603480;57226165979;56360681700;57226160670;55350497300;24333907400;57369420600;","Identifying schizophrenia stigma on Twitter: a proof of principle model using service user supervised machine learning",2022,"npj Schizophrenia","8","1","1","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124453167&doi=10.1038%2fs41537-021-00197-6&partnerID=40&md5=22afef66c17b34fbe156206cb33a1ccb","Stigma has negative effects on people with mental health problems by making them less likely to seek help. We develop a proof of principle service user supervised machine learning pipeline to identify stigmatising tweets reliably and understand the prevalence of public schizophrenia stigma on Twitter. A service user group advised on the machine learning model evaluation metric (fewest false negatives) and features for machine learning. We collected 13,313 public tweets on schizophrenia between January and May 2018. Two service user researchers manually identified stigma in 746 English tweets; 80% were used to train eight models, and 20% for testing. The two models with fewest false negatives were compared in two service user validation exercises, and the best model used to classify all extracted public English tweets. Tweets classed as stigmatising by service users were more negative in sentiment (t (744) = 12.02, p < 0.001 [95% CI: 0.196–0.273]). Our linear Support Vector Machine was the best performing model with fewest false negatives and higher service user validation. This model identified public stigma in 47% of English tweets (n5,676) which were more negative in sentiment (t (12,143) = 64.38, p < 0.001 [95% CI: 0.29–0.31]). Machine learning can identify stigmatising tweets at large scale, with service user involvement. Given the prevalence of stigma, there is an urgent need for education and online campaigns to reduce it. Machine learning can provide a real time metric on their success. © 2022, The Author(s).",,"Article; big data; clinical evaluation; clinical feature; comparative study; controlled study; dimensionality reduction; false negative result; health service; human; linear support vector machine; prevalence; random forest; schizophrenia; sentiment analysis; social media; social stigma; supervised machine learning",Article,Scopus,2-s2.0-85124453167
"Rezaei M., Sanayei A., Aghdaie S.F.A., Ansari A.","57447548800;55823623500;55090108600;55673218300;","Improving the Omnichannel Customers’ Lifetime Value Using Association Rules Data Mining: A Case Study of Agriculture Bank of Iran",2022,"Iranian journal of Management Studies","15","1",,"49","68",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124420044&doi=10.22059%2fIJMS.2021.314405.674317&partnerID=40&md5=aa1c1219e9d2fa56210f51f0392ac03b","Multi-channel marketing causes the customer to lack a unique identity in different channels. This issue overshadows the synergy of the channels in strengthening the positive attitude of the customers. However, an omnichannel marketing strategy can work properly. The main purpose of this study, which was conducted in Agriculture Bank of Iran, was to develop a comprehensive model for calculating customers’ lifetime values, analyzing customers’ behaviors in different channels by association rules data mining, and analyzing the relationship between omnichannel strategy and CLV. First, the association rules in the big data of customers’ banking transactions in different channels were identified using association rules data mining. Then, the CLV indicators were identified and prioritized using interviews, questionnaires, and AHP methods, and the lifetime values of omnichannel and other customers were calculated and compared using t-test. Then, omnichannel customers were categorized based on the association rules and the lifetime values of omnichannel customers of different categories was compared using ANOVA method. Eleven association rules regarding the use of banking channels by omnichannel customers were identified. The results show that there is a significant difference between the lifetime values of omnichannel customers and other customers and the lifetime values of omnichannel customers is 134% more. © 2021 University of Tehran, College of Farabi. All Rights Reserved.","Association rule data mining; Big data; CLV; Keyword: Omnichannel marketing; Omnichannel banking",,Article,Scopus,2-s2.0-85124420044
"Xie H., Cui X., Ying X., Hu X., Xuan J., Xu S.","57443783400;57443205300;57263793100;57443009000;55618446000;57262708900;","Development of a novel hospital payment system – Big data diagnosis & intervention Packet",2022,"Health Policy OPEN","3",,"100066","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124235584&doi=10.1016%2fj.hpopen.2022.100066&partnerID=40&md5=ff230a7b47c11ecbf7571e99e1029d84","The diagnosis related group (DRG) was the most commonly used prospective hospital payment platform in developed countries. One of the major limitations of the DRG system is that the DRG grouping is not sufficiently homogeneous in benchmarking underlying resource needs. We developed a novel hospital payment and management system called Big Data Diagnosis & Intervention Packet (BD-DIP) by applying the similar case mix index (CMI) principles but the grouping is based on unique combination of ICD-10 and ICD-9 v3 codes. The initial prototype of BD-DIP was developed using hospital discharge records in Shanghai and then piloted in Guangzhou, China. The average coefficient of variation of the DB-DIP is about one-third smaller than the US DRG system. Results from the pilot evaluation showed that introduction of the BD-DIP lead to about 5% hospital budget savings and notable improvement in hospital care efficiency, including increased institutional CMI, lower admission rates, smaller variation in hospital charges, and lower patient cost-sharing burdens. The implementation of hospital monitoring tools resulted in identification of potential irregular practices to enable further auditing and investigation. The BD-DIP platform has a number of advantages over DRG-based payment models in terms of more homogeneous resource utilization within groups, design simplicity, dynamic in grouping, and reimbursement value in reflecting real-world treatment pathways and costs, and easy to implement. © 2022 The Author(s)","Big Data; Case Mix; Diagnosis-Related Groups; International Classification of Diseases; Novel Hospital Payment Platform",,Article,Scopus,2-s2.0-85124235584
"Zhang J., Olatosi B., Yang X., Weissman S., Li Z., Hu J., Li X.","57345979900;16068980200;57224940955;57441629000;57285179900;57441627800;57204836921;","Studying patterns and predictors of HIV viral suppression using A Big Data approach: a research protocol",2022,"BMC Infectious Diseases","22","1","122","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124113819&doi=10.1186%2fs12879-022-07047-5&partnerID=40&md5=71a41246aabdc82a6b088fb7eb9748d4","Background: Given the importance of viral suppression in ending the HIV epidemic in the US and elsewhere, an optimal predictive model of viral status can help clinicians identify those at risk of poor viral control and inform clinical improvements in HIV treatment and care. With an increasing availability of electronic health record (EHR) data and social environmental information, there is a unique opportunity to improve our understanding of the dynamic pattern of viral suppression. Using a statewide cohort of people living with HIV (PLWH) in South Carolina (SC), the overall goal of the proposed research is to examine the dynamic patterns of viral suppression, develop optimal predictive models of various viral suppression indicators, and translate the models to a beta version of service-ready tools for clinical decision support. Methods: The PLWH cohort will be identified through the SC Enhanced HIV/AIDS Reporting System (eHARS). The SC Office of Revenue and Fiscal Affairs (RFA) will extract longitudinal EHR clinical data of all PLWH in SC from multiple health systems, obtain data from other state agencies, and link the patient-level data with county-level data from multiple publicly available data sources. Using the deidentified data, the proposed study will consist of three operational phases: Phase 1: “Pattern Analysis” to identify the longitudinal dynamics of viral suppression using multiple viral load indicators; Phase 2: “Model Development” to determine the critical predictors of multiple viral load indicators through artificial intelligence (AI)-based modeling accounting for multilevel factors; and Phase 3: “Translational Research” to develop a multifactorial clinical decision system based on a risk prediction model to assist with the identification of the risk of viral failure or viral rebound when patients present at clinical visits. Discussion: With both extensive data integration and data analytics, the proposed research will: (1) improve the understanding of the complex inter-related effects of longitudinal trajectories of HIV viral suppressions and HIV treatment history while taking into consideration multilevel factors; and (2) develop empirical public health approaches to achieve ending the HIV epidemic through translating the risk prediction model to a multifactorial decision system that enables the feasibility of AI-assisted clinical decisions. © 2022, The Author(s).","Data analytics; HIV/AIDS; Pattern analysis; Viral rebound; Viral suppression","acquired immune deficiency syndrome; artificial intelligence; human; Human immunodeficiency virus infection; virus load; Acquired Immunodeficiency Syndrome; Artificial Intelligence; Big Data; HIV Infections; Humans; Viral Load",Article,Scopus,2-s2.0-85124113819
"Brossard P.-Y., Minvielle E., Sicotte C.","57439826200;6603383439;7003460448;","The path from big data analytics capabilities to value in hospitals: a scoping review",2022,"BMC Health Services Research","22","1","134","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123987577&doi=10.1186%2fs12913-021-07332-0&partnerID=40&md5=a54332b42974bf5ddc5c5bef2aca7d1e","Background: As the uptake of health information technologies increased, most healthcare organizations have become producers of big data. A growing number of hospitals are investing in the development of big data analytics (BDA) capabilities. If the promises associated with these capabilities are high, how hospitals create value from it remains unclear. The present study undertakes a scoping review of existing research on BDA use in hospitals to describe the path from BDA capabilities (BDAC) to value and its associated challenges. Methods: This scoping review was conducted following Arksey and O’Malley’s 5 stages framework. A systematic search strategy was adopted to identify relevant articles in Scopus and Web of Science. Data charting and extraction were performed following an analytical framework that builds on the resource-based view of the firm to describe the path from BDA capabilities to value in hospitals. Results: Of 1,478 articles identified, 94 were included. Most of them are experimental research (n=69) published in medical (n=66) or computer science journals (n=28). The main value targets associated with the use of BDA are improving the quality of decision-making (n=56) and driving innovation (n=52) which apply mainly to care (n=67) and administrative (n=48) activities. To reach these targets, hospitals need to adequately combine BDA capabilities and value creation mechanisms (VCM) to enable knowledge generation and drive its assimilation. Benefits are endpoints of the value creation process. They are expected in all articles but realized in a few instances only (n=19). Conclusions: This review confirms the value creation potential of BDA solutions in hospitals. It also shows the organizational challenges that prevent hospitals from generating actual benefits from BDAC-building efforts. The configuring of strategies, technologies and organizational capabilities underlying the development of value-creating BDA solutions should become a priority area for research, with focus on the mechanisms that can drive the alignment of BDA and organizational strategies, and the development of organizational capabilities to support knowledge generation and assimilation. © 2022, The Author(s).","Big data analytics; Capabilities; Hospitals; Resource-based view; Value creation","hospital; medical informatics; Big Data; Data Science; Hospitals; Medical Informatics",Article,Scopus,2-s2.0-85123987577
"Mosharraf S.I.M., Adnan M.A.","57432907300;57211201230;","Improving lookup and query execution performance in distributed Big Data systems using Cuckoo Filter",2022,"Journal of Big Data","9","1","12","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123774848&doi=10.1186%2fs40537-022-00563-w&partnerID=40&md5=110f3a366cfa04470c6335c380920c57","Performance is a critical concern when reading and writing data from billions of records stored in a Big Data warehouse. We introduce two scopes for query performance improvement. One is to improve the performance of lookup queries after data deletion in Big Data systems that use Eventual Consistency. We propose a scheme to improve lookup performance after data deletion by using Cuckoo Filter. Another scope for improvement is to avoid unnecessary network round-trips for querying in remote nodes in a distributed Big Data cluster when it is known that the nodes do not have requested partition of data. We propose a scheme using probabilistic filters that are looked up before querying remote nodes so that queries resulting in no data can be skipped from passing through the network. We evaluate our schemes with Cassandra using real dataset and show that each scheme can improve performance of lookup queries for up to 2x. © 2022, The Author(s).","Big Data; Bloom filter; Cuckoo Filter; Distributed systems; Probabilistic data structure; Query optimization",,Article,Scopus,2-s2.0-85123774848
"Vranopoulos G., Clarke N., Atkinson S.","57410075100;8961310300;22233456400;","Addressing big data variety using an automated approach for data characterization",2022,"Journal of Big Data","9","1","8","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122723984&doi=10.1186%2fs40537-021-00554-3&partnerID=40&md5=de6dd6d0e3548e6918b5e20b102c9018","The creation of new knowledge from manipulating and analysing existing knowledge is one of the primary objectives of any cognitive system. Most of the effort on Big Data research has been focussed upon Volume and Velocity, while Variety, “the ugly duckling” of Big Data, is often neglected and difficult to solve. A principal challenge with Variety is being able to understand and comprehend the data. This paper proposes and evaluates an automated approach for metadata identification and enrichment in describing Big Data. The paper focuses on the use of self-learning systems that will enable automatic compliance of data against regulatory requirements along with the capability of generating valuable and readily usable metadata towards data classification. Two experiments towards data confidentiality and data identification were conducted in evaluating the feasibility of the approach. The focus of the experiments was to confirm that repetitive manual tasks can be automated, thus reducing the focus of a Data Scientist on data identification and thereby providing more focus towards the extraction and analysis of the data itself. The origin of the datasets used were Private/Business and Public/Governmental and exhibited diverse characteristics in relation to the number of files and size of the files. The experimental work confirmed that: (a) the use of algorithmic techniques attributed to the substantial decrease in false positives regarding the identification of confidential information; (b) evidence that the use of a fraction of a data set along with statistical analysis and supervised learning is sufficient in identifying the structure of information within it. With this approach, the issues of understanding the nature of data can be mitigated, enabling a greater focus on meaningful interpretation of the heterogeneous data. © 2022, The Author(s).","Big Data; Contextual integrity; Data characterization; Data confidentiality; Data Format; Data origination; Delimiter determination; Metadata; Variety",,Article,Scopus,2-s2.0-85122723984
"Singh A., Glińska-Neweś A.","57193264185;36020019100;","Modeling the public attitude towards organic foods: a big data and text mining approach",2022,"Journal of Big Data","9","1","2","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122543553&doi=10.1186%2fs40537-021-00551-6&partnerID=40&md5=441003ff6db17d98ca0b5f3723dc4084","This study aims to identify the topics that users post on Twitter about organic foods and to analyze the emotion-based sentiment of those tweets. The study addresses a call for an application of big data and text mining in different fields of research, as well as proposes more objective research methods in studies on food consumption. There is a growing interest in understanding consumer choices for foods which are caused by the predominant contribution of the food industry to climate change. So far, customer attitudes towards organic food have been studied mostly with self-reported methods, such as questionnaires and interviews, which have many limitations. Therefore, in the present study, we used big data and text mining techniques as more objective methods to analyze the public attitude about organic foods. A total of 43,724 Twitter posts were extracted with streaming Application Programming Interface (API). Latent Dirichlet Allocation (LDA) algorithm was applied for topic modeling. A test of topic significance was performed to evaluate the quality of the topics. Public sentiment was analyzed based on the NRC emotion lexicon by utilizing Syuzhet package. Topic modeling results showed that people discuss on variety of themes related to organic foods such as plant-based diet, saving the planet, organic farming and standardization, authenticity, and food delivery, etc. Sentiment analysis results suggest that people view organic foods positively, though there are also people who are skeptical about the claims that organic foods are natural and free from chemicals and pesticides. The study contributes to the field of consumer behavior by implementing research methods grounded in text mining and big data. The study contributes also to the advancement of research in the field of sustainable food consumption by providing a fresh perspective on public attitude toward organic foods, filling the gaps in existing literature and research. © 2021, The Author(s).","Big data; Latent Dirichlet Allocation (LDA); Machine learning; Organic foods; Sentiment analysis; Text mining; Topic modeling",,Article,Scopus,2-s2.0-85122543553
"Batko K., Ślęzak A.","55322139800;7004682145;","The use of Big Data Analytics in healthcare",2022,"Journal of Big Data","9","1","3","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122536835&doi=10.1186%2fs40537-021-00553-4&partnerID=40&md5=e232da0eb0d783d1d397da2a18aba7ad","The introduction of Big Data Analytics (BDA) in healthcare will allow to use new technologies both in treatment of patients and health management. The paper aims at analyzing the possibilities of using Big Data Analytics in healthcare. The research is based on a critical analysis of the literature, as well as the presentation of selected results of direct research on the use of Big Data Analytics in medical facilities. The direct research was carried out based on research questionnaire and conducted on a sample of 217 medical facilities in Poland. Literature studies have shown that the use of Big Data Analytics can bring many benefits to medical facilities, while direct research has shown that medical facilities in Poland are moving towards data-based healthcare because they use structured and unstructured data, reach for analytics in the administrative, business and clinical area. The research positively confirmed that medical facilities are working on both structural data and unstructured data. The following kinds and sources of data can be distinguished: from databases, transaction data, unstructured content of emails and documents, data from devices and sensors. However, the use of data from social media is lower as in their activity they reach for analytics, not only in the administrative and business but also in the clinical area. It clearly shows that the decisions made in medical facilities are highly data-driven. The results of the study confirm what has been analyzed in the literature that medical facilities are moving towards data-based healthcare, together with its benefits. © 2022, The Author(s).","Big Data; Big Data Analytics; Data-driven healthcare",,Article,Scopus,2-s2.0-85122536835
"Wunderlich F., Memmert D.","57190607889;16039986900;","A big data analysis of Twitter data during premier league matches: do tweets contain information valuable for in-play forecasting of goals in football?",2022,"Social Network Analysis and Mining","12","1","23","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122010136&doi=10.1007%2fs13278-021-00842-z&partnerID=40&md5=902afc5b4eac7828196eb978984040f3","Data-related analysis in football increasingly benefits from Big Data approaches and machine learning methods. One relevant application of data analysis in football is forecasting, which relies on understanding and accurately modelling the process of a match. The present paper tackles two neglected facets of forecasting in football: Forecasts on the total number of goals and in-play forecasting (forecasts based on within-match information). Sentiment analysis techniques were used to extract the information reflected in almost two million tweets from more than 400 Premier League matches. By means of wordclouds and timely analysis of several tweet-based features, the Twitter communication over the full course of matches and shortly before and after goals was visualized and systematically analysed. Moreover, several forecasting models including a random forest model have been used to obtain in-play forecasts. Results suggest that in-play forecasting of goals is highly challenging, and in-play information does not improve forecasting accuracy. An additional analysis of goals from more than 30,000 matches from the main European football leagues supports the notion that the predictive value of in-play information is highly limited compared to pre-game information. This is a relevant result for coaches, match analysts and broadcasters who should not overestimate the value of in-play information. The present study also sheds light on how the perception and behaviour of Twitter users change over the course of a football match. A main result is that the sentiment of Twitter users decreases when the match progresses, which might be caused by an unjustified high expectation of football fans before the match. © 2021, The Author(s).","Big data; Data mining; Football forecasting; In-play forecasting; Social networks; Twitter","Big data; Data handling; Data mining; Decision trees; Learning systems; Sentiment analysis; Social networking (online); Sports; Analysis techniques; Football forecasting; Forecasting accuracy; Forecasting models; In-play forecasting; Machine learning methods; Predictive values; Random forest modeling; Sentiment analysis; Social network; Forecasting",Article,Scopus,2-s2.0-85122010136
"Zhu L., Wei J., Wu S., Zhou X., Sun J.","57190133292;56167696400;16403632000;57193730564;57452080200;","Application of unlabelled big data and deep semi-supervised learning to significantly improve the logging interpretation accuracy for deep-sea gas hydrate-bearing sediment reservoirs",2022,"Energy Reports","8",,,"2947","2963",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124619761&doi=10.1016%2fj.egyr.2022.01.139&partnerID=40&md5=67e7fe752967886dddc0d1a079d8e2a1","Due to the extremely complex reservoirs and strong heterogeneity, deep-sea gas hydrate logging porosity calculations still have problems, which further leads to insufficient resource calculation accuracy. Logging reservoir evaluation methods based on intelligence may be able to provide more reliable prediction results, especially the logging evaluation model based on deep learning with great potential. This paper proposes a new method to form unlabelled logging big data, and based on this, establishes a semi-supervised deep learning method suitable for deep-sea gas hydrate-bearing sediments porosity calculation, forming a porosity evaluation model. The method of forming logging big data expands 380 original data samples into 2280 labelled samples and 60050 unlabelled samples, which reduces the sampling requirements for deep-sea sediment formations with high sampling costs. The evaluation results show that the model not only obtains better results than other methods in the inspection wells corresponding to the areas where the training wells are located, but also obtains very good results in other wells that are not involved in the modelling. Compared with traditional prediction methods, the average relative error of porosity prediction is less than 4%. As far as we know, this is the first time that deep learning has been successfully applied to deep-sea hydrate sediment reservoir logging evaluation. It provides a new idea for intelligent logging evaluation of deep-sea hydrate sediment reservoirs. © 2022 The Author(s)","Big data application; Deep learning; Gas hydrate-bearing sediments; India national gas hydrate program","Application programs; Big data; Deep learning; Forecasting; Gas hydrates; Gases; Petroleum reservoir evaluation; Porosity; Sediments; Big data applications; Deep learning; Deep sea; Evaluation models; Gas hydrate bearing sediments; Hydrate sediments; India national gas hydrate program; Logging evaluation; Logging interpretation; Porosity calculations; Hydration",Article,Scopus,2-s2.0-85124619761
"Zhang J., Qu Z., Chen C., Wang H., Zhan Y., Ye B., Guo S.","55682633000;57189048446;57268889400;57203550518;56949825900;12792666700;7403649953;","Edge Learning: The Enabling Technology for Distributed Big Data Analytics in the Edge",2022,"ACM Computing Surveys","54","7","151","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115448495&doi=10.1145%2f3464419&partnerID=40&md5=b07bd802ad77ef7d56f068256603a9f0","Machine Learning (ML) has demonstrated great promise in various fields, e.g., self-driving, smart city, which are fundamentally altering the way individuals and organizations live, work, and interact. Traditional centralized learning frameworks require uploading all training data from different sources to a remote data server, which incurs significant communication overhead, service latency, and privacy issues. To further extend the frontiers of the learning paradigm, a new learning concept, namely, Edge Learning (EL) is emerging. It is complementary to the cloud-based methods for big data analytics by enabling distributed edge nodes to cooperatively training models and conduct inferences with their locally cached data. To explore the new characteristics and potential prospects of EL, we conduct a comprehensive survey of the recent research efforts on EL. Specifically, we first introduce the background and motivation. We then discuss the challenging issues in EL from the aspects of data, computation, and communication. Furthermore, we provide an overview of the enabling technologies for EL, including model training, inference, security guarantee, privacy protection, and incentive mechanism. Finally, we discuss future research opportunities on EL. We believe that this survey will provide a comprehensive overview of EL and stimulate fruitful future research in this field. © 2021 ACM.","edge computing; Edge learning; federated learning; machine learning; security and privacy","Advanced Analytics; Big data; Data Analytics; Data privacy; Machine learning; Surveys; Centralised; Edge computing; Edge learning; Enabling technologies; Federated learning; Learning frameworks; Machine-learning; Security and privacy; Self drivings; Training data; Edge computing",Article,Scopus,2-s2.0-85115448495
"Corallo A., Crespino A.M., Lazoi M., Lezzi M.","14020996400;57192370035;35956315900;57204022817;","Model-based Big Data Analytics-as-a-Service framework in smart manufacturing: A case study",2022,"Robotics and Computer-Integrated Manufacturing","76",,"102331","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124600298&doi=10.1016%2fj.rcim.2022.102331&partnerID=40&md5=acc849d85c8ae4ff8b409c5dc62e6d65","Today, in a smart manufacturing environment based on the Industry 4.0 paradigm, people, technological infrastructure and machinery equipped with sensors can constantly generate and communicate a huge volume of data, also known as Big Data. The manufacturing industry takes advantage of Big Data and analytics evolution by improving its capability to bring out valuable information and knowledge from industrial processes, production systems and sensors. The adoption of model-based frameworks in the Big Data Analytics pipeline can better address user configuration requirements (e.g. type of analysis to perform, type of algorithm to be applied) and also provide more transparency and clearness on the execution of workflows and data processing. In the current state of art, an application of a model-based framework in a manufacturing scenario is missing. Therefore, in this study, by means of a case study research focused on data from sensors associated with Computer Numerical Control machines, the configuration and execution of a Big Data Analytics pipeline with a Model-based Big Data Analytics-as-a-Service framework is described. The case study provides to theoreticians and managerial experts useful evidence for managing real-time data analytics and deploying a workflow that addresses specific analytical goals, driven by user requirements and developer models, in a complex manufacturing domain. © 2022","Anomaly detection; BDA; Big Data Analytics; Industry 4.0; MBDAaaS framework; Smart manufacturing","Advanced Analytics; Anomaly detection; Big data; Computer control systems; Data Analytics; Data handling; Machinery; Pipeline processing systems; Pipelines; Anomaly detection; BDA; Case-studies; Manufacturing environments; MBDAaaS framework; Model-based OPC; Service framework; Smart manufacturing; Technological infrastructure; Work-flows; Industry 4.0",Article,Scopus,2-s2.0-85124600298
"Bai J., Zeng Z., Abualnaja K.M., Xiong N.N.","57352815000;9634861000;57202135898;57225962791;","ADCC: An effective adaptive duty cycle control scheme for real time big data in Green IoT",2022,"Alexandria Engineering Journal","61","8",,"5959","5975",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119895588&doi=10.1016%2fj.aej.2021.11.026&partnerID=40&md5=affa6117ff841e260c9ae4fbd33d0880","Currently, large number of devices have been connected to the Internet of Things (IoT). Ubiquitous IoT devices encounter emergencies and generate much data which may lead to congestion and urgency to be processed timely. To alleviate it, many approaches have been proposed and Adaptive Duty Cycle Control (ADCC) scheme is an effective one. The main contributions of this paper are as follows: (a) Unlike previous studies that mostly the efficiency of congestion control and real-time processing is experiment-oriented, specially, this paper theoretically gives a targeted optimization of duty cycle ratio, which can effectively guide the design of ADCC scheme for real time big data. (b) A general design principle is proposed to guide the scheme designing in order to improve the efficiency and performance of networks. (c) A comprehensive congestion avoidance and real-time processing scheme combining dynamic duty cycle adjustment and full utilization of residual energy is proposed. Thus, these ideas can meet the concept of Green IoT. Through our extensively theoretical and experimental analysis, the principle can effectively guide the design of ADCC scheme, and can reduce the delay, data drop ratio by 20.95% − 77.85% and 29.63% − 100% respectively, without affecting network lifetime, compared with previous scheme. © 2021 THE AUTHORS","Design principle; Duty cycle; Green IoT; Lifetime; Real time big data","Big data; Control schemes; Design Principles; Duty cycle control; Duty-cycle; Green internet of thing; Green internets; Lifetime; Real time big data; Real- time; Realtime processing; Internet of things",Article,Scopus,2-s2.0-85119895588
"Jin X., Hu H.","57218844977;57461054700;","Research and implementation of smart energy investment and financing system design based on energy mega data mining",2022,"Energy Reports","8",,,"1226","1235",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125017504&doi=10.1016%2fj.egyr.2022.02.044&partnerID=40&md5=88252a32d8db35480016f1a238005562","With the accelerated pace of energy reform, State Grid Corporation of China has actively formulated and implemented a big data strategy, established a big data center, tapped the value of massive energy data resources, improved the data asset management system, and used data to drive management reform and transformation and upgrading. In order to optimize the current energy investment and financing system, this paper attempts to analyze the problems existing in the current rural energy investment and financing system and explore the digital transformation of energy enterprises based on data mining technology. Based on energy big data, this paper constructs an intelligent energy investment and financing system with energy investment and financing mechanism, and emphatically analyzes the factors that influence the strategic choice of each participant, in order to seek the strategy of evolution and stability. Higher emission reduction targets will prompt enterprises to increase their emission reduction efforts, increase their demand for carbon quotas, and at the same time increase the total amount of carbon quotas that can be allocated in the market, resulting in a decline in market clearing carbon prices. In the process of socialist development in the new period, we should seize the opportunity, proceed from the whole, make overall plans, clarify the future development direction, and make bold innovations in financing methods. © 2022","Energy enterprises; Energy mega data; Investment and financing system","Big data; Carbon; Commerce; Data mining; Digital storage; Information management; Investments; Metadata; Datacenter; Energy; Energy enterprise; Energy investment and financings; Energy mega data; Energy reforms; Investment and financing system; Investment and financings; Smart energies; Emission control",Article,Scopus,2-s2.0-85125017504
"Wang H., Yuan J., Qi G., Li Y., Yang J., Dong H., Ma Y.","57211368196;57377150200;57363531000;57362656400;56176391300;55926874700;57191491814;","A data-driven load forecasting method for incentive demand response",2022,"Energy Reports","8",,,"1013","1019",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124706505&doi=10.1016%2fj.egyr.2022.01.232&partnerID=40&md5=3116d4e35f17b8610f4123123cbc6411","The participation of incentive demand response (IDR) can improve power grid flexibility, and reduce peak shaving pressure. However, the further development of incentive demand response services will be limited by the uncertainty of user response behavior. To tackle this problem, this paper proposes a data-driven load forecasting method for IDR, which considers consumer behavior. Firstly, we describe the power market auxiliary service operation mechanism for load aggregators, through which the ability of demand-side resources to respond to auxiliary services is improved. Then, add an attention layer to improve the traditional long and short memory (LSTM) model, and propose an IDR load forecasting method based on this model, which considers the model's learning of historical data of user behavior. Finally, establish a simulation model to verify the effectiveness of the improved LSTM model forecasting. The proposed method is simulated and compared with the traditional LSTM method and k-nearest neighbor prediction method. The results show that compared with the other two methods, the mean error, root means square error, and mean absolute percentage error in the improved LSTM method are reduced by 7.31 MW, 8.32 MW, 2.06% and 10.62 MW, 13.46 MW and 3.09%, respectively, which effectively improves the accuracy of forecasting and increase the participation of demand-side resources in the power auxiliary service market. © 2022 The Author(s)","Data-driven; Incentive demand response; Power big data; Power market; User behavior forecasting","Big data; Consumer behavior; Electric power plant loads; Electric power transmission networks; Errors; Long short-term memory; Nearest neighbor search; Power markets; Data driven; Demand response; Incentive demand response; Load forecasting; Long memory; Power; Power big data; Short memory; User behavior forecasting; User behaviors; Forecasting",Article,Scopus,2-s2.0-85124706505
"Zhang M., Liu X., Shang Y., Kang L., Wu Q., Cheng W.","57204196943;57206480667;57212465137;57204196610;57212567751;57454717300;","Research on comprehensive diagnosis model of anti-stealing electricity based on big data technology",2022,"Energy Reports","8",,,"916","925",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124694406&doi=10.1016%2fj.egyr.2022.02.045&partnerID=40&md5=c823414123c77521d8a86c478883e302","For a long time, power supply enterprises have been plagued by the problem of electricity theft and default. Although power supply enterprises have strengthened anti-theft measures from various aspects, the huge benefits brought by electricity theft and default have driven the development trend of electricity theft methods to be more complicated and concealed, which has brought great challenges to the anti-theft work. In order to quickly and accurately locate the suspected users of ”defaulting on electricity consumption and stealing electricity”, based on the massive data of marketing business application system and electricity consumption information collection system, this paper analyzes and studies the existing common means of stealing electricity, and establishes an anti-stealing diagnostic analysis model by combining the correlation analysis algorithm, extracts the abnormal data related to stealing electricity and its prevention and investigation, and accurately locks the suspected users. The aim is to improve the efficiency and accuracy of preventing and investigating electricity theft, so as to effectively curb the occurrence of electricity theft, reduce the line loss rate, ensure the safety of the power grid and improve the efficiency of power supply enterprises. © 2022 The Author(s)","Anti-stealing electricity; Association analysis algorithm; Big data technology; Diagnosis model","Crime; Efficiency; Electric power transmission networks; Electric power utilization; Analysis algorithms; Anti-stealing electricity; Association analyse algorithm; Association analysis; Big data technology; Data technologies; Diagnosis model; Electricity theft; Electricity-consumption; Power supply enterprise; Big data",Article,Scopus,2-s2.0-85124694406
"Wang J.","55888880600;","A novel oscillation identification method for grid-connected renewable energy based on big data technology",2022,"Energy Reports","8",,,"663","671",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124591880&doi=10.1016%2fj.egyr.2022.02.022&partnerID=40&md5=eab6c517558fe64fba89928a1f27bfdf","With the development of big data technology, power system has entered the era of data analysis. With the help of the massive data provided by the wide area measurement system, the power system can be easily evaluated, and the abnormal operation status can be detected and positioned. As the increase of renewable energy permeability, more new abnormal operating status have appeared in the system. Aimed at the abnormal operation state in the development of new energy, this paper proposes an oscillation location scheme based on evidence theory and support vector machine, which makes up for the limitation of single oscillation location method. The result of location analysis of oscillation energy method, oscillation phase difference method and forced oscillation phase difference location method is fused by evidence theory. © 2022 The Author(s)","Big data; Evidence theory; Oscillation identification; Support vector machine","Big data; Location; Renewable energy resources; Abnormal operation; Data technologies; Evidence theories; Location method; Oscillation identification; Oscillation phasis; Phase difference; Power; Renewable energies; Support vectors machine; Support vector machines",Article,Scopus,2-s2.0-85124591880
"Gao J., Gao J., Guo Q., Li L.","57204861855;57210411234;57450205000;57219552677;","Research on operation status and fault deduction system design of transformer in large sports venues",2022,"Energy Reports","8",,,"539","546",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124559053&doi=10.1016%2fj.egyr.2022.01.228&partnerID=40&md5=c42309be54570214660844eff21c49b3","The problem of low transformer load rate (less than 60%) in the actual operation of large stadiums and gymnasiums has affected the waste of power energy and the improvement of power cost in the industry. The transformer failure of large stadiums and gymnasiums may further cause serious power accidents in the future. In order to solve the above problems, firstly, this paper analyzes the transformer load rate data of the two large sports venues(Nanchang International Sports and Tennis Stadium in Jiangxi Province)and probes into the design data and calculation coefficient of the transformer of the large sports venues; Secondly, this paper puts forward some suggestions on how to improve the operation load rate of transformers in stadiums from the aspects of transformer design and selection of demand coefficient; Thirdly, the paper establishes the system hardware environment by using firewall, special switch and information database, and then realizes the design of software deduction program through correlation analysis, fault diagnosis and cluster deduction, and completes the design of transformer fault deduction system in large sports venues. Finally, the results clearly show that the transformer load rate is relatively low in large sports stadiums(the highest value is only about 55% and well below the 60% standard). On other hand, compared with the traditional system and based on big data association, the average relative errors of fault points 1, 2 and 3 decreased by 67.8%, 56.4% and 51.6% respectively and the diagnosis time has also been greatly shorten by the transformer fault deduction system. In short, this paper uses association rules to mine and obtain transformer fault characteristics, combined with the advantages of virtual reality technology, designs an effective and reliable transformer fault deduction visualization system for large sports venues. © 2022 The Author(s)","Big data; Deduction system; Large venues; Load rate; Transformer fault","Big data; Costs; Data visualization; Gymnasiums; Roofs; Sports; Stadiums; Virtual reality; Actual operation; Deduction systems; Large venue; Load rate; Operation status; Power costs; Sports venues; Transformer failure; Transformer faults; Transformer-load; Recreation centers",Article,Scopus,2-s2.0-85124559053
"Goel R.K., Vishnoi S.","55424000700;57210724974;","Urbanization and sustainable development for inclusiveness using ICTs",2022,"Telecommunications Policy","46","6","102311","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123785160&doi=10.1016%2fj.telpol.2022.102311&partnerID=40&md5=7aa0408fbd308a2090288c28218914e7","The arrival of information and communication technology (ICT) as well as the deployments of free and open-source software (FOSS) have brought hope to developing countries that the use of enabling technologies potentially mitigates the impact of global environmental and socio-economic crises, and it drive radical changes in a user's skills or culture. In 2019, with widespread territorial disparities, approximately 53.6% people were connected to the Internet worldwide. The majority of the offline user lives in the least developed countries, and only 19% of them use the internet, compared to 87% in developed nations. Sustainable development depends on successful management of open and inclusive urban development, especially in low-income and middle-income countries, where the fastest urbanization is expected to occur by the year 2050. The application of ICT enhances the independence, dignity, and equal opportunities of all people, thereby promoting their integration into society. An inclusive approach-based citizen participation is extremely important for building an inclusive society. Furthermore, this study highlights the current issues and challenges in developing countries, as well as the role of ICT in promoting socio-economic development, where it can serve as a catalyst for the implementation of the concept of sustainable urbanization. Considering the emerging socio-technological aspect, a framework for a sustainable socio-technical ecosystem is presented here to achieve economic independence and empowerment. © 2022 Elsevier Ltd","Big-data analytics; Communication infrastructure; Corporate sustainability; Socio-institutional context; Sustainable geo-ICT","Advanced Analytics; Big data; Data Analytics; Digital storage; Economic and social effects; Environmental technology; Open source software; Open systems; Planning; Sustainable development; Urban growth; Communication infrastructure; Corporate-sustainability; Enabling technologies; Environmental economics; Free and open source softwares; Geo-information; Information and Communication Technologies; Institutional contexts; Socio-institutional context; Sustainable geo-information and communication technology; Developing countries",Article,Scopus,2-s2.0-85123785160
"Islam M.T., Karunasekera S., Buyya R.","57201196746;55925860100;57194845546;","Performance and Cost-Efficient Spark Job Scheduling Based on Deep Reinforcement Learning in Cloud Computing Environments",2022,"IEEE Transactions on Parallel and Distributed Systems","33","7",,"1695","1710",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118648747&doi=10.1109%2fTPDS.2021.3124670&partnerID=40&md5=fdf8dbf79cc22ec1595853e8a03bf008","Big data frameworks such as Spark and Hadoop are widely adopted to run analytics jobs in both research and industry. Cloud offers affordable compute resources which are easier to manage. Hence, many organizations are shifting towards a cloud deployment of their big data computing clusters. However, job scheduling is a complex problem in the presence of various Service Level Agreement (SLA) objectives such as monetary cost reduction, and job performance improvement. Most of the existing research does not address multiple objectives together and fail to capture the inherent cluster and workload characteristics. In this article, we formulate the job scheduling problem of a cloud-deployed Spark cluster and propose a novel Reinforcement Learning (RL) model to accommodate the SLA objectives. We develop the RL cluster environment and implement two Deep Reinforce Learning (DRL) based schedulers in TF-Agents framework. The proposed DRL-based scheduling agents work at a fine-grained level to place the executors of jobs while leveraging the pricing model of cloud VM instances. In addition, the DRL-based agents can also learn the inherent characteristics of different types of jobs to find a proper placement to reduce both the total cluster VM usage cost and the average job duration. The results show that the proposed DRL-based algorithms can reduce the VM usage cost up to 30%. © 1990-2012 IEEE.","Cloud computing; cost-efficiency; deep reinforcement learning; performance improvement","Big data; Cloud computing; Cost benefit analysis; Cost effectiveness; Cost reduction; Industrial research; Job analysis; Reinforcement learning; Scheduling; Cloud-computing; Cost-efficiency; Cost-efficient; Jobs scheduling; Performance; Performance improvement; Reinforce learning; Servicelevel agreement (SLA); Task analysis; Deep learning",Article,Scopus,2-s2.0-85118648747
"Gao B.","57222350026;","Construction of knowledge service model of guizhou supply chain enterprises based on big data",2022,"International Journal of Information Systems and Supply Chain Management","15","3",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118309957&doi=10.4018%2fIJISSCM.290016&partnerID=40&md5=a5365da238f163565fa68e3e3dc0b8f4","In the era of big data, “knowledge” scope is expanded. To realize the optimization of supply chain collaborative innovation in the era of big data, the platform of the collection, analysis, mining, and application of massive data resources is needed. By analyzing the sources of big data of collaborative innovation of supply chain, a basic framework of knowledge innovation platform of Guizhou supply chain enterprises under the environment of big data is proposed. The effect of big data technology on supply chain logistics mode is analyzed, and the current situation of logistics industry modernization in Guizhou Province is discussed. The mathematical model of big data processing is designed, and an example is simulated to validate the advantage of the proposed method. © 2022 IGI Global. All rights reserved.","Big Data; Guizhou Province; Knowledge Innovation Platform; Supply Chain","Big data; Data handling; Modernization; Collaborative innovation; Guizhou; Guizhou Province; Innovation platforms; Knowledge innovation; Knowledge innovation platform; Knowledge service; Massive data; Optimisations; Service modeling; Supply chains",Article,Scopus,2-s2.0-85118309957
"Karouani Y., Elgarej M.","57202588997;57204637495;","Milk-run collection monitoring system using the internet of things based on swarm intelligence",2022,"International Journal of Information Systems and Supply Chain Management","15","3",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118292141&doi=10.4018%2fIJISSCM.290018&partnerID=40&md5=71fb3baec500ab5a3dd4ff73fdad65bb","In Morocco, several dairy factories are placed in rural regions with a bad road network, which means that milk collection has a significant impact on profit, affecting milk transport costs. Actually, the milk run logistics process has been transformed from a traditional farm to the new cheese factory, so it’s needed efficient methods and models to improve the process of production and collection of milk from those units. For that, the authors apply new technologies such as the internet of things (IoT) and big data to collect and analyze this information to optimize the milk delivery process. The main goal of this work is to design a new smart decision method using the internet of things and big data to optimize the milk run logistics, reduce the cost of transportation, and improve collection density. This method will be based on the swarm artificial intelligence concept to find and calculate the shortest path between units to optimize the collection of milk. © 2022 IGI Global. All rights reserved.","Ant Colony Optimization; Artificial Intelligent; Internet of Things; Milk-Run; Multi-Agent System; Smart Transportation and Logistic; Swarm Intelligence; Vehicle Routing System","Ant colony optimization; Big data; Internet of things; Swarm intelligence; Vehicle routing; Ant colonies; Ant colony optimization; Artificial intelligent; Colony optimization; Milk-run; Monitoring system; Routing system; Smart transportation and logistic; Swarm intelligence; Vehicle routing system; Multi agent systems",Article,Scopus,2-s2.0-85118292141
"Seth A., Seth K.","55372956600;55584372100;","The novel multi-layered approach to enhance the sorting performance of healthcare analysis",2022,"International Journal of Reliable and Quality E-Healthcare","11","3",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117159913&doi=10.4018%2fIJRQEH.289178&partnerID=40&md5=c43a5b51cd74058454424468d64a6871","Emergence of big data in today’s world leads to new challenges for sorting strategies to analyze the data in a better way. For most of the analyzing technique, sorting is considered as an implicit attribute of the technique used. The availability of huge data has changed the way data is analyzed across industries. Healthcare is one of the notable areas where data analytics is making big changes. An efficient analysis has the potential to reduce costs of treatment and improve the quality of life in general. Healthcare industries are collecting massive amounts of data and look for the best strategies to use these numbers. This research proposes a novel non-comparison-based approach to sort a large data that can further be utilized by any big data analytical technique for various analyses. Copyright © 2022, IGI Global.","Analysis; Non-Comparison Sorting; Radix-Sort; Space-Complexity; Time-Complexity","analytic method; article; big data; health care industry; quality of life",Article,Scopus,2-s2.0-85117159913
"Wang W., Tian P., Zhang J., Agathokleous E., Xiao L., Koike T., Wang H., He X.","56948645000;57203958293;57203966387;56427011900;57452636600;55501894600;7501739925;7404408777;","Big data-based urban greenness in Chinese megalopolises and possible contribution to air quality control",2022,"Science of the Total Environment","824",,"153834","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124588651&doi=10.1016%2fj.scitotenv.2022.153834&partnerID=40&md5=5bdd0d63342406d20dfcecad3ea75127","Urban greenness is essential for people's daily lives, while its contribution to air quality control is unclear. In this study, Streetview big data of urban greenness and air quality data (Air Quality Index, PM2.5, PM10, SO2, NO2, O3, CO) from 206 monitoring stations from 27 provincial capital cities in China were analyzed. The national averages for the sky, ground and middle-level (shrub and short trees) view greenness were 5.4%, 5.5%, and 15.4%, respectively, and the sky:ground:middle ratio was 2:2:6. Street-view/bird-view greenness ratio averaged at 1.1. Large inter-city variations were observed in all the greenness parameters, and the weak associations between all street-view parameters and bird-eye greenspace percentage (21%–73%) indicate their representatives of different aspects of green infrastructures. All air quality parameters were higher in winter than in summer, except O3. Over 90% of air quality variation could be explained by socioeconomics and geoclimates, suggesting that air quality control in China should first reduce efflux from social economics, while geoclimatic-oriented ventilation facilitation design is also critical. For different air quality components, greenness had most significant associations with NO2, O3 and CO, and street-view/bird-view ratio was the most powerful indicator of all greenness parameters. Pooled-data analysis at national level showed that street-view greenness was responsible for 2.3% of the air quality variations in the summer and 3.6% in the winter; however, when separated into different regions (North-South China; East-West China), the explaining power increased up to 16.2%. Increased NO2 was accompanied with decreased O3, indicating NO titration effect. The higher O3 aligned with the higher street-view greenness, showing the greenness-related precursor risk for O3 pollution. Our study manifested that big internet data could identify the association of greenness and air pollution from street view scale, which can favor urban greenness management and evaluation in other regions where street-view data are available. © 2022 Elsevier B.V.","Air pollution; Bird-view greenness; Geographical variation; Redundancy ordination; Street-view greenness; Variation partitioning","Air quality; Big data; Data handling; Economics; Nitrogen oxides; Quality assurance; Quality control; Air quality control; Air quality data; Air quality indices; Bird-view greenness; Daily lives; Geographical variations; Quality variation; Redundancy ordination; Street-view greenness; Variation partitioning; Birds",Article,Scopus,2-s2.0-85124588651
"Holmi J.T., Lipsanen H.","57203179787;8229418700;","WITio: A MATLAB data evaluation toolbox to script broader insights into big data from WITec microscopes",2022,"SoftwareX","18",,"101009","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124647970&doi=10.1016%2fj.softx.2022.101009&partnerID=40&md5=7dcea16fc68aaffd52fbbe246bb54583","WITio is a free script-based MATLAB data evaluation toolbox for the files generated by thousand WITec microscopes operated globally by thousands of academic and industrial users. These modular microscopes excel at advanced imaging techniques and correlative studies, but also easily yield unwieldy big data. Though the shipped WITec software can batch process some of it by hand, it is presently limited at big data evaluation due to no scripting capabilities, 32-bit memory limit (mitigated by slow memory mapping) and no data compression. Partial workaround has been to export data elsewhere, but at the loss of time, metadata and access to WITec software due to its manual, lossy and one-way nature. WITio addresses these limitations by bringing in the strengths of MATLAB software (64-bit) and enabling bidirectional interoperation with WITec software via full read/write capability of the WITec Project/Data (*.wip/*.wid) files (including metadata like transformations). This means the toolbox users can use the best of both worlds to perform previously impossible sequential routines, where some tasks are done by scripts in MATLAB and the others in WITec software. The toolbox enables automation scripts, file batch processing, easy access to customized tools and more systematic data evaluation while reducing slow error-prone manual efforts. It even enables some real-time Autosave-based during-measurement task automation. Its code is freed under permissive license to challenge the extra license costs of WITec software and to invite the WITec community to share. Due to the stated benefits, its MATLAB scripting prerequisite is an excellent investment. To conclude, WITio strives to continually shine new insights into ever-increasing big data, bringing many research and commercial benefits to the community in the years to come. © 2022 The Author(s)","Bidirectional interoperability; Big data evaluation; Customized automation scripts; Full file import/export; MATLAB toolbox; WITec Data (*.wid) file; WITec microscopes; WITec Project (*.wip) file","Automation; Batch data processing; Big data; Metadata; Microscopes; Automation scripts; Bidirectional interoperability; Big data evaluation; Customized automation script; Data evaluation; Datafiles; Full file import/export; MATLAB toolbox; Project file; WITec data file; WITec microscope; WITec project file; MATLAB",Article,Scopus,2-s2.0-85124647970
"Fonseca-Galindo J.C., de Castro Surita G., Neto J.M., de Castro C.L., Lemos A.P.","57214693066;57221148565;57215378286;25929885000;25638788000;","A multi-agent system for solving the Dynamic Capacitated Vehicle Routing Problem with stochastic customers using trajectory data mining",2022,"Expert Systems with Applications","195",,"116602","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124583373&doi=10.1016%2fj.eswa.2022.116602&partnerID=40&md5=c0d3ef5d11f638f6c0f052d131e51198","The worldwide growth of e-commerce has created new challenges for logistics companies, such as delivering products quickly and cheaply. This paper presents a heuristic to solve the last-mile route creation problem dynamically. The heuristic is based on a multi-agent system integrated with trajectory data mining techniques to extract territorial patterns and use them to solve the Dynamic Capacitated Vehicle Routing Problem with Stochastic Customers. Our solution approach is focused on a linear-time heuristic that depends only on the Warehouse system configurations and not on the total number of packages processed, which is suitable for express delivery logistics companies that must process a large number of packages per day. We compare our proposal with benchmark algorithms from the literature; additionally, we evaluate its performance and robustness under different scenarios. Results show that our solution approach is effective for scenarios in which routes must be set dynamically from a continuous stream of packages. © 2022","Big Data; Data mining; Dynamic Capacitated Vehicle Routing Problem with stochastic customers; E-commerce logistics; Multi-agent systems","Benchmarking; Big data; Electronic commerce; Multi agent systems; Sales; Stochastic systems; Vehicle routing; Vehicles; Capacitated vehicle routing problem; Data-mining techniques; Dynamic capacitated vehicle routing problem with stochastic customer; E- commerces; E-commerce logistic; Last mile; Logistics company; Solution approach; Stochastics; Trajectory data minings; Data mining",Article,Scopus,2-s2.0-85124583373
"Zhu P., Huang J., Wang J., Liu Y., Li J., Wang M., Qiang W.","36624731400;57191204393;57321552000;55742311000;57452311300;56342596800;57213172586;","Understanding taxi ridership with spatial spillover effects and temporal dynamics",2022,"Cities","125",,"103637","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124579639&doi=10.1016%2fj.cities.2022.103637&partnerID=40&md5=adc8e6cb5178c500df18b166b6eb2706","In urban transportation systems, taxis are regarded as flexible, convenient, and time-saving. Taxi demand is affected by various built-environment factors and by the time of the day. Although many studies have investigated correlations between taxi demand and the built environment, the direct and spillover effects of built environment factors on taxi demand have not been examined at a fine spatial scale. To address this gap in the literature, this paper employs spatial econometric models using GPS-tracked taxi trips, mobile signaling data, and points of interest (POIs) to study taxi demand in Beijing at a 1-kilometer square grid resolution. The results show that, in the morning and evening peak hours, road network density has the strongest (positive) direct and indirect impact on taxi ridership. A relationship is also found between public transportation and taxi ridership: bus coverage has positive direct effects and insignificant indirect effects on taxi pick-ups and drop-offs, while subway coverage has negative indirect effects, suggesting that it may absorb taxi demand from surrounding grids. Results also indicate that various built-environment factors affect taxi demand differently at morning and evening peak times. This study reveals the complex nature of taxi ridership and has important implications for policymakers, transport planners, and other stakeholders in megacities around the world. © 2022 Elsevier Ltd","Big data; Built environment; Spatial autocorrelation; Spatial econometric model; Taxi demand",,Article,Scopus,2-s2.0-85124579639
"Kim S.-H., Lee D.-H., Kim K.-J.","57215072284;55698968500;26662511100;","EWMA-PRIM: Process optimization based on time-series process operational data using the exponentially weighted moving average and patient rule induction method",2022,"Expert Systems with Applications","195",,"116606","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123986979&doi=10.1016%2fj.eswa.2022.116606&partnerID=40&md5=7303d7b31269c14f850f423be1448d8e","Currently, many manufacturing companies are obtaining a large amount of operational data from manufacturing lines due to advances in information technology. Thus, various data mining methods have been applied to analyze the data to optimize the manufacturing process. Most of the existing data mining-based optimization methods assume that the relationships between input and response variables do not change over time. However, because it often takes a long time to collect a large amount of operational data, the relationships may change during the data collection. In such a case, the operational data is regarded as time-series data and recent data should be regarded to be more important than old data. In this study, we employed a patient rule induction method (PRIM), which is one of the data mining methods applied for process optimization. In addition, we employed an exponentially weighted moving average (EWMA) statistic to assign a larger weight to the recent data. Based on the PRIM and EWMA, the proposed method attempts to obtain optimal intervals for input variables where current performance of the response is better. The proposed method is illustrated with a hypothetical example and validated through a real case study of a steel manufacturing process. © 2022","Big data; Data mining; Exponentially weighted moving average; Manufacturing process optimization; Patient rule induction method; Time-series","Big data; Optimization; Process control; Time series; Data mining methods; Exponentially weighted moving average; Large amounts; Manufacturing process; Manufacturing process optimization; Operational data; Patient rule induction method; Process optimisation; Rule Induction Methods; Times series; Data mining",Article,Scopus,2-s2.0-85123986979
"Zhan X., Han S., Rong N., Liu P., Ao W.","57429578600;57191482741;57192257273;57428698500;57428871100;","A Two-Stage transient stability prediction method using convolutional residual memory network and gated recurrent unit",2022,"International Journal of Electrical Power and Energy Systems","138",,"107973","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123605119&doi=10.1016%2fj.ijepes.2022.107973&partnerID=40&md5=229259e61b68217b21ba987afe7a54f0","In order to improve the accuracy as well as the reliability of transient stability prediction (TSP), a two-stage TSP method using convolutional residual memory network (CRMN) and gated recurrent unit (GRU) is proposed. In the first stage, the underlying measurement data are directly used as input features to build a CRMN-based TSP prediction model for qualitative and quantitative analysis. In the second stage, the GRU-based generator rotor angle trajectory (GRAT) prediction model is firstly established. Subsquently, the unstable samples in qualitative analysis and the samples with a confidence interval of 99.66% in quantitative analysis are used for GRAT prediction. As a consequence, more reliable prediction results can be obtained by comprehensive judgment about the results from qualitative analysis, quantitative analysis and GRAT prediction. Case studies conducted on a modified New England 10-machine 39-bus system and an IEEE 50-machine 145-bus system demonstrate superior accuracy, stronger robustness of the proposed model than other traditional models involving LSTM, GRU and CNN. Furthermore, the results of numerical experiments also prove that the proposed two-stage TSP method improves the reliability of prediction results. © 2022","Convolutional residual memory networks; Gated recurrent unit; Spatio-temporal big data; Transient stability prediction; Two-stage prediction","Big data; Forecasting; Long short-term memory; Numerical methods; Transients; Convolutional residual memory network; Gated recurrent unit; Memory network; Prediction methods; Residual memory; Spatio-temporal; Spatio-temporal big data; Stage prediction; Transient stability prediction; Two-stage prediction; Convolution",Article,Scopus,2-s2.0-85123605119
"Coad A., Karlsson J.","22937056800;57195944949;","A field guide for gazelle hunters: Small, old firms are unlikely to become high-growth firms",2022,"Journal of Business Venturing Insights","17",,"e00286","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119374445&doi=10.1016%2fj.jbvi.2021.e00286&partnerID=40&md5=9ecb0d198b75176f085ade3835327e2d","We map the distribution of High-Growth Firms (HGFs, or “gazelles”) across the dimensions of firm size and firm age using contour plots, where firm size and growth are measured in terms of employees. The analysis is based on Swedish total population data for the period 1990–2016, covering approximately 11, 000, 000 firm-year observations. The results show that the majority of HGFs are small, which partly follows the high representation of small firms in the population. Yet when considering the proportions of HGFs across firm ages and sizes, a distinct feature emerges in that the territory of small, old firms appears to be almost completely deserted—despite their large overall numbers—where firms in this category have the (by far) lowest chances of becoming HGFs. © 2021 Elsevier Inc.","Big data; Contour plots; Firm growth; High-growth firms; SME policy",,Article,Scopus,2-s2.0-85119374445
"Sharma S.K.","57211202554;","A novel approach on water resource management with Multi-Criteria Optimization and Intelligent Water Demand Forecasting in Saudi Arabia",2022,"Environmental Research","208",,"112578","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122951032&doi=10.1016%2fj.envres.2021.112578&partnerID=40&md5=918756d590012febddfcd808272f6ee5","Ever-increasing demands for freshwater resources have elevated the likelihood of severe water stress in several places of Saudi Arabia during the last several decades. With effective decision-making processes, development objectives on water resource management emerge. In the following series of research articles, recent innovations in various objective demand forecasting systems are examined and contrasted in terms of their utility in resolving tough challenges in water resource management. Hence, this study proposes a novel approach to water resource management integrating Multi-Criteria Optimization and Intelligent Water Demand Forecasting (MCO-IWDF). This framework addresses the challenges in allocating various water resources to multiple water sectors in a future changing environment. In order to plan for future water needs, water managers use a variety of tools. When forecasting future water demand, the most common method is to estimate current per-capita consumption (gpcd) and multiply this by the expected population growth. Conserving water in the Kingdom of Saudi Arabia to improve irrigation issues. This research analyzes the current situation of available water resources and the water demand in Saudi Arabia. The machine intelligence and big data analytic approach improve the proposed water resource management scheme. The simulation analysis identifies the highest performance in demand prediction accuracy of 98.96% and optimization ratio of 97.87% compared to the existing models. Over time, a mathematical model is used to conduct simulation experiments. Studying the problem, creating a model and collecting data are just some of the steps involved in simulation research. Response analysis and a simulation report are also part of this process. The case study analysis results in a significant satisfactory level of 99.23%. © 2021 Elsevier Inc.","Big data analytics; Decision making; Machine intelligence; Muti-criteria optimization; Saudi Arabia; Water demand forecasting","forecasting method; irrigation system; resource management; water demand; water relations; water resource; water stress; analytical error; Article; artificial intelligence; big data; conceptual framework; decision making; decision support system; feature extraction; forecasting; multiobjective optimization; population growth; prediction; recurrent neural network; resource management; Saudi Arabia; seasonal variation; technique for order preference by similarity to ideal solution; water availability; water conservation; water pollution control; water supply; Saudi Arabia",Article,Scopus,2-s2.0-85122951032
"Suetterlein J., Manzano J., Marquez A., Gao G.R.","42862588500;8586510800;57196785042;7403170980;","Extending an asynchronous runtime system for high throughput applications: A case study",2022,"Journal of Parallel and Distributed Computing","163",,,"214","231",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124701552&doi=10.1016%2fj.jpdc.2022.01.027&partnerID=40&md5=70adc3eab453d28ba2bd50d8e843624a","Current supercomputers are mostly composed of vast numbers of nodes enhanced with accelerators (usually in the form of GPUs). However, having these heterogeneous designs in the forefront have exposed the software toolchains and application designers to the underlying complexities of an ever evolving hardware substrate. The need for a more dynamic view from the system software (i.e., compilers and runtimes) has become more apparent in these environments. Due to this, adaptive, fine grain runtime systems have seen a rise in popularity in the past decades. With low overhead and small tasks, these runtimes help to hide long latency operations by exploiting the massive concurrency presented in different application workflows. Such features allow the reduction of idle time (a result from ever deeper and complex memory hierarchies and memory types) with the execution of unrelated work across the machine. Of these runtimes, the Asynchronous Many Task (AMT) Runtimes are excellent exemplars as they can efficiently map onto hardware substrates and exhibit a high degree of latency hiding. Because of their latency tolerant characteristics, applications such as Graph Analytics and Big Data applications (which are latency sensitive) can use these runtimes very efficiently. Thanks to these characteristics, we present how a careful design can help to exploit the properties of an AMT when running high latency applications such as the ones encountered in the Big Data domain. In addition, when combined with introspection / adaptive capabilities, the runtime can further exploit optimization opportunities during its execution based on the ever changing state of the underlying hardware substrate. As a vehicle for this exploration, we use the Performance Open Community Runtime (P-OCR) to test all these concepts with Big Data workloads. © 2022","Asynchronous runtime systems; Big Data; Performance analysis","Application programs; Program processors; Substrates; Supercomputers; 'current; Asynchronoi runtime system; Case-studies; Finer grains; High-throughput; Low overhead; Performances analysis; Run- time systems; Runtimes; System softwares; Big data",Article,Scopus,2-s2.0-85124701552
"Qiao W., Dong P., Du X., Zhang Y., Zhang H., Guizani M.","57201654572;35317180400;57205479965;57201996770;9233166700;7004750176;","QoS provision for vehicle big data by parallel transmission based on heterogeneous network characteristics prediction",2022,"Journal of Parallel and Distributed Computing","163",,,"83","96",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124297577&doi=10.1016%2fj.jpdc.2022.01.018&partnerID=40&md5=ac5c10f489a6c41888385f020aca186f","Multipath parallel transmission has become an important research direction to improve big data transmission efficiency of connected vehicles. However, due to the heterogeneity and time-varying characteristics of parallel transmission paths, packets transmitted in parallel are usually out-of-order delivered to the destination, which greatly limits the throughput. To Lift the restriction of out-of-order delivery on the efficiency of big data transmission, this paper proposes a packet-granular real-time shortest delay scheduling scheme for multipath transmission based on path characteristics prediction. The scheme first clusters and models the heterogeneous network, which greatly reduces the complexity of the network. Subsequently, a prediction algorithm that can quickly converge to real-time delay is proposed. Then the details of the scheduling scheme are introduced by modules, and the bandwidth aggregation efficiency close to the theoretical upper limit is proved through simulation. Finally, we summarize the applicable scenarios and future work of the scheme. © 2022 Elsevier Inc.","Big data; Multipath parallel transmission; Network bottleneck prediction; Quality of service; Vehicular networks","Big data; Data transfer; Efficiency; Heterogeneous networks; Quality of service; Scheduling; Vehicle transmissions; Data transmission efficiency; Multipath; Multipath parallel transmission; Network bottleneck prediction; Network bottlenecks; Network characteristics; Parallel transmission; Quality-of-service; Scheduling schemes; Vehicular networks; Forecasting",Article,Scopus,2-s2.0-85124297577
"Gu R., Shi J., Chen X., Wang Z., Che Y., Zhang K., Huang Y.","36997784300;57429676400;57430030700;56450325700;57398217800;57244534400;7501574686;","Octopus-DF: Unified DataFrame-based cross-platform data analytic system",2022,"Parallel Computing","110",,"102879","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123611971&doi=10.1016%2fj.parco.2021.102879&partnerID=40&md5=a06e15d3b1530a0bcbffafb2f9cf0efa","Nowadays, DataFrame serves as a core to model and implement numerous machine learning and data analytic algorithms. Traditional data analytic programming languages, such as Python, provide the DataFrame programming model natively. In the big data era, it is a natural demand to introduce the DataFrame model into distributed computing systems for convenient big data analysis. Therefore, various DataFrame libraries have been implemented on Spark and Dask. However, these distributed computing systems contain some parallelism semantics which are not very straightforward for data analysts. Also, a DataFrame-based algorithm may have quite different performance for various datasets over different platforms. And, it is difficult for data analysts to choose the optimal platforms that achieve the best performance for their programs. To address these problems, we build a unified DataFrame-based data analytic system Octopus-DF. Octopus-DF integrates Pandas, Dask, and Spark as the backend computing platforms and exposes the most widely used Pandas-style APIs to users. Then, as DataFrame computation performance plays a critical role in the computing efficiency of DataFrame-based data analytic algorithms, we designed a set of DataFrame computation optimizations which are divided into two parts: (1) multiple indexing and DAG optimizations, and (2) cross-platform scheduling strategy. Experimental results show that Octopus-DF outperformed the existing single platforms with 11.72× speedup on average. Compared with the existing platform combination strategies, Octopus-DF can achieve the optimal one. Moreover, the proposed optimizations can effectively speedup the execution workflow. © 2022 Elsevier B.V.","Cross-platform; DataFrame programming model; Distributed system; Scheduling","Application programming interfaces (API); Big data; Computational efficiency; Distributed computer systems; Machine learning; Scheduling; Semantics; Shellfish; Analytic algorithm; Analytics systems; Cross-platform; Data analysts; Data analytics; Dataframe programming model; Distributed computing systems; Distributed systems; Performance; Programming models; Molluscs",Article,Scopus,2-s2.0-85123611971
"Christou I.T., Kefalakis N., Soldatos J.K., Despotopoulou A.-M.","6602111061;37055926900;8662280800;57216957073;","End-to-end industrial IoT platform for Quality 4.0 applications",2022,"Computers in Industry","137",,"103591","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122590540&doi=10.1016%2fj.compind.2021.103591&partnerID=40&md5=f7c87a6bb925e3bb2dfcdd8566bb3269","Predictive maintenance, quality management, and zero-defect manufacturing are among the most prominent smart manufacturing use cases in the Industry4.0 era. Nevertheless, the development of such systems is still challenging because of the need to integrate multiple fragmented data sources, to apply advanced machine learning techniques for multi-objective optimizations, and to implement configurable digital twins that can flexibly adapt to changing industrial configurations. This paper presents the architecture, design, practical implementation, and evaluation of an end-to-end platform that addresses these challenges. The platform provides the means for collecting, managing, and routing data streams from heterogeneous cyber physical production systems, in configurable and interoperable ways. Moreover, it supports advanced data analytics by means of a novel machine learning framework that leverages quantitative rule mining. The presented platform has been successfully deployed in various industrial settings and has been positively evaluated in terms of its ability to accelerate application development, reduce unscheduled downtimes, provide increased Overall Equipment Efficiency (OEE), compute production process parameter configurations that lower the percentage of product defects, and predict product defects before they occur. © 2021","Artificial intelligence; Big data; Configurable analytics; Industrial IoT; Predictive control; Predictive maintenance; Quality management; Zero-defect manufacturing","Big data; Data Analytics; Defects; Internet of things; Interoperability; Machine learning; Multiobjective optimization; Predictive analytics; Quality management; Configurable analytic; End to end; Maintenance quality; Manufacturing IS; Predictive control; Predictive maintenance; Product defects; Smart manufacturing; Zero defects; Zero-defect manufacturing; Quality control",Article,Scopus,2-s2.0-85122590540
"Chabot D., Stapleton S., Francis C.M.","57213363237;55305813600;57404641600;","Using Web images to train a deep neural network to detect sparsely distributed wildlife in large volumes of remotely sensed imagery: A case study of polar bears on sea ice",2022,"Ecological Informatics","68",,"101547","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122506196&doi=10.1016%2fj.ecoinf.2021.101547&partnerID=40&md5=731eb553e1cf17b952c49ae04973fa2a","Remote sensing can be a valuable alternative or complement to traditional techniques for monitoring wildlife populations, but often entails operational bottlenecks at the image analysis stage. For example, photographic aerial surveys have several advantages over surveys employing airborne observers or other more intrusive monitoring techniques, but produce onerous amounts of imagery for manual analysis when conducted across vast areas, such as the Arctic. Deep learning algorithms, chiefly convolutional neural networks (CNNs), have shown promise for automatically detecting wildlife in large and/or complex image sets. But for sparsely distributed species, such as polar bears (Ursus maritimus), there may not be sufficient known instances of the animals in an image set to train a CNN. We investigated the feasibility of instead providing ‘synthesized’ training data to a CNN to detect polar bears throughout large volumes of aerial imagery from a survey of the Baffin Bay subpopulation. We harvested 534 miscellaneous images of polar bears from the Web that we edited to more closely resemble 21 known images of bears from the aerial survey that were solely used for validation. We combined the Web images of polar bears with 6292 random background images from the aerial survey to train a CNN (ResNet-50), which subsequently correctly classified 20/21 (95%) bear images from the survey and 1172/1179 (99.4%) random background validation images. Given that even a small background misclassification rate could produce multitudinous false positives over many thousands of photos, we describe a potential workflow to efficiently screen out erroneous detections. We also discuss potential avenues to improve CNN accuracy, and the broader applicability of our approach to other image-based wildlife monitoring scenarios. Our results demonstrate the feasibility of using miscellaneously sourced images of animals to train deep neural networks for specific wildlife detection tasks. © 2022","Artificial intelligence; Big data; Computer vision; Conservation; Machine learning; Marine mammals","aerial survey; artificial neural network; bear; image analysis; monitoring; sea ice; training; Arctic; Baffin Bay; Bay; Somalia",Article,Scopus,2-s2.0-85122506196
"Zhang G., Zhang X., Bilal M., Dou W., Xu X., Rodrigues J.J.P.C.","56678100300;35148680700;56562681700;7006874536;57226874719;25930566300;","Identifying fraud in medical insurance based on blockchain and deep learning",2022,"Future Generation Computer Systems","130",,,"140","154",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122297088&doi=10.1016%2fj.future.2021.12.006&partnerID=40&md5=d2988108568f61a255f5e25f4461dd6f","With the rapid growth of medical costs, the control of medical expenses has been becoming an important task of Health Insurance Department. Traditional medical insurance settlement is paid on a per-service basis, which leads to lots of unreasonable expenses. To cope with this problem, the single-disease payment mechanism has been widely used in recent years. However, the single-disease payment also has a risk of fraud. In this work, we propose a framework to identify fraud of medical insurance based on consortium blockchain and deep learning, which can recognize suspicious medical records automatically to ensure valid implementation on single-disease payment and lighten the work of medical insurance auditors. An explainable model BERT-LE is designed to evaluate the reasonability of ICD disease code for Medicare reimbursement by predicting the probability of a disease according to the chief complaint of a patient. We also put forward a storage and management process of medical records based on consortium blockchain to ensure the security, immutability, traceability, and auditability of the data. The experiments on two real datasets from two 3A hospitals demonstrate that the proposed solution can identify fraud effectively and greatly improve the efficiency in medical insurance reviews. © 2021 Elsevier B.V.","Anti-fraud; Blockchain; Deep learning; Medical big data","Big data; Blockchain; Deep learning; Digital storage; Health insurance; Information management; Anti-fraud; Block-chain; Deep learning; Medical big data; Medical expense; Medical insurance; Medical record; Payment mechanism; Rapid growth; Reasonability; Crime",Article,Scopus,2-s2.0-85122297088
"Zhao Y., Zhou Y.","57226184382;57223093127;","Measurement method and application of a deep learning digital economy scale based on a big data cloud platform",2022,"Journal of Organizational and End User Computing","34","3",,"","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110900002&doi=10.4018%2fJOEUC.20220501.oa1&partnerID=40&md5=346127a66a8bccb85d959501109b92d9","In recent years, with the acceleration of the process of economic globalization and the deepening of China’s financial liberalization, the scale of international short-term capital flows has been extremely rapid. This article mainly studies the deep learning digital economy scale measurement method and its application based on the big data cloud platform. This article uses the indirect method to estimate the stock of renminbi circulating abroad. The results show that the application of big data cloud platforms can increase the development share of digital media and digital transactions in the digital economy and optimize the structure of China’s digital economy. © 2022 IGI Global. All rights reserved.","Big Data; Cloud Platform; Deep Learning; Digital Economy; Scale Measurement Method","Big data; Digital storage; E-learning; Learning systems; Capital flow; Data clouds; Digital economy; Economic globalization; Indirect methods; ITS applications; Measurement methods; Short term; Deep learning",Article,Scopus,2-s2.0-85110900002
"Wang Y., Wang Q., Chen X., Chen D., Fang X., Yin M., Zhang N.","57221510549;57194164835;24823835800;56170747500;55609323400;56383403500;56575788300;","ContainerGuard: A Real-Time Attack Detection System in Container-Based Big Data Platform",2022,"IEEE Transactions on Industrial Informatics","18","5",,"3327","3336",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098752212&doi=10.1109%2fTII.2020.3047416&partnerID=40&md5=46de81eddd9fc18409341e6bf2e352df","As a lightweight, flexible, and high-performance operating system virtualization, containers are used to speed up the big data platform. However, due to the imperfection of the resource isolation mechanism and the property of shared kernel, the meltdown and spectre attacks can lead to information leakage of kernel space and coresident containers. In this article, a noise-resilient and real-time detection system, named ContainerGuard, is proposed to detect meltdown and spectre attacks in the container-based big data platform. ContainerGuard uses a nonintrusive manner to collect lifecycle multivariate time-series performance event data of processes in containers and then uses ensemble of variational autoencoders as generative neural networks to learn the robust representations of normal patterns. Therefore, ContainerGuard meets the urgent need for information protection in the container-based big data platform. Our evaluations using real-world datasets show that ContainerGuard achieves excellent detection performance and only introduces about 4.5% of running performance overhead to the platform. © 2005-2012 IEEE.","Anomaly detection; big data platform security; container; meltdown and spectre; variational autoencoder (VAE)","Big data; Life cycle; Attack detection; Detection performance; Information leakage; Information protection; Multivariate time series; Real-time detection; Real-world datasets; System virtualization; Containers",Article,Scopus,2-s2.0-85098752212
"Fu A., Zhang X., Xiong N., Gao Y., Wang H., Zhang J.","14519093500;57218531953;35231569200;56763011100;24833714700;57220094702;","VFL: A Verifiable Federated Learning with Privacy-Preserving for Big Data in Industrial IoT",2022,"IEEE Transactions on Industrial Informatics","18","5",,"3316","3326",,7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096882437&doi=10.1109%2fTII.2020.3036166&partnerID=40&md5=fb2f6cf36198b91e9f86b6288274b5b4","Due to the strong analytical ability of big data, deep learning has been widely applied to model on the collected data in industrial Internet of Things (IoT). However, for privacy issues, traditional data-gathering centralized learning is not applicable to industrial scenarios sensitive to training sets, such as face recognition and medical systems. Recently, federated learning has received widespread attention, since it trains a model by only sharing gradients without accessing training sets. But existing research works reveal that the shared gradient still retains the sensitive information of the training set. Even worse, a malicious aggregation server may return forged aggregated gradients. In this article, we propose the VFL, a verifiable federated learning with privacy-preserving for big data in industrial IoT. Specifically, we use Lagrange interpolation to elaborately set interpolation points for verifying the correctness of the aggregated gradients. Compared with existing schemes, the verification overhead of VFL remains constant regardless of the number of participants. Moreover, we employ the blinding technology to protect the privacy of the privacy gradients. If no more than $\boldsymbol{n}$-2 of $\boldsymbol{n}$ participants collude with the aggregation server, VFL could guarantee the encrypted gradients of other participants not being inverted. Experimental evaluations corroborate the practical performance of the presented VFL with high accuracy and efficiency. © 2005-2012 IEEE.","Big data; federated learning (FL); industrial Internet of Things (IoT); privacy-preserving; verifiable","Big data; Deep learning; Face recognition; Industrial internet of things (IIoT); Interpolation; Privacy by design; Data gathering; Experimental evaluation; Industrial scenarios; Lagrange interpolations; Medical systems; Privacy preserving; Sensitive informations; Set interpolation; Learning systems",Article,Scopus,2-s2.0-85096882437
"de Sousa R.S., Boukerche A., Loureiro A.A.F.","57193435479;7005819374;7006541504;","On the prediction of large-scale road-network constrained trajectories",2022,"Computer Networks","206",,"108337","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123916031&doi=10.1016%2fj.comnet.2021.108337&partnerID=40&md5=e3d7aecb59f3c20e42104447451995e4","Trajectory data mining-based applications benefit from the increasing availability of vehicle trajectory and road network datasets. For instance, the application of trajectory prediction makes it possible to design more efficient routing protocols for vehicular networks. This paper proposes a novel cluster-based framework for the long-term prediction of road-network constrained trajectories. The framework employs a new hierarchical agglomerative clustering algorithm and trains prediction models from historical trajectory datasets. Experimental results show the framework's effectiveness and efficiency to predict trajectories with different characteristics in a new real-world, large-scale scenario. Furthermore, the framework outperformed the related work in terms of prediction accuracy and time complexity. © 2021 Elsevier B.V.","Big data; Clustering; Data mining; Datasets; Prediction; Trajectories","Clustering algorithms; Data mining; Large dataset; Roads and streets; Trajectories; Clusterings; Constrained trajectories; Dataset; Efficient routing; Large-scales; Road network; Routing-protocol; Trajectory data minings; Trajectory prediction; Vehicle trajectories; Forecasting",Article,Scopus,2-s2.0-85123916031
"Li W., Karthik C., Rajalakshmi M.","57463905200;57226546597;57214415469;","Big data visualization for in-situ data exploration for sportsperson",2022,"Computers and Electrical Engineering","99",,"107829","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125116471&doi=10.1016%2fj.compeleceng.2022.107829&partnerID=40&md5=20cd4ae9efcbbd2ccbc096fe4b7939bf","An extraordinary promise is offered in wellness by the fast expansion in the Internet of Things (IoT). The current influence is on life efficiency and the dependability of health systems on wireless technologies, fitness monitors, and body sensors in the sports arena. Wearable gadgets are growing interests in monitoring physical variables, boosting health, and enhancing practical compliance in many demographics, from elite to sick. In this research, Sports activity monitoring using the big data visualization (SAM-BDV) method is suggested in sports athletic fitness tracking systems utilizing IoT. The wearing gadget for the continual accurate tracking of the heart rates, breathing frequency, and motion rate during the physical exercise is evaluated. The gathered sensors are transmitted to the Ethernet component IoT-connection system and accessed over the internet by the authorized person to monitor sports health. The mathematical model and usage of the portable sensors demonstrate how processing costs can be lowered while maintaining the health requirements for accessing healthcare data stored in a fog/edge distribution environment. The evaluation technique is queuing and can calculate the minimal computer resources required to fulfil the Service Level Contract (fog and cloud terminals). The findings of experiments demonstrate that the proposed approach can be routinely used with ease of use, reliability, and cost savings. © 2022","Big data; Data visualization; Sports person; Wearable sensors","Big data; Health; Internet of things; Sports; Visualization; Wearable sensors; 'current; Activity monitoring; Data exploration; Health systems; In-situ data; Physical variables; Sport person; Sports activity; Sports arena; Wireless technologies; Data visualization",Article,Scopus,2-s2.0-85125116471
"Cárdenas P., Ivrissimtzis I., Obara B., Kureshi I., Theodoropoulos G.","57202647444;6603241821;57240985900;37013369900;7006084934;","Big data for human security: The case of COVID-19",2022,"Journal of Computational Science","60",,"101574","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124883450&doi=10.1016%2fj.jocs.2022.101574&partnerID=40&md5=567d106165578812902b0da483f9b122","The COVID-19 epidemic has changed the world dramatically since societies are changing their behaviour according to the new normal, which comes along with numerous challenges and uncertainties. These uncertainties have led to instabilities in several facets of society, most notably health, economy and public order. Measures to contain the pandemic by governments have occasionally met with increasing discontent from societies and have triggered social unrest, imposing serious threats to human security. Big Data Analytics can provide a powerful force multiplier to support policy and decision makers to contain the virus while at the same time dealing with such threats to human security. This paper presents the utilisation of a big data forecasting and analytics framework and its utilisation to deal with COVID-19 triggered social unrest. The paper is an extended version of paper Cárdenas et al. (2021) presented at the 2021 International Conference on Computational Science. © 2022","Big data; COVID-19; Data analytics; Epidemics; Human security; Machine learning","Advanced Analytics; Big data; Decision making; Machine learning; Viruses; Computational science; COVID-19; Data analytics; Decision makers; Extended versions; Force multipliers; Human securities; Policy makers; Support policy; Uncertainty; Data Analytics",Article,Scopus,2-s2.0-85124883450
"Zarindast A., Poddar S., Sharma A.","57194202416;57197621655;35574710700;","A Data-Driven Method for Congestion Identification and Classification",2022,"Journal of Transportation Engineering Part A: Systems","148","4","04022012","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124658264&doi=10.1061%2fJTEPBS.0000654&partnerID=40&md5=ef828a4393259ed7ba454dd3e5c04f0e","Congestion detection is one of the key steps in reducing delays and associated costs in traffic management. With the increasing use of global positioning system (GPS)-based navigation, promising speed data are now available. This study used extensive historical probe data (year 2016) in Des Moines, Iowa. We used Bayesian change point detection to segment the speed signal and detect temporal congestion. The detected congestion events were then classified as recurrent congestion (RC) or nonrecurrent congestion (NRC). This paper thus presents a robust statistical, big-data-driven expert system and a big-data-mining methodology for identifying both recurrent and nonrecurrent congestion. © 2022 American Society of Civil Engineers.","Big data; Congestion classification; Recurrent and nonrecurrent congestion; Traffic congestion detection","Big data; Classification (of information); Data mining; Global positioning system; Traffic congestion; Associated costs; Bayesian; Change point detection; Congestion classification; Congestion detection; Data-driven methods; Delay costs; Recurrent and non-recurrent congestions; Traffic congestion detections; Traffic management; Expert systems; Bayesian analysis; classification; data mining; data set; detection method; GPS; identification method; navigation; traffic congestion; Des Moines; Iowa; United States",Article,Scopus,2-s2.0-85124658264
"Wen L.","57454069200;","Development Analysis of Cross-Border E-Commerce Logistics Based on Big Data Technology Under Safety Law Protection",2022,"International Journal of Information Systems in the Service Sector","14","2",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124646397&doi=10.4018%2fIJISSS.290547&partnerID=40&md5=dbb1dafa04ebc8dbc318a85a2d16fbe9","Big data technology can effectively improve the logistics level of cross-border e-commerce. Firstly, difficulties existing in international logistics management and the necessity of international logistics supply chain management are discussed. Secondly, challenges and opportunities of big data technology to cross-border e-commerce are analyzed respectively. Big data logistics and its characteristics are studied in depth. The cross-border e-commerce sales forecast model is established based on GM(1,1), and the proposed model is verified based on case study. Simulation results show that the prediction model can improve the prediction precision of sales amount. Finally, integration measurements of big data and cross-border e-commerce are put forward under a suitable local law protection. Copyright © 2022, IGI Global.","1); Cross-Border E-Commerce; GM(1; Logistics; Safety Law Protection","Electronic commerce; Forecasting; Sales; Supply chain management; System theory; 1); Cross-border; Cross-border E-commerce; Data technologies; E- commerces; GM(1; Logistic management; Logistics supply chain managements; Safety law protection; Safety laws; Big data",Article,Scopus,2-s2.0-85124646397
"Talón-Ballestero P., Nieto-García M., González-Serrano L.","55680931200;57192656098;55179393900;","The wheel of dynamic pricing: Towards open pricing and one to one pricing in hotel revenue management",2022,"International Journal of Hospitality Management","102",,"103184","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124601923&doi=10.1016%2fj.ijhm.2022.103184&partnerID=40&md5=19d8f93cb41ef7c9f8601bc710d02590","Dynamic pricing is at the core of hotel revenue management (RM). Big data technologies have facilitated information processing and enriched dynamic pricing techniques. One of the challenges in the sector relates to price personalization, i.e., how prices can be adjusted at the customer level. Using a qualitative approach, the study analyzes how dynamic pricing is currently implemented in hotel RM. By doing so, this research shows empirical evidence of the use of recent concepts in the industry like “open pricing” and identifies the opportunities and challenges of a customer-centric approach to pricing. From a theoretical perspective, the study may guide future research on pricing in hotel RM. Finally, this work also presents actionable insights for practitioners. © 2022","Big data; Conceptual framework of pricing in hospitality; Dynamic pricing; Hotel revenue management; One to one pricing; Open pricing",,Article,Scopus,2-s2.0-85124601923
"Cascitti J., Niebler S., Müller A., Schmidt B.","57226379337;57210802321;57193026508;7402828880;","RNACache: A scalable approach to rapid transcriptomic read mapping using locality sensitive hashing",2022,"Journal of Computational Science","60",,"101572","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124466130&doi=10.1016%2fj.jocs.2022.101572&partnerID=40&md5=fde65a6c4068a9d37035c81e02dbb317","Mapping of reads to transcriptomes is a crucial initial step for bioinformatics RNA-seq pipelines. As alignment-based methods exhibit high computational complexities, lightweight alignment-free methods are becoming increasingly important. We present RNACache – a novel approach to the detection of local similarities between transcriptomes and RNA-seq reads based on context-aware locality sensitive hashing. It consists of a three-step processing pipeline consisting of subsampling of k-mers, match-based (online) filtering, and coverage-based filtering in order to identify truly expressed transcript isoforms. Our performance evaluation shows that RNACache produces transcriptomic mappings of high accuracy that include significantly fewer erroneous matches compared to the state-of-the-art lightweight mappers RapMap, Salmon, and Kallisto. Furthermore, it offers good scalability in terms of number of utilized CPU cores and has the best runtime performance at low memory consumption on modern multi-core workstations. This is an extended version of our previously published conference paper (Cascitti et al., 2021). RNACache is available at https://github.com/jcasc/rnacache. © 2022 Elsevier B.V.","Big data; Bioinformatics; Hashing; Next-generation sequencing; Parallelism; Read mapping; RNA-seq; Transcriptomics","Big data; Mapping; Pipeline processing systems; Pipelines; RNA; Alignment-free; Hashing; Locality sensitive hashing; Next-generation sequencing; Parallelism; Read mappings; RNA-seq; Scalable approach; Transcriptomes; Transcriptomics; Bioinformatics",Article,Scopus,2-s2.0-85124466130
"Goudarzi S., Soleymani S.A., Anisi M.H., Azgomi M.A., Movahedi Z., Kama N., Rusli H.M., Khan M.K.","55874697000;55875827500;24469576200;57251481600;25654431500;57216753681;36609218000;8942252200;","A privacy-preserving authentication scheme based on Elliptic Curve Cryptography and using Quotient Filter in fog-enabled VANET",2022,"Ad Hoc Networks","128",,"102782","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124213906&doi=10.1016%2fj.adhoc.2022.102782&partnerID=40&md5=d21624f1ba97cf6fa10ee8c350147e86","Security and privacy are considered as two main challenges in Vehicular Ad Hoc Network (VANET). To cope with these challenges and in order to improve the safety of VANET, we propose a secure and privacy-preserving authentication scheme. In the proposed scheme, Quotient Filter (QF) is used to address node authentication while message authentication is done based on Elliptic Curve Cryptography (ECC). Besides, each vehicle is mapped to a different pseudo-identity to preserve privacy in VANET. Moreover, due to the higher computational capabilities of Fog Nodes (FN) compared to Road-Side Units (RSUs), they are distributed over the side of the road to minimize the latency of the security model and enhance the system throughput. Our security analysis demonstrates that the proposed scheme is able to identify illegitimacy vehicle nodes and invalid messages when the fog-enabled VANET is exposed to attacks. Furthermore, the performance evaluations prove the effectiveness of our work compared to the existing studies. © 2022 Elsevier B.V.","Authentication; Big data; Fog computing; Privacy; Quotient Filter; VANET","Big data; Fog; Fog computing; Geometry; Network security; Privacy-preserving techniques; Public key cryptography; Roads and streets; Vehicular ad hoc networks; Authentication scheme; Computational capability; Message authentication; Node authentications; Privacy; Privacy-preserving authentication; Pseudo identities; Quotient filter; Security and privacy; Vehicular Adhoc Networks (VANETs); Authentication",Article,Scopus,2-s2.0-85124213906
"Singh S.K., Lee C., Park J.H.","57211843264;55700514400;57305596300;","CoVAC: A P2P smart contract-based intelligent smart city architecture for vaccine manufacturing",2022,"Computers and Industrial Engineering","166",,"107967","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123923671&doi=10.1016%2fj.cie.2022.107967&partnerID=40&md5=0ffeeca06b732e81dcd61c825bd3b572","With the Corona Virus Disease 2019 (COVID-19) outbreak, vaccination is an urgent need worldwide. Internet of Things (IoT) has a vital role in the smart city for vaccine manufacturing with wearable sensors. According to the advanced services in intelligent manufacturing, the fourth resolution is also changing in Industry 5.0 and utilizes high-definition connectivity sensors. Traditional manufacturing companies rely on trusted third parties, which may act as a single point of failure. Access control, big data, and scalability are also challenging issues in existing systems because of the demand response data (DRD) in advanced manufacturing. To mitigate these challenges, CoVAC: A P2P Smart Contract-based Intelligent Smart City Architecture for Vaccine Manufacturing is proposed with three layers, including connection, conversion, and intelligent cloud layer. Smart contract-based blockchain is utilized at the conversion layer for resolving access control, security, and privacy issues. Deep learning is adopted in the intelligent cloud layer for big data analysis and increasing production for vaccine manufacturing in smart city environments. A case study is carried out wherein access data are collected from the various smart plants for vaccines using smart manufacturing to validate the effectiveness of the proposed architecture. Simulation of the proposed architecture is performed on the collected advanced sensor IoT plants data to address the challenges above, offering scalable production in the vaccine manufacturing for the smart city. © 2022","Access control; Blockchain; Deep learning; Security; Smart city; Vaccine manufacturing","Access control; Big data; Deep learning; Internet of things; Manufacture; Smart city; Vaccines; Viruses; Block-chain; Cloud layers; Deep learning; High definition; Intelligent Manufacturing; Proposed architectures; Security; Traditional manufacturing; Vaccine manufacturing; Virus disease; Blockchain",Article,Scopus,2-s2.0-85123923671
"Zeng F.J., Ding L., Chai X.G.","55889456500;57435512600;57435374800;","Urban traffic condition recognition and accessibility prediction based on big data",2022,"Advances in Transportation Studies","56",,,"143","157",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123837298&doi=10.53136%2f979125994790110&partnerID=40&md5=231d009ed7eadf2e9238226326566de6","Traffic congestion, frequent traffic accidents, and air pollution problems in the cities have brought opportunities for the development of intelligent transportation technologies and the data mining of massive traffic spatiotemporal data. Few existing studies have talked about the feature layer data fusion that reflects the main characteristics of the big data of traffic flow time series, and the prediction methods of temporal and spatial accessibility from alternative starting points to destinations in urban traffic spatiotemporal network are pending further research. For these reasons, this paper focused on traffic condition recognition and accessibility prediction based on multi-source big data of urban traffic flow. At first, this paper proposed a method for extracting the features of urban traffic spatiotemporal data, and realized preprocessing and fusion of the obtained data of the traffic flow and average vehicle speed of the road network. Then, a combinatorial optimization algorithm of simulated annealing (SA) and particle swarm optimization (PSO), and the Fuzzy C- Means (FCM) algorithm were employed to perform fuzzy classification on urban traffic conditions, and the urban traffic spatiotemporal network was used to describe the optimization objective function of urban traffic accessibility. Finally, this paper used real cases to verify the effectiveness of the proposed algorithm and the constructed model. © 2022, Aracne Editrice. All rights reserved.","Traffic accessibility; Traffic condition recognition; Urban traffic big data","Combinatorial optimization; Data fusion; Data mining; Forecasting; Fuzzy systems; Motor transportation; Particle swarm optimization (PSO); Simulated annealing; Traffic congestion; Intelligent transportation technologies; Pollution problems; Prediction-based; Spatio-temporal data; Spatiotemporal networks; Traffic accessibilities; Traffic condition recognition; Traffic flow; Urban traffic; Urban traffic big data; Big data",Article,Scopus,2-s2.0-85123837298
"Kruger U., Wang X., Embrechts M.J., Almansoori A., Hahn J.","7006392298;56071885800;57429327000;14027809900;56421348000;","Regularized error-in-variable estimation for big data modeling and process analytics",2022,"Control Engineering Practice","121",,"105060","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123628809&doi=10.1016%2fj.conengprac.2021.105060&partnerID=40&md5=1a47e64b2fdb9a70644af4e9e4d0b18d","This article addresses estimating the uncertainty in operational data by introducing a regularized modeling technique. Existing work (i) requires knowing the true dimension of the operational data, (ii) relies on a maximum likelihood estimation that is compromised by a stringent restriction for this true dimension and (iii) is computationally expensive. In contrast, the presented regularized error-in-variable technique (i) allows determining the true data dimension through hypothesis testing, (ii) is not limited by the restriction of existing methods, and (iii) has an objective function that can be solved efficiently. Based on a simulation example and the analysis of two industrial datasets, the paper highlights that the regularized estimation technique outperforms existing work and shows how to embed this technique within an advanced process analytics framework for advanced process control, optimization and general process diagnostics. © 2022 Elsevier Ltd","Big data; Error-in-variable models; Parameter estimation; Process data analytics; Regularization; Source signal extraction","Big data; Data Analytics; Errors; Maximum likelihood estimation; Uncertainty analysis; Data analytics; Error-in-variable model; Parameters estimation; Process data; Process data analytic; Regularisation; Signal extraction; Source signal extraction; Source signals; Variable model; Parameter estimation",Article,Scopus,2-s2.0-85123628809
"Sun T., Hu Q., Libby J., Atashzar S.F.","57226139542;57222197741;57351924200;12040259900;","Deep Heterogeneous Dilation of LSTM for Transient-Phase Gesture Prediction Through High-Density Electromyography: Towards Application in Neurorobotics",2022,"IEEE Robotics and Automation Letters","7","2",,"2851","2858",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123316587&doi=10.1109%2fLRA.2022.3142721&partnerID=40&md5=a04b17cafdb14d5a7ae55fbac5d103cc","Deep networks have been recently proposed to estimate motor intention using conventional bipolar surface electromyography (sEMG) signals for myoelectric control of neurorobots. In this regard, Deepnets are generally challenged by long training times (affecting practicality and calibration), complex model architectures (affecting the predictability of the outcomes), and a large number of trainable parameters (increasing the need for Big Data). Capitalizing on our recent work on homogeneous temporal dilation in a Recurrent Neural Network (RNN) model, this letter proposes, for the first time, heterogeneous temporal dilation in an LSTM model and applies that to high-density surface electromyography (HD-sEMG), allowing for the decoding of dynamic temporal dependencies with tunable temporal foci. In this letter, a 128-channel HD-sEMG signal space is considered due to the potential for enhancing the spatiotemporal resolution of human-robot interfaces. Accordingly, this letter addresses a challenging motor intention decoding problem of neurorobots, namely, transient intention identification. Our approach uses only the dynamic and transient phase of gesture movements when the signals are not stabilized or plateaued, which can significantly enhance the temporal resolution of human-robot interfaces. This would eventually enhance seamless real-time implementations. Additionally, this letter introduces the concept of 'dilation foci' to modulate the modeling of temporal variation in transient phases. In this work a high number (e.g., 65) of gestures is included, which adds to the complexity and significance of the understudied problem. Our results show state-of-the-art performance for gesture prediction in terms of accuracy, training time, and model convergence. © 2016 IEEE.","high density sEMG; Human-centered robotics; neurorobotics; recurrent neural networks; temporal dilation","Big data; Complex networks; Decoding; Electromyography; Long short-term memory; Real time control; Robots; Density surfaces; High density surface electromyography; Human centered robotics; Neurorobotics; Neurorobots; Signal resolution; Surface electromyography; Surface electromyography signals; Temporal dilation; Transient phase; Transient analysis",Article,Scopus,2-s2.0-85123316587
"Jiang X., Ge Z.","57220047571;55918987200;","Information Fingerprint for Secure Industrial Big Data Analytics",2022,"IEEE Transactions on Industrial Informatics","18","4",,"2641","2650",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122575941&doi=10.1109%2fTII.2021.3104056&partnerID=40&md5=7e51df040090f1976c47e785ca4d6462","Data-driven artificial intelligence (AI) models have been widely used in industrial systems helping big data analytic due to its convenience and flexibility. However, adversarial attacks have the ability to mislead AI models to make incorrect predictions just by adding specific perturbation to actual samples. With the high integration of industrial systems and information technology, the reliability and safety of AI models in industrial systems have been seriously threatened. In the article, fault diagnosis models and soft sensing models rely on AI technology are verified to be vulnerable facing adversarial attack. To this end, the concept of information fingerprint for industrial data is introduced to distinguish actual samples from adversarial samples with small perturbation. With fault diagnosis models and soft sensing models as the background, information fingerprint exaction networks based on deep learning is developed to extract the information fingerprint for further analysis. It utilizes supervised contrastive pretraining and unsupervised training to realize parameter learning for the structure of siamese neural network and autoencoder. Finally, the effectiveness and feasibility of the proposed information fingerprint for adversarial sample detection are verified in two industrial benchmark cases. © 2005-2012 IEEE.","Adversarial attack; data-driven artificial intelligence (AI) models; industrial big data analytics; information fingerprint","Accident prevention; Big data; Deep learning; Failure analysis; Adversarial attack; Data driven; Data-driven artificial intelligence model; Fault diagnosis model; Higher integration; Industrial big data analytic; Industrial systems; Information fingerprint; Intelligence models; Soft-sensing model; Fault detection",Article,Scopus,2-s2.0-85122575941
"Odebiri O., Mutanga O., Odindi J.","57215969586;55912148400;36521256000;","Deep learning-based national scale soil organic carbon mapping with Sentinel-3 data",2022,"Geoderma","411",,"115695","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122572272&doi=10.1016%2fj.geoderma.2022.115695&partnerID=40&md5=e48f73b0acb3270c473b885d8deff2b1","Mapping of soil organic carbon (SOC) at the regional level is critical for climate change policy and the mitigation of its adverse effects. However, reliable SOC estimates particularly over a large extent remains a major challenge due to among others limited sample points, quality of simulation data and the algorithm adopted. Remote sensing (RS) strategies have emerged as a suitable alternative to field and laboratory SOC determination, especially at large spatial extent. The use of Sentinel-3 sensor, the latest of the Sentinel series is minimal and has not been fully developed, despite its impressive attributes that include high spectral-temporal resolution and large coverage. Compared to linear and classical ML models, deep learning (DL) models offer a considerable improvement in data analysis due to their ability to extract more representative features and identify complex spatial patterns associated with big data. Yet, there is paucity in literature on the application of DL-based remote sensing strategies for SOC prediction. Consequently, this study adopted a deep neural network (DNN) to predict SOC at a national scale, using Sentinel-3 image, and compared the results with random forest (RF), support vector machine (SVM) and artificial neural network (ANN) models. The models were trained based on 10-fold cross-validation with 1936 soil samples and 31 predictors. The DNN model generated the best result with a root mean square error (RMSE) of 10.35 t/ha (26 % of the mean), followed by RF (RMSE = 11.2 t/ha), ANN (RMSE = 11.6 t/ha) and SVM (RMSE = 13.6 t/ha). The analytical prowess of the DNN, together with its ability to handle big data by learning patterns through a series of hidden layers (10) to draw conclusions, gives it an edge over other classical ML models. The study concluded that the DNN model with Sentinel-3 data is promising and provides an effective framework for continuous national level SOC modelling. © 2022 Elsevier B.V.","Deep neural network; Remote sensing; Sentinel-3; Soil organic carbon","Big data; Climate change; Decision trees; Mapping; Mean square error; Organic carbon; Remote sensing; Soils; Support vector machines; Adverse effect; Climate change policies; Neural network model; Random forests; Regional levels; Remote-sensing; Root mean square errors; Sentinel-3; Soil organic carbon; Support vectors machine; Deep neural networks; artificial neural network; climate change; mapping; Sentinel; soil organic matter",Article,Scopus,2-s2.0-85122572272
"Russell M., Wang P.","57219414746;56204328400;","Physics-informed deep learning for signal compression and reconstruction of big data in industrial condition monitoring",2022,"Mechanical Systems and Signal Processing","168",,"108709","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122453498&doi=10.1016%2fj.ymssp.2021.108709&partnerID=40&md5=c8bc7d65d278f54d3c44e7f9687cb27e","The onset of the Internet of Things enables machines to be outfitted with always-on sensors that can provide health information to cloud-based monitoring systems for prognostics and health management (PHM), which greatly improves reliability and avoids downtime of machines and processes on the shop floor. On the other hand, real-time monitoring produces large amounts of data, leading to significant challenges for efficient and effective data transmission (from the shop floor to the cloud) and analysis (in the cloud). Restricted by industrial hardware capability, especially Internet bandwidth, most solutions approach data transmission from the perspective of data compression (before transmission, at local computing devices) coupled with data reconstruction (after transmission, in the cloud). However, existing data compression techniques may not adapt to domain-specific characteristics of data, and hence have limitations in addressing high compression ratios where full restoration of signal details is important for revealing machine conditions. This study integrates Deep Convolutional Autoencoders (DCAE) with local structure and physics-informed loss terms that incorporate PHM domain knowledge such as the importance of frequency content for machine fault diagnosis. Furthermore, Fault Division Autoencoder Multiplexing (FDAM) is proposed to mitigate the negative effects of multiple disjoint operating conditions on reconstruction fidelity. The proposed methods are evaluated on two case studies, and autocorrelation-based noise analysis provides insight into the relative performance across machine health and operating conditions. Results indicate that physically-informed DCAE compression outperforms prevalent data compression approaches, such as compressed sensing, Principal Component Analysis (PCA), Discrete Cosine Transform (DCT), and DCAE with a standard loss function. FDAM can further improve the data reconstruction quality for certain machine conditions. © 2021 Elsevier Ltd","Big data; Data compression; Physics-informed deep learning; Prognostics and health management","Condition monitoring; Data communication systems; Data compression; Data transfer; Deep learning; Discrete cosine transforms; Discrete wavelet transforms; Floors; Health; Information management; Principal component analysis; Signal reconstruction; Systems engineering; Auto encoders; Condition; Data reconstruction; Data-transmission; Operating condition; Physic-informed deep learning; Prognostic and health management; Shopfloors; Signal compression; Signals reconstruction; Big data",Article,Scopus,2-s2.0-85122453498
"Gil M., Kozioł P., Wróbel K., Montewka J.","57204392278;57395276500;57193855711;57205016800;","Know your safety indicator – A determination of merchant vessels Bow Crossing Range based on big data analytics",2022,"Reliability Engineering and System Safety","220",,"108311","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122202448&doi=10.1016%2fj.ress.2021.108311&partnerID=40&md5=a13741532018d531853db4a18be7dc02","Even in the era of automatization maritime safety constantly needs improvements. Regardless of the presence of crew members on board, both manned and autonomous ships should follow clear guidelines (no matter as bridge procedures or algorithms). To date, many safety indicators, especially in collision avoidance have been proposed. One of such parameters commonly used in day-to-day navigation but usually omitted by researchers is Bow Crossing Range (BCR). Therefore, this paper aims to investigate, what are typical, empirical values of BCR during routine operations of merchant ships, as well as investigate what factors impact this indicator and to what extent. To this end, a ten-year big dataset of real maritime traffic obtained from the Automatic Identification System (AIS) was used to provide statistical and spatiotemporal analyses. The results indicate that BCR is strongly related to the type of navigational area (open sea or restricted waters) but not with the dimensions or speed of ships. Among analyzed vessel types, passenger ships were noted as vessels that cross other bows at the closes ranges. Results of this study may be found interesting by fleet managers and developers of Maritime Autonomous Surface Ships (MASS). The former could utilize the results to provide revised operational guidelines for deck officers while the latter - propose an early-detection warning system based on empirical data for prospective MASS. © 2021 The Author(s)","Automatic Identification System (AIS); big data analysis; Bow Crossing Range (BCR); Maritime Autonomous Surface Ships (MASS); maritime risk and safety; ship collision avoidance","Automation; Big data; Collision avoidance; Data Analytics; Fleet operations; Risk assessment; Safety engineering; Automatic identification system; Big data analyse; Bow crossing range; Maritime autonomous surface ship; Maritime risks; Maritime safety; Safety indicator; Ship collision avoidance; Surface ship; Ships",Article,Scopus,2-s2.0-85122202448
"Cozzini P., Cavaliere F., Spaggiari G., Morelli G., Riani M.","6602175988;57195128077;57193772586;56927826800;6604094178;","Computational methods on food contact chemicals: Big data and in silico screening on nuclear receptors family",2022,"Chemosphere","292",,"133422","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122095543&doi=10.1016%2fj.chemosphere.2021.133422&partnerID=40&md5=148d8cb90c27528453f0aebdfd6ac6b3","According to Eurostat, the EU production of chemicals hazardous to health reached 211 million tonnes in 2019. Thus, the possibility that some of these chemical compounds interact negatively with the human endocrine system has received, especially in the last decade, considerable attention from the scientific community. It is obvious that given the large number of chemical compounds it is impossible to use in vitro/in vivo tests for identifying all the possible toxic interactions of these chemicals and their metabolites. In addition, the poor availability of highly curated databases from which to retrieve and download the chemical, structure, and regulative information about all food contact chemicals has delayed the application of in silico methods. To overcome these problems, in this study we use robust computational approaches, based on a combination of highly curated databases and molecular docking, in order to screen all food contact chemicals against the nuclear receptor family in a cost and time-effective manner. © 2021","Computational chemistry; Consensus prediction; Database; Nuclear receptors; Toxicology","Big data; Chemical contamination; Computational chemistry; Computational methods; Indicators (chemical); Industrial chemicals; Metabolites; Consensus prediction; Endocrine systems; Food contact; In-silico screening; In-vitro; In-vivo tests; Nuclear receptors; Scientific community; Toxic interactions; Toxicology; Database systems; chemical compound; data set; detection method; endocrine system; European Union; toxicity; cell receptor; endocrine disruptor; food; human; molecular docking; Big Data; Endocrine Disruptors; Food; Humans; Molecular Docking Simulation; Receptors, Cytoplasmic and Nuclear",Article,Scopus,2-s2.0-85122095543
"Georgiadis G., Poels G.","57221775936;6602187929;","Towards a privacy impact assessment methodology to support the requirements of the general data protection regulation in a big data analytics context: A systematic literature review",2022,"Computer Law and Security Review","44",,"105640","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122019514&doi=10.1016%2fj.clsr.2021.105640&partnerID=40&md5=8816a48a0b99ef1ea2352767798d0b63","Big Data Analytics enables today's businesses and organisations to process and utilise the raw data that is generated on a daily basis. While Big Data Analytics has improved efficiency and created many opportunities, it has also increased the risk of personal data being compromised or breached. The General Data Protection Regulation (GDPR) mandates Data Protection Impact Assessment (DPIA) as a means of identifying appropriate controls to mitigate risks associated with the protection of personal data. However, little is currently known about how to conduct such a DPIA in a Big Data Analytics context. To this end, we conducted a systematic literature review with the aim of identifying privacy and data protection risks specific to the Big Data Analytics context that could negatively impact individuals' rights and freedoms when they occur. Based on a sample of 159 articles, we applied a thematic analysis to all identified risks which resulted in the definition of nine Privacy Touch Points that summarise the identified risks. The coverage of these Privacy Touch Points was then analysed for ten Privacy Impact Assessment (PIA) methodologies. The insights gained from our analysis will inform the next phase of our research, in which we aim to develop a comprehensive DPIA methodology that will enable data processors and data controllers to identify, analyse and mitigate privacy and data protection risks when storing and processing data involving Big Data Analytics. © 2021 Georgios Georgiadis and Geert Poels","Big data analytics; Data protection; Data protection directive; General data protection regulation; Governance; Information security; Privacy; Privacy impact assessment; Systematic literature review","Big data; Data Analytics; Risk assessment; Data Protection Directive; Data protection impact assessments; General data protection regulations; Governance; Impact assessment methodologies; Impact assessments; Privacy; Privacy impact assessment; Systematic literature review; Touch-points; Data privacy",Article,Scopus,2-s2.0-85122019514
"Li Q., Xiong D., Shang M.","56505726100;57390585000;12761788400;","Adjusted stochastic gradient descent for latent factor analysis",2022,"Information Sciences","588",,,"196","213",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121963727&doi=10.1016%2fj.ins.2021.12.065&partnerID=40&md5=f78b87d56cbd5dc467edd627fff4efc8","A high-dimensional and incomplete (HDI) matrix is a common form of big data in most industrial applications. Stochastic gradient descent (SGD) algorithm optimized latent factor analysis (LFA) model is often adopted in learning the abundant knowledge in HDI matrix. Despite its computational tractability and scalability, when solving a bilinear problem such as LFA, the regular SGD algorithm tends to be stuck in a local optimum. To address this issue, the paper innovatively proposes an Adjusted Stochastic Gradient Descent (ASGD) for Latent Factor Analysis, where the adjustment mechanism is implemented by considering the bi-polar gradient directions during optimization, such mechanism is theoretically proved for its efficiency in overstepping local saddle points and avoiding premature convergence. Also, the hyper-parameters of the model are implemented in a self-adaptive manner using the particle swarm optimization (PSO) algorithm, for higher practicality. Experimental results show that the proposed model outperforms other state-of-the-art approaches on six different HDI matrices from industrial applications, especially in prediction accuracy for missing data. © 2021 The Author(s)","Adaptive model; Big data analysis; Gradient adjustment; High-dimensional and incomplete matrix; Latent factor analysis; Local optima; Particle swarm optimization; Stochastic gradient descent","Big data; Factor analysis; Gradient methods; Matrix algebra; Multivariant analysis; Stochastic models; Stochastic systems; Adaptive models; Big data analyse; Gradient adjustment; High-dimensional; High-dimensional and incomplete matrix; Higher-dimensional; Latent factor analysis; Local optima; matrix; Stochastic gradient descent; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-85121963727
"Zhao J., Zhang Y., Ding Y., Yu Q., Hu M.","57377823900;57221126347;55515805700;57390557000;56375185700;","A novel in-depth analysis approach for domain-specific problems based on multidomain data",2022,"Information Sciences","588",,,"142","158",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121962810&doi=10.1016%2fj.ins.2021.12.013&partnerID=40&md5=0d0b492d7802ca4f10e70074439fe50a","When addressing analysis and prediction problems in a specific domain based on big data processing, the following problems often arise: only relationships between features in the domain itself are considered, and existing methods are not effective for training models on small sample data sets. The traditional approach usually obtains the relationships between single-domain features. Analysis and forecasting in the problem domain alone can quickly achieve good accuracy, but due to the limitations of the analysis domain, it becomes increasingly difficult to further improve the prediction accuracy. This paper proposes a novel data analysis approach compatible with small sample sets called multidomain data depth analysis (MODE). In contrast to traditional approaches, MODE emphasizes multidomain data and considers the relationships among feature domains in the original data. The features in each domain are orthogonally extracted, and feature dimensions are expanded in accordance with the characteristics of small data sets. A better prediction model can be obtained by using the expanded and strengthened features. We apply this approach to real big data from the field of sociology to predict annual income based on census data in experiments. The experimental results show that MODE offers a better prediction effect based on small multidomain samples. © 2021 Elsevier Inc.","Minimum sample set; Multiple domain; Original data; Prediction","Big data; Data handling; Population statistics; Analysis approach; Domain specific; In-depth analysis; Minimum sample set; Multi-domains; Multiple domains; Original data; Sample sets; Specific problems; Traditional approaches; Forecasting",Article,Scopus,2-s2.0-85121962810
"Ran X., Xu Y., Liu Y., Jiang J.","57201744654;57193134361;35096451400;8650700700;","Examining online social behavior changes after migration: An empirical study based on OSN big data",2022,"Computers in Human Behavior","129",,"107158","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121910345&doi=10.1016%2fj.chb.2021.107158&partnerID=40&md5=9faf90724fd21676fd679442c145c673","With fast urbanization and decreasing transportation cost, migration becomes more common. Previous studies have shown the important role of social networks in the process of migration, but little is known about the effect of migration on social networks. To fill the research gap, this study examines the effect of migration on online social behaviors (in terms of network evolution and social interaction), as well as the moderating effect of migrants' characteristics. We collect a four-month big dataset with 2.29 million records from one of the largest online social networks in China. We apply the propensity score matching combined with the difference-in-differences method to compare online social behavior changes after migration. Our results show that, for network evolution behavior, migration positively impacts on the number of tie formation, but non-significantly impacts on the number of tie decay; for social interaction behavior, migration increases the number of contacts but decreases the number of messages. We also find some moderating effects of migrants’ characteristics, including gender, age, and degree. This study provides big data empirical evidence and some new insights to our understandings of the impact of migration on online social network behavior. © 2021 Elsevier Ltd","Migration; Network evolution; Online social network; Social interaction","Big data; Economic and social effects; Behaviour changes; Empirical studies; Migration; Moderating effect; Networks evolutions; Online social behaviors; Propensity score matching; Research gaps; Social interactions; Transportation cost; Social networking (online); article; behavior change; big data; China; controlled study; empiricism; female; gender; human; male; migrant; online social network; propensity score; social interaction",Article,Scopus,2-s2.0-85121910345
"Glavind S.T., Sepulveda J.G., Faber M.H.","57204416281;37035086900;57377676400;","On a simple scheme for systems modeling and identification using big data techniques",2022,"Reliability Engineering and System Safety","220",,"108219","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121470135&doi=10.1016%2fj.ress.2021.108219&partnerID=40&md5=66e015d533dcfd0cafcfa3727c92b647","In the field of reliability engineering and systems safety, it is a common challenge to identify the state of a system with basis in a limited set of observations of system performances. Often, there are a multitude of different possible system states, including states of damages, which compete in explaining the observations. To account for these in the context of risk-informed management of systems, the probabilities of the relevant possible different states are needed. In the present contribution, an idea on how this might be supported through big data techniques is presented. Here, systems are considered more holistically and not only as the relationship between input and output. The starting point is to establish a knowledge-consistent probabilistic representation of the system, its key performance characteristics, and the observations (exposures, condition state and performances) that may be collected from the system in reality. Monte Carlo simulations are then employed to establish the relevant scenarios of realizations of the random variables describing possible system states, system performance characteristics, and observations. Using big data classification on the simulated scenarios, the probabilities of the system being in a given state, given particular outcomes of observations, may then be straightforwardly evaluated. The application of the presented idea is illustrated through two examples considering damage identification in structural systems subject to extreme loading. © 2021","Big data; Observations; Structural damage identification; System identification; Systems modeling","Damage detection; Intelligent systems; Monte Carlo methods; Structural analysis; Modelling and identifications; Observation; Performance characteristics; Reliability systems; Simple schemes; Structural damage identification; System models; System state; System-identification; Systems performance; Big data",Article,Scopus,2-s2.0-85121470135
"Gong C., Su Z.-G., Wang P.-H., You Y.","57214316342;57189905566;8314442100;56147030400;","Distributed evidential clustering toward time series with big data issue",2022,"Expert Systems with Applications","191",,"116279","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121107923&doi=10.1016%2fj.eswa.2021.116279&partnerID=40&md5=7aab6d9a03deb7c1cb360c3ca9740826","To analyze time series data with large volume, most of the existing clustering algorithms focus on data reduction techniques or multi-level strategies. However, the destruction of raw data structure is inevitable, leading to the information loss, even an abnormal and unaccountable clustering result. To tackle above issues, we propose a distributed evidential clustering algorithm that can be directly adopted on the raw big data, and parallelize it under the Apache Spark, which is a processing engine built for sophisticated data analysis. Concretely, in a parallel way, the possibility of becoming a cluster center is first calculated for each data object under the framework of evidence theory. After drawing a decision graph, the cluster centers are determined and then a credal partition of the time series data is derived. Without simplifying the data structure, the proposed algorithm can not only detect the number of clusters, but also describe the ambiguity and uncertainty in memberships of every data object. Experiments on several benchmark datasets and one real-world problem show the strong scalability and well clustering performance of the introduced algorithm. © 2021 Elsevier Ltd","Apache spark; Big data; Distributed calculation; Evidential clustering; Warped dissimilarity measure","Benchmarking; Clustering algorithms; Data structures; Time series; Apache spark; Cluster centers; Clusterings; Data objects; Dissimilarity measures; Distributed calculation; Evidential clustering; Time-series data; Times series; Warped dissimilarity measure; Big data",Article,Scopus,2-s2.0-85121107923
"Bi W., Lu W., Zhao Z., Webster C.J.","57353366600;24173836000;55810497200;7201838784;","Combinatorial optimization of construction waste collection and transportation: A case study of Hong Kong",2022,"Resources, Conservation and Recycling","179",,"106043","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119920721&doi=10.1016%2fj.resconrec.2021.106043&partnerID=40&md5=52e75bf278e7edb923ffab028c4d2c41","Although construction waste consistently contributes the highest proportion of all solid waste landfilled, its collection and transportation has received little attention. This research conducts a case study of Hong Kong with the aim of unraveling the causes of ineffective construction waste management from a logistics perspective and developing strategies to improve efficiency of waste collection and transportation. We analyze 112,892 individual trips undertaken by 2,563 construction waste hauling trucks and find three critical issues, namely irrational facility choice, disorganized trip chains, and serious underloading. We design an order-to-order distance matrix, based on which we develop a combinatorial approach to optimizing construction waste collection and transportation through (1) optimal facility choice, (2) proper order sequencing, and (3) increased loading ratio. Simulation results indicate that optimal facility choice is most effective of the three strategies, reducing travel distance by 15.2% (15,256 km) and saving 15.3% (21,467 kg) in CO2-eq emissions. Combining the three strategies creates the best optimization effects, saving 20.3% of travel distance (20,346 km) and 18.2% of CO2-eq emissions (25,544 kg). Our findings provide valuable insights for construction waste management and suggest strategies (e.g., developing a work dispatch system like Uber or proper vehicle routing algorithms) for improving waste collection efficiency and reducing carbon emissions. © 2021 Elsevier B.V.","Carbon emission; Construction waste; Order-to-order distance matrix; Routing optimization; Waste collection and transportation","Carbon; Carbon dioxide; Combinatorial optimization; Efficiency; Solid wastes; Waste management; Carbon emissions; Case-studies; Construction waste managements; Construction wastes; Distance matrix; Hong-kong; Order-to-order distance matrix; Routing optimization; Waste collection; Waste transportation; Vehicle routing; carbon dioxide; algorithm; Article; benchmarking; big data; carbon emission; construction and demolition waste; data analysis; process optimization; simulation; travel; waste management",Article,Scopus,2-s2.0-85119920721
"Papadopoulos T., Balta M.E.","16240044400;36052320100;","Climate Change and big data analytics: Challenges and opportunities",2022,"International Journal of Information Management","63",,"102448","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118756970&doi=10.1016%2fj.ijinfomgt.2021.102448&partnerID=40&md5=65729181eeacce9abd684d511b5dd614","Scholars and practitioners have long acknowledged the impact of climate change on businesses, operations, and supply chains. Nevertheless, there is still scant research on the role of Big Data and Analytics (BDA) in addressing these challenges but also opportunities created by Climate Change for operations and supply chains as they strive to become more sustainable. We address this gap in this opinion paper by identifying and discussing how these challenges and opportunities can be better pursued. We then propose thematic foci that future research on BDA and climate change could follow to facilitate the transition to a sustainable future. © 2021 Elsevier Ltd","Big data; Big data analytics; Challenges; Climate change; Opportunities; Sustainability","Advanced Analytics; Climate change; Data Analytics; Supply chains; Business operation; Challenge; Opportunity; Big data",Article,Scopus,2-s2.0-85118756970
"He J., Qian Y., Yin X.","57223036733;57295302600;57223052102;","Supplier evaluation and selection system of embedded e-commerce platform based on big data",2022,"International Journal of Information Systems and Supply Chain Management","15","2",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117002584&doi=10.4018%2fIJISSCM.287629&partnerID=40&md5=9994f2b931830b85efbc4d85bda77b81","For e-commerce companies, it is easier to obtain a large amount of aggregated data about user behavior with the help of embedded network platforms, which contains valuable information that helps to form effective decision-making. This article first gives a detailed introduction to the evaluation and selection of e-commerce and suppliers then puts forward the analytic hierarchy process and entropy method; finally, the AHP analytic method is used to build a supplier evaluation system and a selection system. The experimental results of this paper show that after obtaining the entropy AHP weights through the analytic hierarchy process, these eight suppliers can be ranked and selected. Using the ABC classification method, classification is based on the ranking of suppliers. Among them, Class A suppliers account for 12.5%, which plays a key role in the construction of the evaluation and selection system of e-commerce suppliers. Copyright © 2022, IGI Global.","Analytic Hierarchy Process; Big Data; E-Commerce Platform; Entropy Method Supplier Evaluation System","Analytic hierarchy process; Behavioral research; Decision making; Electronic commerce; Hierarchical systems; Aggregated datum; Commerce platforms; E- commerces; E-commerce platform; Entropy method supplier evaluation system; Entropy methods; Large amounts; Selection systems; Supplier evaluation and selections; Supplier Evaluations; Big data",Article,Scopus,2-s2.0-85117002584
"Hu Z., Li D.","56703911800;57204110845;","Improved heuristic job scheduling method to enhance throughput for big data analytics",2022,"Tsinghua Science and Technology","27","2",,"344","357",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116355236&doi=10.26599%2fTST.2020.9010047&partnerID=40&md5=c5cdfaa226687a280091beccfeb4e589","Data-parallel computing platforms, such as Hadoop and Spark, are deployed in computing clusters for big data analytics. There is a general tendency that multiple users share the same computing cluster. The schedule of multiple jobs becomes a serious challenge. Over a long period in the past, the Shortest-Job-First (SJF) method has been considered as the optimal solution to minimize the average job completion time. However, the SJF method leads to a low system throughput in the case where a small number of short jobs consume a large amount of resources. This factor prolongs the average job completion time. We propose an improved heuristic job scheduling method, called the Densest-Job-Set-First (DJSF) method. The DJSF method schedules jobs by maximizing the number of completed jobs per unit time, aiming to decrease the average Job Completion Time (JCT) and improve the system throughput. We perform extensive simulations based on Google cluster data. Compared with the SJF method, the DJSF method decreases the average JCT by 23.19% and enhances the system throughput by 42.19%. Compared with Tetris, the job packing method improves the job completion efficiency by 55.4%, so that the computing platforms complete more jobs in a short time span. © 1996-2012 Tsinghua University Press.","Big data; Job completion efficiency; Job completion time; Job scheduling; Job throughput","Cluster computing; Efficiency; Heuristic methods; Job shop scheduling; Throughput; Completion efficiencies; Completion time; Job completion; Job completion efficiency; Job completion time; Job throughput; Jobs scheduling; Scheduling methods; System throughput; Big data",Article,Scopus,2-s2.0-85116355236
"Obaid K., Pukthuanthong K.","57217994585;14825851600;","A picture is worth a thousand words: Measuring investor sentiment by combining machine learning and photos from news",2022,"Journal of Financial Economics","144","1",,"273","297",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110986461&doi=10.1016%2fj.jfineco.2021.06.002&partnerID=40&md5=77fdfb2abdba2f1d9fe717bd3e08171b","By applying machine learning to the accurate and cost-effective classification of photos based on sentiment, we introduce a daily market-level investor sentiment index (Photo Pessimism) obtained from a large sample of news photos. Consistent with behavioral models, Photo Pessimism predicts market return reversals and trading volume. The relation is strongest among stocks with high limits to arbitrage and during periods of elevated fear. We examine whether Photo Pessimism and pessimism embedded in news text act as complements or substitutes for each other in predicting stock returns and find evidence that the two are substitutes. © 2021","Behavioral finance; Big data; Deep learning; Investor sentiment; Machine learning; Return predictability",,Article,Scopus,2-s2.0-85110986461
"Katsikopoulos K.V., Şimşek Ö., Buckmann M., Gigerenzer G.","6602799549;8236724300;56674617200;24368890600;","Transparent modeling of influenza incidence: Big data or a single data point from psychological theory?",2022,"International Journal of Forecasting","38","2",,"613","619",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100316442&doi=10.1016%2fj.ijforecast.2020.12.006&partnerID=40&md5=a11a9744a270bd7d3787b5a99e0b9adb","Simple, transparent rules are often frowned upon while complex, black-box models are seen as holding greater promise. Yet in quickly changing situations, simple rules can protect against overfitting and adapt quickly. We show that the surprisingly simple recency heuristic forecasts more accurately than Google Flu Trends (GFT) which used big data analytics and a black-box algorithm. This heuristic predicts that “this week's proportion of flu-related doctor visits equals the proportion from the most recent week.” It is based on psychological theory of how people deal with rapidly changing situations. Other theory-inspired heuristics have outperformed big data models in predicting outcomes, such as U.S. presidential elections, or other uncertain events, such as consumer purchases, patient hospitalizations, and terrorist attacks. Heuristics are transparent, clearly communicating the underlying rationale for their predictions. We advocate taking into account psychological principles that have evolved over millennia and using these as a benchmark when testing big data models. © 2020 International Institute of Forecasters","Big data; Google Flu Trends; Naïve forecasting; Recency; Simple heuristics",,Article,Scopus,2-s2.0-85100316442
"Nagendra N.P., Narayanamurthy G., Moser R., Singh A.","55933884700;36638192700;35226404600;56497315100;","Open Innovation Using Satellite Imagery for Initial Site Assessment of Solar Photovoltaic Projects",2022,"IEEE Transactions on Engineering Management","69","2",,"338","350",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091417491&doi=10.1109%2fTEM.2019.2955315&partnerID=40&md5=de9cf3b3abc3a7d3a2e1a907e51647b8","One of the responses to the fight against climate change by the developing world has been the large-scale adoption of solar energy. The adoption of solar energy in countries like India is propagating mainly through the development of energy producing photovoltaic farms. The realization of solar energy producing sites involves complex decisions and processes in the selection of sites whose knowhow may not rest with all the stakeholders supporting (e.g., banks financing the project) the industry value chain. In this article, we use the region of Bangalore in India as the case study to present how open innovation using satellite imagery can provide the necessary granularity to specifically aid in an independent initial assessment of the solar photovoltaic sites. We utilize the established analytical hierarchy process over the information extracted from open satellite data to calculate an overall site suitability index. The index takes into account the topographical, climatic, and environmental factors. Our results explain how the intervention of satellite imagery-based big data analytics can help in buying the confidence of investors in the solar industry value chain. Our study also demonstrates that open innovation using satellites can act as a platform for social product development. © 1988-2012 IEEE.","Big data analytics; energy; open innovation; satellite imagery; solar industry","Big data; Climate change; Data Analytics; Developing countries; Satellite imagery; Solar power generation; Technology transfer; Complex decision; Developing world; Energy; Industry value chains; Large-scales; Open innovation; Photovoltaics; Site assessment; Solar industries; Solar photovoltaics; Solar energy",Article,Scopus,2-s2.0-85091417491
"Ma P., Lau C.P., Yu N., Li A., Sheng J.","57190740233;57224867518;57212420627;57224174425;24067754100;","Application of deep learning for image-based Chinese market food nutrients estimation",2022,"Food Chemistry","373",,"130994","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118512513&doi=10.1016%2fj.foodchem.2021.130994&partnerID=40&md5=a537b635d7bb5f33fb1369038697457a","With commercialization of deep learning (DL) models, daily precision dietary record based on images from smartphones becomes possible. This study took advantage of DL techniques on visual recognition tasks and proposed a suite of big-data-driven DL models regressing from food images to their nutrient estimation. We established and publicized the first food image database from the Chinese market, named ChinaMartFood-109. It contained 10,921 images with 23 nutrient contents, covering 18 main food groups. Inception V3 was optimized using other state-of-the-art deep convolutional neural networks, achieving up to 78 % and 94 % for top-1 and top-5 accuracy, respectively. Besides, this research compared three nutrient estimation algorithms and achieved the best regression coefficient (R2) by normalization + AM compared with arithmetic mean and harmonic mean, validating applicability in practice as well as theory. These encouraging results provide further evidence supporting artificial intelligence in the field of food analysis. © 2021 Elsevier Ltd","Chinese market food; Convolutional neural network; Deep learning; Food composition; Food image; Food nutrients; Nutrients","Commerce; Convolution; Convolutional neural networks; Deep neural networks; Chinese market food; Chinese markets; Commercialisation; Convolutional neural network; Deep learning; Food compositions; Food image; Food nutrient; Image-based; Learning models; Nutrients; arithmetic; article; artificial intelligence; big data; convolutional neural network; deep learning; food analysis; food composition; nutrient content; visual memory; artificial intelligence; China; Artificial Intelligence; China; Deep Learning; Neural Networks, Computer; Nutrients",Article,Scopus,2-s2.0-85118512513
"Yang X., Yang Y., Sun M., Jia J., Cheng X., Pei Z., Li Q., Xu D., Xiao K., Li X.","57191441881;57196442918;57212793531;56005111500;13408933000;57200628094;57214933039;57223429877;57260252900;52063654000;","A new understanding of the effect of Cr on the corrosion resistance evolution of weathering steel based on big data technology",2022,"Journal of Materials Science and Technology","104",,,"67","80",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114995803&doi=10.1016%2fj.jmst.2021.05.086&partnerID=40&md5=1dd8202d7d5d66961af5cab3422433d5","In this work, we studied the effect of Cr element on the corrosion resistance evolution of weathering steel based on corrosion big data technology. It suggested that corrosion big data technology is suitable for evaluation of the effect of microalloying Cr element on the corrosion evolution behavior of weathering steel. New understandings prove that the effect of Cr on the corrosion process is dynamic rather than static, the processes is affected by both of the environmental factors and the electrochemical or chemical reactions in the rust layer. Besides, Cr element has both beneficial effect and detrimental effect on the corrosion resistance of weathering steel. The beneficial effect is that the general corrosion resistance of Cr-additional steel is better than that of Cr-free steel, while the detrimental effect is that localized corrosion is intensified as the increase of Cr content in the Cr-additional steel. © 2021","Atmospheric corrosion; Big data; Cr-addition; Weathering steel","Big data; Chromium steel; Corrosion resistance; Localized corrosion; Weathering; Weathering steel; Beneficial effects; Corrosion evolutions; Corrosion process; Cr content; Cr-free; Data technologies; Environmental factors; Rust layers; Steel corrosion",Article,Scopus,2-s2.0-85114995803
"Du M.","57193575313;","Application of information communication network security management and control based on big data technology",2022,"International Journal of Communication Systems","35","5","e4643","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097257504&doi=10.1002%2fdac.4643&partnerID=40&md5=191f482fd0b659edcc81cf5355ff7b45","With the continuous development of communication technology and computer network technology and the continuous progress of social economy, information technology has penetrated into all aspects of people's lives. The basic task of communication network is to ensure the reliability and security of information transmission. Based on the above background, the purpose of this study is to study the application of information communication network security management and control based on big data technology. Based on the concept and management and control structure of information communication network, combined with the data collection and preprocessing in big data technology, this study analyzes the feasibility of big data technology applied in communication network management and control and expounds the specific application of big data technology from the four aspects of troubleshooting, security protection, business management, and resource allocation. At last, the test environment of information communication network security management and control platform based on big data technology is built in the laboratory to test the network management performance and routing transmission performance of the system. The experimental results show that the communication success rate of the information communication network security management and control platform based on big data technology remains to 91.78%. It can be seen from the test results that the information communication network security management and control platform based on big data technology can not only accurately collect sensor data but also have the function of real-time monitoring network status and reconfiguration of nodes. © 2020 John Wiley & Sons Ltd.","big data technology; detection system; information communication; network security","Big data; Data acquisition; Data communication systems; Economics; Industrial management; Information management; Network management; Communication technologies; Computer network technology; Continuous development; Information communication; Information transmission; Management and controls; Network security management; Transmission performance; Network security",Article,Scopus,2-s2.0-85097257504
"Guo D., Li J., Li X., Li Z., Li P., Chen Z.","57216208059;8665048900;57220033828;57193788762;57211978013;7409488632;","Advance prediction of collapse for TBM tunneling using deep learning method",2022,"Engineering Geology","299",,"106556","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124487700&doi=10.1016%2fj.enggeo.2022.106556&partnerID=40&md5=2c6e2480799468261ba71963c95731a7","Collapse hazards pose a serious threat to the tunnel construction, especially when a tunnel boring machine (TBM) is used. The current methods are mainly focus on the prediction of adverse geology, which cannot provide the specific area that collapse may happen. Recent development of TBM is capable to offer immense amounts of monitored data by the sensors in the machine. A three-stage method was proposed in this study to predict the collapse area in a real-time manner by combining the TBM-generated data with a deep learning algorithm. The method starts with constructing a long short-term memory model to predict torque and thrust based on the data in non-collapse area. This model can then be used to predict the torque and thrust in a real-time manner for the following tunneling. The predicted parameters will be consistent with the measured ones when the tunneling are stable. On the contrary, the prediction accuracy will be decreased when a collapse is prone to happen. A three-stage method is then proposed based on this principle, which is further examined using the big data from Yin-song tunneling Project in China. There were 18 collapses along this 20 km tunnel, 12 of which have been successfully predicted in the second stage and the remaining 6 have been predicted in the third stage. Essentially, the proposed method here is capable to accurately predict tunnel collapse and provide early warning in advance for tunneling, paving the way to a self-driving TBM in harsh geological conditions. © 2022 Elsevier B.V.","Big data; Collapse; Deep learning; Geological hazard; TBM tunneling","Big data; Construction equipment; Deep learning; Forecasting; Geology; Hazards; Learning algorithms; Tunneling (excavation); Tunneling machines; Tunnels; 'current; Collapse; Collapse hazards; Deep learning; Geological hazards; Learning methods; Real- time; Three-stage method; Tunnel boring machine tunneling; Tunnel construction; Boring machines (machine tools)",Article,Scopus,2-s2.0-85124487700
"Ciccullo F., Fabbri M., Abdelkafi N., Pero M.","57190743223;57445854000;6505544275;33568247200;","Exploring the potential of business models for sustainability and big data for food waste reduction",2022,"Journal of Cleaner Production","340",,"130673","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124262869&doi=10.1016%2fj.jclepro.2022.130673&partnerID=40&md5=553b2ac50d672ef90449c27f7c02e4c9","Because the volume of food waste is increasing, actions are required to mitigate the environmental and social impact of food waste generation. This paper investigates the business models of 41 selected startups (technology and service providers) to capture how companies avoid food waste or use food waste as a resource. The case study analysis, based on secondary sources, shows that some startup business models leverage Big Data Analytics (BDA) to avoid food waste by optimizing an existing linear supply chain, while other business models create value out of food waste by leveraging a circular food supply chain. We found that the latter business models are not fully exploiting the potential of BDA. Based on our findings, we derive three propositions and one corollary. Whereas BDA seems a necessary requirement for business models that are focused on optimizing a linear supply chain, it appears optional for business models closing the supply chain loop. The propositions also discuss the timing when startups should start developing BDA capabilities depending on the business model type. © 2022","Big data; Food waste reduction; Sustainable business model","Big data; Data Analytics; Food supply; Supply chains; Business models; Food waste; Food waste reduction; Linear supplies; Social impact; Sustainable business; Sustainable business model; Technology providers; Waste generation; Waste reduction; Sustainable development",Article,Scopus,2-s2.0-85124262869
"Ignacz G., Szekely G.","57202774374;50162445100;","Deep learning meets quantitative structure–activity relationship (QSAR) for leveraging structure-based prediction of solute rejection in organic solvent nanofiltration",2022,"Journal of Membrane Science","646",,"120268","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122703164&doi=10.1016%2fj.memsci.2022.120268&partnerID=40&md5=c1118da21b62395ff4df1824fa2d541a","Methods for determining solute rejection in organic solvent nanofiltration (OSN) are time-consuming and expensive and still rely on wet-lab measurements, resulting in the slow development of membrane processes. OSN, similar to other membrane technologies, requires precise and comprehensive predictive models that can function on various solutes, membranes, and solvents. We present two prediction methods based on the quantitative structure–activity relationship (QSAR) using traditional machine learning (ML) and deep learning (DL) models. The partial least-squares regression model combined with the variable importance in projection and genetic algorithm achieves a slightly lower root-mean-square error score (8.04) than the DL-based graph neural network (10.40). For the first time, we visualize the effect of different solute functional groups on rejection, providing a new platform for a more in-depth investigation into the membrane–solute interactions, potentially enabling the design of membranes with improved selectivity. Our ML model is freely accessible on the OSN database website (www.osndatabase.com) for everyone. © 2022 Elsevier B.V.","Big data; Deep learning; Machine learning; Partial least squares regression; Quantitative structure–activity relationship",,Article,Scopus,2-s2.0-85122703164
"Yang Y., Gong N., Xie K., Liu Q.","57218995066;57465957100;57440700800;57440800500;","Predicting Gasoline Vehicle Fuel Consumption in Energy and Environmental Impact Based on Machine Learning and Multidimensional Big Data",2022,"Energies","15","5","1602","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125187218&doi=10.3390%2fen15051602&partnerID=40&md5=feda39bb671fd6b8a0f67bbb9ec44189","The underestimation of fuel consumption impacts various aspects. In the vehicle market, manufacturers often advertise fuel economy for marketing. In fact, the fuel consumption reference value provided by the manufacturer is quite different from the real-world fuel consumption of the vehicles. The divergence between reference fuel consumption and real-world fuel consumption also has negative effect on the aspects of policy and environment. In order to effectively promote the sustainable development of transport, it is urged to recognize the real-world fuel consumption of vehicles. The gaps in previous studies includes small sample size, single data dimension, and lack of feature weight evaluation. To fill the research gap, in this study, we conduct a comparative analysis through building five regression models to forecast the real-world fuel consumption rate of light-duty gasoline vehicles in China based on big data from the perspectives of vehicle factors, environment factors, and driving behavior factors. Results show that the random forest regression model performs best among the five candidate models, with a mean absolute error of 0.630 L/100 km, a mean absolute percentage error of 7.5%, a mean squared error of 0.805, an R squared of 0.776, and a 10-fold cross-validation score of 0.791. Further, we capture the most important features affecting fuel consumption among the 25 factors from the above three perspectives. According to the relative weight of each factor in the most optimal model, the three most important factors are brake and accelerator habits, engine power, and the fuel economy consciousness of vehicle owners in sequence. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Energy and environmental; Fuel consumption; Machine learning","Automotive industry; Big data; Commerce; Decision trees; Environmental impact; Errors; Fuel economy; Gasoline; Regression analysis; Sustainable development; Energy; Energy and environmental; Gasoline vehicle; Machine-learning; On-machines; Real-world; Reference values; Regression modelling; Small Sample Size; Vehicle fuels; Machine learning",Article,Scopus,2-s2.0-85125187218
"Demissie M.G., Kattan L.","55746411100;6603440769;","Estimation of truck origin-destination flows using GPS data",2022,"Transportation Research Part E: Logistics and Transportation Review","159",,"102621","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124869815&doi=10.1016%2fj.tre.2022.102621&partnerID=40&md5=df0d1bfc15da296f609c07fd5f8ec014","Large trucking vehicles have a comparatively more significant impact on safety, traffic congestion, pollution, and pavement wear than passenger vehicles. Appropriate planning and operation of truck movement are necessary to reduce these impacts. While heavy truck movement has traditionally been measured through surveys, these remain limited because they are costly and time-consuming. In this study, we propose the use of large streams of GPS data to estimate truck origin–destination flows. Large streams of GPS data have typically been difficult to use as they lack descriptors for key events during a trip unless the data is accompanied by travel diaries. We address this problem by developing a heuristic-based approach to identify the key events, such as truck stops, trips, and other trucking activities. Then, a Pearson correlation coefficient and an entropy measure are applied to compare trucks’ mobility patterns and to determine whether changes in trucks travel patterns have occurred over one year. Finally, we use a multinomial logit structure to estimate destination choice models for five time periods. This research provides a strong case study of how GPS data can be used along with outputs of existing travel demand model (a model created with data collected using traditional techniques) to estimate origin–destination and destination choice models of truck movement in a provincial model setting. © 2022 The Author(s)","Big Data; Destination choice; GPS data; Multinomial logit; Truck origin–destination flow; Truck trip distribution",,Article,Scopus,2-s2.0-85124869815
"Bernot A., Siqueira Cassiano M.","57222523336;57209203029;","China's COVID-19 pandemic response: A first anniversary assessment",2022,"Journal of Contingencies and Crisis Management","30","1",,"10","21",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124506643&doi=10.1111%2f1468-5973.12396&partnerID=40&md5=aa78021ca9dca5d20b54557125be3728","The literature on crisis management reports that crises can be critical for organizations, including state and extra-state actors; they either break down or reinvent themselves. Successful organizations, those that do not break down, use situations of crisis to restructure themselves and improve their performance. Applicable to all crises, this reasoning is also valid for the COVID-19 pandemic and for government organizations in China. Drawing on documentary analysis, this article examines China's pandemic response from the social–political, technological and psychological perspectives using a holistic crisis management framework. It demonstrates that the Chinese state bureaucracy has assembled, expanded and strengthened its surveillance strategies to strive for comprehensive crisis response. © 2022 John Wiley & Sons Ltd","big data; China; COVID-19; crisis management; pandemic","assessment method; conceptual framework; COVID-19; crisis management; data set; pandemic; China",Article,Scopus,2-s2.0-85124506643
"Braverman M., Chassang S.","7006504757;23484163800;","Data-driven incentive alignment in capitation schemes",2022,"Journal of Public Economics","207",,"104584","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124479419&doi=10.1016%2fj.jpubeco.2021.104584&partnerID=40&md5=a26791f047577360227a33ff7bba44d6","This paper explores whether big data, taking the form of extensive high dimensional records, can reduce the cost of adverse selection by private insurers in government-run capitation schemes, such as Medicare Advantage. We argue that using data to improve the ex ante precision of capitation regressions is unlikely to be helpful. Even if types become essentially observable, the high dimensionality of covariates makes it infeasible to precisely estimate the cost of serving a given type: big data makes types observable, but not necessarily interpretable. This gives an informed private operator scope to select types that are relatively cheap to serve. Instead, we argue that data can be used to align incentives by forming unbiased and non-manipulable ex post estimates of a private operator's gains from selection. © 2022 Elsevier B.V.","Adverse selection; Big data; Capitation; Delegated model selection; Detail-free mechanism design; Health-care regulation",,Article,Scopus,2-s2.0-85124479419
"Saputra F.A., Salman M., Hasim J.A.N., Nadhori I.U., Ramli K.","56340452400;57202277218;57189246326;56572833800;55909526900;","The Next‐Generation NIDS Platform: Cloud‐Based Snort NIDS Using Containers and Big Data",2022,"Big Data and Cognitive Computing","6","1","19","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124392474&doi=10.3390%2fbdcc6010019&partnerID=40&md5=ac94a5dabaa571ad7b826e9f2996303a","Snort is a well‐known, signature‐based network intrusion detection system (NIDS). The Snort sensor must be placed within the same physical network, and the defense centers in the typical NIDS architecture offer limited network coverage, especially for remote networks with a restricted bandwidth and network policy. Additionally, the growing number of sensor instances, followed by a quick increase in log data volume, has caused the present system to face big data challenges. This research paper proposes a novel design for a cloud‐based Snort NIDS using containers and implementing big data in the defense center to overcome these problems. Our design consists of Docker as the sensor’s platform, Apache Kafka, as the distributed messaging system, and big data technology orchestrated on lambda architecture. We conducted experiments to measure sensor deploy-ment, optimum message delivery from the sensors to the defense center, aggregation speed, and efficiency in the data‐processing performance of the defense center. We successfully developed a cloud‐based Snort NIDS and found the optimum method for message‐delivery from the sensor to the defense center. We also succeeded in developing the dashboard and attack maps to display the attack statistics and visualize the attacks. Our first design is reported to implement the big data architecture, namely, lambda architecture, as the defense center and utilize rapid deployment of Snort NIDS using Docker technology as the network security monitoring platform. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; Cloud‐based IDS; Docker; Lambda architecture; Snort",,Article,Scopus,2-s2.0-85124392474
"Sreedevi A.G., Nitya Harshitha T., Sugumaran V., Shankar P.","57209650658;57444436400;57210215570;57200081677;","Application of cognitive computing in healthcare, cybersecurity, big data and IoT: A literature review",2022,"Information Processing and Management","59","2","102888","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124250178&doi=10.1016%2fj.ipm.2022.102888&partnerID=40&md5=cb95403b5d56208c522568f2f444e0f1","Human Intelligence is considered superior compared to Artificial Intelligence (AI) because of its ability to adapt faster to changes. Due to increasing data deluge, it is cumbersome for humans to analyse the vast amount of data and hence AI systems are in demand in today's world. However, these AI systems lack self-awareness, social skills, multitasking and faster adaptability. Cognitive Computing (CC), a subset of AI, acts as an effective solution in solving these challenges by serving as an important driver for knowledge-rich automation work. Knowing the latest research and state of the art in CC is one of the initial steps needed for researchers to make progress in this front. Thus, this paper presents a comprehensive survey of prior research in the CC domain along with the challenges, solutions and future research directions. Specifically, CC-based techniques solving real-world problems in four widely-researched application areas, namely, healthcare, cybersecurity, big data and IoT, have been reviewed in detail and the open research issues are discussed. © 2022 Elsevier Ltd","Artificial intelligence; Big data; Cognitive computing; Cybersecurity; Healthcare; IoT","Artificial intelligence; Big data; Cybersecurity; Internet of things; Artificial intelligence systems; Cognitive Computing; Computing domain; Cyber security; Effective solution; Human intelligence; Literature reviews; Self awareness; Social skills; State of the art; Health care",Article,Scopus,2-s2.0-85124250178
"Sun G., Zeng Q., Zhou J.-X.","57203962719;7401806648;31767769800;","Machine learning coupled with mineral geochemistry reveals the origin of ore deposits",2022,"Ore Geology Reviews","142",,"104753","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124244192&doi=10.1016%2fj.oregeorev.2022.104753&partnerID=40&md5=d60047c934d3cfa095e6663a8911214e","As geosciences enter the era of big data, machine learning (ML) that is successful in big data, is now contributing to solving problems in the geosciences, yet there have been few applications in economic geology. This paper highlights the effectiveness of ML-based methods coupled with mineral geochemistry in revealing the origin of the Qingchengzi Pb–Zn ore field in China, which are either metamorphosed sedimentary exhalative (SEDEX) or magmatic–hydrothermal fluid related deposits. Laser ablation–inductively coupled plasma–mass spectrometry (LA–ICP–MS) pyrite trace elements coupled with decision tree (DT), K-nearest neighbors (KNN), and support vector machine (SVM) algorithms were applied to train the classification models. Testing of the DT, KNN, and SVM classifiers yielded accuracies of 98.2%, 96.4%, and 93.6%, respectively. The trained classifiers predict that the strata-bound and vein-type ore bodies at Qingchengzi ore field have a magmatic–hydrothermal origin, with DT, KNN, and SVM values of 100%, 97.4%, and 97.4%. In situ δ34S values of pyrite from strata-bound and vein-type ore bodies are 4.04‰ to 9.10‰ and 6.31‰ to 9.29‰, respectively, slightly higher than those of magmatic intrusions. In situ Pb isotopic ratios plot on the upper crust curve and yield two-stage model ages that are younger than metamorphic events in the region. Principal component (PC) analysis was used to determine the formation of the two types of mineralization. Pyrite from vein-type ore bodies (Py1) has lower contents of PC1 elements (Cu, Zn, Ge, Ag, Cd, Sn, Sb, and Pb) and higher contents of PC2 elements (Co, Ni, and Se) compared with pyrite from strata-bound ore bodies (Py2). Combined with previous fluid inclusion data, the vein-type ore bodies are inferred to have formed at higher temperatures than the strata-bound ore bodies. This study presents three visual classifiers to discriminate metamorphosed SEDEX and magmatic–hydrothermal Pb–Zn deposits. The prediction of classifiers and in situ S–Pb isotopic compositions suggest that the Qingchengzi Pb–Zn deposits have a magmatic–hydrothermal origin. The results demonstrate the effective application of ML-based methods to examine the origin of ore deposits. © 2022","In situ S–Pb isotopes; Machine learning; Pb–Zn deposits; Pyrite trace elements; Qingchengzi","Antimony compounds; Big data; Copper compounds; Data mining; Deposits; Germanium compounds; Isotopes; Laser ablation; Lead compounds; Mass spectrometry; Mineralogy; Nearest neighbor search; Principal component analysis; Selenium compounds; Sulfur compounds; Support vector machines; Tin compounds; Trace elements; Zinc compounds; Geosciences; In situ S–pb isotope; Nearest-neighbour; Orebodies; Origin of ore deposits; Pb isotopes; Pb-zn deposits; Pyrite trace element; Qingchengzi; Traces elements; Decision trees",Article,Scopus,2-s2.0-85124244192
"Blanco C., García-Saiz D., Rosado D.G., Santos-Olmo A., Peral J., Maté A., Trujillo J., Fernández-Medina E.","57213021063;55027407200;15048635400;36609220500;8979711300;42961909600;7103051196;6508031693;","Security policies by design in NoSQL document databases",2022,"Journal of Information Security and Applications","65",,"103120","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123897974&doi=10.1016%2fj.jisa.2022.103120&partnerID=40&md5=fb54c4f898d4384870e002ab07a4715f","The importance of data security is currently increasing owing to the number of data transactions that are continuously taking place. Large amounts of data are generated, stored, modified and transferred every second, signifying that databases require an appropriate capacity, control and protection that will enable them to maintain a secure environment for so much data. Big Data is becoming a prominent trend in our society, and increasing amounts of data, including sensitive and personal information, are being loaded into NoSQL and other Big Data technologies for analysis and processing. However, current security approaches do not take into account the special characteristics of these technologies, leaving sensitive and personal data unprotected and consequently risking considerable financial losses and brand damage. In this paper, we focus on NoSQL document databases and present a proposal for the design and implementation of security policies in this type of databases. We first follow the concept of security by design in order to propose a metamodel that allows the specification of both the structure and the security policies required for document databases. We also define an implementation model by analysing the implementation features provided by a specific NoSQL document database management system (MongoDB). Having obtained the design and implementation models, we follow the model-driven development philosophy and propose a set of transformation rules that allow the automatic generation of the final implementation of security policies. We additionally provide a technological solution in which the Eclipse Modelling Framework environment is employed in order to implement both the design metamodel (Emfatic) and the transformations (Epsilon, EGL). Finally, we apply the proposed framework to a case study carried out in the airport domain. This proposal, in addition to saving development time and costs, generates more robust solutions by considering security by design. This, therefore, abstracting the designer from both specific aspects of the target tool and having to choose the best strategies for the implementation of security policies. © 2022 The Authors","Big Data; Conceptual modelling; Document databases; NoSQL; Security","Airport security; Big data; Database systems; Losses; Conceptual model; Data transaction; Design and implementations; Document database; Implementation models; Meta model; NoSQL; Number of datum; Security; Security policy; Security systems",Article,Scopus,2-s2.0-85123897974
"Regona M., Yigitcanlar T., Xia B., Li R.Y.M.","57221192835;6505536041;57217409275;35189135600;","Artificial Intelligent Technologies for the Construction Industry: How Are They Perceived and Utilized in Australia?",2022,"Journal of Open Innovation: Technology, Market, and Complexity","8","1","16","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123804987&doi=10.3390%2fjoitmc8010016&partnerID=40&md5=7b011f13ce6528a010327d8d8f4bcc71","Artificial intelligence (AI) is a powerful technology that can be utilized throughout a construction project lifecycle. Transition to incorporate AI technologies in the construction industry has been delayed due to the lack of know-how and research. There is also a knowledge gap regarding how the public perceives AI technologies, their areas of application, prospects, and constraints in the construction industry. This study aims to explore AI technology adoption prospects and constraints in the Australian construction industry by analyzing social media data. This study adopted social media analytics, along with sentiment and content analyses of Twitter messages (n = 7906), as the methodological approach. The results revealed that: (a) robotics, internet-of-things, and machine learning are the most popular AI technologies in Australia; (b) Australian public sentiments toward AI are mostly positive, whilst some negative perceptions exist; (c) there are distinctive views on the opportunities and constraints of AI among the Australian states/territories; (d) timesaving, innovation, and digitalization are the most common AI prospects; and (e) project risk, security of data, and lack of capabilities are the most common AI constraints. This study is the first to explore AI technology adoption prospects and constraints in the Australian construction industry by analyzing social media data. The findings inform the construction industry on public perceptions and prospects and constraints of AI adoption. In addition, it advocates the search for finding the most efficient means to utilize AI technologies. The study helps public perceptions and prospects and constraints of AI adoption to be factored in construction industry technology adoption. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Automation; Big data; Digital twin; Industry 4.0; Machine learning; Robotics; Social media analytics",,Article,Scopus,2-s2.0-85123804987
"Di Modica G., Tomarchio O.","57209339075;6602163319;","A Hierarchical Hadoop Framework to Process Geo-Distributed Big Data",2022,"Big Data and Cognitive Computing","6","1","5","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123779737&doi=10.3390%2fbdcc6010005&partnerID=40&md5=162fd68ba8cb24180cbaab1f2c9c60ad","In the past twenty years, we have witnessed an unprecedented production of data world-wide that has generated a growing demand for computing resources and has stimulated the design of computing paradigms and software tools to efficiently and quickly obtain insights on such a Big Data. State-of-the-art parallel computing techniques such as the MapReduce guarantee high performance in scenarios where involved computing nodes are equally sized and clustered via broadband network links, and the data are co-located with the cluster of nodes. Unfortunately, the mentioned techniques have proven ineffective in geographically distributed scenarios, i.e., computing contexts where nodes and data are geographically distributed across multiple distant data centers. In the literature, researchers have proposed variants of the MapReduce paradigm that obtain awareness of the constraints imposed in those scenarios (such as the imbalance of nodes computing power and of interconnecting links) to enforce smart task scheduling strategies. We have designed a hierarchical computing framework in which a context-aware scheduler orchestrates computing tasks that leverage the potential of the vanilla Hadoop framework within each data center taking part in the computation. In this work, after presenting the features of the developed framework, we advocate the opportunity of fragmenting the data in a smart way so that the scheduler produces a fairer distribution of the workload among the computing tasks. To prove the concept, we implemented a software prototype of the framework and ran several experiments on a small-scale testbed. Test results are discussed in the last part of the paper. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; Geographical computing environment; Hierarchical Hadoop; Job scheduling; MapReduce",,Article,Scopus,2-s2.0-85123779737
"jiao P.","57428899100;","Research on electronic decision system for effective data visualization and analysis process",2022,"Computers and Electrical Engineering","98",,"107737","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123628045&doi=10.1016%2fj.compeleceng.2022.107737&partnerID=40&md5=93f4ad252ed5fe707e4f28d79b2c8b63","Today, more leaders, managers, and directors can decide, even though they cannot decide for every situation. When decisions are scattered and unstructured, decisions become complicated and so that a single decision-maker based on expertise alone cannot address them adequately. This research proposes the Electronic Decision System (EDS) for effective data visualization and analysis process. EDS helps to decide during a complicated situation using Big Data Analytics-based Artificial Neural Network Framework (BDA-ANNF). The BDA-ANNF collects and analyses the data provided, and Artificial Neural Network (ANN) is used to predict and take accurate decisions in every complicated situation. These systems allow decision-makers to understand the market, direct and indirect climate, analyze the numerous trade-offs in the sector, evaluate costs linked to the various choices for decision-making, and eventually help people improve their decision-making outcome. EDS's approaches and techniques share one key challenge that must successfully provide consumers and policymakers with an appropriate and understandable simulation solution. The finding indicates that the BDA-ANNF decides the complicated situation up to the accuracy rate of 95.23%, which is not attainable by existing methods, and the error rate is 4.77%. © 2022","Analysis process; Artificial neural network; Big data analytics; Data visualization; Decision making; Effective data; Electronic decision system; Leaders; Managers","Behavioral research; Big data; Commerce; Data Analytics; Data visualization; Economic and social effects; Managers; Neural networks; Visualization; Analysis process; Data-visualization process; Decision makers; Decision systems; Decisions makings; Effective data; Electronic decision system; Leader; Network frameworks; Visualization and analysis; Decision making",Article,Scopus,2-s2.0-85123628045
"Jiang R., Xin Y., Chen Z., Zhang Y.","57200225926;57213820458;56099274300;55684119900;","A medical big data access control model based on fuzzy trust prediction and regression analysis",2022,"Applied Soft Computing","117",,"108423","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123609408&doi=10.1016%2fj.asoc.2022.108423&partnerID=40&md5=25cfa434a54350b2f0a6cc43b6ccb562","One of the important issues facing HIS (Hospital Information System) in the context of big data is how to ensure that massive data and resources are protected from internal attacks and reduce the abuse of medical information. However, the existing single-value quantitative access control model based on trust or risk may not well reflect the true trust or risk situation because it cannot describe the timeliness and trend of the quantitative value. In response to this problem, we propose an access control model based on the credibility of the requesting user. Quantify the trust based on the user's historical visit records on the HIS, and introduce the user's historical behavior trend into the trust evaluation model through the corresponding regression analysis model. Comparative experiments show that in predicting linear, geometric, exponential, and mixed trends, the regression model in this paper is better than existing methods in predicting trust accuracy and predicting trust trends. Different from the working system of trusted access control model proposed in the existing literature, the model in this paper adds “Behavior warning module (BWM)”. The working mode that “User-visit, Early-warning, Trust-evaluation, Access-control” is very effective in reducing information leakage caused by visitors with non-profit purposes (such as curiosity) and purposeless (such as habitual browsing). And this also has a positive effect on improving the overall behavior level of users in the system. © 2022","Access control; Medical big data; Regression analysis; Trust evaluation; Trust prediction","Access control models; Behavioral research; Big data; Forecasting; Medical information systems; Access control models; Data access control; Fuzzy trusts; Hospital information systems; Internal attacks; Massive data; Medical big data; Model-based OPC; Trust evaluation; Trust predictions; Regression analysis",Article,Scopus,2-s2.0-85123609408
"Malik P.K., Singh R., Gehlot A., Akram S.V., Kumar Das P.","55137130500;55613230383;56613481200;57222756774;57222493459;","Village 4.0: Digitalization of village with smart internet of things technologies",2022,"Computers and Industrial Engineering","165",,"107938","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123425205&doi=10.1016%2fj.cie.2022.107938&partnerID=40&md5=af61bf89bec72e288c4037ff0a41dd68","The United Nations (UN) 2030 Agenda makes it clear that growth and sustainable management are not confined to cities, but also to those living in rural and villages. In addition, the villages are the heart of every nation, the villages not only support and maintain the geological ecosystem but also have a great impact on the economic and social ecosystem. Currently, digital technologies have had a big influence on smart cities in terms of digitalization, and with the same motivation, these technologies may also help to build the digital and smart village. In this study, we present a detailed discussion of the implementation of smart and digital villages with different new digital technologies. In addition, we also present the possible enhancements in the village with the digitized village concept. From the discussion on the smart village, it is concluded that digitization is only possible if a reliable and robust network and communication infrastructure is installed in the village area. © 2022","Artificial Intelligence; Big data; Blockchain; Digitalization; Smart ecosystem; Village","Big data; Blockchain; Ecosystems; Internet of things; Block-chain; Digital technologies; Digitalization; Growth management; Internet of things technologies; Management IS; Smart ecosystem; Social ecosystems; Sustainable management; United Nations; Rural areas",Article,Scopus,2-s2.0-85123425205
"Bai J., Hager W.W., Zhang H.","56337105900;7202227743;57196154282;","An inexact accelerated stochastic ADMM for separable convex optimization",2022,"Computational Optimization and Applications","81","2",,"479","518",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122854000&doi=10.1007%2fs10589-021-00338-8&partnerID=40&md5=6ad30beec702185134539ec9b218a9a4","An inexact accelerated stochastic Alternating Direction Method of Multipliers (AS-ADMM) scheme is developed for solving structured separable convex optimization problems with linear constraints. The objective function is the sum of a possibly nonsmooth convex function and a smooth function which is an average of many component convex functions. Problems having this structure often arise in machine learning and data mining applications. AS-ADMM combines the ideas of both ADMM and the stochastic gradient methods using variance reduction techniques. One of the ADMM subproblems employs a linearization technique while a similar linearization could be introduced for the other subproblem. For a specified choice of the algorithm parameters, it is shown that the objective error and the constraint violation are O(1 / k) relative to the number of outer iterations k. Under a strong convexity assumption, the expected iterate error converges to zero linearly. A linearized variant of AS-ADMM and incremental sampling strategies are also discussed. Numerical experiments with both stochastic and deterministic ADMM algorithms show that AS-ADMM can be particularly effective for structured optimization arising in big data applications. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Accelerated gradient method; Accelerated stochastic ADMM; AS-ADMM; Big data; Complexity; Convex optimization; Inexact stochastic ADMM; Separable structure","Big data; Convex optimization; Data mining; Functions; Gradient methods; Linearization; Accelerated gradient method; Accelerated stochastic ADMM; Accelerated stochastic alternating direction method of multiplier; Alternating directions method of multipliers; Complexity; Convex optimisation; Gradient's methods; Inexact stochastic ADMM; Separable structure; Stochastics; Stochastic systems",Article,Scopus,2-s2.0-85122854000
"Shen Y., Song Z., Kusiak A.","57222014024;35321474100;26643635700;","Enhancing the generalizability of predictive models with synergy of data and physics",2022,"Measurement Science and Technology","33","3","034002","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122626087&doi=10.1088%2f1361-6501%2fac3944&partnerID=40&md5=638fd6fc809182a942c0d6ebf3745e0d","Wind farms require predictive models for predictive maintenance. There is a need to predict values of non-observable parameters beyond ranges reflected in available data. A predictive model developed for one machine many not perform well for another similar machine. This is usually due to a lack of generalizability of data-driven models. To increase the generalizability of predictive models, this research integrates data mining with first-principle knowledge. Physics-based principles are combined with machine learning algorithms through feature engineering, strong rules and divide-and-conquer. The proposed synergy concept is illustrated with a wind turbine blade icing prediction and achieves significant predictive accuracy across different turbines. The proposed process should be widely accepted by wind energy predictive maintenance practitioners because of its simplicity and efficiency. Furthermore, the testing scores of KNN, CART and DNN algorithms are increased by 44.78%, 32.72% and 9.13%, respectively, with our proposed process. We demonstrate the importance of embedding physical principles within the machine learning process, and also highlight an important point that the need for more complex machine learning algorithms in industrial big data mining is often much less than it is in other applications, making it essential to incorporate physics and follow the 'less is more' philosophy. © 2021 IOP Publishing Ltd.","industrial big data; machine learning; physical principle; predictive model; process engineering; wind turbine blade icing","Big data; Data mining; Learning algorithms; Machine learning; Turbine components; Wind power; Wind turbines; Industrial big data; Machine learning algorithms; Non-observable; Observable parameters; Physical principles; Predictive maintenance; Predictive models; Wind farm; Wind turbine blade icing; Wind turbine blades; Turbomachine blades",Article,Scopus,2-s2.0-85122626087
"Sandhu A.K.","57407603600;","Big Data with Cloud Computing: Discussions and Challenges",2022,"Big Data Mining and Analytics","5","1",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122612400&doi=10.26599%2fBDMA.2021.9020016&partnerID=40&md5=a0a03259e5e0f7f27ee4ee118d31f985","With the recent advancements in computer technologies, the amount of data available is increasing day by day. However, excessive amounts of data create great challenges for users. Meanwhile, cloud computing services provide a powerful environment to store large volumes of data. They eliminate various requirements, such as dedicated space and maintenance of expensive computer hardware and software. Handling big data is a time-consuming task that requires large computational clusters to ensure successful data storage and processing. In this work, the definition, classification, and characteristics of big data are discussed, along with various cloud services, such as Microsoft Azure, Google Cloud, Amazon Web Services, International Business Machine cloud, Hortonworks, and MapR. A comparative analysis of various cloud-based big data frameworks is also performed. Various research challenges are defined in terms of distributed database storage, data security, heterogeneity, and data visualization. © The author(s) 2022.","big data; cloud computing; data analysis; Hadoop","Cloud computing; Cloud data security; Data visualization; Digital storage; Distributed database systems; Web services; Windows operating system; Cloud computing services; Cloud services; Cloud-computing; Computational cluster; Computer technology; Data storage; Hadoop; Hardware and software; Large volumes; Time-consuming tasks; Big data",Article,Scopus,2-s2.0-85122612400
"Li F., Yu X., Ge R., Wang Y., Cui Y., Zhou H.","13408731100;57214826350;57208129083;57218680963;57222631836;23062556900;","BCSE: Blockchain-based trusted service evaluation model over big data",2022,"Big Data Mining and Analytics","5","1",,"1","14",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122609903&doi=10.26599%2fBDMA.2020.9020028&partnerID=40&md5=b0febc9a3ceaab8fbc30ffcd1e8296f1","The blockchain, with its key characteristics of decentralization, persistence, anonymity, and auditability, has become a solution to overcome the overdependence and lack of trust for a traditional public key infrastructure on third-party institutions. Because of these characteristics, the blockchain is suitable for solving certain open problems in the service-oriented social network, where the unreliability of submitted reviews of service vendors can cause serious security problems. To solve the unreliability problems of submitted reviews, this paper first proposes a blockchain-based identity authentication scheme and a new trusted service evaluation model by introducing the scheme into a service evaluation model. The new trusted service evaluation model consists of the blockchain-based identity authentication scheme, evaluation submission module, and evaluation publicity module. In the proposed evaluation model, only users who have successfully been authenticated can submit reviews to service vendors. The registration and authentication records of users' identity and the reviews for service vendors are all stored in the blockchain network. The security analysis shows that this model can ensure the credibility of users' reviews for service vendors, and other users can obtain credible reviews of service vendors via the review publicity module. The experimental results also show that the proposed model has a lower review submission delay than other models. © 2018 Tsinghua University Press.","blockchain; identity authentication; social network; trusted service evaluation model","Authentication; Big data; Public key cryptography; Social networking (online); Authentication scheme; Block-chain; Decentralisation; Evaluation models; Identity authentication; Key characteristics; Public key infrastructure; Service evaluation; Social network; Trusted service evaluation model; Blockchain",Article,Scopus,2-s2.0-85122609903
"Lai X., Zhang S., Mao N., Liu J., Chen Q.","49861520200;57211147871;57404893800;55850202300;57404893900;","Kansei engineering for new energy vehicle exterior design: An internet big data mining approach",2022,"Computers and Industrial Engineering","165",,"107913","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122539330&doi=10.1016%2fj.cie.2021.107913&partnerID=40&md5=5eb1cdde636d5540f517cd582e14e9c5","New energy vehicles (NEVs) such as electronic cars represent a major trend in the automobile industry, where most their exterior designs still follow those of convention fuelled vehicles (FVs). It is important to investigate whether NEV users have unique requirements that differ from those of traditional users. Kansei engineering is a practical tool for perceptual demand analysis. However, the conventional method requires questionnaires or surveys to perform limited data collection. In this study, we utilised massive internet data to collect user Kansei requirements for NEV exterior design. The Scrapy crawler was adopted for data collection and a bidirectional long short-term memory, conditional random field, and multilayer perceptron framework was developed for text mining. To quantify design features and Kansei image scores, a hybrid Apriori + structural equation model (SEM) system is proposed, where the data-driven Apriori algorithm can explore the hidden relationships in big user generated comments, while the SEM model captures the users’ behaviour and decision procedure so that to provide interpretable results. In addition, the association rules mined from user comments by Apriori can facilitate the specification of a complicated SEM model, substantially reducing the modelling and calibration effort. Goodness-of-fit results suggest that the proposed model outperforms conventional models. A case study on 1805 automobiles, 287 brands, and 369105 comments was conducted and the results suggest that some design features that would increase the Kansei image scores for conventional FVs may have the opposite effect on NEVs. Discussions on engineering and managerial insights are presented and the discovered rules and relationships are employed to develop a design-aided system. © 2021 Elsevier Ltd","Automobile exterior; Deep learning; Kansei engineering; Structural equation model; Text mining","Automobiles; Automotive industry; Big data; Data mining; Deep learning; Surveys; Apriori; Automobile exterior; Data collection; Deep learning; Design features; Exterior designs; Kansei Engineering; Kansei images; New energy vehicles; Structural equation models; Data acquisition",Article,Scopus,2-s2.0-85122539330
"Wang W., Guo H., Li X., Tang S., Xia J., Lv Z.","56828395700;57403680100;56829171600;56699170500;42561913500;55925162500;","Deep learning for assessment of environmental satisfaction using BIM big data in energy efficient building digital twins",2022,"Sustainable Energy Technologies and Assessments","50",,"101897","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122466729&doi=10.1016%2fj.seta.2021.101897&partnerID=40&md5=865b7415f5d32a8a6cecf98380800c0a","Energy efficient Building Digital Twins (BDTs) are researched using Building Information Model (BIM) to explore the key techniques of Digital Twins (DTs). DTs in buildings can be regarded as an expression of “BIM+,” born to digital descriptions. Comprehensive perception of physical systems is the preconditions for DTs implementation. BIM's energy-saving design includes the selection of building orientation and building shape. BIM energy consumption analysis can compare different materials, examine the performance of various materials, and select the most suitable and most energy-efficient materials for building structure maintenance. Data Fusion Algorithm (DFA) in Wireless Sensor Networks (WSNs) is improved. A novel DFA is constructed by combining Backpropagation Neural Network (BPNN) with Dynamic Host Configuration Protocol (DCHP), recorded as BP-DCHP. Simulation experiment proves that BP-DCHP can prolong sensor nodes’ survival time and provide the highest data fusion quality. BP-DCHP runs for about 310 s, 500 s, and 705 s in WSNs consisting of 20, 50, and 100 WSNs, respectively. Moreover, BP-DCHP can provide higher quality given insufficient data fusion degree. Once the WSNs consume 50% of the total initial energy, BP-DCHP presents a shorter network delay, only 0.6 s on average in the 100-sensor-node-WSN. To validate BDTs’ effectiveness, the environmental satisfaction of residents from two Beijing intelligent communities is assessed using Deep Learning (DL) approach. Taking the data as the clue, the study establishes DTs serving the application of urban scene, which plays a certain role in promoting the technological innovation of BDTs, better optimizing the city and managing the city. © 2021 Elsevier Ltd","Assessment of environmental satisfaction; BIM big data; Building digital twins; Deep learning; Energy efficient building","Architectural design; Data fusion; Deep learning; E-learning; Energy efficiency; Energy utilization; Neural networks; Sensor nodes; Assessment of environmental satisfaction; Building digital twin; Building information model big data; Building Information Modelling; Data fusion algorithm; Deep learning; Dynamic host configuration protocols; Energy efficient building; In-buildings; Protocol cans; Big data; artificial neural network; building; comparative study; computer simulation; energy efficiency; environmental assessment; experimental study; information system; numerical model; technological development; urban area; Beijing [Beijing (ADS)]; Beijing [China]; China",Article,Scopus,2-s2.0-85122466729
"Wu H., Luo X., Zhou M., Rawa M.J., Sedraoui K., Albeshri A.","57212467482;36460171200;7403506743;55290678700;6507325355;36617092600;","A PID-incorporated Latent Factorization of Tensors Approach to Dynamically Weighted Directed Network Analysis",2022,"IEEE/CAA Journal of Automatica Sinica","9","3",,"533","546",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122361420&doi=10.1109%2fJAS.2021.1004308&partnerID=40&md5=4e05e5b1a8965eb5558f2c8fd5bd5591","A large-scale dynamically weighted directed network (DWDN) involving numerous entities and massive dynamic interaction is an essential data source in many big-data-related applications, like in a terminal interaction pattern analysis system (TIPAS). It can be represented by a high-dimensional and incomplete (HDI) tensor whose entries are mostly unknown. Yet such an HDI tensor contains a wealth knowledge regarding various desired patterns like potential links in a DWDN. A latent factorization-of-tensors (LFT) model proves to be highly efficient in extracting such knowledge from an HDI tensor, which is commonly achieved via a stochastic gradient descent (SGD) solver. However, an SGD-based LFT model suffers from slow convergence that impairs its efficiency on large-scale DWDNs. To address this issue, this work proposes a proportional-integral-derivative (PID)-incorporated LFT model. It constructs an adjusted instance error based on the PID control principle, and then substitutes it into an SGD solver to improve the convergence rate. Empirical studies on two DWDNs generated by a real TIPAS show that compared with state-of-the-art models, the proposed model achieves significant efficiency gain as well as highly competitive prediction accuracy when handling the task of missing link prediction for a given DWDN. © 2014 Chinese Association of Automation.","Big data; high dimensional and incomplete (HDI) tensor; latent factorization-of-tensors (LFT); machine learning; missing data; optimization; proportional-integral-derivative (PID) controller","Artificial intelligence; Big data; Data mining; Efficiency; Factorization; Learning systems; Optimization; Proportional control systems; Stochastic models; Stochastic systems; Three term control systems; Two term control systems; Directed network; High dimensional and incomplete tensor; High-dimensional; Higher-dimensional; Latent factorization-of-tensor; Missing data; Optimisations; Proportional-integral-derivatives controllers; Stochastic gradient descent; Tensor model; Tensors",Article,Scopus,2-s2.0-85122361420
"Rafiq R., Ahmed T., Yusuf Sarwar Uddin M.","57202333012;57219296934;57195871567;","Structural modeling of COVID-19 spread in relation to human mobility",2022,"Transportation Research Interdisciplinary Perspectives","13",,"100528","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122196955&doi=10.1016%2fj.trip.2021.100528&partnerID=40&md5=17ec86c06787029574fbbcfbe3e018b3","Human mobility is considered as one of the prominent non-pharmaceutical interventions to control the spread of the pandemic (positive effect from mobility to infection). Conversely, the spread of the pandemic triggered massive changes to people's daily schedules by limiting their movement (negative effect from infection to mobility). The purpose of this study is to investigate this bi-directional relationship between human mobility and COVID-19 spread across U.S. counties during the early phase of the pandemic when infection rates were stabilizing and activity-travel behavior reflected a fairly steady return to normal following the drastic changes observed during the pandemic's initial shock. In particular, we applied Structural Regression (SR) model to investigate a bi-directional relationship between COVID-19 infection rate and the degree of human mobility in a county in association with socio-demographic and location characteristics of that county, and state-wide COVID-19 policies. Combining U.S. county-level cross-sectional data from multiple sources, our model results suggested that during the study period, human mobility and infection rate in a county both influenced each other, but in an opposite direction. Metropolitan counties experienced higher infection and lower mobility than non-metropolitan counties in the early stage of the pandemic. Counties with highly infected neighboring counties and more external trips had a higher infection rate. During the study period, community mitigation strategies, such as stay at home order, emergency declaration, and non-essential business closure significantly reduced mobility whereas public mask mandate significantly reduced infection rates. The findings of this study will provide important insights to policy makers in understanding the two-way relationship between human mobility and COVID-19 spread and to derive mobility-driven policy actions accordingly. © 2021 The Author(s)","Big data; COVID-19 pandemic; COVID-19 policies; Human mobility; Infection rate; Latent factor; SEM; Spatial effect",,Article,Scopus,2-s2.0-85122196955
"Ma L., Yuan F., Yan X., Zhang J.","55976377000;57226567395;8889780700;57194192006;","Disentangling City-Level Macroscopic Traffic Performance Patterns through a Trigonometric Multiseasonal Filtering Algorithm: Inspiration from Big Data of Ride-Sourcing Trips",2022,"Journal of Transportation Engineering Part A: Systems","148","3","04021120","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122038804&doi=10.1061%2fJTEPBS.0000628&partnerID=40&md5=319cd928e68d0812710ecdec2bb8d628","This study seeks to design a city-level trip speed performance index (CTSPI) providing an alternative aspect in quantifying the traffic performance of an entire city. Another objective is to disentangle the original CTSPI time series into several featured patterns, including a trend pattern, two seasonality patterns, and a remainder pattern. The big data in the form of observations of ride-sourcing trips in Beijing, China were adopted. This study also introduces a state-space model, the TBATS [trigonometric, Box-Cox transformation, auto-regressive moving average (ARMA) errors, trend, and seasonal components] filtering procedure to decompose the CTSPI time series. This study adopts Beijing as a representative example because the city has very typical and complicated traffic performance patterns. The proposed CTSPI directly reflects the average trip speed, normalized by the best performance supplied by the corresponding infrastructure systems. After filtering out fluctuation, noise, and irregular patterns, it reveals a smooth and clear-cut trend in the evolving process of the city's traffic condition, which was never previously disclosed and is important in understanding the macroscopic long-term tendencies of the city's traffic performance. The results indicate that the CTSPI is capable of capturing the traffic performance of the city well and can sense the influence of special dates or major events, such as the Beijing 2022 Olympic Winter Games, advising tremendous application of traffic management. City-level macroscopic traffic performance is usually measured as index quantities and used to assess traffic situations in different cities. Most often, it is utilized to provide a quantified impression of the degree of congestion to the public, or as traffic-congestion criteria for ranking cities. This study illustrates the importance of measuring city-level macroscopic traffic performance, especially on a daily basis as is appropriate for gauging the impacts of many macroscopic factors on city-level traffic situations. © 2021 American Society of Civil Engineers.","Big data; City-level traffic performance; Ride-sourcing trip; Temporal pattern filtering","Signal filtering and prediction; State space methods; Time series; Traffic congestion; City traffic; City-level traffic performance; Performance indices; Performance patterns; Ride-sourcing trip; Speed performance; Temporal pattern; Temporal pattern filtering; Times series; Traffic performance; Big data; algorithm; flow pattern; Olympic Games; temporal variation; time series analysis; traffic congestion; traffic management; Beijing [Beijing (ADS)]; Beijing [China]; China",Article,Scopus,2-s2.0-85122038804
"Teng F., Teng J., Qiao L., Du S., Li T.","36134106600;57390612700;57390704300;57194798613;7406372548;","A multi-step forecasting model of online car-hailing demand",2022,"Information Sciences","587",,,"572","586",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121967524&doi=10.1016%2fj.ins.2021.12.044&partnerID=40&md5=11f7743c329498edc013941374288b76","Multi-step forecasting of online car-hailing demand is necessary for the long-term planning of traffic resources. However, accurate forecasting is very challenging, because it is difficult to extract the dynamic spatial-temporal dependencies from high-dimensional nonlinear data. This paper proposes a regional traffic demand forecasting framework (RTDFF) to predict the boarding and alighting demand of online car-hailing. The RTDFF can learn robust representations of traffic patterns with multi-resolution spatial dependencies. It uses a 3D deconvolution component to enhance the spatial resolution of intra-regions and a 2D residual component to connect the large-scale spatial dependence. This model investigates the effect of the 3D convolution in capturing the long-term temporal dependence. Hybrid modeling is conducted with multi-granularity temporal segmentation and fusion. The experiments were carried out using real-world datasets. The results verify that RTDFF is accurate and stable, and that it has slower deterioration than baseline models, with the raise of prediction steps. © 2021","3D deconvolution; Demand forecasting; Multi-step prediction; Spatio-temporal dependence; Urban big data","Big data; Deterioration; 3d deconvolution; Deconvolutions; Demand forecasting; Multi-step forecasting; Multi-step prediction; Spatio-temporal; Spatio-temporal dependence; Temporal dependence; Traffic demands; Urban big data; Forecasting",Article,Scopus,2-s2.0-85121967524
"Ushizima D., Chen Y., Alegro M., Ovando D., Eser R., Lee W., Poon K., Shankar A., Kantamneni N., Satrawada S., Junior E.A., Heinsen H., Tosun D., Grinberg L.T.","14827670900;57219812102;54996627500;57219808725;57200523802;57388758400;57388358100;57388518700;57388518800;57388916000;57193569606;7003971491;6507410776;14829091300;","Deep learning for Alzheimer's disease: Mapping large-scale histological tau protein for neuroimaging biomarker validation",2022,"NeuroImage","248",,"118790","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121930856&doi=10.1016%2fj.neuroimage.2021.118790&partnerID=40&md5=5b7116b5268ccb05466f1c79008c1e99","Abnormal tau inclusions are hallmarks of Alzheimer's disease and predictors of clinical decline. Several tau PET tracers are available for neurodegenerative disease research, opening avenues for molecular diagnosis in vivo. However, few have been approved for clinical use. Understanding the neurobiological basis of PET signal validation remains problematic because it requires a large-scale, voxel-to-voxel correlation between PET and (immuno) histological signals. Large dimensionality of whole human brains, tissue deformation impacting co-registration, and computing requirements to process terabytes of information preclude proper validation. We developed a computational pipeline to identify and segment particles of interest in billion-pixel digital pathology images to generate quantitative, 3D density maps. The proposed convolutional neural network for immunohistochemistry samples, IHCNet, is at the pipeline's core. We have successfully processed and immunostained over 500 slides from two whole human brains with three phospho-tau antibodies (AT100, AT8, and MC1), spanning several terabytes of images. Our artificial neural network estimated tau inclusion from brain images, which performs with ROC AUC of 0.87, 0.85, and 0.91 for AT100, AT8, and MC1, respectively. Introspection studies further assessed the ability of our trained model to learn tau-related features. We present an end-to-end pipeline to create terabytes-large 3D tau inclusion density maps co-registered to MRI as a means to facilitate validation of PET tracers. © 2021 The Author(s)","Alzheimer's disease; Big data; Convolutional neural networks; Deep learning; Digital pathology; Histopathology; Imaging; Machine learning","tau protein; Alzheimer disease; area under the curve; Article; artificial neural network; convolutional neural network; deep learning; degenerative disease; histopathology; human; image analysis; immunohistochemistry; in vivo study; introspection; molecular diagnosis; neuroimaging; nonhuman; nuclear magnetic resonance imaging; positron emission tomography; protein targeting; quantitative analysis; receiver operating characteristic; semantics; validation process; x-ray computed tomography",Article,Scopus,2-s2.0-85121930856
"de Souza Neto J.B., Martins Moreira A., Vargas-Solar G., Musicante M.A.","57191826673;8345307700;14632835700;8586992500;","A two-level formal model for Big Data processing programs",2022,"Science of Computer Programming","215",,"102764","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121866492&doi=10.1016%2fj.scico.2021.102764&partnerID=40&md5=80155bc58be910531e966033dbb55387","This paper proposes a model for specifying data flow-based parallel data processing programs agnostic of target Big Data processing frameworks. The paper focuses on the formal abstract specification of non-iterative and iterative programs, generalizing the strategies adopted by data flow Big Data processing frameworks. The proposed model relies on Monoid Algebra and Petri Nets to abstract Big Data processing programs in two levels: a higher level representing the program data flow and a lower level representing data transformation operations (e.g., filtering, aggregation, join). We extend the model for data processing programs proposed in [1], for modeling iterative data processing programs. The general specification of these programs implemented by data flow-based parallel programming models is essential given the democratization of iterative and greedy Big Data analytics algorithms. Indeed, these algorithms call for revisiting parallel programming models to express iterations. The paper gives a comparative analysis of the iteration strategies proposed by Apache Spark, DryadLINQ, Apache Beam, and Apache Flink. It discusses how the model achieves to generalize these strategies. © 2021 Elsevier B.V.","Big Data processing; Data flow programming models; Monoid algebra; Petri nets","Algebra; Data Analytics; Data flow analysis; Data handling; Data transfer; Iterative methods; Metadata; Parallel programming; Petri nets; Specifications; Big data processing; Data flow programming model; Dataflow; Dataflow programming; Flow based; Formal modeling; Monoid algebra; Parallel data processing; Parallel-programming models; Programming models; Big data",Article,Scopus,2-s2.0-85121866492
"Morawska L., Miller W., Riley M., Vardoulakis S., Zhu Y.-G., Marks G.B., Garnawat P., Kumar P., Thynell M.","7004773965;37056087400;24333160700;16835481000;57218097296;7202971580;57203983605;7403961319;13605720700;","How to build Urbanome, the genome of the city?",2022,"Science of the Total Environment","810",,"152310","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121784204&doi=10.1016%2fj.scitotenv.2021.152310&partnerID=40&md5=eb5f50cd38c77b3116f47a44a34eb59c","The world's population is shifting to the cities, and consequently, cities worldwide are growing in number and in size. Cities are complex systems, making it extremely difficult to build and run cities in a way that all the elements of the system operate in harmony. Recently a concept of urbanome, the genome of the city was proposed to address this complexity. Here we first explore this concept and analogy, taking advantage of the potential of other ‘omics, modern data collection techniques, Big Data analysis methods and a transdisciplinary approach. Then, we propose a theoretical approach to build the urbanome as a means of quantifying and qualifying population outcomes, being a function of the form of an urban area including the built environment, the physical and social services it provides, and the population density. © 2021 Elsevier B.V.","Characterization of the city; City performance framework; City performance indicators; Future cities; Ranking of the cities; Urban system","Data acquisition; Population statistics; 'omics'; Characterization of the city; City performance framework; City performance indicator; Future city; Performance frameworks; Performance indicators; Ranking of the city; Urban systems; World population; Genes; complexity; conceptual framework; performance assessment; population density; urban development; urban pollution; urban system; urbanization; article; big data; built environment; city; human; performance indicator; population density; quantitative analysis; social work; urban area; city; city planning; Cities; City Planning; Population Density",Article,Scopus,2-s2.0-85121784204
"Mai Y., Izumi K., Sawada K., Akasaka E., Mai S., Sawamura D., Ihara K., Nakaji S., Nishie W.","55032627500;36461001900;57217828906;24068357900;57202385351;7006076487;57384720800;7005587112;55962803000;","A 1,035-Subject Study Suggesting a History of Bone Fracture as a Possible Factor Associated with the Development of Anti-BP180 Autoantibodies",2022,"Journal of Investigative Dermatology","142","3",,"984","987.e3",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121664760&doi=10.1016%2fj.jid.2021.11.028&partnerID=40&md5=3cfcbe549d2a8343b1eb0c4dcfbefd85",[No abstract available],,"anti bp180 autoantibody; antibiotic agent; antidiabetic agent; antihypertensive agent; autoantibody; bullous pemphigoid antigen 1; collagen type 7; desmoglein 1; desmoglein 3; nc16a protein; protein; unclassified drug; adult; aged; antiosteoporotic activity; Article; big data; blood analysis; clinical evaluation; comorbidity; disease association; enzyme linked immunosorbent assay; esophagus; female; fracture; human; immunofluorescence; Japanese (people); life expectancy; major clinical study; male; medical history; middle aged; osteoporosis; physical examination; prevalence; questionnaire; risk factor",Article,Scopus,2-s2.0-85121664760
"Bi Z., Zhang C.W.J., Wu C., Li L.","7102348549;57375537800;7405579058;55760654300;","New digital triad (DT-II) concept for lifecycle information integration of sustainable manufacturing systems",2022,"Journal of Industrial Information Integration","26",,"100316","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121397304&doi=10.1016%2fj.jii.2021.100316&partnerID=40&md5=d141ed797bd62ae4b2f29b5a1e2e0e38","A system paradigm is a typical pattern or model of enterprise architectures (EAs) that describes constitutive system elements and their relations in achieving missions and goals of enterprises. A well-defined system paradigm and EA help enterprises in adopting appropriate manufacturing resources and technologies, optimizing plans, schedules, and controls of production lines, and coping with complexity, changes, and uncertainties of business processes cost-effectively. It is understandable that a system paradigm should be evolved along with the availability and advancement of advanced manufacturing technologies; on the other hand, new system paradigms are mostly inspired by some successful manufacturing applications of new information technologies (ITs). One of the recently developed system paradigms is Digital Manufacturing (DM) where Digital Twin (DT-I) is used to describe the interactions of virtual and physical entities. This paper discusses the concepts of DM and DT-I and rationalizes the relations of DM and DT-I; in particular, the origin and evolution of DT-I are explored in details to identify its limitations to be used as an EA for DM. It is our finding that existing IT concepts show their limitations in supporting smooth transitions when systems have to be reconfigured in dealing with long-term changes in Sustainable Manufacturing, and this is evidenced by the fact of the trend of the shortened lifespans of modern enterprises even with the aids of rapidly developed digital technologies in decades. To overcome these limitations, a new concept so called Digital Triad (DT-II) is coined and the Internet of Digital Triad Things (IoDTT) is proposed as an information integration (II) solution for digital manufacturing enterprises, and their application is illustrated through a reconfigurable robotic system example. The rationales and significances of DT-II and IoDTT as well as future research directions of DM are summarized as a conclusion. © 2021 Elsevier Inc.","Big data analytics (BDA); Blockchain technology (BCT); Cloud computing (CC); Cyber-physical system (CPS); Digital manufacturing (DM); Digital triad (DT-II); Digital twin (DT-I); Enterprise architecture (EA); Information integration (II); Internet of digital triad things (IoDTT); Internet of Things (IoT); Machine learning (ML)","Big data; Blockchain; Computer architecture; Data Analytics; E-learning; Embedded systems; Engineering education; Integration; Internet of things; Life cycle; Machine learning; Manufacture; Search engines; Big data analytic; Block-chain; Blockchain technology; Cloud computing; Cloud-computing; Cybe-physical system; Digital manufacturing; Digital triad (DT-information integration); Digital twin; Enterprise Architecture; Information integration; Internet of digital triad thing; Internet of thing; Machine learning; Information retrieval",Article,Scopus,2-s2.0-85121397304
"Choi Y., Nguyen H., Bui X.-N., Nguyen-Thoi T.","23484346300;57209589544;36892679300;24171745600;","Optimization of haulage-truck system performance for ore production in open-pit mines using big data and machine learning-based methods",2022,"Resources Policy","75",,"102522","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121289463&doi=10.1016%2fj.resourpol.2021.102522&partnerID=40&md5=3e527cb881db57d337a5ef1b1c45646d","Ore haulage systems are considered critical when evaluating the efficiency of the investment and design of open-pit mines. Smart mines are also adopted to increase mine production, and the, optimization of the production of equipment is necessary. Therefore, this study proposes an unsupervised intelligent system for predicting the performance of a truck-haulage system in the ore transportation process in open-pit mines using a combination of Harris hawks optimization (HHO) and support vector machine (SVM), named the HHO-SVM model. Different kernel functions were investigated with the proposed HHO–SVM model, including radial basis, polynomial, and linear functions. Random forest (RF) and back-propagation neural network (BPNN) models were also developed and compared with the proposed model. To demonstrate the performance in practice, 16005 datasets of a truck-haulage system was collected, and the downscaling method was applied to downscale the size of the dataset into 3000 observations, aiming to improve the computing cost of the models. The results revealed that the BPNN, RF, SVM (without optimization), and HHO–SVM models are potential intelligent models for predicting ore production. The comparisons between the models indicated that the radial basis function was the best fit of the HHO-SVM model in predicting ore production with a root-mean-squared error (RMSE) of 197.213, determination coefficient (R2) of 0.991, and mean absolute error (MAE) of 154.256. Meanwhile, the polynomial function achieved lower performance with an RMSE of 275.427, R2 of 0.982, and MAE of 205.460; the linear function achieved the lowest performance overall with an RMSE of 844.111, R2 of 0.841, and MAE of 595.173. Similar results were also obtained in practice through the validation datasets, with an accuracy in the range of 98–99% for the proposed HHO-SVM model with the radial basis function. However, accuracies in the range of only 84–85% for the linear function and 97–98% for the polynomial function were achieved. © 2021 Elsevier Ltd","Artificial intelligence; Harris hawks optimization; HHO-SVM; Ore production; Support-vector-machine; Truck-haulage system","Backpropagation; Big data; Decision trees; Forecasting; Functions; Intelligent systems; Mean square error; Mine trucks; Open pit mining; Ores; Polynomials; Support vector regression; Harris hawk optimization; Harris hawk optimization-support vector machine; Haulage system; Optimisations; Ore production; Support vectors machine; Truck haulage; Truck-haulage system; Radial basis function networks; artificial intelligence; computer simulation; machine learning; model validation; open pit mine; optimization; ore deposit; ore mineral; support vector machine",Article,Scopus,2-s2.0-85121289463
"Pascucci F., Nardi L., Marinelli L., Paolanti M., Frontoni E., Gregori G.L.","57192997985;57226141525;57217121390;57188734162;9737451300;57220512878;","Combining sell-out data with shopper behaviour data for category performance measurement: The role of category conversion power",2022,"Journal of Retailing and Consumer Services","65",,"102880","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121285888&doi=10.1016%2fj.jretconser.2021.102880&partnerID=40&md5=9326a383e8ecca7ef7cb0fde3e2a06f7","Retailers need to manage a series of complex decisions relating to numerous products. To reduce this complexity, they have introduced category management practices, which consider groups of similar products (categories) that can be managed separately as single business units (SBUs). Although the concept that the store offer should be organised as a category mix and that this strategy allows for better overall store management is already consolidated, retailers still struggle to adopt an approach to the store performance measurement starting from a category level perspective. Nowadays, the available methods for measuring categories’ performance are quite limited. The current trend sees the measurement of category performance mainly based on sell-out data that are ill-equipped to fully address category management issues. Retailers should broaden their field of analysis not only by focusing on the product/sales perspective but also by including other methodologies such as shopper behaviour analysis. In this regard, the use of technology offers the retail sector new perspectives for those analysis. Therefore, we intend to contribute to the ongoing debate on the retail analytics topic by presenting a shopper behaviour analytics system for category management performance monitoring. More in detail, we could derive a new key performance indicator, category conversion power (CCP), aimed at analysing and comparing the single categories organised within the store. The research is based on a unique dataset obtained from a real-time locating system (RTLS), which allowed us to collect behavioural data togheter with sell-out data (from POS scanner). We argue that retailers could exploit this new analytical method to gain more understanding at the category level and therefore make data-driven decisions aimed at improving performance at the store level. © 2021 Elsevier Ltd","Big data; Category management; Performance measurement; Retail marketing; RTLS technology; Shopper behaviour","data set; industrial performance; management practice; marketing; research; retailing; shopping activity",Article,Scopus,2-s2.0-85121285888
"Ji C., Hong T., Kim H.","36537402100;7202830574;57199153903;","Statistical analysis of greenhouse gas emissions of South Korean residential buildings",2022,"Renewable and Sustainable Energy Reviews","156",,"111981","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120620114&doi=10.1016%2fj.rser.2021.111981&partnerID=40&md5=68243fd1bda47228fd0e38441c1768d1","This study analyzed the status of greenhouse gas (GHG) emissions of residential buildings in South Korea from 2015 to 2019, based on energy consumption data for all residential buildings in the country from the Korea National Building Energy Database. The analysis revealed that new residential buildings had a lower GHG emission intensity than old residential buildings and that strengthening the Building Energy Conservation Code (BECC) contributes to the reduction of GHG emissions from heating energy use in residential buildings. The GHG emission intensity for heating energy use of apartments to which the latest standard (0.21 W/m2·K U-value of exterior walls) was applied was 35.8% lower than that of apartments to which the insulation standard was not applied. However, despite the strengthening of the BECC, the GHG emission intensity for baseload and cooling energy use of apartments increased. This means that the BECC does not contribute to reducing GHG emissions from baseload and cooling energy use. Moreover, as the number of residential buildings continues to increase, their consolidated GHG emissions also increase. Since heating energy consumption is significantly influenced by the weather, GHG emissions from heating energy use can vary depending on the weather. For instance, GHG emissions from heating energy use can increase in extremely cold winter despite the strengthening of the BECC. Therefore, a policy is needed for strengthening the BECC as well as for enhancing building occupants' understanding of GHG emissions reduction in buildings and for promoting the occupants’ behavior change. © 2021 Elsevier Ltd","Big data analysis; Building energy conservation Code; Greenhouse gas emission Intensity; Greenhouse gas emissions; Performance gap; Residential buildings","Big data; Codes (symbols); Emission control; Energy conservation; Energy utilization; Gas emissions; Greenhouse gases; Heating; Historic preservation; Big data analyse; Building energy conservation; Building energy conservation code; Emissions intensity; Energy use; Greenhouse gas emission intensity; Greenhouse gas emissions; Performance gaps; Residential building; Apartment houses",Article,Scopus,2-s2.0-85120620114
"Wang X., Pei T., Li K., Cen Y., Shi M., Zhuo X., Mao T.","57226873941;7004466382;57362783600;57362928200;57363658100;57363366200;57363222500;","Analysis of changes in population's cross-city travel patterns in the pre- and post-pandemic era: A case study of China",2022,"Cities","122",,"103472","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120615737&doi=10.1016%2fj.cities.2021.103472&partnerID=40&md5=7a69cd11decac10223d2d890b3a1e79c","The coronavirus disease (COVID-19) outbreak has immensely changed people's travel behaviour. The changes in travel behaviour have had a huge impact on different industries, such as consumption, entertainment, commerce, office, and education. This study investigates the impact of COVID-19 on population travel patterns from three aspects: total trips, travel recovery degree, and travel distance. The result indicates that COVID-19 has reduced the total number of cross-city trips and flexible non-work travel; in the post-pandemic era, cross-city travel is mainly short-distance (distance <100 km). This study has significant policymaking implications for governments in countries where the population shares a similar change in travel behaviour. © 2021 Elsevier Ltd","Big data; COVID-19; Crowd travel; Mobile signalling data; Travel recovery degree",,Article,Scopus,2-s2.0-85120615737
"Dokuz A.S.","46061138200;","Weighted spatio-temporal taxi trajectory big data mining for regional traffic estimation",2022,"Physica A: Statistical Mechanics and its Applications","589",,"126645","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120491109&doi=10.1016%2fj.physa.2021.126645&partnerID=40&md5=f7a75ad8267f4a22cd6d03ec3035170b","The estimation of traffic conditions in cities is becoming essential to establish a sustainable transportation system and to help traffic management authorities plan the traffic of cities. Recently, taxi trajectory big datasets are being collected during taxi drivers are routing around the cities. Taxi trajectory datasets provide behavioral information about the city residents, urban flows of the taxi passengers, and infrastructure for traffic condition estimation. This study aims to estimate regional traffic velocity of New York City using New York taxi trajectory dataset. A new method is proposed that uses weighted spatio-temporal trajectory big data mining approach and scores each region of the cities in terms of traffic velocity. A new algorithm is proposed, namely Regional Traffic Velocity Estimation (RTVE) algorithm, which uses proposed regional spatio-temporal velocity estimation method and experimentally evaluated using New York taxi trajectory dataset. Experimental results show that each region in New York have different velocity and usage characteristics in terms of hourly and daily analyses. Also, borough-level analyses are performed that reveal knowledge about the boroughs of New York. The estimated regional traffic velocity of cities based on taxi trajectory datasets would provide a decision support system for decision-makers in terms of regional hourly and daily evaluation of cities with cost-free and widespread city traffic dataset. © 2021 Elsevier B.V.","Big data mining; Regional traffic condition monitoring; Regional traffic velocity estimation; Taxi trajectory dataset; Weighted spatio-temporal pattern mining","Artificial intelligence; Big data; Condition monitoring; Data mining; Decision making; Decision support systems; Taxicabs; Velocity; Big data mining; New York; Regional traffic condition monitoring; Regional traffic velocity estimation; Spatiotemporal patterns; Taxi trajectory dataset; Temporal pattern minings; Traffic conditions; Velocity estimation; Weighted spatio-temporal pattern mining; Trajectories",Article,Scopus,2-s2.0-85120491109
"Clausen J.B.B., Li H.","57357760000;36961344300;","Big data driven order-up-to level model: Application of machine learning",2022,"Computers and Operations Research","139",,"105641","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120350278&doi=10.1016%2fj.cor.2021.105641&partnerID=40&md5=aa3f4458714f5060a9b63ea46c6dd2c5","Data driven optimisation has become one of the research frontiers in operations management and operations research. Likewise, the recent academic interest in big data has created a desire for big data driven operations research. A new data driven methodology, which employs the empirical risk minimisation (ERM) principle, has recently been introduced in the inventory management literature. It has been used to formulate data driven inventory models which can take multiple features into account and do not need classical distributional assumptions. However, the research on big data driven inventory models is currently confined to the newsvendor model. In this paper, we aim to generalise the previous results on the big data driven newsvendor model and to expand the research by solving a big data driven dynamic order-up-to level inventory model. We show how the ERM methodology is employed to formulate a big data driven order-up-to level inventory model and design a machine learning algorithm to solve the model. The performance of our big data driven inventory model and solution algorithm is demonstrated by an experimental study based on real business data. The numerical results show that our integrated big data driven model generates up to 60% cost savings compared to the best performing univariate benchmark model, and up to 6.37% cost savings compared to the best performing big data driven benchmark model. © 2021 The Authors","Big data; Data driven operations research; Empirical risk minimisation; Inventory models; Machine learning; Neural networks","Big data; Learning algorithms; Machine learning; Benchmark models; Cost saving; Data driven; Data driven operation research; Empirical risk minimization; Inventory modeling; Level model; Neural-networks; Newsvendor models; Operation research; Inventory control",Article,Scopus,2-s2.0-85120350278
"Taylor D., Abarno D.","23483131800;55538163200;","Using big data from probabilistic genotyping to solve crime",2022,"Forensic Science International: Genetics","57",,"102631","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120169046&doi=10.1016%2fj.fsigen.2021.102631&partnerID=40&md5=5cf1a30de958c93e6672105f91492190","Forensic Science South Australia (FSSA) has been using STRmix™ software to deconvolute all reported DNA mixtures since 2012. Almost a decade of deconvolutions had led to a substantial repository of analysed profile data that can be interrogated to observe trends in case type, location or occurrence. In addition, deconvolutions can be compared in order to identify common DNA donors and reveal new intelligence information in cases where DNA profiling has previously provided no investigative information. As a proof of concept all samples deconvoluted as part of criminal casework (suspect or no-suspect) were interrogated and compared to each other using the mixture-to-mixture comparison feature in STRmix™. Within the Adelaide region there were 32 groups of cases that had evidence samples linked by a common DNA donor with LR > 1 million which was in addition to direct links and mixture searching links identified previously. These groups of cases can then be interrogated to reveal additional information to inform Police intelligence gathering. Our paper reports on the findings of this proof-of-concept study. © 2021 Elsevier B.V.","Deconvolution; DNA mixtures; Mixture to mixture; Police intelligence; STRmix","adult; article; big data; clinical article; controlled study; crime; deconvolution; DNA fingerprinting; female; genotype; human; intelligence; male; police; proof of concept; social worker",Article,Scopus,2-s2.0-85120169046
"Yu H.-C., Qiu K.-F., Pirajno F., Zhang P.-C., Dong W.-Q.","57211294748;49662040600;7003614919;57346811500;57346328900;","Revisiting Phanerozoic evolution of the Qinling Orogen (East Tethys) with perspectives of detrital zircon",2022,"Gondwana Research","103",,,"426","444",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119439184&doi=10.1016%2fj.gr.2021.10.022&partnerID=40&md5=da61280c7a31e296731286854ab9eb7d","This paper presents 5383 detrital zircon U–Pb and 2079 Lu–Hf isotopic data, shedding new light on the Phanerozoic tectonic evolution of the Qinling Orogen. The Silurian rocks are characterized by negative εHf(t) values of early Paleozoic and Neoproterozoic zircon, which in the Devonian to Carboniferous samples show both positive and negative εHf(t) values. Late Paleozoic zircon peak with negative εHf(t) values occurs in the upper Carboniferous to Cretaceous strata. The Permian to Cretaceous strata show peaks at 1.9 Ga and 2.5 Ga. Early Paleozoic and Neoproterozoic peaks are back in Upper Triassic to Cretaceous rocks. A Mesozoic peak with negative εHf(t) values occurs in Cretaceous rocks. In terms of the sources of detrital zircon, the South Qinling Belt and Yangtze Block are major contributors for Silurian strata; South Qinling and North Qinling belts are major sources for Lower Devonian to upper Carboniferous rocks; early Carboniferous detrital zircon related to subduction of Mianlue oceanic slab emerges in upper Carboniferous rocks; North China Block is an important source for Lower Permian to Upper Triassic rocks; Lower Cretaceous strata exhibit diverse provenance, including North Qinling Belt, North China Block, and South Qinling Belt. Provenance changes suggest that the initial spreading of the Mianlue Ocean occurred in the Early Silurian during which the South Qinling Terrane and Yangtze Block had not completely separated. The Mianlue Ocean evolved into a mature stage of spreading in the Early Devonian. Meanwhile, the closure of the Shangdan Ocean took place. The Mianlue Ocean started to subduct in the early Carboniferous. Its final closure occurred in the Late Triassic, leading to uplift and exhumation of Qinling Orogen and cutting off detritus transport from North China Block in the Jurassic. The Qinling Orogen underwent collapse in the late Early Cretaceous. © 2021 International Association for Gondwana Research","Big data; Detrital zircon; Phanerozoic; Qinling Orogen; Tectonic evolution",,Article,Scopus,2-s2.0-85119439184
"Corradini C., Folmer E., Rebmann A.","56957930800;55908359500;56721231300;","Listening to the buzz: Exploring the link between firm creation and regional innovative atmosphere as reflected by social media",2022,"Environment and Planning A","54","2",,"347","369",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119274090&doi=10.1177%2f0308518X211056653&partnerID=40&md5=214875ef92598246d50d98621e9dea3d","This paper presents a novel approach to capture ‘buzz’, the vibrancy and knowledge exchange propensity of localised informal communication flows. Building on a conceptual framework based on relational economic geography, we argue the content of buzz may allow to probe into the character of places and investigate what is ‘in the air’ within regional entrepreneurial milieux. In particular, we analyse big data to listen for the presence of buzz about innovation – defined by discursive practices that reflect an innovative atmosphere – and explore how this may influence regional firm creation. Using information from 180 million geolocated Tweets comprising almost two billion words across NUTS3 regions in the UK for the year 2014, our results offer novel evidence, robust to different model specifications, that regions characterised by a relatively higher intensity of discussion and vibrancy around topics related to innovation may provide a more effective set of informal resources for sharing and recombination of ideas, defining regional capabilities to support and facilitate entrepreneurial processes. The findings contribute to the literature on the intangible dimensions in the geography of innovation and offer new insights on the potential of natural language processing for economic geography research. © The Author(s) 2021.","big data; buzz; entrepreneurship; knowledge spillovers; Twitter","economic geography; entrepreneur; innovation; knowledge based system; regional development; social media; spillover effect; United Kingdom",Article,Scopus,2-s2.0-85119274090
"Amen B., Faiz S., Do T.-T.","57207456603;57337775600;56647672700;","Big data directed acyclic graph model for real-time COVID-19 twitter stream detection",2022,"Pattern Recognition","123",,"108404","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119054993&doi=10.1016%2fj.patcog.2021.108404&partnerID=40&md5=83e7933227c2b47ce1f35d41bafa022b","Every day, large-scale data are continuously generated on social media as streams, such as Twitter, which inform us about all events around the world in real-time. Notably, Twitter is one of the effective platforms to update countries leaders and scientists during the coronavirus (COVID-19) pandemic. Other people have also used this platform to post their concerns about the spread of this virus and a rapid increase of death cases globally. The aim of this work is to detect anomalous events associated with COVID-19 from Twitter. To this end, we propose a distributed Directed Acyclic Graph topology framework to aggregate and process large-scale real-time tweets related to COVID-19. The core of our system is a novel lightweight algorithm that can automatically detect anomaly events. In addition, our system can also identify, cluster, and visualize important keywords in tweets. On 18 August 2020, our model detected the highest anomaly since many tweets mentioned the casualties’ updates and the debates on the pandemic that day. We obtained the three most commonly listed terms on Twitter: “covid”, “death”, and “Trump” (21,566, 11,779, and 4761 occurrences, respectively), with the highest TF-IDF score for these terms: “people” (0.63637), “school” (0.5921407) and “virus” (0.57385). From our clustering result, the word “death”, “corona”, and “case” are grouped into one cluster, where the word “pandemic”, “school”, and “president” are grouped as another cluster. These terms were located near each other on vector space so that they were clustered, indicating people's most concerned topics on Twitter. © 2021","Anomaly detection; Big data; COVID-19; Directed acyclic graph; Event stream","Anomaly detection; Big data; Clustering algorithms; Directed graphs; Vector spaces; Viruses; Anomalous events; Anomaly detection; Coronaviruses; COVID-19; Directed acyclic graph model; Event streams; Large scale data; Real- time; Social media; Stream detections; Social networking (online)",Article,Scopus,2-s2.0-85119054993
"Paragliola G., Coronato A.","55884798500;8878672000;","Definition of a novel federated learning approach to reduce communication costs",2022,"Expert Systems with Applications","189",,"116109","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118668661&doi=10.1016%2fj.eswa.2021.116109&partnerID=40&md5=0a5b0f9b579c63ed2de68e14865e26ea","Background and Objective: Contemporary Machine Learning approaches (e.g., Deep Learning) need huge volumes of data to build accurate and robust statistical models. Nowadays, very often, such data are collected by distinct and geographically distributed entities and successively transmitted to and stored by centralized nodes that implement the learning process. This practice, however, exposes data to security and privacy risks that may be even unacceptable in those environments regulated by the General Data Protection Regulation (GDPR). Methods: This paper defines a novel Federated Learning approach that avoids the transmission of sensitive data over the network and improves over the classic federated learning schemes by reducing the communication costs. This approach has been validated concerning a healthcare case study that aimed at building a Time-Series based predictive model to identify the level of risk for patients suffering from hypertension. Results: Experimental validation has shown that the proposed approach achieves excellent results both in terms of classification accuracy, superior to the state-of-the-art models with an improvement ranging from 3.01% to 11.09%, and in terms of communication costs with a reduction of about 34%. Conclusion: The analysis of the proposed approach shows promising results in terms of performance and communication cost. © 2021 Elsevier Ltd","Communication costs; Federated learning; Healthcare informatics; Self-adaptive systems; Time series analysis and classification","Adaptive systems; Big data; Cost benefit analysis; Cost reduction; Deep learning; Health care; Communication cost; Distributed entity; Federated learning; Health care informatics; Learning approach; Machine learning approaches; Self-adaptive system; Statistic modeling; Time series classifications; Time-series analysis; Time series analysis",Article,Scopus,2-s2.0-85118668661
"Sun D., Gao S., Liu X., Buyya R.","9249545100;36623652300;57191335027;57194845546;","A multi-level collaborative framework for elastic stream computing systems",2022,"Future Generation Computer Systems","128",,,"117","131",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117586711&doi=10.1016%2fj.future.2021.10.005&partnerID=40&md5=bdfe2b5ec95d9c494a9b84d45dd155dd","An elastic stream computing system is expected to process dynamic and volatile data streams with low latency and high throughput in timely manner. Effective management of stream application is considered one of the keys to achieve elastic computing by scaling in/out the workload of each computing node properly during runtime. Many existing work tried to build an elastic stream computing system from one perspective or at one level, which limited to some extent the system performance improvement. To address the problems brought by single level management, in this paper, we propose and implement a multi-level collaborative framework (called Mc-Stream) for elastic stream computing systems. This paper introduces our solution from the following aspects: (1) Extensive experiments show that system performance is affected by multiple factors locating at different levels. A multi-level collaborative optimization strategy can coordinate those factors and optimize the performance to a greater extent. (2) A system model is constructed to explain the multi-level collaborative framework, with the creation of topology model, data model and grouping model. The process of multi-level collaborative framework is formalized, including optimizing instances number, determining data stream load ratio among instances and deploying instances. (3) The system performance is optimized at multiple levels (user level, instance level, scheduling level, and resource level). It is further improved by the components of lightweight instances management, available resource-aware data stream redirection, fast and effective scheduling management, and asynchronous runtime redeployment without state loss. (4) Mc-Stream is implemented on top of Apache Storm platform. Metrics are evaluated with real-world stream applications, such as the fulfillment of system latency, throughput and resources utilization. Experimental results show the significant improvements made by Mc-Stream: reducing average system latency by 32%, increasing average system throughput by 26% and average resources utilization by 34%, compared with existing state-of-the-art scheduling strategies. © 2021 Elsevier B.V.","Big data; Distributed system; Elastic processing; Multi-level framework; Stream computing","Data streams; Distributed computer systems; Information management; Scheduling; Topology; Collaborative framework; Computing system; Data stream; Distributed systems; Elastic processing; Multi-level framework; Multilevels; Stream application; Stream computing; Systems performance; Big data",Article,Scopus,2-s2.0-85117586711
"Li X., Liu H., Wang W., Zheng Y., Lv H., Lv Z.","56829171600;57304491200;56828395700;57309218200;57020785300;55925162500;","Big data analysis of the Internet of Things in the digital twins of smart city based on deep learning",2022,"Future Generation Computer Systems","128",,,"167","177",,9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117582124&doi=10.1016%2fj.future.2021.10.006&partnerID=40&md5=e11437ea56bb395757d1cf03c2c1da8a","The study aims to conduct big data analysis (BDA) on the massive data generated in the smart city Internet of things (IoT), make the smart city change to the direction of fine governance and efficient and safe data processing. Aiming at the multi-source data collected in the smart city, the study introduces the deep learning (DL) algorithm while using BDA, and puts forward the distributed parallelism strategy of convolutional neural network (CNN). Meantime, the digital twins (DTs) and multi-hop transmission technology are introduced to construct the smart city DTs multi-hop transmission IoT-BDA system based on DL, and further simulate and analyze the performance of the system. The results reveal that in the energy efficiency analysis of model data transmission, the energy efficiency first increases and then decrease as the minimum energy collected α0 increases. But a more suitable power diversion factor ρ is crucial to the signal transmission energy efficiency of the IoT-BDA system. The prediction accuracy of the model is analyzed and it suggests that the accuracy of the constructed system reaches 97.80%, which is at least 2.24% higher than the DL algorithm adopted by other scholars. Regarding the data transmission performance of the constructed system, it is found that when the successful transmission probability is 100% and the exponential distribution parameters λ is valued 0.01∼0.05, it is the closest to the actual result, and the data delay is the smallest, which is maintained at the ms level. To sum up, improving the smart city's IoT-BDA system using the DL approach can reduce data transmission delay, improve data forecasting accuracy, and offer actual efficacy, providing experimental references for the digital development of smart cities in the future. © 2021 Elsevier B.V.","Big data analysis; Deep learning; Digital twins; Internet of Things; Smart city","Big data; Convolutional neural networks; Data communication systems; Data handling; Deep learning; E-learning; Energy efficiency; Information analysis; Probability distributions; Smart city; Big data analyse; Convolutional neural network; Data analysis system; Data-transmission; Deep learning; Distributed parallelism; Massive data; Multihop transmission; Multisource data; Parallelism strategies; Internet of things",Article,Scopus,2-s2.0-85117582124
"Gökalp M.O., Gökalp E., Kayabay K., Gökalp S., Koçyiğit A., Eren P.E.","57119769400;56403164300;57193878685;57287362800;15755652300;6603471003;","A process assessment model for big data analytics",2022,"Computer Standards and Interfaces","80",,"103585","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116589452&doi=10.1016%2fj.csi.2021.103585&partnerID=40&md5=f16bb1e3c32a81d4dae33ffb768e2697","Today, business success is essentially powered by data-centric software. Big data analytics (BDA) grasp the potential of generating valuable insights and empowering businesses to support their strategic decision-making. However, although organizations are aware of BDAs’ potential opportunities, they face challenges to satisfy the BDA-specific processes and integrate them into their daily software development lifecycle. Process capability/ maturity assessment models are used to assist organizations in assessing and realizing the value of emerging capabilities and technologies. However, as a result of the literature review and its analysis, it was observed that none of the existing studies in the BDA domain provides a complete, standardized, and objective capability maturity assessment model. To address this research gap, we focus on developing a BDA process capability assessment model grounded on the well-accepted ISO/IEC 330xx standard series. The proposed model comprises two main dimensions: process and capability. The process dimension covers six BDA-specific processes: business understanding, data understanding, data preparation, model building, evaluation, and deployment and use. The capability dimension has six levels, from not performed to innovating. We conducted case studies in two different organizations to validate the applicability and usability of the proposed model. The results indicate that the proposed model provides significant insights to improve the business value generated by BDA via determining the current capability levels of the organizations' BDA processes, deriving a gap analysis, and creating a comprehensive roadmap for continuous improvement in a standardized way. © 2021","Big data; Data analytics; Software development; Software process assessment; Software process improvement","Advanced Analytics; Big data; Decision making; ISO Standards; Life cycle; Software design; Assessment models; Business success; Centric softwares; Data analytics; Data centric; Maturity assessments; Process assessments; Software process assessment; Software Process Improvement; Strategic decision making; Data Analytics",Article,Scopus,2-s2.0-85116589452
"Uljarević M., Frazier T.W., Jo B., Billingham W.D., Cooper M.N., Youngstrom E.A., Scahill L., Hardan A.Y.","42263043800;7005941394;7005015695;57224440127;57209307625;7003735731;7006830321;6701799729;","Big Data Approach to Characterize Restricted and Repetitive Behaviors in Autism",2022,"Journal of the American Academy of Child and Adolescent Psychiatry","61","3",,"446","457",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116195780&doi=10.1016%2fj.jaac.2021.08.006&partnerID=40&md5=5305951223ea5effbc0c87a69677b36a","Objective: Despite being a core diagnostic feature of autism spectrum disorder (ASD), demographic, developmental and clinical correlates of restricted and repetitive behaviors and interests (RRB) remain poorly characterized. This study aimed to utilize the largest available RRB data set to date to provide a comprehensive characterization of how distinct RRB domains vary according to a range of individual characteristics. Method: Data were obtained from 17,581 children and adolescents with ASD (meanage= 8.24 years, SDage= 4.06) from the Simons Foundation Powering Autism Research for Knowledge cohort. Caregivers completed the Repetitive Behavior Scale−Revised questionnaire as a measure of repetitive motor behaviors, self-injurious behaviors, compulsions, insistence on sameness, and circumscribed interests RRB domains. Caregivers also provided information on children's cognitive functioning, language ability, and social and communication impairments. Results: Male sex was associated with higher severity of repetitive motor behaviors and restricted interests and with lower severity of compulsions and self-injurious behaviors; no sex differences were found for the insistence on sameness domain. Although repetitive motor behaviors showed a mostly linear (negative) association with age, other RRB domains showed more complex and nonlinear pattern of associations. Higher severity of social and communication impairments provided significant independent contribution in predicting higher severity of all RRB domains at the p &lt; .001 level; however, these effects were small (d &lt; 0.25). The strongest of these effects was observed for insistence on sameness (d = 0.24), followed by repetitive motor behaviors (d = 0.21), compulsions (d = 0.17), restricted interests (d = 0.14), and self-injurious behaviors (d = 0.12). Conclusion: Findings reported here provide further evidence that RRB subdomains show a somewhat distinct pattern of associations with demographic, developmental, and clinical variables, with a key implication that separate consideration of these domains can help to facilitate efforts to understand diverse ASD etiology and to inform the design of effective interventions. © 2021","circumscribed interest; compulsions; insistence on sameness; repetitive motor behavior; self-injurious behaviors","adolescent; Article; autism; automutilation; behavior; behavior disorder assessment; big data; caregiver; child; cognition; cohort analysis; compulsion; female; human; interpersonal communication; language ability; locomotion; major clinical study; male; questionnaire; Repetitive Behavior Scale Revised; restricted and repetitive behavior; sex difference",Article,Scopus,2-s2.0-85116195780
"Ryabchikov M.Y., Ryabchikova E.S.","55833347900;57190069977;","Big Data-Driven Assessment of Proposals to Improve Enterprise Flexibility Through Control Options Untested in Practice",2022,"Global Journal of Flexible Systems Management","23","1",,"43","74",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114824468&doi=10.1007%2fs40171-021-00287-5&partnerID=40&md5=9deacd7e8ebedab4a1a1d871d329081f","The study discusses how the process information reserves can be used for improving production flexibility on the basis of production component variation control. Building upon the assumption that the data in question describe the functions of the production components as those experience perturbations, we submit that it may be possible to predict the productivity and economic efficiency for new, previously unused control options. We elaborate on the production components that ensure production flexibility, approaching them as a highly autonomous holon. We determine the conditions that make it possible to analyze the holon control options in isolation from controlling all other components. We also suggest a solution to the problem of predicting the employees’ impact on production. Our suggestion is based on limiting the types of impact accessible to a human operator. The study looks at the reasons behind the uncertainty of production control simulation modeling results, which stem from the peculiarities of the data collected. We propose a criterion for assessing the impact of uncertainty on the productivity indicators’ variation. This study offers a systemic overview of the aspects of a production simulation aimed at assessing the impact of uncertainty on productivity predictions. As an example, we review how uncertainty affects the variation of coke consumption and productivity of a blast furnace, which is ensured by selecting a suitable option for iron ore sinter quality control. © 2021, Global Institute of Flexible Systems Management.","Big data; Flexible production; Holonic control; Human behavior model; Industry 4.0; Productivity prediction; Simulation modeling; Uncertainty",,Article,Scopus,2-s2.0-85114824468
"Ding X., Wang H., Su J., Wang M., Li J., Gao H.","57007529500;14061534000;55021818300;57211681933;57211142666;34769881200;","Leveraging Currency for Repairing Inconsistent and Incomplete Data",2022,"IEEE Transactions on Knowledge and Data Engineering","34","3",,"1288","1302",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112868584&doi=10.1109%2fTKDE.2020.2992456&partnerID=40&md5=847bb45e591ca1deadadb55adab5d717","Data quality plays a key role in big data management today. With the explosive growth of data from a variety of sources, the quality of data is faced with multiple problems. Motivated by this, we study the multiple data cleaning on incompleteness and inconsistency with currency reasoning and determination in this paper. We introduce a 4-step framework, named Imp3C, for errors detection and quality improvement in incomplete and inconsistent data without timestamps. We achieve an integrated currency determining method to compute the currency orders among tuples, according to currency constraints. Thus, the inconsistent data and missing values are repaired effectively considering the temporal impact. For both effectiveness and efficiency consideration, we carry out inconsistency repair ahead of incompleteness repair. A currency-related consistency distance metric is defined to measure the similarity between dirty tuples and clean ones more accurately. In addition, currency orders are treated as an important feature in the missing imputation training process. The solution algorithms are introduced in detail with case studies. A thorough experiment on three real-life datasets verifies our method Imp3C improves the performance of data repairing with multiple quality problems. Imp3C outperforms the existing advanced methods, especially in the datasets with complex currency orders. © 2020 IEEE.","Currency determining; Data cleaning; Data quality management; Temporal data repairing","Big data; Quality management; Repair; Currency determining; Currency orders; Data cleaning; Data quality; Data quality management; Explosive growth; Incomplete data; Inconsistent data; Temporal Data; Temporal data repairing; Information management",Article,Scopus,2-s2.0-85112868584
"Dai D., Boroomand S.","57225997289;57205456727;","A Review of Artificial Intelligence to Enhance the Security of Big Data Systems: State-of-Art, Methodologies, Applications, and Challenges",2022,"Archives of Computational Methods in Engineering","29","2",,"1291","1309",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110130676&doi=10.1007%2fs11831-021-09628-0&partnerID=40&md5=f9183265fc9061304616015cd159beeb","Technological advancements modernize the way we live with the changes made both globally and nationwide. These technological improvements also cause adverse effects in the form of security threats. To overcome this problem, many researchers integrated both big data and artificial intelligence (AI) techniques to enhance the security of internet-connected devices. Big data normally represents the massive information collected from the web which is both structured and unstructured and big data analytics deal with the processing of this information which is often cumbersome for the traditional data processing techniques. AI technique helps machines to function similarly to humans in solving complex problems. Recent studies show that the AI technique identifies different attacks which compromise the security of the application and system in an organization. AI techniques respond to different attacks in real-time using machine learning and deep learning techniques. Machine learning is a subfield of AI which can identify the patterns present in the input data with less manual intervention. Deep Learning is a subfield of machine learning in which the algorithm used to construct an artificial neural network (ANN) is trained using a large amount of data to solve complex problems with less manual intervention. This study evaluates the security issues faced by Big data systems using AI techniques focusing on different attacks, defense strategies, and security evaluation models. The AI-based techniques for security enhancement in big data-based systems are divided into eight categories: reinforcement learning, swarm intelligence, deep learning, multi-agent, game theory, ML, and ANN. The review is systematically conducted using the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analysis) technique. The open issues present in the security domain can be used by different authors as a potential area for future research. © 2021, CIMNE, Barcelona, Spain.",,"Advanced Analytics; Big data; Complex networks; Data Analytics; Data handling; Deep learning; Game theory; Multi agent systems; Neural networks; Reinforcement learning; Swarm intelligence; Data processing techniques; Learning techniques; Manual intervention; Security enhancements; Security evaluation; Systematic Review; Technological advancement; Technological improvements; Learning systems",Article,Scopus,2-s2.0-85110130676
"Munir K., Ghafoor M., Khafagy M., Ihshaish H.","24477551700;55001313100;36617440800;56084360800;","AgroSupportAnalytics: A Cloud-based Complaints Management and Decision Support System for Sustainable Farming in Egypt",2022,"Egyptian Informatics Journal","23","1",,"73","82",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108946190&doi=10.1016%2fj.eij.2021.06.002&partnerID=40&md5=99c55fcac101c0603e35d8ee8ca15c32","Sustainable Farming requires up-to-date advice on crop diseases, patterns, and adequate prevention actions to face developing circumstances. Currently, in developing countries like Egypt, farmers’ access to such information is extremely limited due to the agriculture support being either not available, inconsistent, or unreliable. The presented Cloud-based Complaints Management and Decision Support System for Sustainable Farming in Egypt, named as AgroSupportAnalytics, aims to resolve the problem of both the lack of support and advice for farmers, and the inconsistencies in doing so by current manual approach provided by agricultural experts. Key contribution is the development of an automated complaint management and decision support strategy, on the basis of extensive research on requirement analysis tailored for Egypt. The solution is grounded on the application of knowledge discovery and analysis on agricultural data and farmers’ complaints, deployed on a Cloud platform, to provide farming stakeholders in Egypt with timely and suitable support. This paper presents the overall system architectural framework along with the information and storage services, which have been based on the requirements specifications phases of the project along with the historical data sets of past 10 year of farmers complaints and enquiries in Egypt. © 2022","AgriTech; Analytics; Big Data; Decision Support Systems; Sustainable Farming","Agricultural robots; Agriculture; Decision making; Developing countries; Digital storage; Research and development management; Storage as a service (STaaS); Architectural frameworks; Cloud platforms; Decision supports; Historical data; Requirement analysis; Requirements specifications; Storage services; Sustainable Farming; Decision support systems",Article,Scopus,2-s2.0-85108946190
"Lee Y., Park J.","57224823146;57224816034;","Using Big Data to Prevent Crime: Legitimacy Matters",2022,"Asian Journal of Criminology","17","1",,"61","80",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108315151&doi=10.1007%2fs11417-021-09353-4&partnerID=40&md5=9a9e6867db250328a1e4126b1fad2628","This study uses a citizens’ awareness survey to gauge the impact of crime prevention initiatives based on big data. Crime prevention activities using large datasets inevitably involve reasonable concerns over: (a) the excessive concentration of information power to law enforcement agencies and (b) possible privacy violations by the state. This study explores the trends in this area through the application of the police legitimacy theory. It aims to gather insights into the use of big data to strengthen preventive responses to crimes by law enforcement agencies while also considering the citizens initiatives based on big data abuses. The survey conducted by the Korean Institute of Criminology in August 2015 was used for analysis through structural equation modeling (SEM). The results showed that, among important factors in police legitimacy theory, three primary items drew support for crime prevention activities based on big data: “distributive fairness,” “lawfulness,” and “effectiveness.” These overshadowed the “procedural fairness” variable, which has been emphasized in previous police legitimacy studies. This suggests that the law enforcement agencies should focus on the promulgation of the following: (1) big data as an aid to guaranteeing citizens’ rights in civil society, rather than the unilateral strengthening of national power, and (2) the effective and lawful use of big data as a method of stabilizing citizens’ personal security. © 2021, The Author(s), under exclusive licence to Springer Nature B.V.","Big data; Crime prevention; Distributive fairness; Legitimacy; Police",,Article,Scopus,2-s2.0-85108315151
"Minutoli M., Castellana V.G., Saporetti N., Devecchi S., Lattuada M., Fezzardi P., Tumeo A., Ferrandi F.","56893657600;48760925400;57221981064;57221976026;36907433300;56434316900;22036780900;7004636771;","Svelto: High-Level Synthesis of Multi-Threaded Accelerators for Graph Analytics",2022,"IEEE Transactions on Computers","71","3",,"520","533",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100854189&doi=10.1109%2fTC.2021.3057860&partnerID=40&md5=632ecbfe63d34b26a122dd5372ea518c","Graph analytics are an emerging class of irregular applications. Operating on very large datasets, they present unique behaviors, such as fine-grained, unpredictable memory accesses, and highly unbalanced task level parallelism, that make existing high-performance general-purpose processors or accelerators (e.g., GPUs) suboptimal. To address these issues, research and industry are developing a variety of custom accelerator designs for this application area, including solutions based on reconfigurable devices (Field Programmable Gate Arrays). These new approaches often employ High-Level Synthesis (HLS) to Speed up the development of the accelerators. In this paper, we propose a novel architecture template for the automatic generation of accelerators for graph analytics and irregular applications. The architecture template includes a dynamic task scheduling mechanism, a parallel array of accelerators that enables supporting task-level parallelism with context switching, and a related multi-channel memory interface that decouples communication from computation and provides support for fine-grained atomic memory operations. We discuss the integration of the architectural template in an HLS flow, presenting the necessary modifications to enable automatic generation of the custom architectures starting from OpenMP annotated code. We evaluate our approach first by synthesizing and exploring triangle counting, a common graph algorithm, and then by synthesizing custom designs for a set of graph database benchmark queries, representing series of graph pattern matching routines. We compare the synthesized accelerators with previous state-of-the-art methodologies for the synthesis of parallel architectures, showing that the proposed approach allows reducing resource usage by optimizing the number of accelerators replicas without any performance penalty. © 1968-2012 IEEE.","Big data; Context switching; Dynamic task scheduling; High performance data analytics; Multi-threading; Parallel architectures; RDF; SPARQL","Acceleration; Application programming interfaces (API); Automatic programming; Field programmable gate arrays (FPGA); General purpose computers; Graph Databases; Industrial research; Large dataset; Memory architecture; Parallel architectures; Program processors; Query languages; Automatic Generation; Context switching; General purpose processors; Irregular applications; Novel architecture; Reconfigurable devices; State of the art; Task level parallelisms; High level synthesis",Article,Scopus,2-s2.0-85100854189
"Griffin T.W., Harris K.D., Ward J.K., Goeringer P., Richard J.A.","24537061200;57192211168;55449099400;57210904743;57221956458;","Three Digital Agriculture Problems in Cotton Solved by Distributed Ledger Technology",2022,"Applied Economic Perspectives and Policy","44","1",,"237","252",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099435175&doi=10.1002%2faepp.13142&partnerID=40&md5=7ece98560c971067f7bc00c74719ec9e","Distributed ledger technology applied to Big Data in agriculture presents challenges and opportunities. Opportunities exist to solve decades-old farm data management problems. Real-world examples of applying distributed ledger technology to current farm data problems in cotton include (1) yield monitor data quality assurance, (2) sustainability metrics and resource tracking of cotton lint quality data from ginner back to subfield locations, and (3) increasing supply chain coordination by providing more information to warehouse managers. The culmination of the discussion across three aspects of cotton production data is of interest to farmers, researchers, policy makers, and consumers. © 2021 Agricultural and Applied Economics Association","Big Data; blockchain; cotton; distributed ledger technology (DLT); traceability",,Article,Scopus,2-s2.0-85099435175
"Cao S., Fan Q., Jin YU W., Tao Wang L., Ni S., Chen J.","57424293000;57424438500;57424586100;57423547800;57424438600;57424142900;","Multi-Sensor fusion and data analysis for operating conditions of low power transmission lines",2022,"Measurement: Journal of the International Measurement Confederation","190",,"110586","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123370134&doi=10.1016%2fj.measurement.2021.110586&partnerID=40&md5=743203a2ee475c19b17ca8830aabed36","The research of the safety distance among drones and cable attracted wide attention with the rising deployment of unmanned aerial vehicle (UAV) to check for high-voltage overhead power systems. To increase the inspection operation's dependability and guarantee a safe and stable functioning of the transmission grid and inspections equipment, it is essential to determine the safety distance among the UAV and the driveway. Due to UAV patrol safety distances from overhead power lines, it is difficult to offer precise surfing information because of the lack of quantitative assistance. The author of the relevant study uses multi-sensor fusion data analysis (MFDA-LPTL). There have been discussions on the properties of large data sets, smart networks, and gigantic data sets before the emergence of low-cost power transmission lines and the benefits they may give. For example, using the adaptive weighted fusional method, which combines first-level data on homogenous sensor input based on critical UAV-influencing factors such as maximum inspection speed, wind speed, positioning error, and drone size, can help achieve this goal. As a secondary benefit, the theory makes use of more robust evidence than before. However, the use of big data analytics in present smart grids must be expanded due to numerous challenges, such as the need for new technologies and increased public awareness. The experimental analysis shows that the proposed MFDA-LPTL model increases the performance ratio of 98.9%, efficiency ratio of 97.2%, reduces the detection failure analysis of 10.2%, processing time of 7.8%, positioning error rate of 12.3% compared to other existing methods. © 2021","Big data; Fusion algorithm; Low power transmission line","Aircraft detection; Antennas; Data Analytics; Data handling; Drones; Electric lines; Electric power transmission networks; Information analysis; Inspection; Power transmission; Sensor data fusion; Wind; Fusion algorithms; High-voltages; Low power transmission line; Low power transmissions; Multi-sensor fusion; Operating condition; Positioning error; Power; Power transmission lines; Safety distances; Big data",Article,Scopus,2-s2.0-85123370134
"Sun J., Li Y., Li Q., Li Y., Jia Y., Xia D.","57385632000;57384942300;57219872348;57219873918;57384243500;57384711700;","Fine Clustering Analysis of Internet Financial Credit Investigation Based on Big Data",2022,"Big Data Research","27",,"100297","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121676753&doi=10.1016%2fj.bdr.2021.100297&partnerID=40&md5=779b9a07cf01b191bd96dc1d75ea4d7f","Given that the traditional methods cannot perform clustering analysis on the Internet financial credit reporting directly and effectively, precise clustering analysis of internet financial credit reporting dependent on multidimensional attribute sparse large data is proposed. By measuring the overall distance between Internet financial credit reporting through the sparse extensive data with multidimensional attributes, the multidimensional attribute sparse large data are used to perform clustering analysis on the overall distance matrix and the component approximate distance matrix between the data, respectively. Numerical experiments show that the method (MASLD) proposed in this paper can reflect the overall data features effectively but also improve the clustering effect of the original Internet financial credit reporting data through the analysis of the correlation relationship between the vital components attribute sequences. The (MASLD) method proposed in this paper could be applied to medical, voice, text, fault monitoring, and other Internet financial credit reporting pattern recognition and knowledge discovery properly. © 2021","Big data; Clustering analysis; Credit investigation; Internet financial; Multidimensional attribute",,Article,Scopus,2-s2.0-85121676753
"Steuber F., Schneider S., Schoenfeld M.","57213141194;57343735000;56211165900;","Embedding Semantic Anchors to Guide Topic Models on Short Text Corpora",2022,"Big Data Research","27",,"100293","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119253728&doi=10.1016%2fj.bdr.2021.100293&partnerID=40&md5=5cc9f6f76d43be6878e6dff255102f1a","Documents on the social media platform Twitter are formulated in short and simple style, instead of being written extensively and elaborately. Further, the core message of a post is often encoded into characteristic phrases called hashtags. These hashtags illustrate the semantics of a post or tie it to a specific topic. In this paper, we propose multiple approaches of using hashtags and their surrounding texts to improve topic modeling of short texts. We use transfer learning by applying a pre-trained word embedding of hashtags to derive preliminary topics. These function as supervising information, or seed topics and are passed to Archetypal LDA (A-LDA), a recent variant of Latent Dirichlet Allocation. We demonstrate the effectiveness of our approach using a large corpus of posts exemplarily on Twitter. Our approaches improve the topic model's qualities in terms of various quantitative metrics. Moreover, the presented algorithms used to extract seed topics can be utilized as form of lightweight topic model by themselves. Hence, our approaches create additional analytical opportunities and can help to gain a more detailed understanding of what people are talking about on social media. By using big data in terms of millions of tweets for preprocessing and fine-tuning, we enable the classification algorithm to produce topics that are very coherent to the reader. © 2021 Elsevier Inc.","Big data; Short text; Topic modeling; Transfer learning; Word embedding",,Article,Scopus,2-s2.0-85119253728
"Wan Y., Qu Y., Gao L., Xiang Y.","57395787500;57193560269;36133254500;57395280200;","Privacy-preserving blockchain-enabled federated learning for B5G-Driven edge computing",2022,"Computer Networks","204",,"108671","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122215564&doi=10.1016%2fj.comnet.2021.108671&partnerID=40&md5=d833befae35c11bdbbd0751e3f8a5cef","The arrival of the fifth-generation technology standard for broadband cellular networks (5G) and beyond 5G networks (B5G) rises the speed and robustness ceiling of communicating networks and thereby empowers the rapid popularization of edge computing. Consequently, B5G-Driven edge computing allows a growing volume of data to be collected from and transmitted among pervasive edge devices for big data analytics. The collected big data becomes the driving force of artificial intelligence (AI) by training high-quality machine learning (ML) models, which is followed by severe individual privacy leakage. Federated learning(FL) is then proposed to achieve privacy-preserving machine learning by avoiding the exchange of raw data. Unfortunately, several major issues remain outstanding. Centralized processing costs significant communication resources between cloud and edge while data falsification problems persist. In addition, the private data may be reconstructed by malicious participants by exploiting the context of model parameters in FL. To solve the identified problems, we propose to integrate blockchain-enabled FL with Wasserstein generative adversarial network (WGAN) enabled differential privacy (DP) to protect the model parameters of edge devices in B5G networks. Blockchain enables decentralized FL to reduce communication costs between cloud and edge while alleviating the data falsification issues, and it also provides an incentive mechanism to alleviate the data island issue in B5G-Driven edge computing. WGAN is used to generate controllable random noise complying with DP requirements, which is then injected to model parameters. WGAN-enabled DP is able to achieve an optimized trade-off between differential privacy protection and improved data utility of model parameters. Time delay analysis is conducted to show the efficiency of the proposed model. Extensive evaluation results from simulations demonstrate superior performances from aspects of convergence efficiency, accuracy, and data utility. © 2021 Elsevier B.V.","Blockchain; Differential privacy protection; Federated learning; Wasserstein generative adversarial nets","Big data; Blockchain; Crime; Data Analytics; Economic and social effects; Edge computing; Efficiency; Machine learning; Privacy-preserving techniques; Block-chain; Data utilities; Differential privacies; Differential privacy protection; Edge computing; Federated learning; Modeling parameters; Privacy preserving; Privacy protection; Wasserstein generative adversarial net; 5G mobile communication systems",Article,Scopus,2-s2.0-85122215564
"van Elten H.J., Sülz S., van Raaij E.M., Wehrens R.","15842451600;36136023100;56030372900;36195780700;","Big Data Health Care Innovations: Performance Dashboarding as a Process of Collective Sensemaking",2022,"Journal of medical Internet research","24","2",,"e30201","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125020366&doi=10.2196%2f30201&partnerID=40&md5=d5333404b86691e11a410b09944a38d4","INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): RR2-10.2196/16779. ©Hilco J van Elten, Sandra Sülz, Erik M van Raaij, Rik Wehrens. Originally published in the Journal of Medical Internet Research (https://www.jmir.org), 22.02.2022.","balanced scorecard; big data; dashboarding; dashboards; digital health; health care; health information; key performance indicators; knowledge translation; performance measurement; stakeholders","article; balanced scorecard; big data; key performance indicator; medical information",Article,Scopus,2-s2.0-85125020366
"Yabe T., Rao S.C.P., Ukkusuri S.V., Cutter S.L.","57189643794;57457722000;14720321700;7003270541;","Toward data-driven, dynamical complex systems approaches to disaster resilience",2022,"Proceedings of the National Academy of Sciences of the United States of America","119","8","e2111997119","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124278143&doi=10.1073%2fpnas.2111997119&partnerID=40&md5=fd80212bd0e6fe09d9a28eac82d647f5","With rapid urbanization and increasing climate risks, enhancing the resilience of urban systems has never been more important. Despite the availability of massive datasets of human behavior (e.g., mobile phone data, satellite imagery), studies on disaster resilience have been limited to using static measures as proxies for resilience. However, static metrics have significant drawbacks such as their inability to capture the effects of compounding and accumulating disaster shocks; dynamic interdependencies of social, economic, and infrastructure systems; and critical transitions and regime shifts, which are essential components of the complex disaster resilience process. In this article, we argue that the disaster resilience literature needs to take the opportunities of big data and move toward a different research direction, which is to develop data-driven, dynamical complex systems models of disaster resilience. Data-driven complex systems modeling approaches could overcome the drawbacks of static measures and allow us to quantitatively model the dynamic recovery trajectories and intrinsic resilience characteristics of communities in a generic manner by leveraging large-scale and granular observations. This approach brings a paradigm shift in modeling the disaster resilience process and its linkage with the recovery process, paving the way to answering important questions for policy applications via counterfactual analysis and simulations. © 2022 National Academy of Sciences. All rights reserved.","Big data; Complex systems; Disaster resilience; Urban science",,Article,Scopus,2-s2.0-85124278143
"Wang F.Q., Choudhary K., Liu Y., Hu J., Hu M.","57460689200;57425806800;57460650300;12242142100;57189991935;","Large scale dataset of real space electronic charge density of cubic inorganic materials from density functional theory (DFT) calculations",2022,"Scientific data","9","1",,"59","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125004552&doi=10.1038%2fs41597-022-01158-z&partnerID=40&md5=ccf0f1e1ca13f6dc1023978f27a042b6","Driven by the big data science, material informatics has attracted enormous research interests recently along with many recognized achievements. To acquire knowledge of materials by previous experience, both feature descriptors and databases are essential for training machine learning (ML) models with high accuracy. In this regard, the electronic charge density ρ(r), which in principle determines the properties of materials at their ground state, can be considered as one of the most appropriate descriptors. However, the systematic electronic charge density ρ(r) database of inorganic materials is still in its infancy due to the difficulties in collecting raw data in experiment and the expensive first-principles based computational cost in theory. Herein, a real space electronic charge density ρ(r) database of 17,418 cubic inorganic materials is constructed by performing high-throughput density functional theory calculations. The displayed ρ(r) patterns show good agreements with those reported in previous studies, which validates our computations. Further statistical analysis reveals that it possesses abundant and diverse data, which could accelerate ρ(r) related machine learning studies. Moreover, the electronic charge density database will also assists chemical bonding identifications and promotes new crystal discovery in experiments. © 2022. The Author(s).",,"achievement; article; big data; crystal; density functional theory; infancy; information science; machine learning",Article,Scopus,2-s2.0-85125004552
"Duan H., Yang J., Yang H.","57450044400;57451296700;57450941100;","A Blockchain-Based Privacy Protection Application for Logistics Big Data",2022,"Journal of Cases on Information Technology","24","5",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124548643&doi=10.4018%2fJCIT.295249&partnerID=40&md5=7d3c806f408e93e146182e233db513c8","Logistics business is generally managed by logistics orders in plain text, and there is a risk of disclosure of customer privacy information in every business link. In order to solve the problem of privacy protection in logistics big data systems, a new kind of logistics user privacy data protection scheme is proposed. First of all, an access rights management mechanism is designed by combining blockchain and anonymous authentication to realize the control and management of users’ access rights to private data. Then, the privacy and confidentiality protection between different services is realized by dividing and storing the data of different services. Finally, the participants of the intra-chain private data are specified by embedding fields in the logistics information. The blockchain node receiving the transaction is used as the transit node to synchronize the intra-chain privacy data, so as to improve the intra-chain privacy protection within the business. Experimental results show that the proposed method can satisfy the privacy requirements and ensure considerable performance. © 2022 IGI Global. All rights reserved.","Big data; Blockchain; Logistics; Privacy protection","Big data; Blockchain; Block-chain; Business links; Customer privacy; Different services; Plain text; Privacy information; Privacy protection; Private data; Problem of privacy; Protection application; Data privacy",Article,Scopus,2-s2.0-85124548643
"Cesare N., Were L.P.O.","57191966160;57194680784;","A multi-step approach to managing missing data in time and patient variant electronic health records",2022,"BMC research notes","15","1",,"64","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124777497&doi=10.1186%2fs13104-022-05911-w&partnerID=40&md5=294dcffb808e873b8e29559c68522505","OBJECTIVE: Electronic health records (EHR) hold promise for conducting large-scale analyses linking individual characteristics to health outcomes. However, these data often contain a large number of missing values at both the patient and visit level due to variation in data collection across facilities, providers, and clinical need. This study proposes a stepwise framework for imputing missing values within a visit-level EHR dataset that combines informative missingness and conditional imputation in a scalable manner that may be parallelized for efficiency. RESULTS: For this study we use a subset of data from AMPATH representing information from 530,812 clinic visits from 16,316 Human Immunodeficiency Virus (HIV) positive women across Western Kenya who have given birth. We apply this process to a set of 84 clinical, social and economic variables and are able to impute values for 84.6% of variables with missing data with an average reduction in missing data of approximately 35.6%. We validate the use of this imputed dataset by predicting National Hospital Insurance Fund (NHIF) enrollment with 94.8% accuracy. © 2022. The Author(s).","Big data; Electronic medical records; HIV; Imputation","electronic health record; female; human; information processing; Kenya; Data Collection; Electronic Health Records; Female; Humans; Kenya",Article,Scopus,2-s2.0-85124777497
"Nasejje J.B., Mbuvha R., Mwambi H.","56879011400;57200961334;6506564852;","Use of a deep learning and random forest approach to track changes in the predictive nature of socioeconomic drivers of under-5 mortality rates in sub-Saharan Africa",2022,"BMJ open","12","2",,"e049786","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124776105&doi=10.1136%2fbmjopen-2021-049786&partnerID=40&md5=b0e6021ae01982dd0d4cbbe0a83e7c85","OBJECTIVES: We used machine learning algorithms to track how the ranks of importance and the survival outcome of four socioeconomic determinants (place of residence, mother's level of education, wealth index and sex of the child) of under-5 mortality rate (U5MR) in sub-Saharan Africa have evolved. SETTINGS: This work consists of multiple cross-sectional studies. We analysed data from the Demographic Health Surveys (DHS) collected from four countries; Uganda, Zimbabwe, Chad and Ghana, each randomly selected from the four subregions of sub-Saharan Africa. PARTICIPANTS: Each country has multiple DHS datasets and a total of 11 datasets were selected for analysis. A total of n=85 688 children were drawn from the eleven datasets. PRIMARY AND SECONDARY OUTCOMES: The primary outcome variable is U5MR; the secondary outcomes were to obtain the ranks of importance of the four socioeconomic factors over time and to compare the two machine learning models, the random survival forest (RSF) and the deep survival neural network (DeepSurv) in predicting U5MR. RESULTS: Mother's education level ranked first in five datasets. Wealth index ranked first in three, place of residence ranked first in two and sex of the child ranked last in most of the datasets. The four factors showed a favourable survival outcome over time, confirming that past interventions targeting these factors are yielding positive results. The DeepSurv model has a higher predictive performance with mean concordance indexes (between 67% and 80%), above 50% compared with the RSF model. CONCLUSIONS: The study reveals that children under the age of 5 in sub-Saharan Africa have favourable survival outcomes associated with the four socioeconomic factors over time. It also shows that deep survival neural network models are efficient in predicting U5MR and should, therefore, be used in the big data era to draft evidence-based policies to achieve the third sustainable development goal. © Author(s) (or their employer(s)) 2022. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.","community child health; health policy; public health","article; artificial neural network; big data; child; child health; controlled study; cross-sectional study; deep learning; education; educational status; female; forest; Ghana; health care policy; health survey; human; human experiment; machine learning; major clinical study; mortality rate; outcome assessment; outcome variable; preschool child; public health; random forest; randomized controlled trial (topic); residence characteristics; socioeconomics; survival; sustainable development goal; Uganda; Zimbabwe",Article,Scopus,2-s2.0-85124776105
"Chen G., Pine D.S., Brotman M.A., Smith A.R., Cox R.W., Taylor P.A., Haller S.P.","57198649514;57376301500;15062301600;56007937700;57375537400;57376149300;56233380700;","Hyperbolic trade-off: The importance of balancing trial and subject sample sizes in neuroimaging",2022,"NeuroImage","247",,"118786","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121349430&doi=10.1016%2fj.neuroimage.2021.118786&partnerID=40&md5=7a0db12427740a4bd181941b714de516","Here we investigate the crucial role of trials in task-based neuroimaging from the perspectives of statistical efficiency and condition-level generalizability. Big data initiatives have gained popularity for leveraging a large sample of subjects to study a wide range of effect magnitudes in the brain. On the other hand, most task-based FMRI designs feature a relatively small number of subjects, so that resulting parameter estimates may be associated with compromised precision. Nevertheless, little attention has been given to another important dimension of experimental design, which can equally boost a study's statistical efficiency: the trial sample size. The common practice of condition-level modeling implicitly assumes no cross-trial variability. Here, we systematically explore the different factors that impact effect uncertainty, drawing on evidence from hierarchical modeling, simulations and an FMRI dataset of 42 subjects who completed a large number of trials of cognitive control task. We find that, due to an approximately symmetic hyperbola-relationship between trial and subject sample sizes in the presence of relatively large cross-trial variability, 1) trial sample size has nearly the same impact as subject sample size on statistical efficiency; 2) increasing both the number of trials and subjects improves statistical efficiency more effectively than focusing on subjects alone; 3) trial sample size can be leveraged alongside subject sample size to improve the cost-effectiveness of an experimental design; 4) for small trial sample sizes, trial-level modeling, rather than condition-level modeling through summary statistics, may be necessary to accurately assess the standard error of an effect estimate. We close by making practical suggestions for improving experimental designs across neuroimaging and behavioral studies. © 2021",,"adult; article; attention; big data; brain; clinical article; controlled study; cost effectiveness analysis; drawing; executive function; experimental design; female; human; human experiment; male; neuroimaging; sample size; simulation; uncertainty",Article,Scopus,2-s2.0-85121349430
"Dhanalakshmi J., Ayyanathan N.","57215354573;56195811300;","A systematic review of big data in energy analytics using energy computing techniques",2022,"Concurrency and Computation: Practice and Experience","34","4","e6647","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116370449&doi=10.1002%2fcpe.6647&partnerID=40&md5=de756b5a7e91fddb5c3cfd9b8aa42092","In big data and machine learning, energy analytics has shown rapid development in the past decade. With this development in energy analytics, the energy data has exponentially increased well in different areas and domains. Energy computing techniques have been used in various domains, particularly in energy data to handle the data analysis process. This paper focuses on explaining the concept and evolution of various research questions formulation, data extraction, quality valuation, search strategy, study selection and reporting the results in energy-oriented domains to evaluate energy-computing techniques. The thermal power plant, power system and smart grid datasets were highly used out of plenty of other datasets discussed and the ANN algorithm is most commonly used to classify the dataset. Different sets of algorithms are used for data analysis, classification, converting structured data to unstructured data, regression, visualization, and forecasting. Journal publications from the year 2012 to 2020 have reviewed and adopted a systematic approach to identify the findings of all relevant research methodologies in the defined research domain. © 2021 John Wiley & Sons Ltd.",,"Big data; Classification (of information); Data visualization; Electric power transmission networks; Machine learning; Neural networks; Quality control; Smart power grids; Thermoelectric power plants; Time series analysis; Clusterings; Data analytics; Electricity grids; Energy; Energy analytic; Energy computing; Indian electricity grid; Neural-networks; Smart grid; Time-series analysis; Data Analytics",Article,Scopus,2-s2.0-85116370449
"Xia Q., Zhou L., Ren W., Wang Y.","56109379900;57218795948;57219267278;57211420047;","Proactive and intelligent evaluation of big data queries in edge clouds with materialized views",2022,"Computer Networks","203",,"108664","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121549592&doi=10.1016%2fj.comnet.2021.108664&partnerID=40&md5=a9fc9ae9bf51024abfa0ea7c47442fee","The rise of big data brings extraordinary benefits and opportunities to businesses and governments. Enterprise users can analyze their consumers’ data and infer the business value, such as purchasing goods correlations and customer preferences. To unveil such hidden values from big data, various big data processing frameworks, such as Hadoop and Tensor-flow, are developed. One fundamental approach of accelerating such big data processing frameworks is to efficiently evaluate queries for big data analytics based on materialization of intermediate results. In this paper, we consider problems of dynamic and proactive QoS-aware query evaluation of big data analytics with intermediate result materialization in a mobile edge cloud. We propose a one-shot online algorithm for the dynamic QoS-aware big data query evaluation within a finite time horizon, which can intelligently determine whether some immediate results during query evaluation need to be materialized for later use of other queries, by making use of the Reinforcement Learning (RL) method with predictions. We also devise an online learning algorithm based on the technique of multi-armed bandits, for the proactive QoS-aware big data query evaluation problem with resource reservations, where the query arrival information and their required datasets are uncertain before the actual evaluation of the queries. We finally investigate the performance of the proposed algorithms by experimental simulations, and results show that the performance of the proposed algorithms is promising, by achieving 12% higher system throughput while reducing 50% and 40% average evaluation cost and time per query compared to the comparison benchmarks. © 2021","Big data analytics; Mobile edge clouds; Online learning; Query evaluation; View materialization","Advanced Analytics; Benchmarking; Data Analytics; Data handling; E-learning; Learning algorithms; Mobile edge computing; Reinforcement learning; Data query; Edge clouds; Intelligent evaluation; Intermediate results; Mobile edge cloud; Online learning; Performance; QoS-aware; Query evaluation; View materializations; Big data",Article,Scopus,2-s2.0-85121549592
"Mcbride K., Philippou C.","15826885800;57209106240;","“Big results require big ambitions”: big data, data analytics and accounting in masters courses",2022,"Accounting Research Journal","35","1",,"71","100",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100536670&doi=10.1108%2fARJ-04-2020-0077&partnerID=40&md5=fa81f7eeed32be89f227363f1f9bf372","Purpose: Accounting education is re-inventing itself as technology impacts the practical aspects of accounting in the real world and education tries to keep up. Big Data and data analytics have begun to influence elements of accounting including audit, accounting preparation, forensic accounting and general accountancy consulting. The purpose of this paper is to qualitatively analyse the current skills provision in accounting Masters courses linked to data analytics compared to academic and professional expectations of the same. Design/methodology/approach: The academic expectations and requirements of the profession, related to the impact of Big Data and data analytics on accounting education were reviewed and compared to the current provisions of this accounting education in the form of Masters programmes. The research uses an exploratory, qualitative approach with thematic analysis. Findings: Four themes were identified of the skills required for the effective use of Big Data and data analytics. These were: questioning and scepticism; critical thinking skills; understanding and ability to analyse and communicating results. Questioning and scepticism, as well as understanding and ability to analyse, were frequently cited explicitly as elements for assessment in various forms of accounting education in the Masters courses. However, critical thinking and communication skills were less explicitly cited in these accounting education programmes. Research limitations/implications: The research reviewed and compared current academic literature and the requirements of the professional accounting bodies with Masters programmes in accounting and data analytics. The research identified key themes relevant to the accounting profession that should be explicitly developed and assessed within accounting education for Big Data and data analytics at both university and professional levels. Further analysis of the in-depth curricula, as opposed to the explicitly stated topic coverage, could add to this body of research. Practical implications: This paper considers the potential combined role of professional qualification examinations and master’s degrees in skills provision for future practitioners in accounting and data analysis. This can be used to identify the areas in which accounting education can be further enhanced by focus or explicit mention of skills that are both developed and assessed within these programmes. Social implications: The paper considers the interaction between academic and professional practice in the areas of accounting education, highlighting skills and areas for development for students currently considering accounting education and data analytics. Originality/value: While current literature focusses on integrating data analysis into existing accounting and finance curricula, this paper considers the role of professional qualification examinations with Masters degrees as skills provision for future practitioners in accounting and data analysis. © 2021, Emerald Publishing Limited.","Accounting education; Audit; Big data; Data analytics; Future careers; Masters; Profession; Qualifications; Technology",,Article,Scopus,2-s2.0-85100536670
"Hamdam A., Jusoh R., Yahya Y., Abdul Jalil A., Zainal Abidin N.H.","57221694129;23481779200;57210390904;57211751659;57195636028;","Auditor judgment and decision-making in big data environment: a proposed research framework",2022,"Accounting Research Journal","35","1",,"55","70",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099866954&doi=10.1108%2fARJ-04-2020-0078&partnerID=40&md5=6a4bb9dc2ab96571c8673365fb02fc7f","Purpose: The role of big data and data analytics in the audit engagement process is evident. Notwithstanding, understanding how big data influences cognitive processes and, consequently, on the auditors’ judgment decision-making process is limited. The purpose of this paper is to present a conceptual framework on the cognitive process that may influence auditors’ judgment decision-making in the big data environment. The proposed framework predicts the relationships among data visualization integration, data processing modes, task complexity and auditors’ judgment decision-making. Design/methodology/approach: The methodology to accomplish the conceptual framework is based on a thorough literature review that consists of theoretical discussions and comparative studies of other authors’ works and thinking. It also involves summarizing and interpreting previous contributions subjectively and narratively and extending the work in some fashion. Based on this approach, this paper formulates four propositions about data visualization integration, data processing modes, task complexity and auditors’ judgment decision-making. The proposed framework was built from cognitive theory addressing how auditors process data into useful information to make judgment decision-making. Findings: The proposed framework expects that the cognitive process of data visualization integration and intuitive data processing mode will improve auditors’ judgment decision-making. This paper also contends that task complexity may influence the cognitive process of data visualization integration and processing modes because of the voluminous nature of data and the complexity of business processes. Hence, it is also expected that the relationships between data visualization integration and audit judgment decision-making and between processing mode and audit judgment decision-making will be moderated by task complexity. Research limitations/implications: There is a dearth of studies examining how big data and big data analytics affect auditors’ cognitive processes in making decisions. This paper will help researchers and auditors understand the behavioral consequences of data visualization integration and data processing mode in making judgment decision-making, given a certain level of task complexity. Originality/value: With the advent of big data and the evolution of innovative audit procedures, the constructed framework can be used as a theoretical foundation for future empirical studies concerning auditors’ judgment decision-making. It highlights the potential of big data to transform the nature and practice of accounting and auditing. © 2021, Emerald Publishing Limited.","Auditor judgment; Big data; Cognitive theory; Data processing mode; Data visualization; Task complexity",,Article,Scopus,2-s2.0-85099866954
"Basuony M.A.K., Mohamed E.K.A., Elragal A., Hussainey K.","56082906100;56082184400;36147145200;16444230900;","Big data analytics of corporate internet disclosures",2022,"Accounting Research Journal","35","1",,"4","20",,6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085003300&doi=10.1108%2fARJ-09-2019-0165&partnerID=40&md5=8f96a2584871743c7e8ee31adea6f512","Purpose: This study aims to investigate the extent and characteristics of corporate internet disclosure via companies’ websites as well via social media and networks sites in the four leading English-speaking stock markets, namely, Australia, Canada, the UK and the USA. Design/methodology/approach: A disclosure index comprising a set of items that encompasses two facets of online disclosure, namely, company websites and social media sites, is used. This paper adopts a data science approach to investigate corporate internet disclosure practices among top listed firms in Australia, Canada, the UK and the USA. Findings: The results reveal the underlying relations between the determining factors of corporate disclosure, i.e. profitability, leverage, liquidity and firm size. Profitability in its own has no great effect on the degree of corporate internet disclosure whether via company websites or social media sites. Liquidity has an impact on the degree of disclosure. Firm size and leverage appear to be the most important factors driving better disclosure via social media. American companies tend to be on the cutting edge of technology when it comes to corporate disclosure. Practical implications: This paper provides new insights into corporate internet disclosure that will benefit all stakeholders with an interest in corporate reporting. Social media is an influential means of communication that can enable corporate office to get instant feedback enhancing their decision-making process. Originality/value: To the best of the authors’ knowledge, this study is amongst few studies of corporate disclosure via social media platforms. This study has adopted disclosure index incorporating social media as well as applying data science approach in disclosure in an attempt to unfold how accounting could benefit from data science techniques. © 2020, Emerald Publishing Limited.","Australia; Big data; Canada; Corporate disclosure; Data science; Disclosure; Internet; Social media; UK; USA",,Article,Scopus,2-s2.0-85085003300
"Kataria S., Ravindran V.","57203209292;57432654900;","Harnessing of real-world data and real-world evidence using digital tools: utility and potential models in rheumatology practice",2022,"Rheumatology (Oxford, England)","61","2",,"502","513",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124433321&doi=10.1093%2frheumatology%2fkeab674&partnerID=40&md5=78b4f8bf0789a04d4f8feb97cf319a51","The diversity of diseases in rheumatology and variability in disease prevalence necessitates greater data parity in disease presentation, treatment responses including adverse events to drugs and various comorbidities. Randomized controlled trials are the gold standard for drug development and performance evaluation. However, when the drug is applied outside the controlled environment, the outcomes may differ in patient populations. In this context, the need to understand the macro and micro changes involved in disease evolution and progression becomes important and so is the need for harvesting and harnessing the real-world data from various resources to use them in generating real-world evidence. Digital tools with potential relevance to rheumatology can potentially be leveraged to obtain greater patient insights, greater information on disease progression and disease micro processes and even in the early diagnosis of diseases. Since the patients spend only a minuscule portion of their time in hospital or in a clinic, using modern digital tools to generate realistic, bias-proof, real-world data in a non-invasive patient-friendly manner becomes critical. In this review we have appraised different digital mediums and mechanisms for collecting real-world data and proposed digital care models for generating real-world evidence in rheumatology. © The Author(s) 2021. Published by Oxford University Press on behalf of the British Society for Rheumatology. All rights reserved. For permissions, please email: journals.permissions@oup.com.","artificial intelligence; big data; data analytics; electronic health records; machine learning",,Article,Scopus,2-s2.0-85124433321
"Sujatha K., Udayarani V.","57222667795;57195214713;","Deep restricted and additive homomorphic ElGamal privacy preservations over big healthcare data",2022,"International Journal of Intelligent Computing and Cybernetics","15","1",,"1","16",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113721819&doi=10.1108%2fIJICC-05-2021-0094&partnerID=40&md5=915454ee693095dfe84ed0f3341fe3df","Purpose: The purpose of this paper is to improve the privacy in healthcare datasets that hold sensitive information. Putting a stop to privacy divulgence and bestowing relevant information to legitimate users are at the same time said to be of differing goals. Also, the swift evolution of big data has put forward considerable ease to all chores of life. As far as the big data era is concerned, propagation and information sharing are said to be the two main facets. Despite several research works performed on these aspects, with the incremental nature of data, the likelihood of privacy leakage is also substantially expanded through various benefits availed of big data. Hence, safeguarding data privacy in a complicated environment has become a major setback. Design/methodology/approach: In this study, a method called deep restricted additive homomorphic ElGamal privacy preservation (DR-AHEPP) to preserve the privacy of data even in case of incremental data is proposed. An entropy-based differential privacy quasi identification and DR-AHEPP algorithms are designed, respectively, for obtaining privacy-preserved minimum falsified quasi-identifier set and computationally efficient privacy-preserved data. Findings: Analysis results using Diabetes 130-US hospitals illustrate that the proposed DR-AHEPP method is more significant in preserving privacy on incremental data than existing methods. A comparative analysis of state-of-the-art works with the objective to minimize information loss, false positive rate and execution time with higher accuracy is calibrated. Originality/value: The paper provides better performance using Diabetes 130-US hospitals for achieving high accuracy, low information loss and false positive rate. The result illustrates that the proposed method increases the accuracy by 4% and reduces the false positive rate and information loss by 25 and 35%, respectively, as compared to state-of-the-art works. © 2021, Emerald Publishing Limited.","Additive homomorphic ElGamal; Big data; Deep restricted; Entropy-based differential privacy; Privacy preservation; Quasi-identifier","Additives; Big data; Health care; Hospitals; Comparative analysis; Computationally efficient; Design/methodology/approach; Differential privacies; False positive rates; Information sharing; Privacy preservation; Sensitive informations; Privacy by design",Article,Scopus,2-s2.0-85113721819
"Akundi A., Euresti D., Luna S., Ankobiah W., Lopes A., Edinbarough I.","55858784600;57464229300;24490642900;57464179700;7201958464;57205010315;","State of Industry 5.0—Analysis and Identification of Current Research Trends",2022,"Applied System Innovation","5","1","27","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125142092&doi=10.3390%2fasi5010027&partnerID=40&md5=02b1206c0592b49efa52ea3d5f6982e4","The term Industry 4.0, coined to be the fourth industrial revolution, refers to a higher level of automation for operational productivity and efficiency by connecting virtual and physical worlds in an industry. With Industry 4.0 being unable to address and meet increased drive of personalization, the term Industry 5.0 was coined for addressing personalized manufacturing and empowering humans in manufacturing processes. The onset of the term Industry 5.0 is observed to have various views of how it is defined and what constitutes the reconciliation between humans and machines. This serves as the motivation of this paper in identifying and analyzing the various themes and research trends of what Industry 5.0 is using text mining tools and techniques. Toward this, the abstracts of 196 published papers based on the keyword “Industry 5.0” search in IEEE, science direct and MDPI data bases were extracted. Data cleaning and preprocessing were performed for further analysis to apply text mining techniques of key terms extraction and frequency analysis. Further topic mining i.e., unsupervised machine learning method was used for exploring the data. It is observed that the terms artificial intelligence (AI), big data, supply chain, digital transformation, machine learning, internet of things (IoT), are among the most often used and among several enablers that have been identified by researchers to drive Industry 5.0. Five major themes of Industry 5.0 addressing, supply chain evaluation and optimization, enterprise innovation and digitization, smart and sustainable manufacturing, transformation driven by IoT, AI, and Big Data, and Human-machine connectivity were classified among the published literature, highlighting the research themes that can be further explored. It is observed that the theme of Industry 5.0 as a gateway towards human machine connectivity and co-existence is gaining more interest among the research community in the recent years. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Artificial intelligence; Big data; Human-machine coexistence; Industry 5.0; Internet of things; Smart manufacturing",,Article,Scopus,2-s2.0-85125142092
"Gvishiani A.D., Dobrovolsky M.N., Dzeranov B.V., Dzeboev B.A.","6602466383;55802396900;56161025800;55803344200;","Big Data in Geophysics and Other Earth Sciences",2022,"Izvestiya, Physics of the Solid Earth","58","1",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124977240&doi=10.1134%2fS1069351322010037&partnerID=40&md5=863b0cfb5cdda3e1001156c10fd83a46","Abstract—The term “Big Data” has become very popular over the past decade. The frequency of its use in the research papers, reports, and broad press has been steadily increasing. This work describes the origin and development of the theory and practice of Big Data as a scientific discipline, outlines the main characteristics and methods for Big Data processing and analysis, discusses the formalism and family of Big Data V-characteristics, and presents the examples of the sources of the growing Big Data which have fundamental effect on the development of geophysics and related Earth sciences. The examples of the sources of Big Data in the Earth sciences are remote sensing, meteorology, geoecology (in terms of the global hierarchical network SMEAR (Stations Measuring Earth surfaces and Atmosphere Relations)), and seismic exploration. Besides, we discuss seismic monitoring data which can become Big Data when combined with other geophysical information and consider geomagnetic data which are not Big Data but nevertheless have a great scientific value. © 2022, Pleiades Publishing, Ltd.","Big Data; Earth sciences; Earth’s remote sensing; geoecology; geomagnetic observations; meteorological observations; seismic exploration; seismic monitoring",,Article,Scopus,2-s2.0-85124977240
"Jyothi R., Krishnamurthy G.N.","57459681300;57459814600;","An Efficient Multi-Objective Optimization-Based Framework for Stock Market Prediction",2022,"International Journal of Engineering Trends and Technology","70","2",,"294","304",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124936143&doi=10.14445%2f22315381%2fIJETT-V70I2P235&partnerID=40&md5=05dd4524466daeb07b2020f632c32a91","Stock prediction is an important parameter for all business applications to improve business deals. To analyze the reason for the available stocks, the stocks should be estimated in the primary section. Several statistical and neural models were implemented to meet these issues, but the data's complexity can still reduce the prediction outcomes. So, Long Short term memory (LSTM) has been introduced. However, the LSTM model has observed the worst results in some cases by consuming more time and less prediction accuracy. The current work has focused on designing a novel Artificial Bee and Buffalo-based Recurrent LSTM (AB-BRL) for the stock prediction framework to enhance the stock forecasting exactness score up to the desired level. In addition, to analyze the stability of the designed model, different error statistics were measured and compared with other models. Besides, the planned design is executed in the python environment. Finally, the novel AB-BRL has gained a good outcome by reducing the error percentage and maximizing the forecasting accuracy. © 2022 Seventh Sense Research Group®","Big Data; Hybrid Optimization; Mean Square Error; Stock Prediction; Stock Prediction Accuracy",,Article,Scopus,2-s2.0-85124936143
"Remoundou K., Alexakis T., Peppes N., Demestichas K., Adamopoulou E.","57216202579;57217833821;57217829833;15126722600;15126943800;","A Quality Control Methodology for Heterogeneous Vehicular Data Streams",2022,"Sensors","22","4","1550","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124907151&doi=10.3390%2fs22041550&partnerID=40&md5=9ab3b884f61bbf46f823ffda0f65610c","The rapid evolution of sensors and communication technologies has led to the production and transfer of mass data streams from vehicles either inside their electronic units or to the outside world using the internet infrastructure. The “outside world”, in most cases, consists of third-party applications, such as fleet or traffic management control centers, which utilize vehicular data for reporting and monitoring functionalities. Such applications, in most cases, in order to facilitate their needs, require the exchange and processing of vast amounts of data which can be handled by the so-called Big Data technologies. The purpose of this study is to present a hybrid platform suitable for data collection, storing and analysis enhanced with quality control actions. In particular, the collected data contain various formats originating from different vehicle sensors and are stored in the aforementioned platform in a continuous way. The stored data in this platform must be checked in order to determine and validate them in terms of quality. To do so, certain actions, such as missing values checks, format checks, range checks, etc., must be carried out. The results of the quality control functions are presented herein, and useful conclusions are drawn in order to avoid possible data quality problems which may occur in further analysis and use of the data, e.g., for training of artificial intelligence models. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; Data quality control; Data streams; Vehicle sensors","Big data; Fleet operations; Quality assurance; Vehicle to vehicle communications; Vehicles; Communicationtechnology; Control methodology; Data quality control; Data stream; Electronic units; Internet infrastructure; Mass data; Sensor technologies; Third party application (Apps); Vehicle sensors; Quality control",Article,Scopus,2-s2.0-85124907151
"Dokuz A.S.","46061138200;","Social velocity based spatio-temporal anomalous daily activity discovery of social media users",2022,"Applied Intelligence","52","3",,"2745","2762",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124793250&doi=10.1007%2fs10489-021-02535-8&partnerID=40&md5=6593668ccca93c643c595b165b98a435","Anomalous daily activities are the activities that do not fit into normal daily behavior of social media users. Discovering anomalous daily activities is important for protecting social media users from harmful content and providing correct information about populated accounts, products, or hashtags. However, discovering anomalous daily activities is challenging due to hardness of detection of bot applications, complexity of anomalous activities, and the big data nature of social media datasets. In this study, a novel method that discovers anomalous daily activities with respect to spatio-temporal information of social media datasets is proposed. For this purpose, an interest measure, named as social velocity, is proposed to discover anomalous daily activities that is based on spatial distance and temporal difference of successive posts. Two novel algorithms are proposed that use proposed method and interest measure and experimentally evaluated on a real Twitter dataset. The experimental results show that proposed algorithms are successful for discovering anomalous activities of social media users with respect to spatio-temporal information. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Anomalous daily activity discovery; Social media anomaly detection; Social media big data; Social velocity; Spatial social media mining; Twitter","Big data; Social networking (online); Activity discoveries; Anomalous daily activity discovery; Anomaly detection; Daily activity; Social media; Social media minings; Social medium anomaly detection; Social medium big data; Social velocity; Spatial social medium mining; Anomaly detection",Article,Scopus,2-s2.0-85124793250
"Gnap J., Jagelčák J., Marienka P., Frančák M., Vojteková M.","6505759592;23990511600;57210562527;55526934500;56968227900;","Global Assessment of Bridge Passage in Relation to Oversized and Excessive Transport: Case Study Intended for Slovakia",2022,"Applied Sciences (Switzerland)","12","4","1931","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124754471&doi=10.3390%2fapp12041931&partnerID=40&md5=3fe591846a23318b0e08653c9d349683","The development of an economy and, in particular, the construction of new infrastructure as well as industrial enterprises creates demand for the road transport of oversized freight that ex-ceeds the maximum permissible total mass of vehicle combinations with its share on the axles. Failure to comply with the defined technological processes and a deficiency in the assessment of per-mitting such forms of transportation can have a large adverse effect, predominantly on the lifetime of bridges in a road network, which can have international implications as well. There is no legisla-tion adopted by the EU Member States, which would at least partially unify the authorisation procedures of these forms of transportation and, therefore, it results in problems when crossing borders and leads to differences related to the assessment of bridge passages. If there is no systematic in-spection of this kind of transportation, it can lead to permanent damage of these bridges as well. Currently, and not only in Slovakia but also in other states, the assessment of bridge passage for certain routes is used for heavy and oversized transportation. It means that if we use 100 transports, 100 assessments of individual routes are needed, although some are the same routes or the same vehicles/vehicle combinations used for a number of transports. Thus, the authors designed a global assessment for bridge passage in relation to heavy and oversized road transport while verifying it in the conditions of the EU Member State from Central Europe–Slovakia. Roads are full of different types of vehicles/vehicle combinations for which the axle loads and distances of the axles (wheel-bases) are important. Thus, there were vehicle/vehicle combinations parameters (big data) ob-served, for which the routes relating to heavy and/or oversized transportation were assessed from 1 January 2016 to 31 December 2020 in Slovakia. The global assessment of bridge passage introduces an entirely new approach within the procedure for obtaining a special permission for road use as well as within transport use itself. Given the low presence of freight with an abnormal axle load or enormous total mass, it is appropriate to define the limited conditions under which it would be possible to implement the global assessment in practice as well. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Assessment of bridge passage; Big data; Cumulative axle load; Excessive transport; Oversized transport",,Article,Scopus,2-s2.0-85124754471
"Wang J., Shu T., Zhao W., Zhou J.","57193136786;57456122500;57455182100;57455336400;","Research on Chinese Consumers’ Attitudes Analysis of Big-Data Driven Price Discrimination Based on Machine Learning",2022,"Frontiers in Psychology","12",,"803212","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124750607&doi=10.3389%2ffpsyg.2021.803212&partnerID=40&md5=558613902a94ba8fe2431a2a211260b9","From the end of 2018 in China, the Big-data Driven Price Discrimination (BDPD) of online consumption raised public debate on social media. To study the consumers’ attitude about the BDPD, this study constructed a semantic recognition frame to deconstruct the Affection-Behavior-Cognition (ABC) consumer attitude theory using machine learning models inclusive of the Labeled Latent Dirichlet Allocation (LDA), Long Short-Term Memory (LSTM), and Snow Natural Language Processing (NLP), based on social media comments text dataset. Similar to the questionnaires published results, this article verified that 61% of consumers expressed negative sentiment toward BDPD in general. Differently, on a finer scale, this study further measured the negative sentiments that differ significantly among different topics. The measurement results show that the topics “Regular Customers Priced High” (69%) and “Usage Intention” (67%) occupy the top two places of negative sentiment among consumers, and the topic “Precision Marketing” (42%) is at the bottom. Moreover, semantic recognition results that 49% of consumers’ comments involve multiple topics, indicating that consumers have a pretty clear cognition of the complex status of the BDPD. Importantly, this study found some topics that had not been focused on in previous studies, such as more than 8% of consumers calling for government and legal departments to regulate BDPD behavior, which indicates that quite enough consumers are losing confidence in the self-discipline of the platform enterprises. Another interesting result is that consumers who pursue solutions to the BDPD belong to two mutually exclusive groups: government protection and self-protection. The significance of this study is that it reminds the e-commerce platforms to pay attention to the potential harm for consumers’ psychology while bringing additional profits through the BDPD. Otherwise, the negative consumer attitudes may cause damage to brand image, business reputation, and the sustainable development of the platforms themselves. It also provides the government supervision departments an advanced analysis method reference for more effective administration to protect social fairness. Copyright © 2022 Wang, Shu, Zhao and Zhou.","big data; consumer attitude analysis; Labeled LDA; LSTM; price discrimination; Snow NLP",,Article,Scopus,2-s2.0-85124750607
"Borovkov A.I., Bolsunovskaya M.V., Gintciak A.M.","8840090300;57191609119;57203897426;","Intelligent Data Analysis for Infection Spread Prediction",2022,"Sustainability (Switzerland)","14","4","1995","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124729151&doi=10.3390%2fsu14041995&partnerID=40&md5=ff8cea96f3b9fd174a005afafc43953b","Intelligent data analysis based on artificial intelligence and Big Data tools is widely used by the scientific community to overcome global challenges. One of these challenges is the worldwide coronavirus pandemic, which began in early 2020. Data science not only provides an opportunity to assess the impact caused by a pandemic, but also to predict the infection spread. In addition, the model expansion by economic, social, and infrastructural factors makes it possible to predict changes in all spheres of human activity in competitive epidemiological conditions. This article is devoted to the use of anonymized and personal data in predicting the coronavirus infection spread. The basic “Susceptible–Exposed–Infected–Recovered” model was extended by including a set of demographic, administrative, and social factors. The developed model is more predictive and applicable in assessing future pandemic impact. After a series of simulation experiment results, we concluded that personal data use in high-level modeling of the infection spread is excessive. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big Data; Data analysis; Infection spread; Personal data; Simulation modeling; System dynamics","artificial intelligence; COVID-19; economic analysis; epidemiological phenomena; human activity; modeling",Article,Scopus,2-s2.0-85124729151
"Chen Z.-G., Wu W.-P., Li J., Zeng Y.-H.","57211024187;57211025561;57456273000;57455331700;","Dynamic Supervision and Control of VOCs Emission From China’s Furniture Manufacturing Based on Big Data and IoT",2022,"Frontiers in Environmental Science","10",,"807216","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124723165&doi=10.3389%2ffenvs.2022.807216&partnerID=40&md5=d6b3d30381f96995c83a09810ead4df4","How to effectively monitor and prevent volatile organic compounds (VOCs) in furniture manufacturing has become the key to solving the dilemma between furniture industry development and public health. Based on the combination of VOCs emission standards and prevention policies of China’s furniture manufacturing, this article explores the path of dynamic monitoring, intelligent prevention, and control of VOCs embedded in furniture manufacturing with big data and IoT and puts forward feasible VOCs control technologies in the whole process of China’s furniture manufacturing, including source, process, and end. Herein, dynamic monitoring, intelligent prevention, and control of VOCs in furniture manufacturing have important reference significance for the prevention and control of VOCs in other industries, such as coal chemical industry, rubber products, engineering machinery, and coiled materials. Copyright © 2022 Chen, Wu, Li and Zeng.","big data; dynamic supervision; furniture manufacturing; IoT; VOCs emission",,Article,Scopus,2-s2.0-85124723165
"Choi H.","57455373100;","Wireless Technology: Tablets Provide Productivity Pluses Mobile devices improve access management and more for Korean company",2022,"Chemical Processing","84","2",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124718800&partnerID=40&md5=c133f2a6a31d96062faf4e0cd60e1ce8",[No abstract available],,"Digital devices; Access management; Digital platforms; Environment management system; Optimisations; Petrochemical producers; South Korea; Wireless technologies; Big data",Article,Scopus,2-s2.0-85124718800
"Jiang W., Luo J.","57188758602;57222138197;","Big Data for Traffic Estimation and Prediction: A Survey of Data and Tools",2022,"Applied System Innovation","5","1","23","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124627439&doi=10.3390%2fasi5010023&partnerID=40&md5=fd667dc8fb5e36482446ca53415394b8","Big data have been used widely in many areas, including the transportation industry. Using various data sources, traffic states can be well estimated and further predicted to improve the overall operation efficiency. Combined with this trend, this study presents an up-to-date survey of open data and big data tools used for traffic estimation and prediction. Different data types are categorized, and off-the-shelf tools are introduced. To further promote the use of big data for traffic estimation and prediction tasks, challenges and future directions are given for future studies. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; Call detail records; Census data; GPS trajectory data; Location-based service data; Open data; Public transport transaction data; Road sensor data; Survey data",,Article,Scopus,2-s2.0-85124627439
"Raju M.P., Laxmi A.J.","57217177485;6507199177;","BIG DATA DRIVEN HEMS TOWARDS DG INTEGRATION USING ML ALGORITHM BASED LMA",2022,"Journal of Engineering Science and Technology","17","1",,"798","819",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124606491&partnerID=40&md5=2767a4e22c7d3d399821b47630299d31","In Smart Grid (SG) environment, digitalization of micro grid to effectively balance the energy sources, loads and accommodating renewable Distributed Generation (DG) integration are few growing concerns. Active management of distributed network has provided numerous solutions to achieve DG integration. But, the advent of Information and Communication Technologies (ICT), Artificial Intelligence (AI) and data analytics has totally changed the paradigm of renewable DG integration. On the other hand, equipping the existing Home Energy Management System (HEMS) with the said ICT and AI based prediction abilities has equally grown. Towards that, HEMS with latest Demand Response (DR) and Demand Side Management (DSM) algorithms enhancing the penetration of renewable DGs into micro grid is also seen in the literature. To move a step further, this paper proposes Big Data driven HEMS to accommodate the said DG integration by implementing DSM based Load Management Algorithm (LMA) and Load Priority Assignment Algorithms (LPAA). LMA and LPAA used in this paper are designed using load priority and peak clipping Demand Side Management (DSM) techniques. Hourly Load Priority Table (LPT) and Hourly Threshold Power (PTh) required to run LMA are obtained from ML based predictions. Temperature (T) based LPAA is used to obtain hourly LPT, where T is predicted using ML based prediction model. And to obtain PTh, ML based total Load Demand (PT) predictions are used along with LPT resulted from LPAA. Four ML algorithms namely Linear Regression (LR), Ensemble Bagged Tree (EBT), SVR (Support Vector Regression), and GPR (Gaussian Process Regression) are trained with load and weather Big Data in order to make T and PT predictions. SIMULINK model of the Data driven HEMS is developed in MATLAB-SIMULINK environment using MATLAB 2018b. This model can predict the PT, PTh, LPT and CO2 emissions for any given date, time and weather parameters. Thus, size of the DGs for any given date can be predicted. A hardware model of HEMS is fabricated to demonstrate proposed AI based DG integration. Simulation and experimental results obtained are presented to showcase the ability of Big Data driven HEMS in achieving DG integration. Inferences drawn in terms of reduction in CO2 emissions resulted using proposed Big Data driven HEMS are also presented. © School of Engineering, Taylor’s University","Big data; Demand side management algorithms; DG integration; Home energy management system; Machine learning algorithms",,Article,Scopus,2-s2.0-85124606491
"Wei X., Ye M., Yuan L., Bi W., Lu W.","57452264800;57006013900;57221944503;57353366600;24173836000;","Analyzing the Freight Characteristics and Carbon Emission of Construction Waste Hauling Trucks: Big Data Analytics of Hong Kong",2022,"International Journal of Environmental Research and Public Health","19","4","2318","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124596298&doi=10.3390%2fijerph19042318&partnerID=40&md5=c7fe77dde5b18f6184d69a6d6b1b1131","Unlike their counterparts that are used for container or municipal solid waste hauling, or their peers of taxies and other commercial vehicles, construction waste hauling trucks (CWHTs) are heterogeneous in that they transport construction waste from construction sites to designated disposal facilities. Depending on the intensity of the construction activities, there are many CWHTs in operation, imposing massive impacts on a region’s transportation system and natural environment. However, such impacts have rarely been documented. This paper has analyzed CWHTs’ freight characteristics and their carbon emission by harnessing a big dataset of 112,942 construction waste transport trips in Hong Kong in May 2015. It has been observed that CWHTs generate 4,544 daily trips with 307.64 tons CO2-eq emitted on working days, and 553 daily trips emitting 28.78 tons CO2-eq on non-working days. Freight carbon emission has been found to be related to the vehicle type, transporting weight, and trip length, while the trip length is the most influential metric to carbon emission. This research contributes to the understanding of freight characteristics by exploiting a valuable big dataset and providing important benchmarking metrics for monitoring the effectiveness of policy interventions related to construction waste transportation planning and carbon emission. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; Carbon emission; Construction waste hauling trucks; Freight characteristics","article; benchmarking; big data; carbon dioxide equivalent; carbon emission; construction and demolition waste; Hong Kong",Article,Scopus,2-s2.0-85124596298
"Sarkar M., Roy A., Agrebi M., Alqaheri H.","56294873100;57198515510;56126786400;57202387986;","Exploring New Vista of Intelligent Recommendation Framework for Tourism Industries: An Itinerary through Big Data Paradigm",2022,"Information (Switzerland)","13","2","70","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124482191&doi=10.3390%2finfo13020070&partnerID=40&md5=5a571fb790f26978faa1c376d1dfcb7d","Big Data is changing how organizations conduct operations. Data are assembled from multiple points of view through online quests, investigation of purchaser purchasing conduct, and then some, and industries utilize it to improve their net revenue and give an overall better experience to clients. Each of these organizations must figure out how to improve the general client experience and meet every client’s novel necessities, and big data helps with this cycle. Through the utilization and reviews of Big Data, travel industry organizations can study the inclinations of more modest portions of their intended interest group or even about people in some cases. In this paper, a Crow Search Optimization-based Hybrid Recommendation Model is proposed to get accurate suggestions based on clients’ preferences. The hybrid recommendation is performed by combining collaborative filtering and content-based filtering. As a result, the advantages of collaborative filtering and content-based filtering are utilized. Moreover, the intelligent behavior of Crows’ assists the proper selection of neighbors, rating prediction, and in-depth analysis of the contents. Accordingly, an optimized recommendation is always provided to the target users. Finally, performance of the proposed model is tested using the TripAdvisor dataset. The experimental results reveal that the model provides 58%, 58.5%, 27%, 24.5%, and 25.5% better Mean Absolute Error, Root Mean Square Error, Precision, Recall, and F-Measure, respectively, compared to similar algorithms. © 2022 by the authors.","Big Data; Crow search algorithm; Hybrid recommendation; Statistical analysis; Tourism industries","Collaborative filtering; Mean square error; Recommender systems; Content based filtering; Crow search algorithm; Hybrid recommendation; Intelligent behavior; Interest groups; Multiple points; Net revenue; Search Algorithms; Search optimization; Tourism industry; Big data",Article,Scopus,2-s2.0-85124482191
"Morán-Reyes A.A.","36185950700;","Towards an ethical framework about Big Data era: metaethical, normative ethical and hermeneutical approaches",2022,"Heliyon","8","2","e08926","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124454850&doi=10.1016%2fj.heliyon.2022.e08926&partnerID=40&md5=f5be2f670d48a04139da62652ce1ca54","The main ethical challenges that arise for Information Sciences (with the daily use in different areas of Big Data applications) are not about the reliability of its professionals to carry out tasks in the organization area in an impartial way or about the obligation to train themselves technologically in the area of Data Science. The most important problems are related to the concept of moral responsibility, especially from a metaethical perspective, in line with the reflection of the implementation of technology with respect to human autonomy. In this paper it is stated that the challenges of Big Data go beyond the individual spectrum of responsibility of a professional in Information Sciences (specifically, due to the negative social consequences), so that the changes brought about by massive data sets are essentially problems of a group ethics, so they require approaches from the theoretical postulates of these disciplines. In addition to this, the moral challenges in dealing with Big Data are usually approached from applied ethics (such as information ethics), but in this article it will be approached as a problem of metaethics and normative ethics (as a foundation for its application in professional codes), and also from some ideas of digital hermeneutics and the philosophy of technology. © 2022 The Author(s)","Big Data; Disruptive technology; Information ethics; Interdisciplinarity; Metaethics",,Article,Scopus,2-s2.0-85124454850
"Rutherford S., Fraza C., Dinga R., Kia S.M., Wolfers T., Zabihi M., Berthet P., Worker A., Verdi S., Andrews D., Han L.K., Bayer J.M., Dazzan P., McGuire P., Mocking R.T., Schene A., Sripada C., Tso I.F., Duval E.R., Chang S.-E., Penninx B.W., Heitzeg M.M., Burt S.A., Hyde L.W., Amaral D., Wu Nordahl C., Andreasssen O.A., Westlye L.T., Zahn R., Ruhe H.G., Beckmann C., Marquand A.F.","57201995341;57224123405;57204581268;55904731100;56801110700;57206663940;57225405594;56433382400;57448747300;56534600600;56573181400;57201549031;6602196376;7101880438;57265032900;7004533788;8764377600;57265225300;55408408600;57448705300;55800249300;6507774116;55700682700;7006815112;7007019852;57225036209;55642479500;14068210200;7102439467;9037349900;57338273100;36914890300;","Charting brain growth and aging at high spatial precision",2022,"eLife","11",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124444210&doi=10.7554%2feLife.72904&partnerID=40&md5=72defc96ab73106ab74af1678b61d3f9","Defining reference models for population variation, and the ability to study individual deviations is essential for understanding inter-individual variability and its relation to the onset and progression of medical conditions. In this work, we assembled a reference cohort of neuroimaging data from 82 sites (N=58,836; ages 2-100) and used normative modeling to characterize lifespan trajectories of cortical thickness and subcortical volume. Models are validated against a manually quality checked subset (N=24,354) and we provide an interface for transferring to new data sources. We showcase the clinical value by applying the models to a transdiagnostic psychiatric sample (N=1985), showing they can be used to quantify variability underlying multiple disorders whilst also refining case-control inferences. These models will be augmented with additional samples and imaging modalities as they become available. This provides a common reference platform to bind results from different studies and ultimately paves the way for personalized clinical decision-making. © 2022, Rutherford et al.","big data; brain chart; growth chart; human; individual prediction; lifespan; neuroscience; normative model","aging; article; brain growth; clinical decision making; cohort analysis; controlled study; lifespan; major clinical study; neuroimaging; neuroscience; thickness",Article,Scopus,2-s2.0-85124444210
"Siegismund D., Fassler M., Heyse S., Steigele S.","57200315711;35080302500;6701749354;8948929400;","Benchmarking feature selection methods for compressing image information in high-content screening",2022,"SLAS technology","27","1",,"85","93",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124438235&doi=10.1016%2fj.slast.2021.10.015&partnerID=40&md5=1e60006984ed4f8e10f36a5074d05916","Biopharmaceutical drug discovery, as of today is a highly automated, high throughput endeavor, where many screening technologies produce a high-dimensional measurement per sample. A striking example is High Content Screening (HCS), which utilizes automated microscopy to systematically access the wealth of information contained in biological assays. Exploiting HCS to its full potential traditionally requires extracting a high number of features from the images to capture as much information as possible, then performing algorithmic analysis and complex data visualization in order to render this high-dimensional data into an interpretable and instructive information for guiding drug development. In this process, automated feature selection methods condense the feature set to reduce non-useful or redundant information and render it more meaningful. We compare 12 state-of-the-art feature selection methods (both supervised and unsupervised) by systematically testing them on two HCS datasets from drug screening imaging assays of high practical relevance. Considering as evaluation metrics standard plate-, assay- or compound statistics on the final results, we assess the generalizability and importance of the selected features by use of automated machine learning (AutoML) to achieve an unbiased evaluation across methods. Results provide practical guidance on experiment design, optimal sizing of a reduced feature set and choice of feature selection method, both in situations where useful experimental control states are available (enabling use of supervised algorithms) or where such controls are unavailable, using unsupervised techniques. Copyright © 2021. Published by Elsevier Inc.","Automated machine learning; Big data; Drug discovery; Feature selection; High-content screening",,Article,Scopus,2-s2.0-85124438235
"Zheng L., Zhang L., Chen K., He Q.","57193029758;57250035300;57211364148;56368227900;","Unmasking unexpected health care inequalities in China using urban big data: Service-rich and service-poor communities",2022,"PLoS ONE","17","2 February","e0263577","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124403175&doi=10.1371%2fjournal.pone.0263577&partnerID=40&md5=be13697fb2816977a451af3ebb63646f","Geographic accessibility plays a key role in health care inequality but remains insufficiently investigated in China, primarily due to the lack of accurate, broad-coverage data on supply and demand. In this paper, we employ an innovative approach to local supply-and-demand conditions to (1) reveal the status quo of the distribution of health care provision and (2) examine whether individual households from communities with different housing prices can acquire equal and adequate quality health care services within and across 361 cities in China. Our findings support previous conclusions that quality hospitals are concentrated in cities with high administrative rankings and developmental levels. However, after accounting for the population size an “accessible” hospital serves, we discern “pro-poor” inequality in accessibility to care (denoted as GAPSD) and that GAPSD decreases along with increases in administrative rankings of cities and in community ratings. This paper is significant for both research and policy-making. Our approach successfully reveals an “unexpected” pattern of health care inequality that has not been reported before, and our findings provide a nationwide, detailed benchmark that facilitates the assessment of health and urban policies, as well as associated policy-making. © 2022 Zheng et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"article; big data; China; city; health care quality; household; housing; management; population size; price",Article,Scopus,2-s2.0-85124403175
"Pellizzoni P., Pietracaprina A., Pucci G.","57219268445;6701396337;7004379817;","k-Center Clustering with Outliers in Sliding Windows",2022,"Algorithms","15","2","52","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124087905&doi=10.3390%2fa15020052&partnerID=40&md5=cb6cbb1b64b09fb05877e3e169dd071a","Metric k-center clustering is a fundamental unsupervised learning primitive. Although widely used, this primitive is heavily affected by noise in the data, so a more sensible variant seeks for the best solution that disregards a given number z of points of the dataset, which are called outliers. We provide efficient algorithms for this important variant in the streaming model under the sliding window setting, where, at each time step, the dataset to be clustered is the window W of the most recent data items. For general metric spaces, our algorithms achieve O(1) approximation and, remarkably, require a working memory linear in k + z and only logarithmic in |W|. For spaces of bounded doubling dimension, the approximation can be made arbitrarily close to 3. For these latter spaces, we show, as a by-product, how to estimate the effective diameter of the window W, which is a measure of the spread of the window points, disregarding a given fraction of noisy distances. We also provide experimental evidence of the practical viability of the improved clustering and diameter estimation algorithms. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Approximation algorithms; Big data; Coreset; Data stream model; Doubling dimension; Effective diameter; K-center with outliers; Sliding windows","Big data; Clustering algorithms; Decoding; Statistics; Clusterings; Core set; Data stream model; Doubling dimensions; Effective diameter; K-center; K-center with outlier; Sliding Window; Streaming model; Window settings; Approximation algorithms",Article,Scopus,2-s2.0-85124087905
"Munjal R., Liu W., Li X., Gutierrez J., Chong P.H.J.","57200437872;56962720400;56046929300;7401653270;7102970085;","Multi-Attribute Decision Making for Energy-Efficient Public Transport Network Selection in Smart Cities",2022,"Future Internet","14","2","42","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124085345&doi=10.3390%2ffi14020042&partnerID=40&md5=0cb360c8d3739b651fd22f5939fcb690","Smart cities use many smart devices to facilitate the well-being of society by different means. However, these smart devices create great challenges, such as energy consumption and carbon emissions. The proposed research lies in communication technologies to deal with big data-driven applications. Aiming at multiple sources of big data in a smart city, we propose a public transport-assisted data-dissemination system to utilize public transport as another communication medium, along with other networks, with the help of software-defined technology. Our main objective is to minimize energy consumption with the maximum delivery of data. A multi-attribute decision-making strategy is adopted for the selction of the best network among wired, wireless, and public transport networks, based upon users’ requirements and different services. Once public transport is selected as the best network, the Capacitated Vehicle Routing Problem (CVRP) will be implemented to offload data onto buses as per the maximum capacity of buses. For validation, the case of Auckland Transport is used to offload data onto buses for energy-efficient delay-tolerant data transmission. Experimental results show that buses can be utilized efficiently to deliver data as per their demands and consume 33% less energy in comparison to other networks. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; Delay-tolerant network (DTN); Energy consumption; Multi-attribute decision making; Public trans-port","Big data; Buses; Decision making; Delay tolerant networks; Energy efficiency; Power management (telecommunication); Smart city; Vehicle routing; Delay-tolerant network; Energy efficient; Energy-consumption; Multi attribute decision making; Network selection; Public trans-port; Public transport; Public transport networks; Smart devices; Energy utilization",Article,Scopus,2-s2.0-85124085345
"Azeroual O., Nikiforova A.","57201378256;57204202481;","Apache Spark and MLlib-Based Intrusion Detection System or How the Big Data Technologies Can Secure the Data",2022,"Information (Switzerland)","13","2","58","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124074936&doi=10.3390%2finfo13020058&partnerID=40&md5=67482bf7b557ecf35730313a6f820681","Since the turn of the millennium, the volume of data has increased significantly in both industries and scientific institutions. The processing of these volumes and variety of data we are dealing with are unlikely to be accomplished with conventional software solutions. Thus, new technologies belonging to the big data processing area, able to distribute and process data in a scalable way, are integrated into classical Business Intelligence (BI) systems or replace them. Furthermore, we can benefit from big data technologies to gain knowledge about security, which can be obtained from massive databases. The paper presents a security-relevant data analysis based on the big data analytics engine Apache Spark. A prototype intrusion detection system is developed aimed at detecting data anomalies through machine learning by using the k-means algorithm for clustering analysis implemented in Sparks MLlib. The extraction of features to detect anomalies is currently challenging because the problem of detecting anomalies is not actively and exhaustively monitored. The detection of abnormal data can be effectuated by using relevant data that are already in companies’ and scientific organizations’ possession. Their interpretation and further processing in a continuous manner can sufficiently contribute to anomaly and intrusion detection. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; Clustering; Data anomaly; IDS; Intrusion; Intrusion detection; K-means; Machine learning; Security","Big data; Computer crime; Data Analytics; K-means clustering; Machine learning; Network security; Clusterings; Data anomalies; Data technologies; IDS; Intrusion; Intrusion Detection Systems; Intrusion-Detection; K-means; Scientific institutions; Security; Intrusion detection",Article,Scopus,2-s2.0-85124074936
"Brackenridge R.E., Demyanov V., Vashutin O., Nigmatullin R.","35791171100;6701452362;57439499500;57142025100;","Improving Subsurface Characterisation with ‘Big Data’ Mining and Machine Learning",2022,"Energies","15","3","1070","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124023265&doi=10.3390%2fen15031070&partnerID=40&md5=3f7860ad2efaf89502e4e584308e5107","Large databases of legacy hydrocarbon reservoir and well data provide an opportunity to use modern data mining techniques to improve our understanding of the subsurface in the presence of uncertainty and improve predictability of reservoir properties. A data mining approach provides a way to screen dependencies in reservoir and fluid data and enable subsurface specialists to estimate absent properties in partial or incomplete datasets. This allows for uncertainty to be managed and reduced. An improvement in reservoir characterisation using machine learning results from the capacity of machine learning methods to detect and model hidden dependencies in large multivariate datasets with noisy and missing data. This study presents a workflow applied to a large basin‐scale reservoir characterization database. The study aims to understand the dependencies between reservoir attributes in order to allow for predictions to be made to improve the data coverage. The machine learning workflow comprises the following steps: (i) exploratory data analysis; (ii) detection of outliers and data partitioning into groups showing similar trends using clustering; (iii) identification of dependencies within reservoir data in multivariate feature space with self‐organising maps; and (iv) feature selection using supervised learning to identify relevant properties to use for predictions where data are absent. This workflow provides an opportunity to reduce the cost and increase accuracy of hydrocarbon exploration and production in mature basins. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; Hydrocarbon exploration; Machine learning; Multivariant analysis; Reservoir; Subsurface characterisation; Supervised learning; Unsupervised learning","Data mining; Hydrocarbons; Large dataset; Multivariant analysis; Petroleum prospecting; Hydrocarbon exploration; Hydrocarbon reservoir; Large database; Machine-learning; Property; Reservoir characterization; Reservoir data; Subsurface characterizations; Uncertainty; Work-flows; Supervised learning",Article,Scopus,2-s2.0-85124023265
"Pettit C.J., Leao S.Z., Lock O., Ng M., Reades J.","7003759636;7003834636;57205543378;57219342060;18936742300;","Big Data: The Engine to Future Cities—A Reflective Case Study in Urban Transport",2022,"Sustainability (Switzerland)","14","3","1727","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124010393&doi=10.3390%2fsu14031727&partnerID=40&md5=360bbb67b1ea8470af57d91282fc156b","In an era of smart cities, artificial intelligence and machine learning, data is purported to be the ‘new oil’, fuelling increasingly complex analytics and assisting us to craft and invent future cities. This paper outlines the role of what we know today as big data in understanding the city and includes a summary of its evolution. Through a critical reflective case study approach, the research examines the application of urban transport big data for informing planning of the city of Sydney. Specifically, transport smart card data, with its diverse constraints, was used to understand mobility patterns through the lens of the 30 min city concept. The paper concludes by offering reflections on the opportunities and challenges of big data and the promise it holds in supporting data-driven approaches to planning future cities. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; Dashboards; Data analytics smart cities; Urban modelling; Visualisation","artificial intelligence; data set; future prospect; machine learning; urban area; urban transport; Australia; New South Wales; Sydney [New South Wales]",Article,Scopus,2-s2.0-85124010393
"Muzhoffar D.A.F., Hamada K., Wada Y., Miyake Y., Kawamura S.","57219451999;55725791900;57194728927;57439499300;57438983400;","Basic Ship-Planning Support System Using Big Data in Maritime Logistics for Simulating Demand Generation",2022,"Journal of Marine Science and Engineering","10","2","186","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124004405&doi=10.3390%2fjmse10020186&partnerID=40&md5=55b16c1937fcd5a78dc940a363887e8f","Dynamic changes in the global market demand affect ship development. Correspond-ingly, big data have provided the ability to comprehend the current and future conditions in nu-merous sectors and understand the dynamic circumstances of the maritime industry. Therefore, we have developed a basic ship-planning support system utilizing big data in maritime logistics. Pre-vious studies have used a ship allocation algorithm, which only considered the ship cost (COST) along limited target routes; by contrast, in this study, a basic ship-planning support system is reinforced with particularized COST attributes and greenhouse gas (GHG) features incorporated into a ship allocation algorithm related to the International Maritime Organization GHG reduction strat-egy. Additionally, this system is expanded to a worldwide shipping area. Thus, we optimize the operation-level ship allocation using the existing ships by considering the COST and GHG emis-sions. Finally, the ship specifications demanded worldwide are ascertained by inputting the new ships instance. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; GHG emissions; Greedy algorithm; Maritime logistics; Ship allocation",,Article,Scopus,2-s2.0-85124004405
"Dixon S., Keshavamurthy R., Farber D.H., Stevens A., Pazdernik K.T., Charles L.E.","57439324800;57207696583;57439671200;56978716400;56078896600;57223054827;","A Comparison of Infectious Disease Forecasting Methods across Locations, Diseases, and Time",2022,"Pathogens","11","2","185","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123995300&doi=10.3390%2fpathogens11020185&partnerID=40&md5=6edaeab13f8ac0bed8828a6b9532ba8c","Accurate infectious disease forecasting can inform efforts to prevent outbreaks and mitigate adverse impacts. This study compares the performance of statistical, machine learning (ML), and deep learning (DL) approaches in forecasting infectious disease incidences across different countries and time intervals. We forecasted three diverse diseases: campylobacteriosis, typhoid, and Q-fever, using a wide variety of features (n = 46) from public datasets, e.g., landscape, climate, and socioeconomic factors. We compared autoregressive statistical models to two tree-based ML models (extreme gradient boosted trees [XGB] and random forest [RF]) and two DL models (multi-layer perceptron and encoder–decoder model). The disease models were trained on data from seven different countries at the region-level between 2009–2017. Forecasting performance of all models was assessed using mean absolute error, root mean square error, and Poisson deviance across Australia, Israel, and the United States for the months of January through August of 2018. The overall model results were compared across diseases as well as various data splits, including country, regions with highest and lowest cases, and the forecasted months out (i.e., nowcasting, short-term, and long-term forecasting). Overall, the XGB models performed the best for all diseases and, in general, tree-based ML models performed the best when looking at data splits. There were a few instances where the statistical or DL models had minutely smaller error metrics for specific subsets of typhoid, which is a disease with very low case counts. Feature importance per disease was measured by using four tree-based ML models (i.e., XGB and RF with and without region name as a feature). The most important feature groups included previous case counts, region name, population counts and density, mortality causes of neonatal to under 5 years of age, sanitation factors, and elevation. This study demonstrates the power of ML approaches to incorporate a wide range of factors to forecast various diseases, regardless of location, more accurately than traditional statistical approaches. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; Campylobacteriosis; Deep learning; GLARMA; Infectious disease forecasting; Machine learning; Multi-feature fusion; Prediction; Q-fever; Typhoid","algorithm; antibiotic resistance; Article; artificial neural network; campylobacteriosis; cause of death; climate change; comparative study; decision making; decision tree; deep learning; disease surveillance; DNA extraction; Echinococcus granulosus; forecasting; geographic distribution; health care personnel; hospitalization; human; infection; intermethod comparison; learning algorithm; machine learning; morbidity; mortality; nonhuman; population density; prediction; prevalence; Q fever; random forest; risk factor; sanitation; sea surface temperature; seasonal variation; socioeconomics; time series analysis; typhoid fever",Article,Scopus,2-s2.0-85123995300
"Gusc J., Bosma P., Jarka S., Biernat-Jarka A.","40761449800;57439466600;57224094704;57224081507;","The Big Data, Artificial Intelligence, and Blockchain in True Cost Accounting for Energy Transition in Europe",2022,"Energies","15","3","1089","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123992812&doi=10.3390%2fen15031089&partnerID=40&md5=3bde6007302538b4847f71680bd684f3","The current energy prices do not include the environmental, social, and economic short and long-term external effects. There is a gap in the literature on the decision-making model for the energy transition. True Cost Accounting (TCA) is an accounting management model supporting the decision-making process. This study investigates the challenges and explores how big data, AI, or blockchain could ease the TCA calculation and indirectly contribute to the transition towards more sustainable energy production. The research question addressed is: How can IT help TCA applications in the energy sector in Europe? The study uses qualitative interpretive methodology and is performed in the Netherlands, Germany, and Poland. The findings indicate the technical feasibilities of a big data infrastructure to cope with TCA challenges. The study contributes to the literature by identifying the challenges in TCA application for energy production, showing the readiness potential for big data, AI, and blockchain to tackle them, revealing the need for cooperation between accounting and technical disciplines to enable the energy transition. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","AI; Big data; Blockchain; Energy production; Sustainability; True Cost Accounting","Big data; Blockchain; Cost accounting; Costs; Decision making; Accounting IS; Block-chain; Current energy; Decision-making modeling; Decision-making process; Energy prices; Energy productions; Energy transitions; Management Model; True cost accounting; Sustainable development",Article,Scopus,2-s2.0-85123992812
"Kharel T.P., Ashworth A.J., Owens P.R.","49963643500;56047858300;14219522600;","Linking and Sharing Technology: Partnerships for Data Innovations for Management of Agricultural Big Data",2022,"Data","7","2","12","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123921674&doi=10.3390%2fdata7020012&partnerID=40&md5=c8427eacd36c1f1f781a52ef6f690ced","Combining data into a centralized, searchable, and linked platform will provide a data exploration platform to agricultural stakeholders and researchers for better agricultural decision making, thus fully utilizing existing data and preventing redundant research. Such a data repository requires readiness to share data, knowledge, and skillsets and working with Big Data infrastructures. With the adoption of new technologies and increased data collection, agricultural workforces need to update their knowledge, skills, and abilities. The partnerships for data innovation (PDI) effort integrates agricultural data by efficiently capturing them from field, lab, and greenhouse studies using a variety of sensors, tools, and apps and provides a quick visualization and summary of statistics for real-time decision making. This paper aims to evaluate and provide examples of case studies currently using PDI and use its long-term continental US database (18 locations and 24 years) to test the cover crop and grazing effects on soil organic carbon (SOC) storage. The results show that legume and rye (Secale cereale L.) cover crops increased SOC storage by 36% and 50%, respectively, compared with oat (Avena sativa L.) and rye mixtures and low and high grazing intensities improving the upper SOC by 69–72% compared with a medium grazing intensity. This was likely due to legumes providing a more favorable substrate for SOC formation and high grazing intensity systems having continuous manure deposition. Overall, PDI can be used to democratize data regionally and nationally and therefore can address large-scale research questions aimed at addressing agricultural grand challenges. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big Data; Precision agriculture; Remote sensing; Spatial; Temporal",,Article,Scopus,2-s2.0-85123921674
"Hayajneh J.A.M., Elayan M.B.H., Abdellatif M.A.M., Abubakar A.M.","55758830400;57204914324;57432953500;57193113146;","Impact of business analytics and π-shaped skills on innovative performance: Findings from PLS-SEM and fsQCA",2022,"Technology in Society","68",,"101914","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123760126&doi=10.1016%2fj.techsoc.2022.101914&partnerID=40&md5=731aaf4138ccbc3240d70f3f519ba5ec","This paper proposes that the relationship between business analytics and innovative performance is somewhat more complex than originally thought, as firms still struggle to leverage the benefits of business analytics and artificial intelligence capabilities. To expand on the scholarship in this area of knowledge, our paper cross-fertilizes the literature by amalgamating business analytics capabilities with π-shaped skills. We draw on resource orchestration theory to examine the effects of business analytics and π-shaped skills on a firm's innovative performance, and the moderating role of π-shaped skills. Field data (n = 450) were obtained from individuals with supervisory positions in large Saudi firms and SMEs and analyzed with PLS-SEM and fsQCA techniques. PLS-SEM results reveal that business analytics and π-shaped skills are relevant antecedents for innovative performance. However, the expected moderating role of π-shaped skills on the relationship between business analytics and innovative performance did not hold. FsQCA results reveal that business analytics and π-shaped skills are sufficient but not necessary conditions for high innovative performance. This paper contributes not only to empirical evidence, but also to theory by furthering our understanding of the emergent π-shaped skills concept. Our findings echo the need to expand inquiry into business analytics and skill sets capabilities for better innovative outputs. Implications for theory and practice are discussed. © 2022 Elsevier Ltd","Big data; Business analytics; Innovative performance; Saudi vision 2030; π-shaped skills","Advanced Analytics; Data Analytics; Information management; Business analytics; Business skills; Condition; Field data; Innovative performance; Saudi vision 2030; Skill sets; Theory and practice; Π-shaped skill; Big data",Article,Scopus,2-s2.0-85123760126
"Zhang Q., Yang L.T., Chen Z., Li P.","56017374400;57203323020;56020772800;57214069572;","PPHOPCM: Privacy-Preserving High-Order Possibilistic c-Means Algorithm for Big Data Clustering with Cloud Computing",2022,"IEEE Transactions on Big Data","8","1",,"25","34",,46,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123739288&doi=10.1109%2fTBDATA.2017.2701816&partnerID=40&md5=9fa6f36165faca6950573eed1a5e5b07","As one important technique of fuzzy clustering in data mining and pattern recognition, the possibilistic c-means algorithm (PCM) has been widely used in image analysis and knowledge discovery. However, it is difficult for PCM to produce a good result for clustering big data, especially for heterogenous data, since it is initially designed for only small structured dataset. To tackle this problem, the paper proposes a high-order PCM algorithm (HOPCM) for big data clustering by optimizing the objective function in the tensor space. Further, we design a distributed HOPCM method based on MapReduce for very large amounts of heterogeneous data. Finally, we devise a privacy-preserving HOPCM algorithm (PPHOPCM) to protect the private data on cloud by applying the BGV encryption scheme to HOPCM, In PPHOPCM, the functions for updating the membership matrix and clustering centers are approximated as polynomial functions to support the secure computing of the BGV scheme. Experimental results indicate that PPHOPCM can effectively cluster a large number of heterogeneous data using cloud computing without disclosure of private data. © 2015 IEEE.","Big data clustering; Cloud computing; Possibilistic c-means; Privacy preserving; Tensor space","Big data; Cluster analysis; Cluster computing; Clustering algorithms; Data mining; Fuzzy clustering; Pattern recognition; Privacy-preserving techniques; Tensors; Big data clustering; C-means algorithms; Cloud-computing; Heterogeneous data; High-order; Higher-order; Possibilistic C-means; Privacy preserving; Private data; Tensor spaces; Cloud computing",Article,Scopus,2-s2.0-85123739288
"Shen J., Wang C., Castiglione A., Liu D., Esposito C.","55964982500;56582853800;16027997000;57148794600;57213839597;","Trustworthiness Evaluation-Based Routing Protocol for Incompletely Predictable Vehicular Ad Hoc Networks",2022,"IEEE Transactions on Big Data","8","1",,"48","59",,6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123729682&doi=10.1109%2fTBDATA.2017.2710347&partnerID=40&md5=5ca5d8004f25c096e4559f153271ced1","Incompletely predictable vehicular ad hoc networks is a type of networks where vehicles move in a certain range or just in a particular tendency, which is very similar to some circumstances in reality. However, how to route in such type of networks more efficiently according to the node motion characteristics and related historical big data is still an open issue. In this paper, we propose a novel routing protocol named trustworthiness evaluation-based routing protocol (TERP). In our protocol, trustworthiness of each individual is calculated by the cloud depending on the attribute parameters uploaded by the corresponding vehicle. In addition, according to the trustworthiness provided by the cloud, vehicles in the network choose reliable forward nodes and complete the entire route. The analysis shows that our protocol can effectively improve the fairness of the trustworthiness judgement. In the simulation, our protocol has a good performance in terms of the packet delivery ratio, normalized routing overhead and average end-to-end delay. © 2015 IEEE.","Incompletely predicable networks; Routing protocol; Self-configured networks; Trustworthiness evaluation; VANETs","Big data; Internet protocols; Vehicles; Vehicular ad hoc networks; Incompletely predicable network; Motion characteristics; Packet delivery ratio; Performance; Protocol cans; Routing-protocol; Self-configured network; Trustworthiness evaluations; VANET; Vehicular Adhoc Networks (VANETs); Routing protocols",Article,Scopus,2-s2.0-85123729682
"Huang C., Min G., Wu Y., Ying Y., Pei K., Xiang Z.","55522928700;7102053498;23092568400;13905725500;57204879392;57431601300;","Time Series Anomaly Detection for Trustworthy Services in Cloud Computing Systems",2022,"IEEE Transactions on Big Data","8","1",,"60","72",,22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123706186&doi=10.1109%2fTBDATA.2017.2711039&partnerID=40&md5=e7fe4b4ec33d318019fbe6b3fa2ae16c","As a powerful architecture for large-scale computation, cloud computing has revolutionized the way that computing infrastructure is abstracted and utilized. Coupled with the challenges caused by Big Data, the rocketing development of cloud computing boosts the complexity of system management and maintenance, resulting in weakened trustworthiness of cloud services. To cope with this problem, a compelling method, i.e., Support Vector Data Description (SVDD), is investigated in this paper for detecting anomalous performance metrics of cloud services. Although competent in general anomaly detection, SVDD suffers from unsatisfactory false alarm rate and computational complexity in time series anomaly detection, which considerably hinders its practical applications. Therefore, this paper proposes a relaxed form of linear programming SVDD (RLPSVDD) and presents important insights into parameter selection for practical time series anomaly detection in order to monitor the operations of cloud services. Experiments on the Iris dataset and the Yahoo benchmark datasets validate the effectiveness of our approaches. Furthermore, the comparison of RLPSVDD and the methods obtained from Twitter, Numenta, Etsy and Yahoo, shows the overall preference for RLPSVDD in time series anomaly detection. © 2015 IEEE.","Anomaly detection; Cloud computing systems; Time series analysis; Trustworthiness","Big data; Cloud computing; Data description; Distributed database systems; Linear programming; Time series analysis; Web services; Anomaly detection; Cloud computing system; Cloud services; Cloud-computing; Computing system; Linear-programming; Support vector data description; Time-series analysis; Times series; Trustworthiness; Information management",Article,Scopus,2-s2.0-85123706186
"Frantz D., Hostert P., Rufin P., Ernst S., Röder A., van der Linden S.","56428816500;6602798575;56503442700;57204393107;7003364227;22952273100;","Revisiting the Past: Replicability of a Historic Long-Term Vegetation Dynamics Assessment in the Era of Big Data Analytics",2022,"Remote Sensing","14","3","597","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123705088&doi=10.3390%2frs14030597&partnerID=40&md5=a7bd59ab83ca5e00f58c92625906fd71","Open and analysis-ready data, as well as methodological and technical advancements have resulted in an unprecedented capability for observing the Earth’s land surfaces. Over 10 years ago, Landsat time series analyses were inevitably limited to a few expensive images from carefully selected acquisition dates. Yet, such a static selection may have introduced uncertainties when spatial or inter-annual variability in seasonal vegetation growth were large. As seminal pre-open-data-era papers are still heavily cited, variations of their workflows are still widely used, too. Thus, here we quantitatively assessed the level of agreement between an approach using carefully selected images and a state-of-the-art analysis that uses all available images. We reproduced a representative case study from the year 2003 that for the first time used annual Landsat time series to assess long-term vegetation dynamics in a semi-arid Mediterranean ecosystem in Crete, Greece. We replicated this assessment using all available data paired with a time series method based on land surface phenology metrics. Results differed fundamentally because the volatile timing of statically selected images relative to the phenological cycle introduced systematic uncertainty. We further applied lessons learned to arrive at a more nuanced and information-enriched vegetation dynamics description by decomposing vegetation cover into woody and herbaceous components, followed by a syndrome-based classification of change and trend parameters. This allowed for a more reliable interpretation of vegetation changes and even permitted us to disentangle certain land-use change processes with opposite trajectories in the vegetation components that were not observable when solely analyzing total vegetation cover. The long-term budget of net cover change revealed that vegetation cover of both components has increased at large and that this process was mainly driven by gradual processes. We conclude that study designs based on static image selection strategies should be critically evaluated in the light of current data availability, analytical capabilities, and with regards to the ecosystem under investigation. We recommend using all available data and taking advantage of phenology-based approaches that remove the selection bias and hence reduce uncertainties in results. © 2022 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/).","Big Data; Crete; Land degradation; Landsat; Long-term; Mediterranean; Phenology; Replicability; Reproducibility; Semi-arid; Time series; Vegetation decomposition","Budget control; Classification (of information); Data Analytics; Dynamics; Ecosystems; Open Data; Surface measurement; Time series; Time series analysis; Vegetation; Crete; Land degradation; LANDSAT; Long-term; Mediterranean; Replicability; Reproducibilities; Semi arid; Times series; Vegetation decomposition; Big data",Article,Scopus,2-s2.0-85123705088
"Wu W., Ma Z., Guo J., Niu X., Zhao K.","57218086979;57431375400;57431774600;57188624006;56208957000;","Evaluating the Effects of Built Environment on Street Vitality at the City Level: An Empirical Research Based on Spatial Panel Durbin Model",2022,"International Journal of Environmental Research and Public Health","19","3","1664","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123688938&doi=10.3390%2fijerph19031664&partnerID=40&md5=d301f3ea0e7da079dad8dda71fe449b3","There is evidence that the built environment has an influence on street vitality. However, previous studies seldom assess the direct, indirect, and total effect of multiple environmental elements at the city level. In this study, the features of the street vitality on Xiamen Island are described based on the location-based service Big Data. Xiamen Island is the central urban area of Xiamen, one of the national central cities in China. With the help of multi-source data such as street view images, the condition of design that is difficult to effectively measure with traditional data can be better explored in detail on a macro scale. The built environment is measured through a 5D system at the city level, including Density, Diversity, Design, Destination accessibility, and Distance to transit. Spatial panel Durbin models are constructed to analyze the influence of the built environment on the street vitality on weekdays and weekends, and the direct, indirect, and total effects are evaluated. Results indicate that at the city level, the built environment plays a significant role in promoting street vitality. Functional density is not statistically significant. Most of the elements have spatial effects, except for several indicators in the condition of the design. Compared with the conclusions of previous studies, some indicators have different effects on different spatial scales. For instance, on the micro scale, greening can enhance the attractiveness of streets. However, on the macro scale, too much greening brings fewer functions along the street, which inhibits the street vitality. The condition of design has the greatest effect, followed by destination accessibility. The differences in the influences of weekdays and weekends are mainly caused by commuting behaviors. Most of the built environment elements have stronger effects on weekends, indicating that people interact with the environment more easily during this period. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","5D; Built environment; Spatial panel Durbin model; Street activity; Street vitality; Xiamen Island","article; big data; built environment; China; city; controlled study; empirical research; human; urban area",Article,Scopus,2-s2.0-85123688938
"Rammer D., Buddhika T., Malensek M., Pallickara S., Pallickara S.L.","57147757400;56316551000;54788044500;6602168326;8982898100;","Enabling Fast Exploratory Analyses over Voluminous Spatiotemporal Data Using Analytical Engines",2022,"IEEE Transactions on Big Data","8","1",,"213","228",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123686201&doi=10.1109%2fTBDATA.2019.2939834&partnerID=40&md5=6059a0fc9c6587283726690c0f5bb000","Fueled by the proliferation of IoT devices and increased adoption of sensing environments the collection of spatiotemporal data has exploded in recent years. Disk based storage systems provide reliable archives but are far too slow for efficient analytics. Furthermore, spatiotemporal datasets quickly exceed the memory capacity of cluster environments. Current solutions focused on in-memory analytics suffer from memory contention and unnecessary network I/O, failing to provide a suitable platform for iterative, exploratory analytics in shared environments. In this work we propose Anamnesis, the first in-memory, sketch aligned, HDFS compliant storage system. Data sketching algorithms reduce dataset sizes by summarizing feature values and inter-feature relationships. Anamnesis leverages data sketches to alleviate memory contention and vastly reduce network I/O during analytics. Upon request, we generate accurate full-resolution datasets with negligible resource and time costs. Datasets are available using a fully HDFS compliant interface allowing Anamnesis to achieve unprecedented compatibility with popular analytics engines. This facilitates adoption into existing workflows by serving as a 'drop-in' replacement for canonical HDFS. We evaluate the system using 2 spatiotemporal datasets, a variety of popular analytics engines, and real-world analytical operations. © 2015 IEEE.","Big Data; Data sketches; Distributed analytics; Spatiotemporal data","Digital storage; Engines; Iterative methods; Data sketch; Disk based storage; Distributed analytic; Exploratory analysis; Memory capacity; Memory contentions; Network I/O; Spatio-temporal data; Spatiotemporal datasets; Storage systems; Big data",Article,Scopus,2-s2.0-85123686201
"Khalajzadeh H., Abdelrazek M., Grundy J., Hosking J., He Q.","55129244500;56080446200;7102156137;7005154879;55217854300;","Survey and Analysis of Current End-User Data Analytics Tool Support",2022,"IEEE Transactions on Big Data","8","1",,"152","165",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123681647&doi=10.1109%2fTBDATA.2019.2921774&partnerID=40&md5=6e88382f167f730e2264f02d89055666","There has been a very large growth in interest in big data analytics to discover patterns and insights. A major challenge in this domain is the need to combine domain knowledge - what the data means (semantics) and what it is used for - with advanced data analytics and visualization techniques to mine and communicate important information from the huge volumes of raw data. Many data analytics tools have been developed for both research and practice to assist in specifying, integrating and deploying data analytics applications. However, delivering such big data analytics applications requires a capable team with different skillsets including data scientists, software engineers and domain experts. Such teams and skillsets usually take a long time to build and have high running costs. An alternative is to provide domain experts and data scientists - the end users - with tools they can use to create and deploy complex data analytics application solutions directly with less technical skills required. In this paper we present a survey and analysis of several current research and practice approaches to supporting data analytics for end-users, identifying key strengths, weaknesses and opportunities for future research. © 2015 IEEE.","Big data; Data analytics; Data science; Data visualization; Domain specific visual languages; Machine learning","Application programs; Data Analytics; Data visualization; Domain Knowledge; Learning systems; Semantics; Surveys; Visual languages; Visualization; 'current; Data analytic tools; Data analytics; Domain experts; Domain-specific visual language; End-users; Skill sets; Survey and analysis; Tool support; User data; Big data",Article,Scopus,2-s2.0-85123681647
"Xiong H., Choo K.-K.R., Vasilakos A.V.","14059276900;57208540261;57200495061;","Revocable Identity-Based Access Control for Big Data with Verifiable Outsourced Computing",2022,"IEEE Transactions on Big Data","8","1",,"1","13",,13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123676739&doi=10.1109%2fTBDATA.2017.2697448&partnerID=40&md5=c6e64968b2f45f2cc6b72a029f884823","To be able to leverage big data to achieve enhanced strategic insight, process optimization and make informed decision, we need to be an efficient access control mechanism for ensuring end-to-end security of such information asset. Signcryption is one of several promising techniques to simultaneously achieve big data confidentiality and authenticity. However, signcryption suffers from the limitation of not being able to revoke users from a large-scale system efficiently. We put forward, in this paper, the first identity-based (ID-based) signcryption scheme with efficient revocation as well as the feature to outsource unsigncryption to enable secure big data communications between data collectors and data analytical system(s). Our scheme is designed to achieve end-to-end confidentiality, authentication, non-repudiation, and integrity simultaneously, while providing scalable revocation functionality such that the overhead demanded by the private key generator (PKG) in the key-update phase only increases logarithmically based on the cardiality of users. Although in our scheme the majority of the unsigncryption tasks are outsourced to an untrusted cloud server, this approach does not affect the security of the proposed scheme. We then prove the security of our scheme, as well as demonstrating its utility using simulations. © 2015 IEEE.","Access control; big data; identity-based signcryption; revocation; verifiable outsourced computing","Authentication; Cryptography; Large scale systems; Optimization; Access control mechanism; End-to-end security; Identity-based; Identity-based signcryption; Information assets; Informed decision; Process optimisation; Revocation; Signcryption; Verifiable outsourced computing; Big data",Article,Scopus,2-s2.0-85123676739
"Perelló‐bratescu A., Dürsteler C., Álvarez‐carrera M.A., Granés L., Kostov B., Sisó‐almirall A.","35410090900;14123081100;57211914953;57225195380;56725835100;17136155000;","Risk Prescriptions of Strong Opioids in the Treatment of Chronic Non‐Cancer Pain by Primary Care Physicians in Catalonia: Opicat Padris Project",2022,"International Journal of Environmental Research and Public Health","19","3","1652","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123614286&doi=10.3390%2fijerph19031652&partnerID=40&md5=94aa6b66d50a8858b4325ce71d0ab363","The prescription of strong opioids (SO) for chronic non‐cancer pain (CNCP) is steadily increasing. This entails a high risk of adverse effects, a risk that increases with the concomitant prescription of SO with central nervous system depressant drugs and with the use of SO for non‐rec-ommended indications. In order to examine this concomitant risk prescription, we designed a de-scriptive, longitudinal, retrospective population‐based study. Patients aged ≥15 years with a con-tinued SO prescription for ≥3 months during 2013–2017 for CNCP were included. Of these, patients who had received concomitant prescriptions of SO and risk drugs (gabapentinoids, benzodiaze-pines and antidepressants) and those who had received immediate‐release fentanyl (IRF) were se-lected. The study included 22,691 patients; 20,354 (89.7%) patients received concomitant risk pre-scriptions. Men and subjects with a higher socioeconomic status received fewer concomitant risk prescriptions. Benzodiazepines or Z‐drugs were prescribed concomitantly with SO in 15,883 (70%) patients, antidepressants in 14,932 (65%) and gabapentinoids in 11,267 (49%), while 483 (21.32%) patients received IRF (2266 prescriptions in total) without a baseline SO. In conclusion, our study shows that a high percentage of patients prescribed SO for CNCP received concomitant prescriptions with known risks, as well as IRF for unauthorized indications. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Analgesics; Big data; Chronic pain; Drug combinations; Inappropriate prescribing; Opioids; Opioid‐related disorders; Pharmacoepidemiology; Physicians; Primary care","alprazolam; amfebutamone; amitriptyline; analgesic agent; antidepressant agent; benzodiazepine derivative; bromazepam; citalopram; clomipramine; clorazepate; diazepam; duloxetine; escitalopram; fentanyl; fluoxetine; gabapentin; gabapentinoid derivative; ketazolam; lorazepam; lormetazepam; mianserin; midazolam; mirtazapine; opiate derivative; paroxetine; pregabalin; pyridoxine; sertraline; sulpiride; unclassified drug; venlafaxine; zolpidem; adolescent; adult; aged; Article; big data; cancer pain; Catalonia; chronic pain; controlled study; descriptive research; drug classification; drug combination; drug indication; drug release; female; general practitioner; human; longitudinal study; major clinical study; male; middle aged; monotherapy; pharmacoepidemiology; population research; prescription; primary medical care; retrospective study; risk assessment; risk benefit analysis; side effect; social status; very elderly",Article,Scopus,2-s2.0-85123614286
"Rungskunroch P., Shen Z.-J., Kaewunruen S.","57205264532;57405978800;55907644600;","Benchmarking Socio-Economic Impacts of High-Speed Rail Networks Using K-Nearest Neighbour and Pearson’s Correlation Coefficient Techniques through Computational Model-Based Analysis",2022,"Applied Sciences (Switzerland)","12","3","1520","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123582607&doi=10.3390%2fapp12031520&partnerID=40&md5=dd4f3c441e82bf450bcb05b627b421ac","Not only have high-speed rail (HSR) services stimulated the economy of many countries, but they have also significantly uplifted quality of lives (QoL) of countless people. For many decades, the aspiration for HSR network development has dramatically risen, and HSR networks have inevitably become an icon of civilisation. However, only a few successful HSR networks globally can truly generate socio-economic impacts on their societies. This research aims to understand the impact of HSR networks on social and economic impacts and to provide recommendations for success. This study is the world’s first to examine the benefits of HSR across all community demographic groups, including young and elderly people. The findings will illustrate the QoL, economic, and educational elements’ advantages in explicit terms. It has established two interconnected models via Python to codify a novel customised model for socio-economic evaluation. ‘Pearson correlation coefficient’ and ‘K-Nearest Neighbour’ techniques are applied to bolster the reliability of the research findings. The outcomes have been reviewed by 30 international HSR specialists. The benchmarking exhibits that socio-economic impacts apparently occur across vast areas. The insight stemming from this benchmarking also offers policy implications and empirical data for long-term HSR improvement, assisting the government in developing new methods for sustainable communities. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; High-speed rail network; Population dynamics; Social impacts; Socio-economic",,Article,Scopus,2-s2.0-85123582607
"Badapanda K., Mishra D.P., Salkuti S.R.","57427862900;56537757800;57218893449;","Agriculture data visualization and analysis using data mining techniques: application of unsupervised machine learning",2022,"Telkomnika (Telecommunication Computing Electronics and Control)","20","1",,"98","108",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123537977&doi=10.12928%2fTELKOMNIKA.v20i1.18938&partnerID=40&md5=8e6cd2424ccadfca191f6e09fa47e07d","Unsupervised machine learning is one of the accepted platforms for applying a broad data analytics challenge that involves the way to identify secret trends, unexplained associations, and other significant data from a wide dispersed dataset. The precise yield estimate for the various crops involved in the planning is a critical problem for agricultural planning. To achieve realistic and effective solutions to this problem, data mining techniques are an essential approach. Applying distplot combined with kernel density estimate (KDE) in this paper to visualize the probability density of disseminated datasets of vast crop deals for crop planning. This paper focuses on analyzing and segmenting agricultural data and determining optimal parameters to maximize crop yield using data mining techniques such as K-means clustering and principal component analysis (PCA). © 2022. All Rights Reserved.","Big data; Distplot; Elbow method; K-means; Kernel density estimate; Principal component analysis",,Article,Scopus,2-s2.0-85123537977
"Park Y.-T., Kim D., Koh S.-J., Kim Y.S., Kim S.M.","55494379000;57427954200;55286016300;57210290786;57312690000;","Patient Factors Associated with Different Hospice Programs in Korea: Analyzing Healthcare Big Data",2022,"International Journal of Environmental Research and Public Health","19","3","1566","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123522261&doi=10.3390%2fijerph19031566&partnerID=40&md5=71606f2a833483269c4bd6fdef2e0bde","The Korean government has implemented a pilot project that introduces a new type of hospice care program called “Consultative Hospice Care” (COHC) since August 2017. The COHC is a new type of hospice program for terminally ill patients in acute care wards, which is different from the Independent Hospice Unit (IHU) care. This study aimed to compare the characteristics of two groups of hospice patients: COHC care only and both IHU care and COHC groups. Healthcare claim data from 1 April 2018 to 31 March 2020 were retrieved from the HIRA data warehouse sys-tem. The main outcome variable was patients receiving COHC only or both COHC and IHU care. The total number of hospice patients was 6482. A multivariate logistic regression analysis was used. Of 6482 hospice care recipients, 3789 (58.5%) received both COHC and IHU care. Those who received both COHC and IHU care were significantly associated with several factors: period from the first evaluation to death (adjusted odds ratio (aOR), 1.026; 95% confidence internal (CI), 1.024–1.029; p <0.0001), disease severity measured by the Charlson Comorbidity Index (aOR, 1.032; CI, 1.017– 1.047; p < 0.0001), consciousness (aOR, 3.654; CI, 3.269–4.085; p < 0.0001), and awareness of end-stage disease (aOR, 1.422; CI, 1.226–1.650; p < 0.0001). The COHC program had a critical role in hospice delivery to terminally ill patients. Policymakers on hospice care need to establish plans that promote efficient hospice care delivery systems. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Hospice; Hospice shared care; Hospice units; Palliative care; Terminal illness","data set; disease severity; health care; health services; aged; Article; big data; Charlson Comorbidity Index; consciousness; consultative hospice care; controlled study; critically ill patient; cross-sectional study; death; disease severity; female; health care delivery; hospice care; hospice patient; human; independent hospice unit care; intermethod comparison; major clinical study; male; South Korea; Korea",Article,Scopus,2-s2.0-85123522261
"Sarosh P., Parah S.A., Bhat G.M.","57218388554;35793844300;7103251059;","An efficient image encryption scheme for healthcare applications",2022,"Multimedia Tools and Applications","81","5",,"7253","7270",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123487517&doi=10.1007%2fs11042-021-11812-0&partnerID=40&md5=b5f1691396e2de3aa5bc9ebacba0ecb6","In recent years, there has been an enormous demand for the security of image multimedia in healthcare organizations. Many schemes have been developed for the security preservation of data in e-health systems however the schemes are not adaptive and cannot resist chosen and known-plaintext attacks. In this contribution, we present an adaptive framework aimed at preserving the security and confidentiality of images transmitted through an e-healthcare system. Our scheme utilizes the 3D-chaotic system to generate a keystream which is used to perform 8-bit and 2-bit permutations of the image. We perform pixel diffusion by a key-image generated using the Piecewise Linear Chaotic Map (PWLCM). We calculate an image parameter using the pixels of the image and perform criss-cross diffusion to enhance security. We evaluate the scheme’s performance in terms of histogram analysis, information entropy analysis, statistical analysis, and differential analysis. Using the scheme, we obtain the average Number of Pixels Change Rate (NPCR) and Unified Average Changing Intensity (UACI) values for an image of size 256 × 256 equal to 99.5996 and 33.499 respectively. Furthermore, the average entropy is 7.9971 and the average Peak Signal to Noise Ratio (PSNR) is 7.4756. We further test the scheme on 50 chest X-Ray images of patients having COVID-19 and viral pneumonia and found the average values of variance, PSNR, entropy, and Structural Similarity Index (SSIM) to be 257.6268, 7.7389, 7.9971, and 0.0089 respectively. Furthermore, the scheme generates completely uniform histograms for medical images which reveals that the scheme can resist statistical attacks and can be applied as a security framework in AI-based healthcare. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Big data; Biomedical systems; Healthcare; Image Encryption; Medical images; Privacy; Security","Big data; Chaotic systems; Cryptography; Graphic methods; Image enhancement; Medical imaging; Piecewise linear techniques; Pixels; Signal to noise ratio; Biomedical systems; e-Health systems; Health care application; Healthcare organizations; Image encryption scheme; Images encryptions; Medical image; Peak signal to noise ratio; Privacy; Security; Health care",Article,Scopus,2-s2.0-85123487517
"Gao C., Liu J., Zhang S., Zhu H., Zhang X.","57425865700;36198367500;57190165323;56266620100;57222351994;","The Coastal Tourism Climate Index (CTCI): Development, Validation, and Application for Chinese Coastal Cities",2022,"Sustainability (Switzerland)","14","3","1425","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123453202&doi=10.3390%2fsu14031425&partnerID=40&md5=0707bd482a9a5e36a444cbf4cf6ca8e8","Climate is an essential component in the sustainability of tourism cities. Coastal tourism cities face unprecedented challenges under a changing climate. The complexity of the tourism–cli-mate interface predicates the need for tools that can assess the weather and climate accurately. Tourism climate indices have been widely developed to evaluate the temporal and spatial distribution of climate resources, but these indices are not entirely applicable to coastal cities facing air pollution. This study developed a Coastal Tourism Climate Index (CTCI) to assess the tourism climate suitability of Chinese coastal cities. The CTCI was developed to include five variables: thermal comfort, sunshine, precipitation, wind, and air quality. This index was applied and verified in the case of nine coastal tourism cities in China compared to the Holiday Climate Index (HCI: Beach). According to the results, the CTCI is more suitable for coastal tourism climate assessment in China. Finally, corresponding countermeasures are put forward for the balanced and sustainable development of Chinese coastal tourism cities. This study takes the lead in applying big data to the development and validation of tourism climate indices. These findings provide novel insights for the tourism climate assessment of coastal destinations facing air pollution. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Air quality; Big data; China; Climate change; Coastal cities; Coastal tourism; Tourism Climate Index","atmospheric pollution; coastal zone; model validation; spatial distribution; sustainability; tourism development; China",Article,Scopus,2-s2.0-85123453202
"Guo J., Huang C., Hou J.","57205419896;55207460700;48861439400;","A Scalable Computing Resources System for Remote Sensing Big Data Processing Using GeoPySpark Based on Spark on K8s",2022,"Remote Sensing","14","3","521","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123382827&doi=10.3390%2frs14030521&partnerID=40&md5=74f567068f9de7d36975a813dc913260","As a result of Earth observation (EO) entering the era of big data, a significant challenge relating to by the storage, analysis, and visualization of a massive amount of remote sensing (RS) data must be addressed. In this paper, we proposed a novel scalable computing resources system to achieve high-speed processing of RS big data in a parallel distributed architecture. To reduce data movement among computing nodes, the Hadoop Distributed File System (HDFS) is established on nodes of K8s, which are also used for computing. In the process of RS data analysis, we innovatively use the tile-oriented programming model instead of the traditional strip-oriented or pixel-oriented approach to better implement parallel computing in a Spark on Kubernetes (K8s) cluster. A large RS raster layer can be abstracted as a user-defined tile format of any size, so that a whole computing task can be divided into multiple distributed parallel tasks. The computing resources applied by users would be immediately assigned in the Spark on K8s cluster by simply configuring and initializing SparkContext through a web-based Jupyter notebook console. Users can easily query, write, or visualize data in any box size from the catalog module in GeoPySpark. In summary, the system proposed in this study can provide a distributed scalable resources system for assembling big data storage, parallel computing, and real-time visualization. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; GeoPySpark; HDFS on K8s; Parallel computing; Remote sensing; Spark on K8s","Big data; Cluster computing; Data handling; Data visualization; Digital storage; High speed cameras; Remote sensing; Visualization; Computing resource; Distributed file systems; Earth observations; Geopyspark; Hadoop distributed file system on k8s; Parallel com- puting; Remote-sensing; Resource system; Scalable computing; Spark on k8s; File organization",Article,Scopus,2-s2.0-85123382827
"Belcastro L., Cantini R., Marozzo F.","48860946600;57215871062;37097646500;","Knowledge Discovery from Large Amounts of Social Media Data",2022,"Applied Sciences (Switzerland)","12","3","1209","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123381026&doi=10.3390%2fapp12031209&partnerID=40&md5=779da2da9dddf55d128e0273dd1b277e","In recent years, social media analysis is arousing great interest in various scientific fields, such as sociology, political science, linguistics, and computer science. Large amounts of data gath-ered from social media are widely analyzed for extracting useful information concerning people’s behaviors and interactions. In particular, they can be exploited to analyze the collective sentiment of people, understand the behavior of user groups during global events, monitor public opinion close to important events, identify the main topics in a public discussion, or detect the most frequent routes followed by social media users. As an example of the countless works in the state-of-the-art on social media analysis, this paper presents three significant applications in the field of opinion and pattern mining from social media data: (i) an automatic application for discovering user mobility patterns, (ii) a novel application for estimating the political polarization of public opinion, and (iii) an application for discovering interesting social media discussion topics through a hashtag recommendation system. Such applications clearly highlight the abundance and wealth of useful information in many application contexts of human life that can be extracted from social media posts. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big Data; Big Data analysis; Knowledge discovery; Social media analysis; Social media applications",,Article,Scopus,2-s2.0-85123381026
"Xing Y., Wang X., Qiu C., Li Y., He W.","57219335049;55950653000;57248391500;57248177700;36985723200;","Research on opinion polarization by big data analytics capabilities in online social networks",2022,"Technology in Society","68",,"101902","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123351677&doi=10.1016%2fj.techsoc.2022.101902&partnerID=40&md5=85c427d088926a1370922696acb9b2af","Opinion polarization in online social networks causes a lot of concerns on its social, economic, and political impacts, and is becoming an important topic for academic research. Based on the system theory, a theoretical framework on analyzing opinion polarization combining big data analytics capabilities (BDAC) is proposed. A web crawler is used to collect data from the Sina Weibo platform on the topic of “Tangping”. Concerning the characteristics of the big data environment, social network analysis (SNA), machine learning, text clustering and content analysis are used to mine opinion polarization of “Tangping” on Weibo. Results show that social network users holding the same opinion indicate the phenomenon of aggregation. Although no influential users support the opinion of “Tangping” on Weibo, a high percentage of people advocate the idea. The supporting group has the most clusters while the opposing group has the highest density of keywords. The research contributes to the existing literature on applying BDAC to analyze online polarization from the perspective of the system from user behavior and interaction to topic clustering and keywords identification. The conceptual system framework shows superiority in the integration of information coordination of microsystem and exosystem. Guidance strategies are put forward to supplement the formation theory of opinion polarization and provide suggestions to reasonably regulate network group polarization. © 2022 Elsevier Ltd","Big data; Online social networks; Opinion polarization; Social network analysis; Systems; Text clustering","Behavioral research; Big data; Cluster analysis; Data Analytics; Online systems; Polarization; Web crawler; Academic research; Economic impacts; Opinion polarization; Political impact; Social impact; Social Network Analysis; Social-economic; System; Text Clustering; Theoretical framework; Social networking (online)",Article,Scopus,2-s2.0-85123351677
"Gültekin G., Bayat O.","57192690292;8291680500;","A Naïve Bayes prediction model on location-based recommendation by integrating multi-dimensional contextual information",2022,"Multimedia Tools and Applications","81","5",,"6957","6978",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123240803&doi=10.1007%2fs11042-021-11676-4&partnerID=40&md5=b2d9bd1158a1cf05c7e955eea1d91f81","In recent years, researchers have been trying to create recommender systems. There are many different recommender systems. Point of Interest (POI) is a new type of recommender systems that focus on personalized and context-aware recommendations to improve user experience. Recommender systems use different types of recommendation methods to obtain information on POI. In this research paper, we introduced a Naïve Bayes Prediction Model based on Bayesian Theory for POI recommendation. Then, we used the Brightkite dataset to make predictions on POI recommendation and compared it with the other two different recommendation methods. Experimental results confirm that our proposed method outperforms on Location-based POI recommendation. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Big data analysis; Collaborative filtering; Factorization; Location-based social networks; Naïve Bayes theorem; Recommendation algorithms","Barium compounds; Bayesian networks; Big data; Classifiers; Collaborative filtering; Forecasting; Location; Social networking (online); Baye's theorem; Big data analyse; Contextual information; Location based; Location-based social networks; Multi dimensional; Naive baye theorem; Naive bayes; Recommendation algorithms; Recommendation methods; Recommender systems",Article,Scopus,2-s2.0-85123240803
"Zhu J., Jin J., Gao Z., Reviriego P.","57206253420;56585143000;57224437981;19639262200;","Single Event Transient tolerant Count Min Sketches",2022,"Microelectronics Reliability","129",,"114486","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123221674&doi=10.1016%2fj.microrel.2022.114486&partnerID=40&md5=200ac520415ed986722c43b9bf737abd","Frequency estimation is a common operation in big data processing. In many big data applications, computing the exact data frequency is not practical as it requires a large computational effort. Instead, a reasonably accurate estimate is commonly used. The Count Min Sketch (CMS) is a popular method for frequency estimation due to its simple implementation and low memory requirements. However, Single Event Transients (SETs) can affect the logic functions of the CMS. Based on our previous research, nearly half of the SETs will lead to CMS estimation errors in the worst case. Moreover, the most frequent elements are more likely to be underestimated when SETs occur, which is not acceptable in practice. Therefore, this paper proposes several fault-tolerant schemes to protect the CMS against SETs, especially to avoid underestimation. In particular, space redundancy-based schemes and partial time redundancy-based schemes are proposed, and the probabilities for underestimation and overestimation are analyzed theoretically. Experiments are performed to compare the reliability of the CMS protected by different schemes and validate the theoretical predictions. Finally, the selection of the best CMS parameters for practical applications is discussed with a comprehensive analysis of the overhead and performance of the different protection schemes. © 2022 Elsevier Ltd","Count Min Sketch; Fault tolerance; Frequency estimation; Single Event Transients","Big data; Computation theory; Fault tolerance; Frequency estimation; Radiation hardening; Redundancy; Transients; Big data applications; Common operations; Computational effort; Count-Min sketch; Estimation errors; Fault tolerant schemes; Logic functions; Lower memory requirement; Simple++; Single event transients; Data handling",Article,Scopus,2-s2.0-85123221674
"Perelló-Bratescu A., Dürsteler C., Álvarez-Carrera M.A., Granés L., Kostov B., Sisó-Almirall A.","35410090900;14123081100;57211914953;57225195380;56725835100;17136155000;","Trends in the Prescription of Strong Opioids for Chronic Non-Cancer Pain in Primary Care in Catalonia: Opicat-Padris-Project",2022,"Pharmaceutics","14","2","237","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123063099&doi=10.3390%2fpharmaceutics14020237&partnerID=40&md5=5873dc777aa33e035b89d766271ca59c","In chronic non-cancer pain (CNCP), evidence of the effectiveness of strong opioids (SO) is very limited. Despite this, their use is increasingly common. To examine SO prescriptions, we designed a descriptive, longitudinal, retrospective population-based study, including patients aged ≥15 years prescribed SO for ≥3 months continuously in 2013–2017 for CNCP in primary care in Catalonia. Of the 22,691 patients included, 17,509 (77.2%) were women, 10,585 (46.6%) were aged >80 years, and most had incomes of <€18,000 per year. The most common diagnoses were muscu-loskeletal diseases and psychiatric disorders. There was a predominance of transdermal fentanyl in the defined daily dose (DDD) per thousand inhabitants/day, with the greatest increase for tapentadol (312% increase). There was an increase of 66.89% in total DDD per thousand inhabitants/day for SO between 2013 (0.737) and 2017 (1.230). The mean daily oral morphine equivalent dose/day dispensed for all drugs was 83.09 mg. Transdermal fentanyl and immediate transmucosal release were the largest cost components. In conclusion, there was a sustained increase in the prescription of SO for CNCP, at high doses, and in mainly elderly patients, predominantly low-income women. The new SO are displacing other drugs. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Analgesics; Big data; Chronic pain; Inappropriate prescribing; Opioid-related disorders; Opioids; Pharmacoeconomics; Pharmacoepidemiology; Physicians; Primary care",,Article,Scopus,2-s2.0-85123063099
"Lu Y., Hong W., Liu Z., Wang Y., Wang H., Chen W., Liu S., Yan Y., Xu B.","57416755800;57416740300;56002353000;57214231809;55862402300;57204763713;35722222300;35207263700;35747344800;","Research on the orientation flights and colony development of Apis cerana based on smart beehives",2022,"Computers and Electronics in Agriculture","193",,"106733","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123032948&doi=10.1016%2fj.compag.2022.106733&partnerID=40&md5=a9cdb76f28ef3b616de98728e684db52","Pollinations as a result of bees collecting pollen from flowers are vital for maintaining the subtle balance of the earth's ecological system. However, apiculture is facing a crisis owing to collapsing honey bee colonies, yet the reasons are still unclear. As young bees are the future backbone of a colony, their behaviors could reveal the development of the colony and the problems causing colony collapse. In this study, we used Internet of Things technology to monitor the data of eight colonies of Apis cerana continuously over three years. We obtained more than 10,000 h of monitoring data to analyze the relationship between the orientation flight activities of young bees and changes in honeycomb weight, time, and local ambient temperature. The results indicated that the young bee ratio was closely associated with the trend in colony development. When the ratio of young bees participating in the orientation flight was low and stable, the colony size was likely to be maintained; in contrast, when the ratio was high and fluctuating considerably, the bee colony tended to decline. This finding can be used to predict the development trend of bee colonies and explore further quantitative factors that trigger abnormal colony losses. © 2022 Elsevier B.V.","Age structure; Apis Cerana; Big data analysis; Colony collapse disorder; Long-term monitoring","Big data; Age structures; Apis cerana; Big data analyse; Colony collapse disorder; Colony development; Ecological systems; Flight activity; Honey bee; Internet of things technologies; Long term monitoring; Monitoring; colony structure; flight behavior; honeybee; Internet",Article,Scopus,2-s2.0-85123032948
"Jalalian Z., Sharifi M.","57428505200;7006324593;","A hierarchical multi-objective task scheduling approach for fast big data processing",2022,"Journal of Supercomputing","78","2",,"2307","2336",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122779312&doi=10.1007%2fs11227-021-03960-9&partnerID=40&md5=5071800fb716edca2ecd84c7d7c3c03d","Due to the rapid growth of production and dissemination of big data from various sources, the speed of data processing must inevitably increase. In distributed big data processing systems such as cloud computing, the task scheduler is responsible for mapping a large set of various tasks to a set of possibly heterogeneous computing nodes in a way to raise resource efficiency and data locality and reduce makespan. Scheduling strategies that try to achieve these goals in one pass have lower performance than multi-pass strategies. To achieve higher performance, we propose MOTS (a hierarchical multi-objective task scheduling scheme) by first clustering tasks using the K-means algorithm alongside a load balancing equation to increase resource efficiency and then optimizing clusters to reduce makespan using evolutionary algorithms. The latter is achieved by using the state of physical machines and sending related consecutive tasks to a physical machine to eliminate data transfer. We have simulated and tested our scheme in Cloudsim. Our experiments show reduction of approximately 10% makespan and 4% higher CPU efficiency compared to Mai’s reinforcement learning approach and Bugerya’s parallel implementation method. The cost of data transfer between consecutive tasks is also decreased by 10% compared to Bugerya’s methods. With respect to the results and the fact that our proposed task scheduling scheme is inspired by the iHadoop method for parallel implementation, it is suitable for use in distributed big data processing systems. Information about previous executions of tasks and current status of computing nodes is highly influential in efficient mapping of tasks to computing nodes. Predictions of future resource needs of tasks and available capacities of computing nodes can complement the historical information in the way of finding a more near-to-optimal mapping, resulting in faster data processing. This issue and evaluation of our proposed scheme using real data will be pursued in the future. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Evolutionary algorithm; Fast big data processing; K-means algorithm; Task scheduling","Balancing; Big data; Data transfer; Efficiency; Evolutionary algorithms; Hierarchical clustering; K-means clustering; Mapping; Multitasking; Reinforcement learning; Available capacity; Data processing systems; Heterogeneous computing; Historical information; Parallel implementations; Reinforcement learning approach; Resource efficiencies; Scheduling strategies; Parallel processing systems",Article,Scopus,2-s2.0-85122779312
"Di Maria E., De Marchi V., Galeazzo A.","6603693524;23484799100;55910770900;","Industry 4.0 technologies and circular economy: The mediating role of supply chain integration",2022,"Business Strategy and the Environment","31","2",,"619","632",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122675683&doi=10.1002%2fbse.2940&partnerID=40&md5=1cbb888e15aee37c9d7ef7adb655cf62","There is a great expectation that Industry 4.0 technologies will enable better circular economy (CE) results at firms. However, it is unclear how these technologies might contribute to CE. We hypothesize that Industry 4.0 technologies are positively related to the level of integration among actors along the supply chain and within the firm supply chain integration (SCI), which, in turn, explains superior CE results. By employing partial least square structural equation models on original survey data based on a sample of more than 1200 Italian manufacturing firms and almost 200 adopters, we find that disentangling for the type of technologies is essential to understanding both their direct and indirect role toward CE. Smart manufacturing technologies have a stronger impact on CE outcomes than data processing technologies; the mediating effect of SCI is verified for the former but not for the latter type, questioning the possibility for those technologies to support sustained CE performance in the long run. © 2021 ERP Environment and John Wiley & Sons Ltd.","big data; circular economy; digital technologies; robots; smart manufacturing; supply chain integration",,Article,Scopus,2-s2.0-85122675683
"Owens K.","56480195400;","The passivists: Managing risk through institutionalized ignorance in genomic medicine",2022,"Social Science and Medicine","294",,"114715","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122641756&doi=10.1016%2fj.socscimed.2022.114715&partnerID=40&md5=c2bb15a222fecc27d7e712ff461dd0b4","As the era of big data transforms modern medicine, clinicians have access to more health data than ever. How do medical providers determine which data are relevant to patient care, which are irrelevant, and which may be inappropriately used to justify potentially harmful interventions? One of the most prominent medical fields to address these questions head on – clinical genomics – is actively debating how to assess the value of genomic data. In-depth interviews with clinicians and a content analysis of policy documents demonstrate that while many clinicians believe that collecting as much patient data as possible will lead to better patient care, a sizeable minority of clinicians preferred to collect less data. These clinicians worried that large genomic tests provided too much data, leading to confusion and inappropriate treatment. Clinical geneticists have also started developing the concept of “actionability” to assess which types of genomic data are worth collecting and interpreting. By classifying data as useful when it can or should lead to action, clinicians can formalize and institutionalize what types of data should be ignored. But achieving consensus about what counts as “actionable” has proven difficult and highlights the different values and risk philosophies of clinicians. At the same time, many clinicians are fighting against the ignorance arising from genomic databases predominantly filled with samples from European ancestry populations. Debates about how and when to institutionalize ignorance of health data are not unique to clinical genomics, but have spread throughout many fields of medicine. As the amount of health data available to clinicians and patients grows, social science research on the politics of knowledge and ignorance should inform debates about the value of data in medicine. © 2022 The Author","Biomedicine; Clinical genomics; Data; Ignorance; Knowledge; Risk","database; genomics; knowledge; medicine; adult; article; big data; consensus; content analysis; genomics; human; interview; medical geneticist; patient care; patient coding; philosophy; politics; sociology; biomedicine; genomic medicine",Article,Scopus,2-s2.0-85122641756
"Sun Z., Sandoval L., Crystal-Ornelas R., Mousavi S.M., Wang J., Lin C., Cristea N., Tong D., Carande W.H., Ma X., Rao Y., Bednar J.A., Tan A., Wang J., Purushotham S., Gill T.E., Chastang J., Howard D., Holt B., Gangodagamage C., Zhao P., Rivas P., Chester Z., Orduz J., John A.","36562536000;56646080900;57194691224;56438882300;57406745500;57406397600;35069266300;57406397700;57389717900;22941707200;57406397800;57217242732;57406659800;57201399639;54882660300;57406397900;57225373411;34769985600;57406918600;21233340700;57406569800;57207780751;57406398000;57406828700;57202705597;","A review of Earth Artificial Intelligence",2022,"Computers and Geosciences","159",,"105034","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122572031&doi=10.1016%2fj.cageo.2022.105034&partnerID=40&md5=7369335e45f0676d2f5500c891d5b8d2","In recent years, Earth system sciences are urgently calling for innovation on improving accuracy, enhancing model intelligence level, scaling up operation, and reducing costs in many subdomains amid the exponentially accumulated datasets and the promising artificial intelligence (AI) revolution in computer science. This paper presents work led by the NASA Earth Science Data Systems Working Groups and ESIP machine learning cluster to give a comprehensive overview of AI in Earth sciences. It holistically introduces the current status, technology, use cases, challenges, and opportunities, and provides all the levels of AI practitioners in geosciences with an overall big picture and to “blow away the fog to get a clearer vision” about the future development of Earth AI. The paper covers all the majorspheres in the Earth system and investigates representative AI research in each domain. Widely used AI algorithms and computing cyberinfrastructure are briefly introduced. The mandatory steps in a typical workflow of specializing AI to solve Earth scientific problems are decomposed and analyzed. Eventually, it concludes with the grand challenges and reveals the opportunities to give some guidance and pre-warnings on allocating resources wisely to achieve the ambitious Earth AI goals in the future. © 2022","Artificial intelligence/machine learning; Atmosphere; Big data; Cyberinfrastructure; Geosphere; Hydrology","Artificial intelligence; Earth atmosphere; Earth system science; NASA; Artificial intelligence/machine learning; Cyberinfrastructure; Earth science data systems; Earth system science; Geospheres; NASA Earth Science; Operations cost; Reducing costs; Scaling-up; Subdomain; Big data; artificial intelligence; computer simulation; machine learning; NSCAT; numerical model",Article,Scopus,2-s2.0-85122572031
"Ge Y., Zhang W.-B., Liu H., Ruktanonchai C.W., Hu M., Wu X., Song Y., Ruktanonchai N.W., Yan W., Cleary E., Feng L., Li Z., Yang W., Liu M., Tatem A.J., Wang J.-F., Lai S.","57322849200;57208577846;57223827848;55822125700;24168602200;57223827689;57200073199;36440324300;57223818051;57281159300;57286561000;57392163600;57405135800;57200370617;6603035928;57350103800;57226487461;","Impacts of worldwide individual non-pharmaceutical interventions on COVID-19 transmission across waves and space",2022,"International Journal of Applied Earth Observation and Geoinformation","106",,"102649","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122528867&doi=10.1016%2fj.jag.2021.102649&partnerID=40&md5=2ceea8146c87912f351dd56f851125cb","Governments worldwide have rapidly deployed non-pharmaceutical interventions (NPIs) to mitigate the COVID-19 pandemic. However, the effect of these individual NPI measures across space and time has yet to be sufficiently assessed, especially with the increase of policy fatigue and the urge for NPI relaxation in the vaccination era. Using the decay ratio in the suppression of COVID-19 infections and multi-source big data, we investigated the changing performance of different NPIs across waves from global and regional levels (in 133 countries) to national and subnational (in the United States of America [USA]) scales before the implementation of mass vaccination. The synergistic effectiveness of all NPIs for reducing COVID-19 infections declined along waves, from 95.4% in the first wave to 56.0% in the third wave recently at the global level and similarly from 83.3% to 58.7% at the USA national level, while it had fluctuating performance across waves on regional and subnational scales. Regardless of geographical scale, gathering restrictions and facial coverings played significant roles in epidemic mitigation before the vaccine rollout. Our findings have important implications for continued tailoring and implementation of NPI strategies, together with vaccination, to mitigate future COVID-19 waves, caused by new variants, and other emerging respiratory infectious diseases. © 2021 The Author(s)","Big data; COVID-19; Effectiveness; Multi-scale; Non-pharmaceutical interventions","COVID-19; data set; disease spread; disease transmission; epidemic; health policy; infectious disease; pandemic; respiratory disease; state role; vaccination; vaccine; United States",Article,Scopus,2-s2.0-85122528867
"Wang X., Sun Y., Luo D., Peng J.","57405080500;35323367300;57405241200;57226715519;","Comparative study of machine learning approaches for predicting short-term photovoltaic power output based on weather type classification",2022,"Energy","240",,"122733","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122507262&doi=10.1016%2fj.energy.2021.122733&partnerID=40&md5=110eaea73ffa7e4035d54ba354fb61ff","To combat the worsening global energy shortage, global photovoltaic (PV) installation capacity has been increasing rapidly every year. Since the instability and intermittence of PV power output have great impacts on utility grids, accurate PV power output prediction is crucial. This paper proposes the use of machine learning approaches, combined with a weather type classification method, to predict short-term PV power output. The datasets are collected from a commercial PV power station located in Yangjiang, Guangdong province of China (latitude 21.56 °N, longitude 112.09 °E). Firstly, daytime meteorological data from 07:30 to 18:00 are divided into six 2-h intervals, and then the meteorological conditions of each interval are divided into four categories using an Extremely randomized Trees Classification model according to the PV power generation in each period. Secondly, nine machine learning models are established based on the weather type classification to predict the PV power output. The results show that weather type classification is vital to the selection of appropriate machine learning models and the accurate prediction of PV power output because the characteristic correlation between the meteorological data and PV power output always changes. In general, the Lasso Regressor, Random Forest Regressor, Gradient Boosting Regressor, and Support Vector Regressor models show better performances than the other models. Furthermore, all the models’ accuracy is relatively high when the local meteorological conditions are relatively stable, such as in October, November, and December, during which time the Mean Relative Error values are 2.07, 1.07, and 1.73, respectively. During the period when the weather is unstable, the performance of the SVR model is better than that of the other models. The prediction accuracy can be significantly improved with integrating the accurate weather classification into the model. With regards to each daytime period, the prediction accuracy in the morning and evening is relatively high and the MREs for these times are small. This study provides a theoretical basis for selecting appropriate machine learning models to predict photovoltaic power generation under different weather conditions. © 2021","Big data; Machine learning; PV output prediction; Weather classification","Adaptive boosting; Decision trees; Machine learning; Meteorology; Photovoltaic cells; Random forests; Solar energy; Thermoelectric power; Weather forecasting; Machine learning approaches; Machine learning models; Meteorological data; Photovoltaic output prediction; Photovoltaic power; Photovoltaics; Power output; Type classifications; Weather classification; Weather types; Big data; comparative study; data acquisition; machine learning; photovoltaic system; power plant; prediction; China; Guangdong; Yangjiang",Article,Scopus,2-s2.0-85122507262
"Zhao P., Yun Q., Li A., Li R., Yan Y., Wang Y., Sun H., Damirin A.","57195247219;57398425000;57398425100;57397102400;57195249189;57203119355;57212453015;6508046114;","LPA3 is a precise therapeutic target and potential biomarker for ovarian cancer",2022,"Medical Oncology","39","2","17","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122335133&doi=10.1007%2fs12032-021-01616-5&partnerID=40&md5=de30b45d524d2cf37b833b86d437366e","Current studies have demonstrated that significant increased LPA levels to be observed in ascites in patients with ovarian cancer. Although several studies have shown that Lysophosphatidic acid (LPA) related to the progression of ovarian cancer, which LPA receptors (LPARs) and G-coupled protein subtypes mediated in LPA actions have not been clearly elucidated. This study aimed to clarify the roles of LPA and it is subtype-specific LPARs mediating mechanisms in ovarian cancer integrated using bioinformatic analysis and biological experimental approaches. The big data analysis shown that LPA3 was the only differentially expressed LPA receptor among the six LPARs in ovarian cancer and further verified in immunohistochemistry of tissue microarrays. Also found that LPA3 was also highly expressed in ovarian cancer tissue and ovarian cancer cells. Importantly, LPA significantly promoted the proliferation and migration of LPA3-overexpressing ovarian cancer cells, while the LPA-induced actions blocked by Ki16425, a LPAR1/3 antagonist treated, and LPA3-shRNA transfected. In vivo study indicated that the LPA3-overexpressing cell-derived tumors metastasis, tumors volume, and tumors mass were apparently increased in xenografted nude mice. In addition, we also observed that LPA3 was differential high expression in ovarian cancer tissue of the patients. Our studies further confirmed the LPA3/Gi/MAPKs/NF-κB signals were involved in LPA-induced oncogenic actions in ovarian cancer cells. Our findings indicated that the LPA3 might be a novel precise therapeutic target and potential biomarker for ovarian cancer. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Bioinformatics; LPA3; Ovarian cancer; Signaling mechanism; Xenografted mice model","antineoplastic agent; immunoglobulin enhancer binding protein; ki 16425; lysophosphatidic acid; lysophosphatidic acid 3; lysophosphatidic acid receptor; mitogen activated protein kinase; protein inhibitor; short hairpin RNA; tumor marker; unclassified drug; animal cell; animal experiment; animal model; animal tissue; Article; ascites; big data; bioinformatics; cancer size; carcinogenesis; cell migration; cell proliferation; controlled study; gene overexpression; histopathology; human; immunohistochemistry; in vivo study; male; metastasis; mouse; nonhuman; ovary cancer; protein expression; protein function; signal transduction; tissue microarray; tumor xenograft",Article,Scopus,2-s2.0-85122335133
"Ponmalar A., Dhanakoti V.","57205556561;56736841100;","An intrusion detection approach using ensemble Support Vector Machine based Chaos Game Optimization algorithm in big data platform",2022,"Applied Soft Computing","116",,"108295","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122267850&doi=10.1016%2fj.asoc.2021.108295&partnerID=40&md5=68b20e22cc9f7c3f0a5910cef5e57c09","The mainstream computing technology is not efficient in managing massive data and detecting network traffic intrusions, often including big data. The intrusions present in sustained network traffic and the massive host log event data cannot be effectively managed by conventional analytical tools, resulting in a huge number of false positives and a longer training time. This paper presents a novel technique to enhance the intrusion detection process by handling the fundamental big data complexities associated with different forms of heterogeneous security data. To achieve the earlier objective, the ensemble Support Vector Machine (SVM) is integrated with the Chaos Game Optimization (CGO) algorithm. The proposed methodology improves the intrusion classification accuracy and also identifies nine different types of attacks present in the UNSW-NB15 dataset. The efficiency of the proposed methodology is evaluated using statistical analysis and different performance metrics such as precision, recall, F1-score, accuracy, ROC curve, and confusion matrix by comparing it with different baseline models. The proposed methodology obtains an accuracy of 96.29% when compared to the chi-SVM (89.12%) and an improvement of 6.47% is noted in the proposed methodology in terms of accuracy when compared with the chi-SVM. The higher classification accuracy shows that the proposed methodology exhibit a fewer number of false positives when handling the security events in big data platforms. © 2021 Elsevier B.V.","Bigdata; Chaos Game Optimization; Classification; Intrusion; Support Vector Machine","Big data; Classification (of information); Computer games; Intrusion detection; Bigdata; Chaos game optimization; Classification accuracy; Data platform; False positive; Intrusion; Network traffic; Optimisations; Optimization algorithms; Support vectors machine; Support vector machines",Article,Scopus,2-s2.0-85122267850
"Zhang S.-N., Li Y.-Q., Ruan W.-Q., Liu C.-H.","57209942380;56578859500;57195468849;57233625200;","Would you enjoy virtual travel? The characteristics and causes of virtual tourists’ sentiment under the influence of the COVID-19 pandemic",2022,"Tourism Management","88",,"104429","","",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122093005&doi=10.1016%2fj.tourman.2021.104429&partnerID=40&md5=74d200f6b746611b59a9d509b2280f1a","There are numerous arguments regarding the attitudes of different tourists and scenic destination managers regarding virtual tourism. However, it remains difficult to understand the public's attitude towards virtual tourism in a crisis situation. In this way, under the influence of COVID-19, this study explores the public sentiment and drivers of virtual tourism using Python and the grounded theory method. The results reveal that tourists' positive sentiment in virtual tourism dominates, with few tourists showing negative or neutral sentiment polarity. Furthermore, there is an obvious law of time decay in the intensity of public sentiment. Especially as the crisis fades, the supplementary effect of virtual tourism on on-site tourism weakens. Moreover, project design, experience quality, travel convenience, travel cost, travel motivation and destination attractiveness are the critical factors affecting tourist sentiment. The findings provide implications for the sustainable development of both destination and virtual tourism in a new world order post-COVID-19. © 2021 Elsevier Ltd","Big data; Cause; COVID-19; Crisis situation; Sentiment analysis; Virtual tourism","COVID-19; sustainable development; tourism development; tourism management; tourism market; tourist behavior; tourist destination; travel behavior; virtual reality",Article,Scopus,2-s2.0-85122093005
"Ullah F., Babar M.A.","57209067685;6602842620;","On the scalability of Big Data Cyber Security Analytics systems",2022,"Journal of Network and Computer Applications","198",,"103294","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121240730&doi=10.1016%2fj.jnca.2021.103294&partnerID=40&md5=9a2c895efa5362a84d1f64b7521011ca","Big Data Cyber Security Analytics (BDCA) systems use big data technologies (e.g., Apache Spark) to collect, store, and analyse a large volume of security event data for detecting cyber-attacks. The volume of digital data in general and security event data in specific is increasing exponentially. The velocity with which security event data is generated and fed into a BDCA system is unpredictable. Therefore, a BDCA system should be highly scalable to deal with the unpredictable increase/decrease in the velocity of security event data. However, there has been little effort to investigate the scalability of BDCA systems to identify and exploit the sources of scalability improvement. In this paper, we first investigate the scalability of a Spark-based BDCA system with default Spark settings. We then identify Spark configuration parameters (e.g., execution memory) that can significantly impact the scalability of a BDCA system. Based on the identified parameters, we finally propose a parameter-driven adaptation approach, SCALER, for optimizing a system's scalability. We have conducted a set of experiments by implementing a Spark-based BDCA system on a large-scale OpenStack cluster. We ran our experiments with four security datasets. We have found that (i) a BDCA system with default settings of Spark configuration parameters deviates from ideal scalability by 59.5% (ii) 9 out of 11 studied Spark configuration parameters significantly impact scalability and (iii) SCALER improves the BDCA system's scalability by 20.8% compared to the scalability with default Spark parameter setting. The findings of our study highlight the importance of exploring the parameter space of the underlying big data framework (e.g., Apache Spark) for scalable cyber security analytics. © 2021 Elsevier Ltd","Adaptation; Big data; Configuration parameter; Cyber security; Scalability; Spark","Cybersecurity; Large dataset; Network security; Adaptation; Analytics systems; Configuration parameters; Cyber security; Data technologies; Large volumes; Security Analytics; Security events; System scalability; System use; Scalability",Article,Scopus,2-s2.0-85121240730
"Wang J.W., Williams M.","57211805784;57226656469;","Registries, Databases and Repositories for Developing Artificial Intelligence in Cancer Care",2022,"Clinical Oncology","34","2",,"e97","e103",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121210719&doi=10.1016%2fj.clon.2021.11.040&partnerID=40&md5=bf9578d35424a7e471df4bc385e69ce4","Modern artificial intelligence techniques have solved some previously intractable problems and produced impressive results in selected medical domains. One of their drawbacks is that they often need very large amounts of data. Pre-existing datasets in the form of national cancer registries, image/genetic depositories and clinical datasets already exist and have been used for research. In theory, the combination of healthcare Big Data with modern, data-hungry artificial intelligence techniques should offer significant opportunities for artificial intelligence development, but this has not yet happened. Here we discuss some of the structural reasons for this, barriers preventing artificial intelligence from making full use of existing datasets, and make suggestions as to enable progress. To do this, we use the framework of the 6Vs of Big Data and the FAIR criteria for data sharing and availability (Findability, Accessibility, Interoperability, and Reuse). We share our experience in navigating these barriers through The Brain Tumour Data Accelerator, a Brain Tumour Charity-supported initiative to integrate fragmented patient data into an enriched dataset. We conclude with some comments as to the limits of such approaches. © 2021 The Royal College of Radiologists","Artificial intelligence; Big Data; database; deep learning; registries; repository","artificial intelligence; factual database; human; neoplasm; register; Artificial Intelligence; Big Data; Databases, Factual; Humans; Neoplasms; Registries",Article,Scopus,2-s2.0-85121210719
"Bai K., Li K., Guo J., Chang N.-B.","36676931900;57192156636;57224911813;7202467963;","Multiscale and multisource data fusion for full-coverage PM2.5 concentration mapping: Can spatial pattern recognition come with modeling accuracy?",2022,"ISPRS Journal of Photogrammetry and Remote Sensing","184",,,"31","44",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121137471&doi=10.1016%2fj.isprsjprs.2021.12.002&partnerID=40&md5=0f8fe97ef671e47e178ea727d59c2297","In spite of a variety of PM2.5 modeling schemes, general guidance for full-coverage PM2.5 concentration mapping from satellite observations is still lacking. The current technical gap is tied to how to integrate multiscale data from multiple sources to generate a spatially contiguous map that can better recognize PM2.5 distribution patterns without compromising modeling accuracy. In this study, ten different PM2.5 concentration data sets were generated using distinct mapping strategies and compared to one another, aiming to facilitate full-coverage PM2.5 concentration mapping with a generalized approach. The inter-comparison results indicated that different mapping strategies could yield comparable modeling accuracy albeit distinct PM2.5 distributions over space. Although the inclusion of PM2.5 autocorrelation terms as predictors can markedly improve the modeling accuracy, spatial patterns of PM2.5 estimations could be apparently distorted under different parameter configurations. In an attempt to balance the conflicting objectives, the optimal PM2.5 mapping scheme was proposed for broadened applications. A daily full-coverage PM2.5 concentration data set with 5-km resolution in China between 2015 and 2020 was generated for a demonstration to infer an apparent decreasing trend of PM2.5 across China over the past five years. Besides, the examination of COVID-19 pandemic impacts on regional air quality variations reveals a pattern of marked PM2.5 concentration decrease that cannot be easily realized by site-based air quality measurements. It is indicative that the proposed approach in this study can offer an optimal framework in support of various full-coverage PM2.5 mapping practices. © 2021 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Aerosol optical depth; Air pollution; Big data analytics; Data fusion; Multiscale prediction; PM2.5","Air quality; Big data; Data Analytics; Mapping; Pattern recognition; Aerosol optical depths; Concentration data; Data set; Mapping strategy; Modeling accuracy; Multi-scale datum; Multiscale predictions; Multisource data; PM 2.5; Spatial patterns; Data fusion; accuracy assessment; air quality; concentration (composition); data assimilation; mapping method; particulate matter; pattern recognition; satellite altimetry; spatial analysis; China",Article,Scopus,2-s2.0-85121137471
"Zhukov D., Khvatova T., Millar C., Andrianova E.","57189660218;25027867600;8893361700;57200555430;","Beyond big data – new techniques for forecasting elections using stochastic models with self-organisation and memory",2022,"Technological Forecasting and Social Change","175",,"121425","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120999207&doi=10.1016%2fj.techfore.2021.121425&partnerID=40&md5=26c5afdd026240163a29790fb6edea15","This paper introduces an innovative social process model addressing population-wide measures of voter preferences that was tested on data from the 2016 US presidential election. Population-wide, “macroscopic” parameters are needed when privacy, ethics or regulatory constraints block “big data” techniques (e.g., in political contexts to counter “micro-targeting”). Confidence will be eroded if existing trend models and other macroscopic approaches frequently fail to predict outcomes, however campaign data reveal mathematical features that suggest a different possible approach. Given that the populations modelled exhibit self-organisation and memory when transmitting viewpoints, our model is based on mathematical representations of such processes. Its validation indicates the applicability and potential generalisability of this theoretical approach. In order to design a stochastic dynamics model of changing voter preferences, we evaluated probability models for transitions between possible system states (magnitudes of voter preferences), formulated the boundary task for probability density functions and derived a second-order non-linear differential equation incorporating self-organisation and memory. We find consistent dependencies between influences on the system and its reaction, and it is congruent with empirical data. The ability to use researchable global parameters indicates the potential for modelling electoral processes and wider applicability for complex social processes, avoiding dependence on “internal” variables. © 2021","Electoral processes; Memory; Probability density oscillations; Self-organization; Social process modelling; Stochastic dynamics","Big data; Differential equations; Population statistics; Probability; Stochastic models; Stochastic systems; Density oscillations; Electoral process; Probability densities; Probability density oscillation; Process-models; Self organizations; Social process; Social process modeling; Stochastic dynamics; Stochastic-modeling; Probability density function; election; electoral geography; empirical analysis; ethics; memory; self organization; stochasticity; United States",Article,Scopus,2-s2.0-85120999207
"Dwivedi A., Moktadir M.A., Chiappetta Jabbour C.J., de Carvalho D.E.","57208524270;57200032197;57219923759;57369160600;","Integrating the circular economy and industry 4.0 for sustainable development: Implications for responsible footwear production in a big data-driven world",2022,"Technological Forecasting and Social Change","175",,"121335","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120955401&doi=10.1016%2fj.techfore.2021.121335&partnerID=40&md5=6f4039b5c37f5668b08d9353d05391e4","In the past few years, the implementation of a more sustainable mode of production has become necessary for manufacturing companies to thrive in the international business context. Professionals and researchers are also paying increasing attention to Industry 4.0 (I4.0) and big data, as well as Circular Economy (CE) concepts, due to the many advantages they can provide for the manufacturing industry. The purpose of the CE is the effective utilization of resources, and the I4.0 concept has been established to help achieve this objective and requires further exploration in the field of sustainable production. Research gaps exist in the literature regarding challenges to I4.0 and CE in terms of Sustainable Footwear Production (SFP) in emerging economies. Therefore, the purpose of this study is to deliver an original analysis of the most significant challenges regarding the interaction of I4.0 and CE for SFP. A comprehensive literature review is performed to establish the challenges to I4.0-CE for SFP in the context of emerging economies. Further, the challenges identified are prioritized based on the assessment of a real case study from the footwear industry. A mixed methodology (i.e., both qualitative and quantitative) is adopted to identify the cause-effect interactions among the identified challenges. The results of our analysis show that the five most important causal challenges obstructing the execution of SFP in the footwear industry are ‘deficient legislative-industry strategies towards SFP adoption’, ‘lack of competence in I4.0 and CE concepts for SFP adoption’, ‘understanding of synchronization among suppliers for attaining SFP’, ‘uncertain economic benefits of I4.0 and CE application for attaining SFP’ and ‘inadequate support for industries towards SFP efforts’. This study may be beneficial for industry managers, practitioners and decision makers to establish an understanding of I4.0-CE initiatives and to mitigate the most significant challenges regarding the successful implementation of I4.0-CE for SFP. © 2021","Big data; Circular economy; Green recovery; Industry 4.0; Sustainable Development Goals; Sustainable production","Decision making; Industrial economics; Industry 4.0; Planning; Sustainable development; Business contexts; Circular economy; Data driven; Emerging economies; Footwear industry; Green recovery; International business; Manufacturing companies; Sustainable development goal; Sustainable production; Big data",Article,Scopus,2-s2.0-85120955401
"Blickensdörfer L., Schwieder M., Pflugmacher D., Nendel C., Erasmi S., Hostert P.","57207688863;55867250100;14048914300;55894060300;24174158300;6602798575;","Mapping of crop types and crop sequences with combined time series of Sentinel-1, Sentinel-2 and Landsat 8 data for Germany",2022,"Remote Sensing of Environment","269",,"112831","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120936631&doi=10.1016%2fj.rse.2021.112831&partnerID=40&md5=57949d1280f0cf1c3bc262c3b1b826f9","Monitoring agricultural systems becomes increasingly important in the context of global challenges like climate change, biodiversity loss, population growth, and the rising demand for agricultural products. High-resolution, national-scale maps of agricultural land are needed to develop strategies for future sustainable agriculture. However, the characterization of agricultural land cover over large areas and for multiple years remains challenging due to the locally diverse and temporally variable characteristics of cultivated land. We here propose a workflow for generating national agricultural land cover maps on a yearly basis that accounts for varying environmental conditions. We tested the approach by mapping 24 agricultural land cover classes in Germany for the three years 2017, 2018, and 2019, in which the meteorological conditions strongly differed. We used a random forest classifier and dense time series data from Sentinel-2 and Landsat 8 in combination with monthly Sentinel-1 composites and environmental data and evaluated the relative importance of optical, radar, and environmental data. Our results show high overall accuracy and plausible class accuracies for the most dominant crop types across different years despite the strong inter-annual meteorological variability and the presence of drought and non-drought years. The maps show high spatial consistency and good delineation of field parcels. Combining optical, SAR, and environmental data increased overall accuracies by 6% to 10% compared to single sensor approaches, in which optical data outperformed SAR. Overall accuracy ranged between 78% and 80%, and the mapped areas aligned well with agricultural statistics at the regional and national level. Based on the multi-year dataset we mapped major crop sequences of cereals and leaf crops. Most crop sequences were dominated by winter cereals followed by summer cereals. Monocultures of summer cereals were mainly revealed in the Northwest of Germany. We showcased that high spatial and thematic detail in combination with annual mapping will stimulate research on crop cycles and studies to assess the impact of environmental policies on management decisions. Our results demonstrate the capabilities of integrated optical time series and SAR data in combination with variables describing local and seasonal environmental conditions for annual large-area crop type mapping. © 2021 The Authors","Agricultural land cover; Analysis-ready data; Big data; Large-area mapping; Multi-sensor; Optical remote sensing; SAR; Time series","Big data; Biodiversity; Climate change; Decision trees; Drought; Environmental protection; Food supply; Mapping; Population statistics; Remote sensing; Sustainable development; Synthetic aperture radar; Time series; Time series analysis; Agricultural land; Agricultural land cover; Analyse-ready data; Area mapping; Land cover; Large-area mapping; Multi sensor; Optical remote sensing; SAR; Times series; Crops; agricultural land; cropping practice; land cover; Landsat; Sentinel; synthetic aperture radar; time series analysis; Germany",Article,Scopus,2-s2.0-85120936631
"EL Azzaoui A., Sharma P.K., Park J.H.","57215058883;57191076911;57305596300;","Blockchain-based delegated Quantum Cloud architecture for medical big data security",2022,"Journal of Network and Computer Applications","198",,"103304","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120905111&doi=10.1016%2fj.jnca.2021.103304&partnerID=40&md5=387a39b8b0fe6b68d8a5857e834d0421","Smart Healthcare systems compromise complex computations such as visualization of molecules, analysis of DNA, and therapy determination. These are considered to be complex problems that today's supercomputers are still facing. On the other hand, Quantum computing promises fast, efficient, and scalable computing resources that are sufficient to compute large and complex operations in exponential time. It is a fact that Quantum computing will adequately innovate the computation perspective. However, it is not a feasible solution yet as it is likely to be rare and highly expensive to produce. This paper presents a Quantum Cloud-as-a-service for an efficient, scalable, and secure solution for complex Smart Healthcare computations. Our novelty resides in the usage of Quantum Terminal Machines (QTM) and Blockchain technology to enhance the feasibility and security of the proposed architecture. Experimental results prove the feasibility of the architecture and the absolute security of the implemented Q-OTP encryption. © 2021 Elsevier Ltd","Blockchain; Cloud computing; Privacy; Quantum cloud-as-a-service; Security; Smart healthcare","Architecture; Big data; Blockchain; Cloud computing; Cloud data security; Computer architecture; Cryptography; Data privacy; Quantum efficiency; Supercomputers; Block-chain; Cloud architectures; Cloud-computing; Complex computation; Privacy; Quantum cloud-as-a-service; Quantum Computing; Security; Smart healthcare; Smart healthcare systems; Health care",Article,Scopus,2-s2.0-85120905111
"Xuan L.","57368035100;","Big data-driven fuzzy large-scale group decision making (LSGDM) in circular economy environment",2022,"Technological Forecasting and Social Change","175",,"121285","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120899745&doi=10.1016%2fj.techfore.2021.121285&partnerID=40&md5=d535f2944e1a1a04201dc441195d1c0a","With the continuous development of big data techniques, more and more group decision-making problems involve multiple decision-makers. In the era of big data, massive data can improve decision-making ability and provide good support for decision-making. Large-Scale Group Decision Making (LSGDM) has been developed, and it discusses many cases where decision-makers are related to the decision-making process. However, the current LSGDM model has some shortcomings. To end this issue, this paper proposed a novel LSGDM method to study the role of the big data-driven decision-making issue. In this regard, existing literature and expert interviews were used to collect research standards. Then, a fuzzy LSGDM judgement matrix decision-making method based on group analysis is proposed. Finally, the method is applied to evaluate the takeout service platform. The results show that seven respondents need to adjust the acceptable consistency of the fuzzy judgement matrix, and they can quickly reach a satisfactory consensus. The results can further enrich and improve the group decision-making method based on a fuzzy judgement matrix. © 2021","Big data; Fuzzy sets; Large-scale group decision making (LSGDM); Policy decision; Social network","Big data; Matrix algebra; Social networking (online); Data driven; Decision makers; Decision-making method; Decisions makings; Fuzzy judgment matrixes; Group Decision Making; Large-scale group; Large-scale group decision making; Policy decisions; Social network; Decision making; data set; decision making; fuzzy mathematics; matrix; policy analysis; social network",Article,Scopus,2-s2.0-85120899745
"Moodi F., Saadatfar H.","57422685600;35318872400;","An improved K-means algorithm for big data",2022,"IET Software","16","1",,"48","59",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120852742&doi=10.1049%2fsfw2.12032&partnerID=40&md5=65d3c1ca2ca8920350913d05e7b3e240","An improved version of K-means clustering algorithm that can be applied to big data through lower processing loads with acceptable precision rates is presented here. In this method, the distances from one point to its two nearest centroids were used along with their variations in the last two iterations. Points with an equidistance threshold greater than the equidistance index were eliminated from the distance calculations and were stabilised in the cluster. Although these points are compared with the research index —cluster radius—again in the algorithm iteration, the excluded points are again included in the calculations if their distances from the stabilised cluster centroid are longer than the cluster radius. This can improve the clustering quality. Computerised tests as well as synthetic and real samples show that this method is able to improve the clustering quality by up to 41.85% in the best-case scenario. According to the findings, the proposed method is very beneficial to big data. © 2021 The Authors. IET Software published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.",,"Iterative methods; K-means clustering; Cluster centroids; Clustering quality; Computerized tests; Distance calculation; Improved K-Means algorithm; K-means clustering algorithms; Precision rates; Processing load; Real samples; Research index; Big data",Article,Scopus,2-s2.0-85120852742
"Naef M., Chadha K., Lefsrud L.","57366392500;57366680600;6506669804;","Decision support for process operators: Task loading in the days of big data",2022,"Journal of Loss Prevention in the Process Industries","75",,"104713","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120787201&doi=10.1016%2fj.jlp.2021.104713&partnerID=40&md5=ff10761b5aaf8513aaa51c20dc29cdf7","Modern chemical processes rely on distributed control systems to make the repetitive and routine adjustments to maintain steady operation. Operators are still required to “supervise the (system) supervisor” and intervene when variables exceed pre-programmed parameters to avert major incidents. Research in human-computer interaction and advanced process control has often focused on data-driven methods for fault detection as distinct from operator effectiveness. In this paper, we explore the application of a novel data-driven fault-detection technique to enhance operator decision support. During a simulated abnormal event, three users attempted to diagnose the root cause of a process upset using a traditional or standard interface, then with the addition of causal maps, in a A-B-A single-subject design. The causal maps were derived using a hierarchical method that could be applied to a wide range of chemical processes as an online, adaptive augmentation for abnormal situation management. Using a think-aloud technique, the three participants developed high quality insights into the process without negatively impacting the overall task load. These preliminary findings challenge prevailing wisdom in process control interface design, which often focuses on de-cluttering displays at the cost of information resolution. © 2021 Elsevier Ltd","Chemical process industry; Cognitive task load; Complex causality; Distributed control system (DCS); Operator training; Transfer entropy","Big data; Chemical industry; Distributed parameter control systems; Fault detection; Human computer interaction; Personnel training; Process control; Causal Maps; Chemical process; Chemical process industry; Cognitive task; Cognitive task load; Complex causality; Decision supports; Distributed control system; Operator training; Transfer entropy; Decision support systems",Article,Scopus,2-s2.0-85120787201
"Li S., Gong Q., Li H., Chen S., Liu Y., Ruan G., Zhu L., Liu L., Chen H.","57222486182;36484463300;57205187177;57209773502;57211088304;57211408500;57364995200;16052834600;57161643600;","Automatic location scheme of anatomical landmarks in 3D head MRI based on the scale attention hourglass network",2022,"Computer Methods and Programs in Biomedicine","214",,"106564","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120746329&doi=10.1016%2fj.cmpb.2021.106564&partnerID=40&md5=f22202ae02ec7630df16da36f625a823","Background and Objective: An anatomical landmark is biologically meaningful point in medical images and often used for medical image registration. The purpose of this study is to automatically locate anatomical landmarks from 3D medical images. Methods: A two-step automatic location scheme of anatomical landmarks in 3D medical image was designed in this study. In the first step, the full convolutional neural network was used for slice detection from a 3D medical image. In the second step, the scale attention hourglass network was used for landmark location in the detected slice and could overcome the difficulty of similar anatomical structures and different image parameters. This method was implemented and tested on four stable anatomical landmarks in 3D head MRI. Results: A total of 500 and 300 3D head volumes were used for training and testing, respectively. Results showed that the slice detection accuracy reached 85.7% and that the maximum location error was less than one slice. The average accuracy of the four anatomical landmarks in the detected slice reached 87.2%, and the spatial distance was 2.4 ± 2.4, which obtained better performance compared with hourglass network and feature pyramid networks. Conclusions: This method can be useful for locating anatomical landmarks in 3D head MRI and provides technical support for medical image registration and big data analysis. © 2021 Elsevier B.V.","Anatomical landmark; Full convolutional neural network; Medical image; Scale attention hourglass network","Convolution; Convolutional neural networks; Image registration; Location; Medical imaging; 3D head; 3D medical image; Anatomical landmarks; Automatic location; Convolutional neural network; Full convolutional neural network; Medical image; Medical image registration; Scale attention hourglass network; Slicer detection; Magnetic resonance imaging; anatomic landmark; article; attention; big data; cephalometry; controlled study; convolutional neural network; human; image registration; nuclear magnetic resonance imaging; attention; image processing; three-dimensional imaging; Attention; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Neural Networks, Computer",Article,Scopus,2-s2.0-85120746329
"Chen Y., Zhang Y., Coffman D.M., Mi Z.","57364606000;57196487644;56603101200;55272317400;","An environmental benefit analysis of bike sharing in New York City",2022,"Cities","121",,"103475","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120677620&doi=10.1016%2fj.cities.2021.103475&partnerID=40&md5=557b308f7d060738093d4990721f894c","Bike-sharing systems provide the public with more transport choices. To elaborate more effectively and comprehensively bike sharing's prospective contribution to urban sustainable development, a quantitative estimation of bike sharing's environmental benefits is performed through a case study of New York City's bike-sharing systems. Using a long-term series of big data, the environmental benefits of bike sharing in New York City are evaluated from a spatiotemporal perspective. Data on a total of 48 million bicycle trips between January 2014 and December 2017 are analysed. During 2014-2017, bike travellers saved 13,370 t of oil equivalent, and decreased 30,070 t of carbon emissions and 80 t of nitrogen oxides. Evaluation of gender dynamics reveals that men produced greater environmental benefits through the bike-sharing initiative. © 2021","Big data; Bike-sharing systems; Carbon emissions; Energy consumption; Gender; Nitrogen oxides","carbon emission; cycle transport; data set; energy use; environmental protection; gender relations; nitrogen oxides; public transport; urban transport; New York [New York (STT)]; New York [United States]; United States",Article,Scopus,2-s2.0-85120677620
"Li X., Li Y., Jia T., Zhou L., Hijazi I.H.","57192490717;56084217400;23394880800;57218079801;36727578900;","The six dimensions of built environment on urban vitality: Fusion evidence from multi-source data",2022,"Cities","121",,"103482","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120620330&doi=10.1016%2fj.cities.2021.103482&partnerID=40&md5=ae1709d1ea7211e6d363609eb73ab2db","Long-standing attention has been given to urban vitality and its association with the built environment (BE). However, the multiplicity and complex impacts of BE factors that shape urban vitality patterns have not been fully explored. For this purpose, multisource data from 1025 communities in Wuhan, China, were combined to explore the BE vitality nexus. A deep learning method was explored to segment street-view images, on which a composite indicator of urban vitality was developed with social media data. Then, six dimensions of BE factors, neighbourhood attributes, urban form and function, landscape, location, and street configuration, were incorporated into a spatial regression model to systematically examine the composite influences. The results show that population density, community age, open space, the sidewalk ratio, streetlights, shopping and leisure density, integration, and proximity to transportation are positive factors that induce urban vitality, whereas the effects of road density, proximity to parks, and green space have the opposite results. This study contributes to an improved understanding of the BE nexus. Managerial implications for mediating the relationship between planning policies and urban design strategies for the optimization of resource allocation and promotion of sustainable development are discussed. © 2021 Elsevier Ltd","Big data; Built environment; Community; Urban vitality; Wuhan","greenspace; open space; optimization; planning process; public space; resource allocation; strategic approach; sustainable development; urban design; urban geography; urban planning; China; Hubei; Wuhan",Article,Scopus,2-s2.0-85120620330
"Yin X., Liu Q., Huang X., Pan Y.","57219119358;55553889600;55255485200;56498216400;","Perception model of surrounding rock geological conditions based on TBM operational big data and combined unsupervised-supervised learning",2022,"Tunnelling and Underground Space Technology","120",,"104285","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120455089&doi=10.1016%2fj.tust.2021.104285&partnerID=40&md5=7837e2934e9a484fa4d11930309f2940","The perception of surrounding rock geological conditions ahead the tunnel face is essential for TBM safe and efficient tunnelling. This paper developed a perception approach of surrounding rock class based on TBM operational big data and combined unsupervised-supervised learning. In data preprocessing, four data mining techniques (i.e., Z-score, K-NN, Kalman filtering, and wavelet packet decomposition) were used to detect outliers, substitute outliers, suppress noise, and extract features, respectively. Then, GMM was used to revise the original surrounding rock class through clustering TBM load parameters and performance parameters in view of the shortcomings of the HC method in the TBM-excavated tunnel. After that, five various ensemble learning classification models were constructed to identify the surrounding rock class, in which model hyper-parameters were automatically tuned by Bayes optimization. In order to evaluate model performance, balanced accuracy, Kappa, F1-score, and training time were taken into account, and a novel multi-metric comprehensive ranking system was designed. Engineering application results indicated that LightGBM achieved the most superior performance with the highest comprehensive score of 6.9066, followed by GBDT (5.9228), XGBoost (5.4964), RF (3.7581), and AdaBoost (0.9946). Through the weighted purity reduction algorithm, the contributions of input features on the five models were quantitatively analyzed. Finally, the impact of class imbalance on model performance was discussed using the ADASYN algorithm, showing that eliminating class imbalance can further improve the model's perception ability. © 2021 Elsevier Ltd","Perception model; Supervised learning; Surrounding rock class; TBM; Unsupervised learning","Adaptive boosting; Big data; Data mining; Geology; Nearest neighbor search; Rocks; Statistics; Wavelet decomposition; Class imbalance; Class-based; Data preprocessing; Geological conditions; Modeling performance; Perception model; Surrounding rock; Surrounding rock class; TBM; Tunnel face; Supervised learning; algorithm; excavation; perception; rock mechanics; supervised learning; TBM",Article,Scopus,2-s2.0-85120455089
"Schwieder M., Wesemeyer M., Frantz D., Pfoch K., Erasmi S., Pickert J., Nendel C., Hostert P.","55867250100;57356961600;56428816500;57218387699;24174158300;57193168426;55894060300;6602798575;","Mapping grassland mowing events across Germany based on combined Sentinel-2 and Landsat 8 time series",2022,"Remote Sensing of Environment","269",,"112795","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120159637&doi=10.1016%2fj.rse.2021.112795&partnerID=40&md5=ffd7ace65d7d6cadd28cf897ddced0d0","Spatially explicit knowledge on grassland extent and management is critical to understand and monitor the impact of grassland use intensity on ecosystem services and biodiversity. While regional studies allow detailed insights into land use and ecosystem service interactions, information on a national scale can aid biodiversity assessments. However, for most European countries this information is not yet widely available. We used an analysis-ready-data cube that contains dense time series of co-registered Sentinel-2 and Landsat 8 data, covering the extent of Germany. We propose an algorithm that detects mowing events in the time series based on residuals from an assumed undisturbed phenology, as an indicator of grassland use intensity. A self-adaptive ruleset enabled to account for regional variations in land surface phenology and non-stationary time series on a pixel-basis. We mapped mowing events for the years from 2017 to 2020 for permanent grassland areas in Germany. The results were validated on a pixel level in four of the main natural regions in Germany based on reported mowing events for a total of 92 (2018) and 78 (2019) grassland parcels. Results for 2020 were evaluated with combined time series of Landsat, Sentinel-2 and PlanetScope data. The mean absolute percentage error between detected and reported mowing events was on average 40% (2018), 36% (2019) and 35% (2020). Mowing events were on average detected 11 days (2018), 7 days (2019) and 6 days (2020) after the reported mowing. Performance measures varied between the different regions of Germany, and lower accuracies were found in areas that are revisited less frequently by Sentinel-2. Thus, we assessed the influence of data availability and found that the detection of mowing events was less influenced by data availability when at least 16 cloud-free observations were available in the grassland season. Still, the distribution of available observations throughout the season appeared to be critical. On a national scale our results revealed overall higher shares of less intensively mown grasslands and smaller shares of highly intensively managed grasslands. Hotspots of the latter were identified in the alpine foreland in Southern Germany as well as in the lowlands in the Northwest of Germany. While these patterns were stable throughout the years, the results revealed a tendency to lower management intensity in the extremely dry year 2018. Our results emphasize the ability of the approach to map the intensity of grassland management throughout large areas despite variations in data availability and environmental conditions. © 2021 The Author(s)","Analysis-ready data; Big data; Common agricultural policy; Germany; Land use intensity; Large-area mapping; Multi-spectral data; Optical remote sensing; PlanetScope; Time series","Agriculture; Big data; Biodiversity; Ecosystems; Land use; Photomapping; Pixels; Remote sensing; Time series analysis; Analyse-ready data; Area mapping; Common agricultural policy; Germany; Land use intensity; Large-area mapping; Multi-spectral data; Optical remote sensing; Planetscope; Times series; Time series; biodiversity; ecosystem service; land surface; land use; Landsat; mapping; mowing; phenology; Sentinel; time series; Germany",Article,Scopus,2-s2.0-85120159637
"Cappa F., Franco S., Rosso F.","56574851700;57211944337;56830260800;","Citizens and cities: Leveraging citizen science and big data for sustainable urban development",2022,"Business Strategy and the Environment","31","2",,"648","667",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119859864&doi=10.1002%2fbse.2942&partnerID=40&md5=3a2acba82c251a8eacb0c79da93842e2","Citizen science (CS), that is, the involvement of citizens in data collection or analysis for research projects, is becoming more widespread. This is due to the increasing digitalization of the general public and due to the increasing number of grand challenges that society is facing. Thanks to the contributions of common citizens in data collection and data analysis conducted through technology-mediated interactions, CS can produce a number of benefits for researchers, public organizations, policymakers, citizens, and society as a whole. Given the high density of socio-economic activities in cities, CS can be implemented in a particularly effective way in urban environments to help tackle many “grand challenges”, namely, the pressing environmental and social issues that societies are facing at present. However, CS still has untapped potential to be explored. Indeed, we contend that even though CS involves citizens for precisely defined scientific objectives, the interaction that occurs can also be leveraged to collect data beyond the original aim, thereby producing big data (BD). Through a multiple case studies analysis, we highlight how CS can be used to collect BD as well, which can be a valuable resource for researchers, public organizations, and policymakers. With this aim in mind, this study proposes the definition of a citizen-sourcing framework that jointly employs CS and BD, and it highlights which processes can be implemented to favor the sustainable development of urban environments. Moreover, we also discuss the looming dangers associated with citizen-sourcing as a result of technology-mediated interactions and the use of digital technologies, and we highlight possible future developments. © 2021 The Authors. Business Strategy and The Environment published by ERP Environment and John Wiley & Sons Ltd.","big data; citizen science; citizen-sourcing; crowd; digitalization; grand challenge; sustainability; urban areas",,Article,Scopus,2-s2.0-85119859864
"Zhu X.X., Qiu C., Hu J., Shi Y., Wang Y., Schmitt M., Taubenböck H.","55696622200;57194601941;57192207722;55495784300;38663687700;55547115907;8698790500;","The urban morphology on our planet – Global perspectives from space",2022,"Remote Sensing of Environment","269",,"112794","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119657449&doi=10.1016%2fj.rse.2021.112794&partnerID=40&md5=7a3d0c2ffbe5e6464d77321e67e7b7bb","Urbanization is the second largest mega-trend right after climate change. Accurate measurements of urban morphological and demographic figures are at the core of many international endeavors to address issues of urbanization, such as the United Nations’ call for “Sustainable Cities and Communities”. In many countries – particularly developing countries –, however, this database does not yet exist. Here, we demonstrate a novel deep learning and big data analytics approach to fuse freely available global radar and multi-spectral satellite data, acquired by the Sentinel-1 and Sentinel-2 satellites. Via this approach, we created the first-ever global and quality controlled urban local climate zones classification covering all cities across the globe with a population greater than 300,000 and made it available to the community (https://doi.org/10.14459/2021mp1633461). Statistical analysis of the data quantifies a global inequality problem: approximately 40% of the area defined as compact or light/large low-rise accommodates about 60% of the total population, whereas approximately 30% of the area defined as sparsely built accommodates only about 10% of the total population. Beyond, patterns of urban morphology were discovered from the global classification map, confirming a morphologic relationship to the geographical region and related cultural heritage. We expect the open access of our dataset to encourage research on the global change process of urbanization, as a multidisciplinary crowd of researchers will use this baseline for spatial perspective in their work. In addition, it can serve as a unique dataset for stakeholders such as the United Nations to improve their spatial assessments of urbanization. © 2021 The Authors","Big data; Data fusion; Deep learning; Global inequality; Global urban LCZ dataset; Local climate zones; Remote sensing; Sentinels; Urban morphology","Big data; Climate change; Data Analytics; Data fusion; Deep learning; Developing countries; Economic and social effects; Morphology; Population statistics; Quality control; Deep learning; Global inequalities; Global perspective; Global urban LCZ dataset; Local climate; Local climate zone; Remote-sensing; Sentinel; United Nations; Urban morphology; Remote sensing; geographical region; global change; morphology; planet; stakeholder; United Nations; urban morphology; urbanization; Satellites",Article,Scopus,2-s2.0-85119657449
"Li L., Lin J., Ouyang Y., Luo X.R.","57200498748;55888042200;57344878300;57225907579;","Evaluating the impact of big data analytics usage on the decision-making quality of organizations",2022,"Technological Forecasting and Social Change","175",,"121355","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119356785&doi=10.1016%2fj.techfore.2021.121355&partnerID=40&md5=4d6fbd26db3184ac4ead7e3c81385d96","Big data initiatives are critical for transforming traditional organizational decision making into data-driven decision making. However, prior information systems research has not paid enough attention to the impact of big data analytics usage on decision-making quality. Drawing on the dynamic capability theory, this study investigated the impact of big data analytics usage on decision-making quality and tested the mediating effect of data analytics capabilities. We collected data from 240 agricultural firms in China. The empirical results showed that big data analytics usage had a positive impact on decision-making quality and that data analytics capabilities played a mediating role in the relationship between big data analytics usage and decision-making quality. Hence, firms should not only popularize big data analytics usage in their business activities but also take measures to improve their data analytics capabilities, which will improve their decision-making quality toward competitive advantages. © 2021 Elsevier Inc.","Agricultural firms; Big data analytics usage; Data analytics capabilities; Decision-making quality","Advanced Analytics; Agriculture; Big data; Competition; Data Analytics; Decision theory; Metadata; Agricultural firm; Big data analytic usage; Data analytic capability; Data analytics; Data driven decision; Decision-making quality; Decisions makings; Information system research; Organizational decision making; Prior information; Decision making; agricultural market; business development; data acquisition; decision analysis; decision making; empirical analysis; organization; China",Article,Scopus,2-s2.0-85119356785
"Shen Y., Guo B., Shen Y., Duan X., Dong X., Zhang H., Zhang C., Jiang Y.","57191664311;55695245700;55055827000;57191651500;57191647259;56999958300;15761381300;55733597600;","Personal big data pricing method based on differential privacy",2022,"Computers and Security","113",,"102529","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119301362&doi=10.1016%2fj.cose.2021.102529&partnerID=40&md5=a7cb26dfd922c9b356ec945ed1c7a2c1","Personal big data can greatly promote social management, business applications, and personal services, and bring certain economic benefits to users. The difficulty with personal big data security and privacy protection lies in realizing the maximization of the value of personal big data and in striking a balance between data privacy protection and sharing on the premise of satisfying personal big data security and privacy protection. Thus, in this paper, we propose a personal big data pricing method based on differential privacy (PMDP). We design two different mechanisms of positive and reverse pricing to reasonbly price personal big data. We perform aggregate statistics on an open dataset and extensively evaluated its performance. The experimental results show that PMDP can provide reasonable pricing for personal big data and fair compensation to data owners, ensuring an arbitrage-free condition and finding a balance between privacy protection and data utility. © 2021","Data privacy; Differential privacy; Personal big data; Positive pricing; Privacy budget; Privacy compensation; Privacy protection; Reverse pricing","Big data; Budget control; Costs; Economics; Data pricing; Data security and privacy; Differential privacies; Positive pricing; Pricing methods; Privacy budget; Privacy compensation; Privacy protection; Reverse pricing; Security and privacy protection; Data privacy",Article,Scopus,2-s2.0-85119301362
"Gocer F., Sener N.","56203457300;56007348600;","Spherical fuzzy extension of AHP-ARAS methods integrated with modified k-means clustering for logistics hub location problem",2022,"Expert Systems","39","2","e12886","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119253560&doi=10.1111%2fexsy.12886&partnerID=40&md5=01d9d8558d6e3a3959e21fcb929423eb","Like most other industries, logistics services are currently encountering enormous transformation. Many companies worldwide are applying big data analytics to implement operational strategies and facilitate location selection. Logistics companies need to focus on now and explore some futures of logistics hub location problems. Creating a profitable logistic network is a crucial task for airline and postal services. To establish a cost-effective network, transportation expenses should be decreased, and networks should be simplified. Hub location problem is born out of these needs. The concept of hub cumulates the flows and makes networks more reliable. In this article, a novel hub location selection approach is introduced in a group decision making (GDM) environment with uncertainty by integrating a modified weighted k-means clustering algorithm with multi-criteria decision-making (MCDM) tools. The combined MCDM method integrates analytic hierarchy process (AHP) to measure criteria weights and Additive Ratio Assessment technique to measure the performance of hub location alternatives in a spherical fuzzy set (SFS) environment. The SFS has shown definite advantages in handling vagueness and uncertainty over crisp, fuzzy, or intuitionistic fuzzy sets to depict experts' evaluations with a richer structure, allowing for more representative decision making. Using Turkish logistics data, big data algorithm facilitates 15 possible locations, and these sites are ranked in order by the integrated GDM methodology. The validation of the proposed evaluation model is illustrated in an application of the network structure in Turkey. Finally, sensitivity and comparison evaluations are introduced to demonstrate the feasibility and effectiveness of the proposed approach. © 2021 John Wiley & Sons Ltd.",,"Additives; Big data; Cost effectiveness; Data Analytics; Decision making; Hierarchical systems; K-means clustering; Location; Spheres; Additive ratio assessment; Analytic hierarchy process; Fuzzy extension; Group Decision Making; Hub location; Hub location problems; K-means++ clustering; Location selection; Logistics hubs; Spherical fuzzy set; Analytic hierarchy process",Article,Scopus,2-s2.0-85119253560
"Munk J.K., Hansen M.F., Buhl H., Lind B.S., Bathum L., Jørgensen H.L.","57209137723;57340614700;57339983300;7101925077;6701672299;7202554813;","The 10 most frequently requested blood tests in the Capital Region of Denmark, 2010–2019 and simulated effect of minimal retesting intervals",2022,"Clinical Biochemistry","100",,,"55","59",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119209874&doi=10.1016%2fj.clinbiochem.2021.11.002&partnerID=40&md5=b26d8adda633caa3764c9bdec09af072","As healthcare costs continue to rise throughout the world, critical assessment of the appropriateness of expenses gain focus. Objectives: We aimed to describe the developments in test numbers of the 10 most frequently requested tests, and to simulate the effect of introducing minimal retesting intervals. Design &amp; Methods: Data from the blood tests – albumin, alanine transaminase, cholesterol, creatinine, C-reactive protein, hemoglobin, hemoglobin A1c, potassium, sodium, and thyrotropin – from 2,687,589 patients handled by the Capital Region of Denmark from 2010 to 2019 was used. Tallies of each test per year were graphed. A simulation of the effect of minimal retesting intervals on test count and blood sampling volume was performed by virtually removing requests made prior to a set of possible minimal retesting intervals. Results: Increases in requests were observed both from hospitals and general practitioners. The number of requests for hemoglobin A1c increased more than the other tests. The increases could not be accounted for by an increase in population size and aging of the population, and therefore suggests possible inappropriate increase in monitoring of patients. The simulated effect of applying minimal retesting intervals showed large reductions in tests and blood sampled. Conclusions: For hospitals, the simulation suggested that applying minimal retesting intervals could lead to significant reductions in both the number of blood tests performed and in the amount of blood drawn for testing. For general practitioners, the simulation showed only minimal reductions in number of tests and blood volume drawn. © 2021 The Author(s)","Big data; Frequently used blood tests; Laboratory information system; Minimal retesting interval; Simulation","glycosylated hemoglobin; hemoglobin A1c protein, human; blood examination; Denmark; female; human; male; metabolism; Denmark; Female; Glycated Hemoglobin A; Hematologic Tests; Humans; Male",Article,Scopus,2-s2.0-85119209874
"Shen F., Zhao L., Wang M., Du W., Qian F.","57205572163;57111082300;25925567300;7202953420;53663472900;","Data-driven adaptive robust optimization for energy systems in ethylene plant under demand uncertainty",2022,"Applied Energy","307",,"118148","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119189286&doi=10.1016%2fj.apenergy.2021.118148&partnerID=40&md5=9639c6cedc6ad1e4808e8111e0a51c88","The operational optimization of energy systems is of great significance for improving the overall efficiency of industrial processes. Facing new challenges brought by widespread uncertainties, a data-driven adaptive robust industrial multi-type energy systems optimization framework was proposed by bridging robust optimization and machine learning methods in this paper. The industrial data were used to capture the demand uncertainty of the actual process. Hybrid models of units were first developed considering the operational characteristics, and the energy system optimization model was then formed as a mixed-integer nonlinear programming problem. The uncertain parameter set of process power demands was formed by the process models using historical data of a whole operating period. Afterward, the uncertainty set was constructed by applying the robust kernel density estimation method, which can reduce conservatism by considering the distributional information. By integrating the derived data-driven uncertainty set, a two-stage adaptive robust optimization model aiming at minimizing the weighted total energy consumption was developed. The multi-level robust optimization model was reformulated as a tractable single-level model by employing the affine decision rule. A case study on a plant-wide industrial energy system in the ethylene plant was performed, and the minimum optimal energy consumption was 25,350 kg/h, whose price of robustness was only 2.18%. The robust optimization results can guide the operational optimization of energy systems under uncertainty for the operators of the ethylene plant. © 2021 Elsevier Ltd","Adaptive robust optimization; Energy systems; Industrial big data; Machine learning; Uncertainty","Big data; Energy utilization; Ethylene; Integer programming; Nonlinear programming; Uncertainty analysis; Adaptive robust optimization; Data driven; Demand uncertainty; Energy system optimizations; Energy systems; Ethylene plants; Industrial big data; Operational optimization; Robust optimization; Uncertainty; Machine learning; ethylene; machine learning; performance assessment; uncertainty analysis",Article,Scopus,2-s2.0-85119189286
"Shah T.R.","57340046700;","Can big data analytics help organisations achieve sustainable competitive advantage? A developmental enquiry",2022,"Technology in Society","68",,"101801","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119182423&doi=10.1016%2fj.techsoc.2021.101801&partnerID=40&md5=cbc03b1f3308edd65ed8c79a4c6523fc","Society is changing radically and fast, triggered by the digital revolution. Firms are thus looking for newer ways to attract, satisfy and retain those customers as their needs and aspirations change. Such newer ways constitute the firms’ quest to achieve competitive advantage. However, in this current digital era almost all the traditional “Porterian” competitive advantage barriers are difficult to sustain. Further, COVID has led to a rapid acceleration in the pace of digitalisation. Can the huge data generated in every digitally enabled entity of today be judiciously combined with other firm resources? Can this big data be transformed into meaningful knowledge which organisations can leverage to sustain and grow? Can big data provide a sustainable competitive advantage? In this conceptual paper, I use a knowledge-based approach analysed through resource-based view from the strategic management domain to search for answers to these questions. I introduce a holistic framework that integrates the following: (i) firm knowledge, (ii) managerial capabilities and decision-making, (iii) sustainable competitive advantage, (iv) big data analytics. I call this framework the “The Perpetual Model of BDA as a Sustainable Competitive Advantage”. I also propose a unique Analytic Maturity Model, which I call the SAMDDC-DAMPPC Model, which can help identify and classify firms based on their analytic maturity. © 2021 Elsevier Ltd","Big data analytics; Digital age; Knowledge-based view; Maturity model; Resource-based view; Sustainable competitive advantage","Big data; Customer satisfaction; Data Analytics; Decision making; Knowledge based systems; Knowledge management; Sustainable development; 'current; Competitive advantage; Digital age; Digital era; Digital revolution; Knowledge-based approach; Knowledge-based views; Maturity model; Resource-based view; Sustainable competitive advantages; Competition; competitiveness; data set; decision making; holistic approach; industrial development; industrial practice; knowledge based system; sustainability",Article,Scopus,2-s2.0-85119182423
"Dong Z., Yun Y., Sun Y., Wang F.","57339776600;57215008314;55849820900;56022925000;","Building use-wellbeing associations: A spatial perspective",2022,"Cities","121",,"103493","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119139356&doi=10.1016%2fj.cities.2021.103493&partnerID=40&md5=16f2078b12591c0a299f10631081511b","This paper examines the effects of mixed and dense building use patterns on life satisfaction of local residents. The analysis is empirically conducted by using a combination of a cross-sectional individual survey data and point-of-interest big data in Beijing. The results show that mixed building uses are positively associated with subjective wellbeing, but residents tend to be less satisfied with dense building use patterns. Additional results quantify evidence that the building use-wellbeing association needs to be contingent upon human lived experiences for consumption and social interaction in the uncertain geographical context. Findings of this study suggest that the hybrid application of location-based big data and traditional survey in urban contexts provides an alternative channel for recovering the built environment-wellbeing associations at fine geographical scales. © 2021 Elsevier Ltd","Big data; Building use; Neighborhood environment; Subjective wellbeing; Uncertainty problem","building; data set; life satisfaction; neighborhood; Beijing [China]; China",Article,Scopus,2-s2.0-85119139356
"Ferretti A., Ienca M., Velarde M.R., Hurst S., Vayena E.","57204154114;57188838443;57338253100;7005848789;6507718293;","The Challenges of Big Data for Research Ethics Committees: A Qualitative Swiss Study",2022,"Journal of Empirical Research on Human Research Ethics","17","1-2",,"129","143",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119041600&doi=10.1177%2f15562646211053538&partnerID=40&md5=38254193c581390e1fb159ee9e4cce2c","Big data trends in health research challenge the oversight mechanism of the Research Ethics Committees (RECs). The traditional standards of research quality and the mandate of RECs illuminate deficits in facing the computational complexity, methodological novelty, and limited auditability of these approaches. To better understand the challenges facing RECs, we explored the perspectives and attitudes of the members of the seven Swiss Cantonal RECs via semi-structured qualitative interviews. Our interviews reveal limited experience among REC members with the review of big data research, insufficient expertise in data science, and uncertainty about how to mitigate big data research risks. Nonetheless, RECs could strengthen their oversight by training in data science and big data ethics, complementing their role with external experts and ad hoc boards, and introducing precise shared practices. © The Author(s) 2021.","big data; biomedical research; ethics; IRBs; research ethics; responsible research","article; big data; data science; human; interview; medical research; professional standard; uncertainty",Article,Scopus,2-s2.0-85119041600
"Wang W., Wang Y., Zhang Y., Liu D., Zhang H., Wang X.","56948696300;57216707638;57214940357;57205502888;57336636200;57337345200;","PPDTS: Predicting potential drug–target interactions based on network similarity",2022,"IET Systems Biology","16","1",,"18","27",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119017291&doi=10.1049%2fsyb2.12037&partnerID=40&md5=1c3e640041bd2dc7f46d29e5fe09cdc7","Identification of drug–target interactions (DTIs) has great practical importance in the drug discovery process for known diseases. However, only a small proportion of DTIs in these databases has been verified experimentally, and the computational methods for predicting the interactions remain challenging. As a result, some effective computational models have become increasingly popular for predicting DTIs. In this work, the authors predict potential DTIs from the local structure of drug–target associations' network, which is different from the traditional global network similarity methods based on structure and ligand. A novel method called PPDTS is proposed to predict DTIs. First, according to the DTIs’ network local structure, the known DTIs are converted into a binary network. Second, the Resource Allocation algorithm is used to obtain a drug–drug similarity network and a target–target similarity network. Third, a Collaborative Filtering algorithm is used with the known drug–target topology information to obtain similarity scores. Fourth, the linear combination of drug–target similarity model and the target–drug similarity model are innovatively proposed to obtain the final prediction results. Finally, the experimental performance of PPDTS has proved to be higher than that of the previously mentioned four popular network-based similarity methods, which is validated in different experimental datasets. Some of the predicted results can be supported in UniProt and DrugBank databases. © 2021 The Authors. IET Systems Biology published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.",,"Big data; Bioinformatics; Collaborative filtering; Computational methods; Drug interactions; Bio-computing; Biology computing; Drug; Drug targets; Drug-target interactions; Local structure; Potential drug targets; Practical importance; Similarity models; Similarity network; Forecasting",Article,Scopus,2-s2.0-85119017291
"Villegas C.A., Martinez M.J.","57336669300;57337233000;","Lessons from Harvey: Improving traditional damage estimates with social media sourced damage estimates",2022,"Cities","121",,"103500","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119015233&doi=10.1016%2fj.cities.2021.103500&partnerID=40&md5=e7608e6cb0f426897d08a00a206c29d6","Social media systems and crowdsourced data sites were incredibly active during disasters. Residents, first responders, and officials all turn to these systems to impart information and make calls for assistance. These systems will likely continue to hold a central informational and communication role in future disasters. Analyzing the trends and information that come from these sources in real-time aids the recovery process and help public agencies, first responders and researchers more quickly assess damages during and immediately after a disaster. Traditional sources, such as the initial FEMA damage estimates can miss areas of heavy impact and are often time delayed by several weeks. Using Harris County, Texas in the Houston region and the 2017 Hurricane Harvey flooding, the study provides a novel use-case in crisis informatics. The study leverages calls for help during the flooding event to mine address-level information to proxy damage estimates at the parcel-level. The study finds 36- to 53% of Twitter-sourced damage estimates are not captured in the FEMA estimates, significantly augmenting initial estimates with new data – feasibly within hours after the information is first tweeted. Empirically, the study evaluates how parcel-level FEMA damage estimates and Twitter-sourced damage estimates complement each other to proxy damaged structures. © 2021 Elsevier Ltd","Big data; Damage assessments; Disaster maps; Flood; Smart City; Social media; Text analysis",,Article,Scopus,2-s2.0-85119015233
"Krishna S., Murthy U.V.","57222667795;57331486600;","Evolutionary tree-based quasi identifier and federated gradient privacy preservations over big healthcare data",2022,"International Journal of Electrical and Computer Engineering","12","1",,"903","913",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118955433&doi=10.11591%2fijece.v12i1.pp903-913&partnerID=40&md5=09a36500b68082644d75019ae70fef92","Big data has remodeled the way organizations supervise, examine and leverage data in any industry. To safeguard sensitive data from public contraventions, several countries investigated this issue and carried out privacy protection mechanism. With the aid of quasi-identifiers privacy is not said to be preserved to a greater extent. This paper proposes a method called evolutionary tree-based quasi-identifier and federated gradient (ETQI-FD) for privacy preservations over big healthcare data. The first step involved in the ETQI-FD is learning quasi-identifiers. Learning quasi-identifiers by employing information loss function separately for categorical and numerical attributes accomplishes both the largest dissimilarities and partition without a comprehensive exploration between tuples of features or attributes. Next with the learnt quasi-identifiers, privacy preservation of data item is made by applying federated gradient arbitrary privacy preservation learning model. This model attains optimal balance between privacy and accuracy. In the federated gradient privacy preservation learning model, we evaluate the determinant of each attribute to the outputs. Then injecting Adaptive Lorentz noise to data attributes our ETQI-FD significantly minimizes the influence of noise on the final results and therefore contributing to privacy and accuracy. An experimental evaluation of ETQI-FD method achieves better accuracy and privacy than the existing methods. © This is an open access article under the CC BY-SA license.","Adaptive Lorentz; Big data; Evolutionary tree; Federated gradient arbitrary; Privacy preservation; Quasi identifier",,Article,Scopus,2-s2.0-85118955433
"Campos A., Ferreira A.R., Gonçalves-Pinho M., Freitas A., Fernandes L.","57329835000;57199147690;57189639715;57326174600;24775678200;","Bipolar Disorder in pediatric patients: A nationwide retrospective study from 2000 to 2015",2022,"Journal of Affective Disorders","298",,,"277","283",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118880905&doi=10.1016%2fj.jad.2021.10.113&partnerID=40&md5=8845db490985f5e52c7c26b8f3d8b706","Background: Pediatric Bipolar Disorder (BD) has been the focus of increased attention. To date, in Portugal, there is no evidence available for pediatricBD-related hospitalization rates. This study aimed to describe and characterize all pediatric hospitalizations with a primary diagnosis of BD registered in Portugal from 2000 to 2015. Methods: A retrospective observational study was conducted. Pediatric (< 18 years) inpatient episodes with a primary diagnosis of BD were selected from a national administrative database. The ICD-9-CM codes 296.x (excluding 296.2x, 296.3x and 296.9x) identified the diagnosis of interest. Additionally, age at discharge, sex, psychiatric comorbidities, length of stay (LoS), admission type and date, in-hospital mortality and hospital charges were analyzed. Results: A total of 348 hospitalizations, representing 258 patients, were identified. The overall population-based rate of hospitalizations was 1.18/100 000 youths. A non-linear increase throughout the study period was found. Patients were mostly female (60.6%), with a median age of 16 years (Q1-Q3:14–17). Admissions were mostly emergent (81%), and the median LoS was 14 days (Q1-Q3:7–24). Moreover, about 26% of all episodes were readmissions. Mean estimated charges per episode were 3503.10€, totalizing 1.20M€. Limitations: Limitations include the use of secondary data and the retrospective nature of the study. Conclusions: Annual rates of pediatric BD hospitalizations showed a non-linear increase. These findings may contribute to better understand the pediatric BD burden. Nevertheless, more research is warranted, to better characterize sociodemographic and clinical trends in pediatric BD to prevent the high number of acute hospitalizations and readmissions of these patients. © 2021 Elsevier B.V.","Adolescent; Big data; Bipolar disorder; Child; Routinely collected health data","adolescent; bipolar disorder; child; female; hospital discharge; hospitalization; human; length of stay; male; retrospective study; Adolescent; Bipolar Disorder; Child; Female; Hospitalization; Humans; Length of Stay; Male; Patient Discharge; Retrospective Studies",Article,Scopus,2-s2.0-85118880905
"Ezerins M.E., Ludwig T.D., O'Neil T., Foreman A.M., Açıkgöz Y.","57250063600;7007149334;57322939800;55854089700;57188653984;","Advancing safety analytics: A diagnostic framework for assessing system readiness within occupational safety and health",2022,"Safety Science","146",,"105569","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118498733&doi=10.1016%2fj.ssci.2021.105569&partnerID=40&md5=0538649b787fed3fce088402f012a383","Big data and analytics have shown promise in predicting safety incidents and identifying preventative measures directed towards specific risk variables. However, the safety industry is lagging in big data utilization due to various obstacles, which may include lack of data readiness (e.g., disparate databases, missing data, low validity) and personnel competencies. This paper provides a primer on the application of big data to safety. We then describe a safety analytics readiness assessment framework that highlights system requirements and the challenges that safety professionals may encounter in meeting these requirements. The proposed framework suggests that safety analytics readiness depends on (a) the quality of the data available, (b) organizational norms around data collection, scaling, and nomenclature, (c) foundational infrastructure, including technological platforms and skills required for data collection, storage, and analysis of health and safety metrics, and (d) measurement culture, or the emergent social patterns between employees, data acquisition, and analytic processes. A safety-analytics readiness assessment can assist organizations with understanding current capabilities so measurement systems can be matured to accommodate more advanced analytics for the ultimate purpose of improving decisions that mitigate injury and incidents. © 2021 Elsevier Ltd","Data analytics; Occupational health; Readiness assessment; Safety analytics","Accident prevention; Big data; Data acquisition; Digital storage; Health; Industrial hygiene; Occupational risks; Personnel; Risk assessment; Data analytics; Data collection; Data utilization; Occupational health; Occupational safety and healths (OSH); Readiness assessment; Risk variables; Safety analytic; Safety incidents; Safety industry; Data Analytics; Article; data quality; health care quality; human; information processing; occupational health; occupational safety; risk reduction",Article,Scopus,2-s2.0-85118498733
"van der Weide R., van der Vlies V., van der Meer F.","55360128400;55608949500;57323355100;","Train driver experience: A big data analysis of learning and retaining the new ERTMS system",2022,"Applied Ergonomics","99",,"103627","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118481648&doi=10.1016%2fj.apergo.2021.103627&partnerID=40&md5=b0663ffc21ba40330aea5322c04f37f4","Now and in the coming years a new European Rail Traffic Management System (ERTMS) is being deployed on a larger scale. In the system level chosen in the Netherlands (Level 2) lineside signals will disappear and the train driver will receive all information needed for safe and efficient train operation through an interface inside the train cab. Furthermore, procedures, driving skills and communication will alter. Many train drivers will thus need to be trained in driving with ERTMS. Train operating companies (TOC) have to decide on efficient and effective strategies to organise this extensive training volume and to reassure that drivers become and remain proficient after training when roll-out of ERTMS may not have been fully completed. In this paper the central questions to be answered are: How many hours/duties does a driver have to do during a certain period of time on an ERTMS track section to become proficient with ERTMS (learning period), and to stay proficient (retaining period)? The approach taken is unique for rail industry (to the authors' knowledge) and is based on big data analyses. In this study, we were able to study the collective performance of train drivers, by big data analysis while at the same time maintaining privacy through pseudonymisation of the train driver database and their individual performances, characteristics and other potential privacy sensitive data. Databases containing detailed information on all planned and accomplished train service times/positions of the last three years (2015–2017) of a large TOC were combined with personal data from HR databases (age, experience, ERTMS training history, job assessment results, etcetera). From the data, delays caused by rolling stock and train operating related issues were identified. The amount and duration of these delays were correlated with the personal data to test research hypotheses. Again, at the same time we maintained privacy of the individuals’ personal data. Although the research has some limitations due to the small amount of existing ERTMS lines, and associated rolling stock, the results show some clear correlations. Thus, quantifiable directions to the minimum amount of ERTMS driving per 3 months during the first year and in the years after are presented. The results are now being translated into models for training and rostering related to the phase of ERTMS roll-out. © 2021 Elsevier Ltd","Big data; Competences; ERTMS; Experience; Rail; Train drivers; Training","Big data; Data privacy; Driver training; Railroad rolling stock; Railroad transportation; Competence; Driver experience; European rail traffic management systems; Experience; Large-scales; Level 2; Netherlands; System levels; Train drivers; Train operating companies; Database systems; adult; anonymization; article; big data; controlled study; human; identifiable information; learning; privacy; car driving; data analysis; learning; Netherlands; Automobile Driving; Data Analysis; Humans; Learning; Netherlands",Article,Scopus,2-s2.0-85118481648
"Liu Y., Wang Y., Shi C., Zhang W., Luo W., Wang J., Li K., Yeung N., Kite S.","57214950025;8980601600;57211659697;57317710500;57206266578;56502541200;55522528800;25028883300;25028017300;","Assessing the CO2 reduction target gap and sustainability for bridges in China by 2040",2022,"Renewable and Sustainable Energy Reviews","154",,"111811","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118246844&doi=10.1016%2fj.rser.2021.111811&partnerID=40&md5=d0580bc4b567a50ad9b288623e7c8780","In China, rapid and continuing infrastructure growth will significantly affect the feasibility of achieving CO2 emission reduction targets. It is uncertain whether the current infrastructure designs are consistent with the national emission targets of China, consequently posing a threat to the national commitment to reaching carbon neutrality. To illustrate the gap in achieving the emission targets with respect to bridges, we present carbon intensity reduction targets relevant to bridges by 2040. By exploring three policy–economic scenarios that combine with CO2 emission reduction policy scenarios of the International Energy Agency and shared socioeconomic pathways, we assess the adaptability of the bridge designs to short-, medium- and long-term carbon intensity targets. We propose a new sustainability design approach based on a bridge big data set that integrates reliability, environmental impacts, and cost. We show that, under the current policy–economic scenario, more than 50% of code-based bridges would meet the five-year CO2 emission target, whereas only one in ten thousand could meet the ten-year target under the new policy–economic scenario. Under the sustainable development policy–economic scenario, none of the bridge designs would meet the carbon reduction targets over the next 20 years. Our findings highlight that the bridges in China need to be duly considered in setting CO2 reduction targets, and in the national move toward zero carbon, careful consideration should be given to implementing our proposed bridge sustainability design. © 2021 Elsevier Ltd","Big data method; Bridge sustainability; CO2 emission target; Policy scenario; Reduction gap assessment; Sustainability design","Bridges; Carbon; Carbon dioxide; Emission control; Sustainable development; % reductions; Big data method; Bridge sustainability; CO 2 emission; CO2 emission target; Emission targets; Policy scenario; Reduction gap assessment; Reduction targets; Sustainability designs; Big data",Article,Scopus,2-s2.0-85118246844
"Lyreskog D.M., Pavarini G., Lorimer J., Jacobs E., Bennett V., Singh I.","56541713500;36816377200;57201352038;57223077455;57234458200;16246029200;","How to build a game for empirical bioethics research: The case of ‘Tracing Tomorrow’",2022,"Health Expectations","25","1",,"304","312",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118230068&doi=10.1111%2fhex.13380&partnerID=40&md5=2175207fcc6cb4cc056d983285b57f6b","It is becoming increasingly clear that the field of empirical bioethics requires methodological innovations that can keep up with the scale and pace of contemporary research in health and medicine. With that in mind, we have recently argued for Design Bioethics—the use of purpose-built, engineered research tools that allow researchers to investigate moral decision-making in ways that are embodied and contextualized. In this paper, we outline the development, testing and implementation of a novel prototype tool in the Design Bioethics Workshop—with each step illustrated with collected data. Titled ‘Tracing Tomorrow’ (www.tracingtomorrow.org), the tool is a narrative game to investigate young people's values and preferences in the context of digital phenotyping for mental health. The process involved (1) Working with young people to discover, validate and define the morally relevant cases or problems, (2) Building and testing the game concept in collaboration with relevant groups and game developers, (3) Developing prototypes that were tested and iterated in partnership with groups of young people and game developers and (4) Disseminating the game to young people to collect data to investigate research questions. We argue that Design Bioethics yields tools that are relevant, representative and meaningful to target populations and provide improved data for bioethics analysis. Patient or Public Contribution: In planning and conducting this study, we consulted with young people from a diverse range of backgrounds, including the NeurOX Young People's Advisory Group, the What Lies Ahead Junior Researchers Team, Censuswide youth participants and young people from the Livity Youth Network. © 2021 The Authors. Health Expectations published by John Wiley & Sons Ltd.","big data; coproduction; digital bioethics; mental health; predictive testing; young people","adolescent; article; big data; bioethics; controlled study; human; juvenile; mental health; narrative; phenotype; female; human experiment; male",Article,Scopus,2-s2.0-85118230068
"Lee L.W.Y., Sharma P., Barnes B.R.","57195267714;26434453900;8427055600;","Adopting big data to create an “outside-in” global perspective of guanxi",2022,"Journal of Business Research","139",,,"614","628",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117201706&doi=10.1016%2fj.jbusres.2021.09.074&partnerID=40&md5=c98e8db190b54a28c250d5df23e8453b","Previous research on the Chinese notion of guanxi has tended to use descriptive approaches to study its prevalence and influence on business and Chinese consumers. Relatively less research has focused on perceptions of guanxi in other parts of the world. This paper addresses this important research gap through adopting an “outside-in” global perspective using big data. In particular, the study draws on 162 million guanxi-related news articles during 2017–2020 extracted from the Global Database of Events, Language and Tone (GDELT), an open-source, real-time current affairs repository of online news and event metadata. The findings reveal that guanxi is heavily influenced by geopolitical and public health issues. The study also discovered a major contrast in the overall tone between China, being slightly positive, and the US and Germany, being largely negative, with the association varying according to changes in the marketing ecosystem. © 2021 Elsevier Inc.","Big data; Business-to-business relationships; Ecosystem; GDELT; Guanxi; Outside-in marketing",,Article,Scopus,2-s2.0-85117201706
"Meurer M.M., Waldkirch M., Schou P.K., Bucher E.L., Burmeister-Lamp K.","57291400200;56816878700;57220084014;55634225900;35110884700;","Digital affordances: how entrepreneurs access support in online communities during the COVID-19 pandemic",2022,"Small Business Economics","58","2",,"637","663",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116891365&doi=10.1007%2fs11187-021-00540-2&partnerID=40&md5=fc8695ee18bc8fed2230e4ef7136d52f","COVID-19 has caused significant and unforeseen problems for entrepreneurs. While entrepreneurs would normally seek social support to help deal with these issues, due to social distancing, physical networks are often not available. Consequently, entrepreneurs must turn to alternative support sources, such as online communities, raising the question of how support is created in such spaces. Drawing on an affordance perspective, we investigate how entrepreneurs interact with online communities and base our qualitative analysis on conversation data (76,365 posts) from an online community of entrepreneurs on Reddit during the COVID-19 pandemic. Our findings draw out four affordances that online communities offer to entrepreneurs (resolving problems, reframing problems, reflecting on situations, refocusing thinking and efforts), resulting in a framework of entrepreneurial support creation in online communities. Thus, our study contributes to debates around (1) entrepreneurs’ support during COVID-19 and (2) digital affordances in the entrepreneurship context. © 2021, The Author(s).","Affordances; Big data; COVID-19; Entrepreneurial support; Online communities",,Article,Scopus,2-s2.0-85116891365
"Neu D., Saxton G.D., Everett J., Rahaman A.","6701826464;55894326600;7102558461;6602103329;","The centrality of ethical utterances within professional narratives",2022,"Accounting History","27","1",,"75","94",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116662201&doi=10.1177%2f10323732211040272&partnerID=40&md5=818939c597f34e14afc0bcc813ca9468","This study examines the centrality of ethics within editorials published in the Canadian Institute of Chartered Accountants’ professional journal, CA Magazine, over the 1912 to 2010 period. Starting from the twin assumptions that editorials speak about appropriate professional behavior using a variety of words such as ‘ethics,’ ‘conduct,’ and ‘codes,’ and that appropriate professional behavior is situational, we use topic modeling techniques to identify these dimensions of ethical discourse. We then use social network analysis methods to map the position and centrality of ethics within the editorials across time. The results show that enunciations about appropriate professional conduct are broader than simply enunciations using the word ‘ethics’. The results also highlight that ethical utterances become more central, not less central, over time. © The Author(s) 2021.","accounting ethics; big data; professional ethics",,Article,Scopus,2-s2.0-85116662201
"Rahman M.G., Islam M.Z.","55338274700;57198634079;","Adaptive Decision Forest: An incremental machine learning framework",2022,"Pattern Recognition","122",,"108345","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116645829&doi=10.1016%2fj.patcog.2021.108345&partnerID=40&md5=6295f7efaa6f8fa881d5f647be7f0ccd","In this study, we present an incremental machine learning framework called Adaptive Decision Forest (ADF), which produces a decision forest to classify new records. Based on our two novel theorems, we introduce a new splitting strategy called iSAT, which allows ADF to classify new records even if they are associated with previously unseen classes. ADF is capable of identifying and handling concept drift; it, however, does not forget previously gained knowledge. Moreover, ADF is capable of handling big data if the data can be divided into batches. We evaluate ADF on nine publicly available natural datasets and one synthetic dataset, and compare the performance of ADF against the performance of eight state-of-the-art techniques. We also examine the effectiveness of ADF in some challenging situations. Our experimental results, including statistical sign test and Nemenyi test analyses, indicate a clear superiority of the proposed framework over the state-of-the-art techniques. © 2021 Elsevier Ltd","Big data; Concept drift; Decision forest algorithm; Incremental learning; Online learning","Big data; Data handling; Forestry; Machine learning; Concept drifts; Decision forest; Decision forest algorithm; Incremental learning; Natural dataset; Online learning; Performance; Splitting strategies; State-of-the-art techniques; Synthetic datasets; E-learning",Article,Scopus,2-s2.0-85116645829
"Jha A.K., Mithun S., Sherkhane U.B., Jaiswar V., Shi Z., Kalendralis P., Kulkarni C., Dinesh M.S., Rajamenakshi R., Sunder G., Purandare N., Wee L., Rangarajan V., Van Soest J., Dekker A.","57197687905;57163516700;57221671059;57221677446;57204588291;57204593162;57273601800;57204668762;25961228200;55920010400;7003293572;57213352753;6603448872;56156330000;57225379184;","Implementation of Big Imaging Data Pipeline Adhering to FAIR Principles for Federated Machine Learning in Oncology",2022,"IEEE Transactions on Radiation and Plasma Medical Sciences","6","2",,"207","213",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115751945&doi=10.1109%2fTRPMS.2021.3113860&partnerID=40&md5=5f59c0de39dade6fe16a82050e02c554","Cancer is a fatal disease and one of the leading causes of death worldwide. The cure rate in cancer treatment remains low; hence, cancer treatment is gradually shifting toward personalized treatment. Artificial intelligence (AI) and radiomics have been recognized as one of the potential areas of research in personalized medicine in oncology. Several researchers have identified the capabilities of AI and radiomics to characterize phenotype and there by predict the outcome of treatment in oncology. Although AI and radiomics have shown promising initial results in diagnosis and treatment in oncology, these technologies are also facing challenges of standardization and scalability. In the last few years, researchers have been trying to develop a research infrastructure for federated machine learning that increases the usability of Big Data for clinical research. These research infrastructures are based on the findable, accessible, interoperable, and reusable (i.e., FAIR) data principles. The India-Dutch 'big imaging data approach for oncology in a Netherlands India collaboration' (BIONIC) is a jointly funded initiative by the Dutch Research Council (NWO) and the Indian Ministry of Electronics and Information Technology (MeitY), aiming to introduce radiomic-based research into clinical environments using federated machine learning on geographically dispersed collections of FAIR data. This article described a prototype end-to-end research infrastructure implemented through the BIONIC partnership into a leading cancer care public hospital in India. © 2017 IEEE.","accessible; and reusable (FAIR) data; Artificial intelligence (AI); findable; interoperable; machine learning; natural language processing (NLP); radiomics","Big data; Clinical research; Diagnosis; Diseases; Hospitals; Learning algorithms; Machine learning; Medical imaging; Natural language processing systems; Oncology; Biomedical imaging; Data pipelines; FAIR data.; Fatal disease; Features extraction; Imaging data; Machine-learning; Radiomic; Research infrastructure; Data mining",Article,Scopus,2-s2.0-85115751945
"Yatracos Y.G.","6602642902;","Residual's influence index (RINFIN), bad leverage and unmasking in high dimensional L2-regression",2022,"Statistical Analysis and Data Mining","15","1",,"125","138",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115677894&doi=10.1002%2fsam.11550&partnerID=40&md5=d3f1be9bc4ec8ebbaf7f223c0a839458","In linear regression of Y on X(∈ Rp) with parameters β(∈ Rp+1), statistical inference is unreliable when observations are obtained from gross-error model, Fϵ,G = (1 − ϵ)F + ϵG, instead of the assumed probability F;G is gross-error probability, 0 &lt; ϵ &lt; 1. Residual's influence index (RINFIN) at (x, y) is introduced, with components measuring also the local influence of x in the residual and large value flagging a bad leverage case (from G), thus causing unmasking. Large sample properties of RINFIN are presented to confirm significance of the findings, but often the large difference in the RINFIN scores of the data is indicative. RINFIN is successful with microarray data, simulated, high dimensional data and classic regression data sets. RINFIN's performance improves as p increases and can be used in multiple response linear regression. © 2021 The Authors. Statistical Analysis and Data Mining published by Wiley Periodicals LLC.",,"Big data; Clustering algorithms; Data mining; Gross errors; High-dimensional; Higher-dimensional; Influence functions; Leverage; Local influence; Masking; P-values; Post-P-value era; Residual influence index; Linear regression",Article,Scopus,2-s2.0-85115677894
"Fishe J., Zheng Y., Lyu T., Bian J., Hu H.","57160424400;57208236332;57265980100;57265868800;57265980200;","Environmental effects on acute exacerbations of respiratory diseases: A real-world big data study",2022,"Science of the Total Environment","806",,"150352","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115319454&doi=10.1016%2fj.scitotenv.2021.150352&partnerID=40&md5=4b233f7ac46ff8db865433cf34b31c3f","Background: The effects of weather periods, race/ethnicity, and sex on environmental triggers for respiratory exacerbations are not well understood. This study linked the OneFlorida network (~15 million patients) with an external exposome database to analyze environmental triggers for asthma, bronchitis, and COPD exacerbations while accounting for seasonality, sex, and race/ethnicity. Methods: This is a case-crossover study of OneFlorida database from 2012 to 2017 examining associations of asthma, bronchitis, and COPD exacerbations with exposures to heat index, PM 2.5 and O 3. We spatiotemporally linked exposures using patients' residential addresses to generate average exposures during hazard and control periods, with each case serving as its own control. We considered age, sex, race/ethnicity, and neighborhood deprivation index as potential effect modifiers in conditional logistic regression models. Results: A total of 1,148,506 exacerbations among 533,446 patients were included. Across all three conditions, hotter heat indices conferred increasing exacerbation odds, except during November to March, where the opposite was seen. There were significant differences when stratified by race/ethnicity (e.g., for asthma in April, May, and October, heat index quartile 4, odds were 1.49 (95% confidence interval (CI) 1.42–1.57) for Non-Hispanic Blacks and 2.04 (95% CI 1.92–2.17) for Hispanics compared to 1.27 (95% CI 1.19–1.36) for Non-Hispanic Whites). Pediatric patients' odds of asthma and bronchitis exacerbations were significantly lower than adults in certain circumstances (e.g., for asthma during June – September, pediatric odds 0.71 (95% CI 0.68-0.74) and adult odds 0.82 (95% CI 0.79-0.85) for the highest quartile of PM 2.5). Conclusion: This study of acute exacerbations of asthma, bronchitis, and COPD found exacerbation risk after exposure to heat index, PM 2.5 and O 3 varies by weather period, age, and race/ethnicity. Future work can build upon these results to alert vulnerable populations to exacerbation triggers. © 2021 Elsevier B.V.","Asthma; Bronchitis; COPD; Heat index; O3; PM2.5","Big data; Pulmonary diseases; Acute exacerbations; Asthma; Bronchiti; Confidence interval; COPD; Environmental triggers; Heat indices; Hispanics; O3; PM 2.5; Pediatrics; ozone; asthma; environmental effect; environmental factor; ethnicity; racial identity; respiratory disease; temperature effect; trigger mechanism; adult; aged; air pollution; Article; asthma; big data; Black person; bronchitis; Caucasian; chronic obstructive lung disease; controlled study; data analysis; disease exacerbation; environmental factor; ethnicity; exposome; factual database; female; health hazard; heat; Hispanic; human; longitudinal study; major clinical study; male; middle aged; neighborhood; particulate matter 2.5; pediatric patient; race; residence characteristics; respiratory tract disease; seasonal variation; sex; study design; very elderly; weather; young adult; asthma; breathing disorder; child; chronic obstructive lung disease; crossover procedure; Florida [United States]; United States; Pierinae; Adult; Asthma; Big Data; Child; Cross-Over Studies; Humans; Pulmonary Disease, Chronic Obstructive; Respiration Disorders",Article,Scopus,2-s2.0-85115319454
"Tu B., Zhao Y., Yin G., Jiang N., Li G., Zhang Y.","57259025000;57259323300;57219225462;57200941236;55757220600;43362063400;","Research on intelligent calculation method of intelligent traffic flow index based on big data mining",2022,"International Journal of Intelligent Systems","37","2",,"1186","1203",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114933012&doi=10.1002%2fint.22665&partnerID=40&md5=24ffad42739c545c9483be54ae04d1a4","To understand the operating status of the road network and measure the traffic congestion problem, an intelligent calculation method for the intelligent traffic flow index based on big data mining is proposed. According to the error data discriminating rules, the error data in the traffic flow data is discriminated, all lanes are detected according to the data discriminating result, the traffic data of each lane are recorded in chronological order, and the traffic data is converted. Fuzzy data mining technology is used to predict the converted traffic flow, combined with traffic flow sequence segmentation and BP neural network model to realize the intelligent calculation of the smart traffic flow index. Experimental results show that the method can achieve accurate calculation of daily and weekly smart traffic index, and the calculation time is short, indicating that it can provide a reliable data basis for traffic operation state estimation and traffic early warning mechanism formulation. © 2021 Wiley Periodicals LLC",,"Backpropagation; Big data; Motor transportation; Traffic congestion; Accurate calculations; BP neural network model; Calculation time; Chronological order; Early warning mechanisms; Fuzzy-data mining; Intelligent traffics; Traffic operation; Data mining",Article,Scopus,2-s2.0-85114933012
"Kumaran P., Chitrakala S.","57216608110;25645652800;","A novel mathematical modeling in shift in emotion for gauging the social influential in big data streams with hybrid sarcasm detection",2022,"Concurrency and Computation: Practice and Experience","34","3","e6597","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114305035&doi=10.1002%2fcpe.6597&partnerID=40&md5=f46f219c79cd914359e383d42dbd5636","Online social network is a platform that plays an essential role in identifying the emotional values of user-generated content such as blogs, posts, and comments along with their influential factors. Especially on Twitter, network users are growing worldwide day by day and creating a massive amount of data that is not analyzed effectively in a quick way. Identifying the most influential persons on the social network is also a challenging task over the wide range of real-time applications like recommendation systems. Now, to handle these situations, this article proposes a novel approach for prediction of information diffusion that includes emotion recognition with sarcasm detection based influence spreader identification (PID-ERSDISI). The proposed method uses the user-generated posts for emotion recognition in tandem with sarcasm detection both implicitly and explicitly. This approach helps to gauge the leverage that influences spreaders and also enhances the prediction accuracy of information diffusion in a better way. The implementation of the proposed work executed their task one after another in the following way, namely, sarcasm detection, emotional-level computation, breakpoint computation, breakpoint validation, influential user generation, and information diffusion. After the successful implementation of this proposed PID-ERSDIS, it produced prominent results against other state-of-art methods. © 2021 John Wiley & Sons, Ltd.",,"Big data; Data streams; Real time systems; Speech recognition; Spreaders; Emotion recognition; Influential factors; Information diffusion; On-line social networks; Prediction accuracy; Real-time application; State-of-art methods; User-generated content; Social networking (online)",Article,Scopus,2-s2.0-85114305035
"Wu H., Wang L.","57193569078;57242635500;","Analysis of lower limb high-risk injury factors of patellar tendon enthesis of basketball players based on deep learning and big data",2022,"Journal of Supercomputing","78","3",,"4467","4486",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114111892&doi=10.1007%2fs11227-021-04029-3&partnerID=40&md5=25b5b10fe1d620e5ad18c31fbf3a86a4","The research purposes were to explore the high-risk injury factors of basketball players’ lower limb patellar tendon enthesis based on medical big data and accurately recognize players’ lower limb injuries. Middle school students in Nanchang, the capital of Jiangxi Province, were included as the research samples. Innovatively, deep learning and artificial intelligence were applied for statistics and analysis of the collected data. The training of included samples was tracked to collect kinetic and biological data and then to analyze the high-risk injury factors of lower limbs. The analysis of basic information of the participants indicated no significant differences between the two groups. Deep learning algorithm analysis suggests that the accuracy of high-risk group and non-high-risk group is 66.82% and 66.20%, respectively. The analysis of the mechanical characteristics of patellar tendon ends of participants indicated that when the maximum flexion angles of the knee joints of the high-risk group were significantly greater than that of the non-high-risk lower limb group, there were statistically significant differences (P < 0.01). The analysis of the dynamic characteristics of the lower limbs revealed that in different action phases, the impulses of the high-risk lower limbs in the stretching and extension phase of drop landing and jump were significantly greater than that of the non-high-risk lower limbs group. In addition, in the buffer phase and the stretching and extension phase of stop jump, the impulses of lower limbs before the injury were smaller than that after the injury in the same action phases, and the differences between the impulses of stretching and extension were statistically significant (P < 0.01). The research results have processed the data through the deep learning algorithm and the parallel computing to find a joint angle that is most appropriate to make the concentration capacities of muscles reach the maximum value during the movements, thereby the damages and injuries of the body would be the lowest. The results provide a new idea for the selection of basketball court material coefficient. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Artificial intelligence; Biomechanical parameter; Deep learning; End of patellar tendon; Statistical analysis; VISA score","Basketball; Big data; Joints (anatomy); Learning algorithms; Physiological models; Risk assessment; Tendons; Dynamic characteristics; Jiangxi Province; Material coefficients; Mechanical characteristics; Middle school students; Research results; Statistically significant difference; Statistics and analysis; Deep learning",Article,Scopus,2-s2.0-85114111892
"Aghdashi A., Mirtaheri S.L.","57212064460;35611976100;","Novel dynamic load balancing algorithm for cloud-based big data analytics",2022,"Journal of Supercomputing","78","3",,"4131","4156",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113741598&doi=10.1007%2fs11227-021-04024-8&partnerID=40&md5=db44c4765da6db437f22617a24c117be","Big data analytics in cloud environments introduces challenges such as real-time load balancing besides security, privacy, and energy efficiency. This paper proposes a novel load balancing algorithm in cloud environments that performs resource allocation and task scheduling efficiently. The proposed load balancer reduces the execution response time in big data applications performed on clouds. Scheduling, in general, is an NP-hard problem. Our proposed algorithm provides solutions to reduce the search area that leads to reduced complexity of the load balancing. We recommend two mathematical optimization models to perform dynamic resource allocation to virtual machines and task scheduling. The provided solution is based on the hill-climbing algorithm to minimize response time. We evaluate the performance of proposed algorithms in terms of response time, turnaround time, throughput metrics, and request distribution with some of the existing algorithms that show significant improvements. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Big data; Cloud computing; Distributed system; Load balancing; Optimization; Real-time processing; Resource allocation; Scheduling","Balancing; Big data; Data Analytics; Energy efficiency; Green computing; Multitasking; NP-hard; Optimization; Privacy by design; Resource allocation; Big data applications; Dynamic load balancing algorithms; Dynamic resource allocations; Hill climbing algorithms; Load balancing algorithms; Mathematical optimization model; Request distributions; Throughput metrics; Advanced Analytics",Article,Scopus,2-s2.0-85113741598
"Grami M.","57193119294;","An energy-aware scheduling of dynamic workflows using big data similarity statistical analysis in cloud computing",2022,"Journal of Supercomputing","78","3",,"4261","4289",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113615790&doi=10.1007%2fs11227-021-04016-8&partnerID=40&md5=d910d7122acb40609764671125811457","Cloud computing is a suitable platform for workflows that work with massive data and big data. Through virtualization, cloud computing converts physical infrastructures to virtual machines (VMs). Virtual machines can meet fluctuating and dynamic requests through simpler management. Workflow scheduling in cloud computing is important, concerning the fact that proper scheduling can enhance the efficiency of the cloud and good scheduling can cause energy consumption reduction. As energy efficiency is one of the most important issues in cloud computing, in this paper a new statistical analysis-based algorithm is suggested for defining similarities of input workflows. The proposed algorithm, which is called massive data similarity statistics analysis algorithm (MSSA), classifies virtual machines into virtual clusters and it executes scheduling by reforming the virtual clusters. Furthermore, MSSA investigates the similarities of message passing in two different periods; it decides for the next period, and finally, carries out the load balancing by a new method for transferring the machines in virtual clusters. The results of simulation with CloudSim show that the proposed algorithm is more energy efficient in comparison with traditional methods, like FIFO, and heuristic methods such as BlindPick, and relatively new method, named eOO as well as makespan. The main parameter for comparing is makespan and energy consumption. The results showed that the proposed method is more energy efficient compared with similar algorithms and it reduced the makespan significantly. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Big data; Cloud computing; Scheduling","Balancing; Big data; Cloud computing; Clustering algorithms; Energy utilization; Green computing; Heuristic methods; Message passing; Metadata; Network security; Power management; Scheduling; Statistical methods; Virtual machine; Data similarity; Dynamic request; Energy efficient; Energy-aware scheduling; Main parameters; Statistics analysis; Virtual clusters; Workflow scheduling; Energy efficiency",Article,Scopus,2-s2.0-85113615790
"Alyami F., Almutairi S.","57227736700;35228865100;","Implementing Integrity Assurance System for Big Data",2022,"Wireless Personal Communications","122","3",,"2585","2601",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113307582&doi=10.1007%2fs11277-021-09013-x&partnerID=40&md5=fe2f2aefd3a607c05bc92c8ae32d70ce","The computation of big data and related services has been the topic of research and popular applications due to the rapid progress of big data technology and statistical data analysis solutions. There are several issues with data quality that contribute to error decisions in organizations and institutions. Current research just covers how to adequately validate data to assure its validity. Data integrity is synonymous with data validity. It is a difficult undertaking that is often performed by national statistics organizations and institutes. There is a significant need to provide a general system for validating the big data integrity. This approach has been dedicated to presenting a model for data integrity, particularly big data, and how to solve the validation process. The data also comprises the validity of the data fields, as well as the validity of measuring the data and determining compliance with the data cycle chain. For the integrity of large data, the processing speed and accuracy of the verification process are taken into account. The research was based on the Python programming language and real test data, and it was based on the use of the most recent technologies and programming languages. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Big data; Data provenance; Decisions; Integrity; V dimensions; Velocity; Volume","Big data; Data technologies; Integrity assurance; National statistics; Processing speed; Python programming language; Statistical data analysis; Validation process; Verification process; Data handling",Article,Scopus,2-s2.0-85113307582
"Finn A.P., Fujino D., Lum F., Rao P.","57061976300;57212762991;6603018245;56732287800;","Etiology, Treatment Patterns, and Outcomes for Choroidal Neovascularization in the Pediatric Population: An Intelligent Research in Sight (IRIS®) Registry Study",2022,"Ophthalmology Retina","6","2",,"130","138",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112495476&doi=10.1016%2fj.oret.2021.05.015&partnerID=40&md5=71adfbddbefe6eb664676ff0da0e3e8c","Objective: Choroidal neovascularization (CNV) is a rare, but devastating, cause of vision loss in children, with most current publications limited to small case series. Using a large clinical registry allowed us to understand the most common causes of this disease and the visual outcomes. Design: Retrospective analysis. Participants: Patients younger than 18 years in the Intelligent Research in Sight Registry diagnosed with CNV between 2013 and 2019. Methods: Cases were identified based on International Classification of Diseases, Ninth and Tenth Revisions, diagnosis codes for CNV or CNV-related etiology and Current Procedural Terminology treatment codes. Main Outcome Measures: Etiology of CNV, treatment patterns, and visual outcomes. Results: Two thousand three hundred fifty-three eyes with pediatric CNV were identified. The most common identifiable causes of pediatric CNV were posterior uveitis or inflammatory chorioretinal disease (19.4%), myopia (18.4%), hereditary dystrophy (5.4%), chorioretinal scar (4.2%), choroidal rupture (3.5%), optic nerve drusen (3.2%), osteoma (1.9%), and solar retinopathy (0.2%). In 38.2% of eyes, CNV was idiopathic, and in 5.7% of eyes, multiple causes were coded. One thousand forty-one eyes (44.4%) underwent treatment. The mean age of mean age of patients whose eyes received treatment 13.6 ± 3.5 years compared with 12.4 ± 4.1 years for the untreated group (P < 0.001). In 88.9% of eyes, anti–vascular endothelial growth factor (VEGF) injections were administered, 7.9% of eyes received laser therapy, 0.3% of eyes received photodynamic therapy, and 2.9% of eyes received combination therapy. In the eyes receiving anti-VEGF agents, 68.4% required 3 injections or fewer (P < 0.0001). Eyes undergoing treatment exhibited worse baseline visual acuity (VA) than eyes that did not undergo treatment (0.62 ± 0.50 logarithm of the minimum angle of resolution [logMAR] vs. 0.44 ± 0.50 logMAR; P < 0.0001). Visual acuity in the treatment group improved significantly from 0.62 ± 0.50 logMAR at baseline to 0.39 ± 0.43 logMAR at year 1 (P < 0.0001). Visual acuity in the untreated group improved significantly from 0.44 ± 0.50 logMAR at baseline to 0.34 ± 0.44 logMAR at year 1 (P < 0.001). Treated eyes showed a statistically significant higher odds of exhibiting a 2-line vision improvement or better compared with the untreated group at 12 months regardless of treatment type and after controlling for baseline VA (odds ratio, 2.4; P < 0.0001). Conclusions: CNV is a rare, sight-threatening condition in children, with the most common causes being idiopathic, inflammatory chorioretinal disease, and myopia. Eyes undergoing treatment tended to be in older patients and showed worse baseline VA compared with eyes that did not undergo treatment. Those that were treated experienced significant improvement in vision that was maintained in the long term. © 2021 American Academy of Ophthalmology","Anti-VEGF; Big data; Choroidal neovascularization; IRIS® Registry; Pediatrics","aflibercept; bevacizumab; ranibizumab; adolescent; Article; child; chorioretinal scar; chorioretinopathy; choroid disease; choroidal rupture; drusen; female; hereditary corneal dystrophy; human; idiopathic disease; laser coagulation; laser therapy; major clinical study; male; myopia; osteoma; pediatrics; photodynamic therapy; retinopathy; retrospective study; solar retinopathy; subretinal neovascularization; time to treatment; uveitis; visual acuity",Article,Scopus,2-s2.0-85112495476
"Sardar T.H., Ansari Z.","56121754300;54415383900;","Distributed Big Data Clustering using MapReduce-based Fuzzy C-Medoids",2022,"Journal of The Institution of Engineers (India): Series B","103","1",,"73","82",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111615672&doi=10.1007%2fs40031-021-00647-w&partnerID=40&md5=ba45b64a3d2621842a8fcdf0298f820d","Efficient big data clustering is a requirement for massive data generating in this digitalized connected world. The traditional clustering algorithms do not scale over massively sized and highly unstructured big data. Thus, to obtain efficiency in clustering big data new architecture and programming paradigm is required. In this work, a novel MapReduce-based Fuzzy C-Medoids clustering algorithm is designed and experimented with to cluster big data repository of documents datasets. The performance of the proposed algorithm is experimentally evaluated for different-sized Hadoop cluster sizes and different-sized document datasets. The algorithm is found to be scalable and efficient in performing clustering jobs. © 2021, The Institution of Engineers (India).","Big data; Distributed computing; Distributed fuzzy C-Medoids; Document clustering; Hadoop; MapReduce","Cluster analysis; Large dataset; Cluster sizes; Data clustering; Data repositories; Document datasets; Map-reduce; Massive data; Programming paradigms; Traditional clustering; Clustering algorithms",Article,Scopus,2-s2.0-85111615672
"Shehab N., Badawy M., Ali H.A.","57211989263;24723484700;57211770938;","Toward feature selection in big data preprocessing based on hybrid cloud-based model",2022,"Journal of Supercomputing","78","3",,"3226","3265",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110958929&doi=10.1007%2fs11227-021-03970-7&partnerID=40&md5=e0bb9092ce1222f82e405b86d1d52892","Recently, big data are widely noticed in many fields like machine learning, pattern recognition, medical, financial, and transportation fields. Data analysis is crucial to converting data into more specific information fed to the decision-making systems. With the diverse and complex types of datasets, knowledge discovery becomes more difficult. One solution is to use feature subset selection preprocessing that reduces this complexity, so the computation and analysis become convenient. Preprocessing produces a reliable and suitable source for any data-mining algorithm. The effective features’ selection can improve a model’s performance and help us understand the characteristics and underlying structure of complex data. This study introduces a novel hybrid feature selection cloud-based model for imbalanced data based on the k nearest neighbor algorithm. The proposed model showed good performance compared with the simple weighted nearest neighbor. The proposed model combines the firefly distance metric and the Euclidean distance used in the k nearest neighbor. The experimental results showed good insights in both time usage and feature weights compared with the weighted nearest neighbor. It also showed improvement in the classification accuracy by 12% compared with the weighted nearest neighbor algorithm. And using the cloud-distributed model reduced the processing time up to 30%, which is deliberated to be substantial compared with the recent state-of-the-art methods. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Analysis; Big data; Classification; Cloud; Feature selection; Firefly; WKNN","Big data; Decision making; Feature extraction; Motion compensation; Nearest neighbor search; Classification accuracy; Data mining algorithm; Decision-making systems; Distributed modeling; Feature subset selection; Hybrid feature selections; K nearest neighbor algorithm; Nearest neighbor algorithm; Data mining",Article,Scopus,2-s2.0-85110958929
"Yu W., Liu Y., Dillon T., Rahayu W., Mostafa F.","57210554592;57210568224;24724020500;6603033227;25928012100;","An Integrated Framework for Health State Monitoring in a Smart Factory Employing IoT and Big Data Techniques",2022,"IEEE Internet of Things Journal","9","3",,"2443","2454",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110879370&doi=10.1109%2fJIOT.2021.3096637&partnerID=40&md5=04ceb1d872870adc7b223d86e0e35e47","With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data & AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues. © 2021 IEEE.","Big Data; Cloud computing; Data analysis; Intelligent sensors; Internet of Things; Sensor phenomena and characterization; Smart manufacturing","Advanced Analytics; Big data; Data acquisition; Data Analytics; Digital storage; Engineering education; Information management; Machine learning; Manufacture; Metadata; Security of data; Industry applications; Integrated frameworks; Internet of thing (IOT); Machine learning techniques; Manufacturing equipment; Real-time environment; Seamless integration; Smart manufacturing; Internet of things",Article,Scopus,2-s2.0-85110879370
"Sardar T.H., Ansari Z.","56121754300;54415383900;","MapReduce-based Fuzzy C-means Algorithm for Distributed Document Clustering",2022,"Journal of The Institution of Engineers (India): Series B","103","1",,"131","142",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110799491&doi=10.1007%2fs40031-021-00651-0&partnerID=40&md5=a8faa591a8b7a4a44a14bd9c91365016","The clustering of big data is a challenging task. The traditional clustering algorithms are inefficient for clustering big data. The recent researches in this field suggest that the traditional clustering algorithms needed to be redesigned for the modern architecture of computing. This wok has proposed a novel MapReduce-based fuzzy C-means algorithm for big document data clustering. The algorithm is extensively experimented with using different sizes of document datasets and executed over the Hadoop cluster of different sizes. The proposed algorithm’s efficiency is compared against serial traditional fuzzy C-means and MapReduce-based K-means algorithms. The proposed design of the fuzzy C-means algorithm is scaled well with the Hadoop platform and documents big datasets and resulted in a performance gain. © 2021, The Institution of Engineers (India).","Distributed computing; Document clustering; Hadoop; MapReduce; Parallel fuzzy C-means","Big data; Cluster analysis; Copying; Fuzzy clustering; Fuzzy systems; Distributed document clustering; Document datasets; Fuzzy C-means algorithms; Hadoop platforms; Modern architectures; Performance Gain; Recent researches; Traditional clustering; K-means clustering",Article,Scopus,2-s2.0-85110799491
"Gomes C., Tavares E., Junior M.N.O., Nogueira B.","57211255915;55775625600;57211251276;25634422000;","Cloud storage availability and performance assessment: a study based on NoSQL DBMS",2022,"Journal of Supercomputing","78","2",,"2819","2839",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110459183&doi=10.1007%2fs11227-021-03976-1&partnerID=40&md5=361cc8b4da605132a4d8d260956c93f9","Cloud storage systems are increasingly adopting NoSQL database management systems (DBMS), since they generally provide superior availability and performance than traditional DBMSs. To the detriment of better consistency guarantees, several NoSQL DBMSs allow eventual consistency, in which an operation is confirmed without checking all nodes. Different consistency levels for an operation (e.g. read) can be adopted, and such levels may distinctly affect system behaviour. Thus, the assessment of a system design taking into account distinct consistency levels is important for developing cloud storage systems. This work proposes an approach based on reliability block diagrams and generalized stochastic Petri nets to evaluate availability and performance of cloud storage systems, considering redundant nodes and eventual consistency based on NoSQL DBMS. Experimental results demonstrate system configuration may influence unavailability from 1 s to 21 h in a year, and performance can be impacted by up to 17.9%. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Availability; Big data; Cloud storage; Consistency; Latency; Model; NoSQL; Reliability block diagram; Stochastic Petri nets","Availability; Petri nets; Stochastic systems; Cloud storage systems; Consistency level; Eventual consistency; Generalized Stochastic Petri nets; Performance assessment; Redundant nodes; Reliability block diagrams; System configurations; Database systems",Article,Scopus,2-s2.0-85110459183
"Chapple K., Poorthuis A., Zook M., Phillips E.","6701815314;37075676000;6701534158;57224985232;","Monitoring streets through tweets: Using user-generated geographic information to predict gentrification and displacement",2022,"Environment and Planning B: Urban Analytics and City Science","49","2",,"704","721",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108797937&doi=10.1177%2f23998083211025309&partnerID=40&md5=ea14473e8817004583210b5a21f9bfdc","The new availability of big data sources provides an opportunity to revisit our ability to predict neighborhood change. This article explores how data on urban activity patterns, specifically, geotagged tweets, improve the understanding of one type of neighborhood change—gentrification—by identifying dynamic connections between neighborhoods and across scales. We first develop a typology of neighborhood change and risk of gentrification from 1990 to 2015 for the San Francisco Bay Area based on conventional demographic data from the Census. Then, we use multivariate regression to analyze geotagged tweets from 2012 to 2015, finding that outsiders are significantly more likely to visit neighborhoods currently undergoing gentrification. Using the factors that best predict gentrification, we identify a subset of neighborhoods that Twitter-based activity suggests are at risk for gentrification over the short term—but are not identified by analysis with traditional census data. The findings suggest that combining Census and social media data can provide new insights on gentrification such as augmenting our ability to identify that processes of change are underway. This blended approach, using Census and big data, can help policymakers implement and target policies that preserve housing affordability and protext tenants more effectively. © The Author(s) 2021.","big data; Gentrification; neighborhood change; social media; Twitter",,Article,Scopus,2-s2.0-85108797937
"Yin F., Shi F.","57202056404;57202055725;","A Comparative Survey of Big Data Computing and HPC: From a Parallel Programming Model to a Cluster Architecture",2022,"International Journal of Parallel Programming","50","1",,"27","64",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106535558&doi=10.1007%2fs10766-021-00717-y&partnerID=40&md5=967254123076b7efe0a6b172f4e9bfd5","With the rapid growth of artificial intelligence (AI), the Internet of Things (IoT) and big data, emerging applications that cross stacks with different techniques bring new challenges to parallel computing systems. These cross-stack functionalities require one system to possess multiple characteristics, such as the ability to process data under high throughput and low latency, the ability to carry out iterative and incremental computation, transparent fault tolerance, and the ability to perform heterogeneous tasks that evolve dynamically. However, high-performance computing (HPC) and big data computing, as two categories of parallel computing architecture, are incapable of meeting all these requirements. Therefore, by performing a comparative analysis of HPC and big data computing from the perspective of the parallel programming model layer, middleware layer, and infrastructure layer, we explore the design principles of the two architectures and discuss a converged architecture to address the abovementioned challenges. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Big data computing; Converged architecture; Cross-stack functionality; Heterogeneous tasks; High-performance computing; Iterative computation; Parallel computing architecture","Artificial intelligence; Big data; Cluster computing; Fault tolerance; Fault tolerant computer systems; Internet of things; Iterative methods; Middleware; Parallel programming; Emerging applications; High performance computing (HPC); Incremental computation; Internet of thing (IOT); Multiple characteristics; Parallel computing architecture; Parallel computing system; Parallel programming model; Parallel architectures",Article,Scopus,2-s2.0-85106535558
"Moriarty F., Thompson W., Boland F.","56018650500;55901664600;54892130100;","Methods for evaluating the benefit and harms of deprescribing in observational research using routinely collected data",2022,"Research in Social and Administrative Pharmacy","18","2",,"2269","2275",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106369607&doi=10.1016%2fj.sapharm.2021.05.007&partnerID=40&md5=4022b4425f964d529f4ab53ac1a6d30d","Deprescribing is defined as “the planned and supervised process of dose reduction or stopping of medication that might be causing harm, or no longer be of benefit”. Barriers to deprescribing include healthcare professional fear and lack of guidance. These may stem from limited available evidence on benefits and harms of deprescribing medications commonly used among older persons. Advances in pharmacoepidemiology and causal inference methods to evaluate comparative effectiveness and safety of prescribing medications have yet to be considered for deprescribing medication. This paper discusses select methods and how they can be applied to deprescribing research, using case studies of benzodiazepines and low-dose acetylsalicylic acid (aspirin). Target trial emulation involves the explicit application of design principles from randomised controlled trials to observational studies. Several design aspects, including defining eligibility criteria and time zero, require additional considerations for deprescribing studies. The active comparator new user design also presents challenges, including selection of an appropriate comparator. This paper discusses these aspects, and others, in relation to deprescribing studies. Furthermore, methods proposed to control for confounding, in particular, the prior event rate ratio and propensity scores, are discussed. Introduction of billing codes or mechanisms for accurately determining when deprescribing has occurred would enhance the ability to conduct research using routinely collected data. Although the approaches discussed in this paper may strengthen observational studies of deprescribing, their use may be best suited to certain scenarios or research questions, where randomised controlled trials may be less feasible. © 2021 The Author(s)","Big data; Causal inference; Deprescribing; Pharmacoepidemiology; Propensity scores","aged; deprescription; human; very elderly; Aged; Aged, 80 and over; Deprescriptions; Humans; Routinely Collected Health Data",Article,Scopus,2-s2.0-85106369607
"Sprecher B., Verhagen T.J., Sauer M.L., Baars M., Heintz J., Fishman T.","56096074600;57217824023;57221784463;57223181666;24279572400;56125365000;","Material intensity database for the Dutch building stock: Towards Big Data in material stock analysis",2022,"Journal of Industrial Ecology","26","1",,"272","280",,5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105154109&doi=10.1111%2fjiec.13143&partnerID=40&md5=5dd3541636c7fb8314d3c2c7fe931984","Re-use and recycling in the construction sector is essential to keep resource use in check. Data availability about the material contents of buildings is significant challenge for planning future re-use potentials. Compiling material intensity (MI) data is time and resource intensive. Often studies end up with only a handful of datapoints. In order to adequately cover the diversity of buildings and materials found in cities, and accurately assess material stocks at detailed spatial scopes, many more MI datapoints are needed. In this work, we present a database on the material intensity of the Dutch building stock, containing 61 large-scale demolition projects with a total of 781 datapoints, representing more than 306,000 square meters of built floor space. This dataset is representative of the types of buildings being demolished in the Netherlands. Our data were empirically sourced in collaboration with a demolition company that explicitly focuses on re-using and recycling materials and components. The dataset includes both the structural building materials and component materials, and covers a wide range of building types, sizes, and construction years. Compared to the existing literature, this paper adds significantly more datapoints, and more detail to the different types of materials found in demolition streams. This increase in data volume is a necessary step toward enabling big data methods, such as data mining and machine learning. These methods could be used to uncover previously unrecognized patters in material stocks, or more accurately estimate material stocks in locations that have only sparse data available. This article met the requirements for a Gold-Gold JIE data openness badge described at http://jie.click/badges. © 2021 The Authors. Journal of Industrial Ecology published by Wiley Periodicals LLC on behalf of Yale University","circular economy; construction; industrial ecology; material stocks and flows; urban mining","Big data; Construction industry; Demolition; Gold; Recycling; Building stocks; Component materials; Construction sectors; Data availability; Demolition projects; Material content; Recycling materials; Structural buildings; Data mining; building; building code; building construction; data mining; database; demolition; resource use; Netherlands",Article,Scopus,2-s2.0-85105154109
"Jiang H., Li L., Xian H., Hu Y., Huang H., Wang J.","57095221600;55730847700;57222561536;57281935600;57205095515;57204096693;","Crowd Flow Prediction for Social Internet-of-Things Systems Based on the Mobile Network Big Data",2022,"IEEE Transactions on Computational Social Systems","9","1",,"267","278",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103221179&doi=10.1109%2fTCSS.2021.3062884&partnerID=40&md5=d5b030eda6089904cfa82f7d9be2a53b","Accurate crowd flow prediction has gained increasing importance for the development of social Internet-of-Things (IoT) systems. In this article, we provide an efficient crowd flow prediction for social IoT systems in urban space based on the mobile network big data. In particular, the usage detail records (UDRs) are used in the prediction. The feasibility of using UDRs in the prediction is first analyzed. Then, a graph data model is exploited to record and represent the mobile behavior of users. In particular, we propose to apply the heterogeneous information network (HIN) representing the UDR data and characterize the users' behavior through the embedding methods of HIN. Moreover, an attention-based spatiotemporal graph convolution network with embedded vectors (EA-STGCN) is proposed for the final prediction. Through experimental evaluation, the advantages of the proposed model are shown in comparison to benchmarks. © 2014 IEEE.","Crowd flow prediction; graph convolution network; graph representation learning; heterogeneous information network (HIN)","Big data; Cellular radio systems; Embeddings; Forecasting; Information services; Mobile telecommunication systems; Wireless networks; Embedding method; Experimental evaluation; Graph data models; Heterogeneous information; Internet of Things (IOT); Mobile behavior; Spatio-temporal graphs; Urban spaces; Internet of things",Article,Scopus,2-s2.0-85103221179
"Zhang L., Li N.","56764784600;57222507546;","Material analysis and big data monitoring of sports training equipment based on machine learning algorithm",2022,"Neural Computing and Applications","34","4",,"2749","2763",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102917774&doi=10.1007%2fs00521-021-05852-8&partnerID=40&md5=22394a1590ee172e59ad5ba410702742","Different machine learning algorithms predict the application effect of perovskite materials in sports training equipment. The sensitivity to material data is different on different ranges of data sets. Therefore, the algorithm needs to be selected according to specific material data samples. This study compares the prediction performance of neural network prediction algorithm (NN), genetic algorithm, and support vector machine-based machine learning algorithm (SVM) and uses statistical analysis to perform data analysis and draw corresponding curves. Moreover, this study uses a single perovskite material to verify the algorithm performance. In addition, based on the real data, the three machine learning algorithms of this study are applied to the related performance prediction, and the comparative analysis method is used to analyze the prediction performance of the machine learning algorithm. Through data analysis and chart analysis, we can see that machine learning algorithms have a certain effect in the application prediction of perovskite materials in sports training equipment. Among the three machine learning algorithms selected in this study, the performance of the machine learning algorithm based on support vector machine in all aspects is more excellent. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Algorithm comparison; Machine learning; Material properties; Perovskite; Physical training; Prediction","Big data; Data handling; Forecasting; Genetic algorithms; Information analysis; Learning systems; Perovskite; Sports; Support vector machines; Algorithm performance; Application effect; Comparative analysis; Corresponding curve; Neural network predictions; Performance prediction; Prediction performance; Specific materials; Learning algorithms",Article,Scopus,2-s2.0-85102917774
"Malik R., Nishi M.","57220699318;57221919836;","Flexible big data approach for geospatial analysis",2022,"Journal of Ambient Intelligence and Humanized Computing","13","2",,"737","756",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100587148&doi=10.1007%2fs12652-021-02925-3&partnerID=40&md5=9f35313749bf43035337c0de8bcb3859","For a long time, the improvement of practical tools to deal with the enormous volumes of information this particular surveying system is significant at collecting has long been view as an issue. Big data systems offered effective management and also computational applications in conditions like this. This paper provides a large scale way for the geological processing of large aerial LiDAR (Light Detection and Ranging) stage clouds. By utilizing Spark and Cassandra, our proposal seeks to assist the execution of any fair time-consuming process; however, we concentrated on the fast ground only raster generation from massive LiDAR datasets, for the initial evaluation. Filtered clouds ensuing from impartial proper care of neighbouring areas might misclassify on the borders on the regions. Generally, semi-automatic or manual procedures take care of this particular type of error. Likewise, we suggest an integrated approach to resolve these faults, raise the classification procedure consistency, and also the digital terrain models (DTMs) obtained while lessening user interaction. These independent look for most computing levels, together with the reduced processing time, opens these chances to discover the framework for an on-demand DTM output or perhaps another geospatial method as an extremely scalable service orientated solution. The strategy is very beneficial and wonderful to other LiDAR programs, and also might be utilized in real-time with adequate computing tools. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.","Big data; Cassandra; Digital terrain models; Error correction; LiDAR; Spark","Antennas; Big data; Information management; Temperature control; Classification procedure; Computational applications; Digital terrain model; Effective management; Geo-spatial analysis; Integrated approach; Semi-automatics; User interaction; Optical radar",Article,Scopus,2-s2.0-85100587148
"Zhu J.","55704649200;","DEA under big data: data enabled analytics and network data envelopment analysis",2022,"Annals of Operations Research","309","2",,"761","783",,14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086166588&doi=10.1007%2fs10479-020-03668-8&partnerID=40&md5=5e738e0a48f015ed597c70c6d67e2e33","This paper proposes that data envelopment analysis (DEA) should be viewed as a method (or tool) for data-oriented analytics in performance evaluation and benchmarking. While computational algorithms have been developed to deal with large volumes of data (decision making units, inputs, and outputs) under the conventional DEA, valuable information hidden in big data that are represented by network structures should be extracted by DEA. These network structures, e.g., transportation and logistics systems, encompass a broader range of inter-linked metrics that cannot be modelled by conventional DEA. It is proposed that network DEA is related to the value dimension of big data. It is shown that network DEA is different from standard DEA, although it bears the name of DEA and some similarity with conventional DEA. Network DEA is big data enabled analytics (big DEA) when multiple (performance) metrics or attributes are linked through network structures. These network structures are too large or complex to be dealt with by conventional DEA. Unlike conventional DEA that are solved via linear programming, general network DEA corresponds to nonconvex optimization problems. This represents opportunities for developing techniques for solving non-linear network DEA models. Areas such as transportation and logistics system as well as supply chains have a great potential to use network DEA in big data modeling. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Big data; Composite index; Data enabled analytics; Data envelopment analysis (DEA); Efficiency; Performance; Productivity; Transportation",,Article,Scopus,2-s2.0-85086166588
"Bashir M., Ashraf J., Habib A., Muzammil M.","57202985082;57031630100;36699661600;36677704900;","An intelligent linear time trajectory data compression framework for smart planning of sustainable metropolitan cities",2022,"Transactions on Emerging Telecommunications Technologies","33","2","e3886","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079464919&doi=10.1002%2fett.3886&partnerID=40&md5=34936df9729bb8045f58739be6285a2d","The urban road networks and vehicles generate exponential amount of spatio-temporal big-data, which invites researchers from diverse fields of interest. Global positioning system devices may transceive data every second thus producing huge amount of trajectory data. Subsequently, it requires optimized computing for various operations such as visualization and mining hidden patterns. This sporadically stored big-data contains invaluable information, which is useful for a number of real-time applications. Compression is a highly important, but knotty task. Optimized compression enables us achieve the desired results in efficient and effective manner by using minimum energy and computational resources without compromising on important information. We present two versions of a compression technique based on the points of intersections (PoI) of urban roads networks. Based on intelligent mining paradigm, we created a compressed lookup lexicon to store the PoIs of dynamically selected region of interests (ROI). An important feature of our lexicon is the key pattern, which is intelligently computed based on the relative geographic position of a spatial geodetic vertex with respect to Euclidean space origin in a given ROI. This compresses trajectories in linear time, making it feasible for mission critical real world applications. Our experimental dataset contained 959 547, 517 436, and 231 740 trajectories for Bikes, Cars, and Taxis, respectively. The Compr10 reduced these trajectories to 17 428, 11 084, and 6565, respectively. Results of Compr15 and Compr20 show promising results. We define the quality of the compression in context of the considered problem. The results show that the proposed technique achieved satisfactory quality of the compression. © 2020 John Wiley & Sons, Ltd.",,"Big data; Data compression; Compression techniques; Computational resources; Euclidean spaces; Important features; Metropolitan cities; Real-time application; Region of interest; Urban road networks; Trajectories",Article,Scopus,2-s2.0-85079464919
"Wang Y., Hu L., Chen J., Ren Y.","55961816100;57211609425;57211600383;57211600649;","Health status diagnosis of distribution transformers based on big data mining",2022,"Transactions on Emerging Telecommunications Technologies","33","2","e3759","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074583531&doi=10.1002%2fett.3759&partnerID=40&md5=fd37c0338bdf00c5fe4ad944cb5a8166","In the fault detection technology of distribution transformer, the traditional artificial intelligence algorithm has been unable to achieve its efficient analysis and processing, but also unable to achieve the elimination of misdiagnosis caused by improper interval segmentation in the process of fault diagnosis of distribution transformer. In a word, the problem that always exists in transformer fault diagnosis technology is the problem of discretization of fault data. In order to solve this problem, this paper creatively proposes a health condition diagnosis method of distribution transformer based on large data. This method innovatively proposes that the dissolved gas analysis value of distribution transformer is the conditional attribute, and the fault type is the decision attribute, and the fault decision table is established. The continuous attribute data in the decision table are discretized by using the optimization behavior of large data sets. Subsequently, the discretized decision table is simplified by using the big data theory, and the decision table of fault diagnosis rules is established, which greatly simplifies the difficulty of attribute simplification of decision table and makes diagnosis more convenient. Finally, an example shows that the proposed method can effectively discretize and reduce samples. Compared with traditional methods, it improves the accuracy of fault diagnosis. © 2019 John Wiley & Sons, Ltd.",,"Artificial intelligence; Big data; Data mining; Decision tables; Decision theory; Electric transformers; Failure analysis; Artificial intelligence algorithms; Continuous attribute; Decision attribute; Detection technology; Dissolved gas analysis; Distribution transformer; Efficient analysis; Transformer fault diagnosis; Fault detection",Article,Scopus,2-s2.0-85074583531
"Bond R.M., Sweitzer M.D.","37107076000;57200387139;","Political Homophily in a Large-Scale Online Communication Network",2022,"Communication Research","49","1",,"93","115",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059054692&doi=10.1177%2f0093650218813655&partnerID=40&md5=2bce2047beac7895fcef4e0ba522a325","As communication increasingly occurs in online environments, it is important to know the structure of such conversations in social networks. Here, we investigate patterns of conversation in online forums concerning politics, as well as patterns of cross-ideological interactions in forums that are not expressly political. First, we demonstrate a method for measuring the latent ideological preferences of more than 690,000 individuals using patterns of political commenting. Using this measure, we find that communication between ideologically dissimilar individuals becomes more common in periods of increased engagement with politics, that political homophily decreases as more individuals contribute to a conversation, and that forums dedicated to nonpolitical topics exhibit substantially less homophily than political forums. Theoretical implications for political communication on online platforms are discussed. © The Author(s) 2018.","big data; political communication; political homophily; social media; social networks",,Article,Scopus,2-s2.0-85059054692
"Guo H., Liu Z., Jiao Z.","57219207405;57212007101;57218585750;","Research on Satisfaction Evaluation Based on Tourist Big Data",2022,"KSII Transactions on Internet and Information Systems","16","1",,"231","244",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124604275&doi=10.3837%2ftiis.2022.01.013&partnerID=40&md5=7dfd4f820651057ea311e98bac2950a4","With the improvement of people's living standards and the development of tourism, tourists have greater freedom in choosing destinations. Therefore, as an indicator of satisfaction with scenic spots, tourist comments are becoming increasingly prominent. This paper aims to compare and analyze the landscape image of the Five Great Mountains in China and provide specific strategies for its development. The online reviews of tourists on the Online Travel Agency (OTA) website about the Five Great Mountains from 2015 to 2018 are collected as research samples. The text analysis method and R language are used to analyze the content of the tourist reviews, while the high-frequency words in the word cloud are used for visual display. In addition, the entropy weight method is used to determine the index weight and tourist satisfaction is evaluated to understand the weaknesses of those scenic spots. The results of the study show that firstly, the tourist satisfaction with the Five Great Mountains is basically consistent with its popularity. Secondly, through weight analysis, tourists pay special attention to the landscape features and environmental health of the scenic area, so that relevant departments should focus on building the landscape characteristics and improving the environmental health of the scenic area. At the same time, the accommodation and service management of the scenic spot cannot be ignored. Finally, according to the analysis results, suggestions are made on how to improve the tourist satisfaction with the Five Great Mountains. Copyright © 2022 KSII.","Online travel agency; Text analysis; The five great mountains; Tourist satisfaction; Word cloud","Big data; Visual languages; Compare and analyze; Environmental health; Living standards; Online travel agency; Scenic areas; Scenic spot; The five great mountain; Tourist satisfaction; Travel agency; Word clouds; Landforms",Article,Scopus,2-s2.0-85124604275
"Tounsi Y., Anoun H., Hassouni L.","57204210757;55788704000;57193431178;","MODELING THE FAIR VALUE OF BANKING COLLATERAL USING REAL ESTATE WEBSITE DATA AND MACHINE LEARNING TECHNIQUES - THE CASE OF CASABLANCA CITY, MOROCCO",2022,"Journal of Theoretical and Applied Information Technology","100","2",,"578","590",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124453627&partnerID=40&md5=95ce7615ea2d5a5e0a56554d74fa1c0c","In recent years, big data has been widely used to understand emerging trends in several markets. With this paper, we consider the use of real estate websites data and machine learning in the problem of knowing the current fair value of banking collateral in Morocco. In our study, we are going to utilize the online data, containing housing sales advertisements, extracted from three websites: (Avito.ma, Mubawab.ma and Sarouty.ma), popular online portals for real estate services in Morocco, in order to develop forecasting models of apartment prices using artificial intelligence for the city of Casablanca. Each record of the database contains information on the listed housing unit (asking price, district, floor, area, number of rooms, balcony, terrace…), on the building (elevator, garage, garden, etc.), on the ad (publication date, apartment sector) and a short description. After collecting the database and the deduplication process, we make additional controls on the dataset to address potential incoherence errors in the data. After validating the clean dataset against official statistical sources, we construct forecasting models for real estate sale prices, based on artificial intelligence and statistical methodologies. © 2022 Little Lion Scientific. All rights reserved.","Big Data; Housing Price Prediction; Machine Learning Algorithms; Mortgage lenders",,Article,Scopus,2-s2.0-85124453627
"Vardas P.E., Asselbergs F.W., van Smeden M., Friedman P.","35391927700;57202567488;55580255200;34975087100;","The year in cardiovascular medicine 2021: digital health and innovation",2022,"European heart journal","43","4",,"271","279",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123969113&doi=10.1093%2feurheartj%2fehab874&partnerID=40&md5=6f1b9aa3b8b2931d3a1ac78a2358e0cf","This article presents some of the most important developments in the field of digital medicine that have appeared over the last 12 months and are related to cardiovascular medicine. The article consists of three main sections, as follows: (i) artificial intelligence-enabled cardiovascular diagnostic tools, techniques, and methodologies, (ii) big data and prognostic models for cardiovascular risk protection, and (iii) wearable devices in cardiovascular risk assessment, cardiovascular disease prevention, diagnosis, and management. To conclude the article, the authors present a brief further prospective on this new domain, highlighting existing gaps that are specifically related to artificial intelligence technologies, such as explainability, cost-effectiveness, and, of course, the importance of proper regulatory oversight for each clinical implementation. © The Author(s) 2022. Published by Oxford University Press on behalf of European Society of Cardiology. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com.","AI-ECG; AI-wearables; Big data; Cardiovascular medicine; Digital health; Machine learning",,Article,Scopus,2-s2.0-85123969113
"Nazari-Ghanbarloo V.","57220734378;","A dynamic performance measurement system for supply chain management",2022,"International Journal of Productivity and Performance Management","71","2",,"576","597",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098498982&doi=10.1108%2fIJPPM-01-2020-0023&partnerID=40&md5=7caed8cb0d4202b034deb4847b062568","Purpose: This paper aims to propose a dynamic model for measurement supply chain performance (SCP) based on a dynamic balanced scorecard (DBSC). Balanced scorecard (BSC) can be defined as a popular performance measurement method that can translate the strategy into a set of performance indicators and manage the status of implementing the various strategies. However, BSC is unable to simulate the complicated environment and the dynamic behavior of performance metrics. Therefore, the author combines BSC with system dynamics (SD) to explore a more efficient tool for measurement SCP. Design/methodology/approach: A dynamic causal model is proposed based on the causal hypotheses. The developed DBSC enables managers to evaluate and measure the SCP in a much-balanced way. Using DBSC makes it possible that different SCP metrics to be reviewed and distributed into the four above-mentioned perspectives. It also enables supply chain (SC) managers to evaluate different strategies to improve SCP. Findings: investigates two strategies to improve SCP as follows: (1) competitive strategy and (2) harvesting big data and using data mining techniques to determine the customer's expectations and then compares the results of these two strategies based on the four perspectives of DBSC and introduce the best strategy. Finally, harvesting big data and data mining is selected as the best strategy. Originality/value: This study proposes a novel strategic management tool for measurement SCP and simulation of the complicated environment and the dynamic behavior of performance metrics. The proposed DBSC model enables managers to compare different strategies and select the best strategy. © 2020, Emerald Publishing Limited.","Big data; Data mining; Dynamic balanced scorecard; Strategy; Supply chain performance; System dynamics",,Article,Scopus,2-s2.0-85098498982
"Yang M., Liu X., Hu Q., Li J., Fu S., Chen D., Wu Y., Luo A., Zhang X., Feng R., Xu G., Liu C., Jiang H., Liu W.","57240443500;55717162500;57430517000;57430663000;57430517100;57430552700;57430446200;57430623200;57430517200;57211202942;57430663100;57430446300;55017673100;56595374800;","Eosinopenia as a biomarker for antibiotic use in COPD exacerbations: Protocol for a retrospective hospital-based cohort study",2022,"BMJ Open","12","1","e21051939","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123675010&doi=10.1136%2fbmjopen-2021-051939&partnerID=40&md5=46f4eaae304651f3a007a9c357004ea5","Introduction The acute exacerbation of chronic obstructive pulmonary disease (AECOPD) has a seriously negative impact on patients' healths condition and disease progression. Bacterial infection is closely related to AECOPD, and antibiotics are frequently used in clinical practice. The lack of specific biomarkers for rational antibiotics use always leads to antibiotics abuse in chronic obstructive pulmonary disease (COPD) flare-ups. Eosinopenia has been considered to be related to increased bacterial load of potentially pathogenic organisms at the onset of COPD exacerbations. Therefore, this study aims to investigate whether eosinopenia could be used as a reference for the use of antibiotics in AECOPD. Methods and analysis In this study, a hospital-based retrospective cohort design will be adopted to analyse the clinical data of inpatients who are primarily diagnosed with AECOPD in West China Hospital of Sichuan University from 1 January 2010 to 31 December 2020. Relevant data will be extracted from the Clinical Big Data Platform for Scientific Research in West China Hospital, including demographic characteristics, blood eosinophil count, procalcitonin, C reactive protein, microbial cultivation, antibiotics use, length of hospital stay, non-invasive ventilation use, intensive care unit transfer and mortality, etc. The collected data will be described and inferred by corresponding statistical methods according to the data type and their distributions. Multiple binary logistic regression models will be used to analyse the relationship between blood eosinophil count and bacterial infection. The antibiotics use, and patient morbidity and mortality will be compared between patients with or without eosinopenia. Ethics and dissemination This study has been approved by the Biomedical Ethics Review Board of West China Hospital of Sichuan University (Approval No. 2020-1056). And the research results will be published in a peer-reviewed journal. Trial registration number ChiCTR2000039379. ©","chronic airways disease; immunology; respiratory infections","bronchodilating agent; C reactive protein; corticosteroid; procalcitonin; theophylline; adjuvant therapy; antibiotic therapy; Article; bacterial infection; bacterium culture; big data; chronic obstructive lung disease; clinical study; cohort analysis; controlled study; corticosteroid therapy; disease exacerbation; drug use; eosinopenia; eosinophil count; hospital patient; human; intensive care unit; length of stay; major clinical study; morbidity; mortality; noninvasive ventilation; retrospective study",Article,Scopus,2-s2.0-85123675010
"Jacky Y., Sulaiman N.A.","57402502400;55968241600;","The use of data analytics in external auditing: a content analysis approach",2022,"Asian Review of Accounting","30","1",,"31","58",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122385416&doi=10.1108%2fARA-11-2020-0177&partnerID=40&md5=5b325b3b7df1958628947b5d16b0406e","Purpose: This study examines the perceptions of interested stakeholders on the factors affecting the use of data analytics (DA) in financial statement audits. Response letters submitted by stakeholders of the auditing services to the International Auditing and Assurance Standards Board's (IAASB) Data Analytics Working Group (DAWG) served as sources for analysis. Design/methodology/approach: The modified information technology audit model was used as a framework to perform a direct content analysis of all the 50 response letters submitted to the DAWG. Findings: The analysis showed that a range of attributes, such as the usefulness of DA in auditing, authoritative guidance (auditing standards), data reliability and quality, auditors' skills, clients' factors and costs, were the factors perceived by stakeholders to be affecting the use of DA in external auditing. Research limitations/implications: This study is subjected to the limitations inherent to all content analysis studies. Nonetheless, the findings offer additional insights about potential factors affecting the adoption of DA in audit practices. Originality/value: The data noted in the published statements highlighted the perceptions of a range of stakeholders with regards to the factors affecting the use of DA in auditing. © 2021, Emerald Publishing Limited.","Auditing; Auditors; Big data; Content analysis; Data analytics; IT audit",,Article,Scopus,2-s2.0-85122385416
"Chen J., Li F., Wang M., Li J., Marquez-Lago T.T., Leier A., Revote J., Li S., Liu Q., Song J.","57215386032;57198720015;57439053900;57226634116;21741172500;14033416500;56133068000;57439222200;55869897600;56023619300;","BigFiRSt: A Software Program Using Big Data Technique for Mining Simple Sequence Repeats From Large-Scale Sequencing Data",2022,"Frontiers in Big Data","4",,"727216","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124025539&doi=10.3389%2ffdata.2021.727216&partnerID=40&md5=6d7240904999b97c7fe3e4462e0bbfa2","Background: Simple Sequence Repeats (SSRs) are short tandem repeats of nucleotide sequences. It has been shown that SSRs are associated with human diseases and are of medical relevance. Accordingly, a variety of computational methods have been proposed to mine SSRs from genomes. Conventional methods rely on a high-quality complete genome to identify SSRs. However, the sequenced genome often misses several highly repetitive regions. Moreover, many non-model species have no entire genomes. With the recent advances of next-generation sequencing (NGS) techniques, large-scale sequence reads for any species can be rapidly generated using NGS. In this context, a number of methods have been proposed to identify thousands of SSR loci within large amounts of reads for non-model species. While the most commonly used NGS platforms (e.g., Illumina platform) on the market generally provide short paired-end reads, merging overlapping paired-end reads has become a common way prior to the identification of SSR loci. This has posed a big data analysis challenge for traditional stand-alone tools to merge short read pairs and identify SSRs from large-scale data. Results: In this study, we present a new Hadoop-based software program, termed BigFiRSt, to address this problem using cutting-edge big data technology. BigFiRSt consists of two major modules, BigFLASH and BigPERF, implemented based on two state-of-the-art stand-alone tools, FLASH and PERF, respectively. BigFLASH and BigPERF address the problem of merging short read pairs and mining SSRs in the big data manner, respectively. Comprehensive benchmarking experiments show that BigFiRSt can dramatically reduce the execution times of fast read pairs merging and SSRs mining from very large-scale DNA sequence data. Conclusions: The excellent performance of BigFiRSt mainly resorts to the Big Data Hadoop technology to merge read pairs and mine SSRs in parallel and distributed computing on clusters. We anticipate BigFiRSt will be a valuable tool in the coming biological Big Data era. Copyright © 2022 Chen, Li, Wang, Li, Marquez-Lago, Leier, Revote, Li, Liu and Song.","big data; Hadoop; next-generation sequencing; read pairs; Simple Sequence Repeats (SSR)",,Article,Scopus,2-s2.0-85124025539
"Saputra A., Wang G., Zhang J.Z., Behl A.","57263346900;57214836304;57208659110;56554009500;","The framework of talent analytics using big data",2022,"TQM Journal","34","1",,"178","198",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115180339&doi=10.1108%2fTQM-03-2021-0089&partnerID=40&md5=d889eb22578fc0017fcef7640e53a542","Purpose: The era of work 4.0 demands organizations to expedite their digital transformation to sustain their competitive advantage in the market. This paper aims to help the human resource (HR) department digitize and automate their analytical processes based on a big-data-analytics framework. Design/methodology/approach: The methodology applied in this paper is based on a case study and experimental analysis. The research was conducted in a specific industry and focused on solving talent analysis problems. Findings: This research conducts digital talent analysis using data mining tools with big data. The talent analysis based on the proposed framework for developing and transforming the HR department is readily implementable. The results obtained from this talent analysis using the big-data-analytics framework offer many opportunities in growing and advancing a company's talents that are not yet realized. Practical implications: Big data allows HR to perform analysis and predictions, making more intelligent and accurate decisions. The application of big data analytics in an HR department has a significant impact on talent management. Originality/value: This research contributes to the literature by proposing a formal big-data-analytics framework for HR and demonstrating its applicability with real-world case analysis. The findings help organizations develop a talent analytics function to solve future leaders' business challenges. © 2021, Emerald Publishing Limited.","Big data; Human resource; Talent; Talent analytics","Big data; Competition; Data Analytics; Data mining; Analysis problems; Analytical process; Business challenges; Competitive advantage; Design/methodology/approach; Digital transformation; Experimental analysis; Talent management; Advanced Analytics",Article,Scopus,2-s2.0-85115180339
"Kim B., Cooks E., Kim S.-K.","57193899119;57131152100;56081945400;","Exploring incivility and moral foundations toward Asians in English-speaking tweets in hate crime-reporting cities during the COVID-19 pandemic",2022,"Internet Research","32","1",,"362","378",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113766853&doi=10.1108%2fINTR-11-2020-0678&partnerID=40&md5=53051693798621c575f5ec50f9006b4f","Purpose: This study aims to explore the extent to which Twitter users engaged in uncivil and morally questionable expressions in their comments about specific Asian countries and citizens. The integrated threat theory (ITT) was used to formulate questions surrounding incivility and moral foundations within Twitter discourses related to the COVID-19 pandemic. Design/methodology/approach: The authors collected tweets and retweets posted by English-speaking Twitter users in the United States (US) across the following three phases: (1) initial discovery of COVID-19 in China, (2) high US mortality rate from COVID-19 and (3) the announcement that a vaccine would soon be available in the US. Findings: The authors found a significant difference in uncivil tweets posted in cities with higher levels of reported hate crimes against Asians than cities with low levels. Lastly, English-speaking Twitter users tended to employ moral virtue words and moral vice words when discussing China and Chinese culture/populations. Research limitations/implications: The bags-of-words employed are limited in capturing nuanced and metaphorical terms. In addition, the analysis focused solely on Tweets composed in English and thus did not capture the thoughts and opinions of non-English speakers. Lastly, this study did not address all Asian countries. In this sense, the findings of this study might not be applicable to Tweets about other nations. Practical implications: Given that many Twitter users tend to use terms of moral virtue in support of Asians and Asian communities, the authors suggest that non-governmental organization administrators provide morally supportive social media campaigns that encourage users to engage in civil discourse. Social implications: These findings have theoretical implications as the frameworks of integrated threats and moral foundations were used to offer group-level explanations for online behavior. Additional research is needed to explore whether these frameworks can be used to explain negativity in other communication environments. Originality/value: This study expands the findings of prior studies that identified the extent to which Twitter users express hate speech, focusing on general Twitter discourse across three specific periods of the pandemic: degrees of incivility and moral foundations, and comparison of incivility based on the prevalence of reported hate crimes. © 2021, Emerald Publishing Limited.","Big data; COVID-19; Incivility; Integrated threat theory; Moral foundations theory; Semantic network analysis; Twitter",,Article,Scopus,2-s2.0-85113766853
"Tian X., He W., Wang F.-K.","56915178700;36985723200;7501311683;","Applying sentiment analytics to examine social media crises: a case study of United Airline's crisis in 2017",2022,"Data Technologies and Applications","56","1",,"1","23",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106473238&doi=10.1108%2fDTA-09-2018-0087&partnerID=40&md5=0a5ad3b1c850916e28ccd7674a5d1606","Purpose: In recent years, social media crises occurred more and more often, which negatively affect the reputations of individuals, businesses and communities. During each crisis, numerous users either participated in online discussion or widely spread crisis-related information to their friends and followers on social media. By applying sentiment analysis to study a social media crisis of airline carriers, the purpose of this research is to help companies take measure against social media crises. Design/methodology/approach: This study used sentiment analytics to examine a social media crisis related to airline carriers. The arousal, valence, negative, positive and eight emotional sentiments were applied to analyze social media data collected from Twitter. Findings: This research study found that social media sentiment analysis is useful to monitor public reaction after a social media crisis arises. The sentiment results are able to reflect the development of social media crises quite well. Proper and timely response strategies to a crisis can mitigate the crisis through effective communication with the customers and the public. Originality/value: This study used the Affective Norms of English Words (ANEW) dictionary to classify the words in social media data and assigned the words with two elements to measure the emotions: valence and arousal. The intensity of the sentiment determines the public reaction to a social media crisis. An opinion-oriented information system is proposed as a solution for resolving a social media crisis in the paper. © 2021, Emerald Publishing Limited.","Big data; Conflicts and crises; Crisis communication; Opinion-oriented information system; Sentiment analytics; Social media",,Article,Scopus,2-s2.0-85106473238
"Erraji A., Maizate A., Ouzzif M.","57436410100;55820871000;24178281900;","MULTI-CRITERIA ANALYSIS BETWEEN NOSQL DATABASES CATEGORIES TOWARD A COMPLETE MIGRATION FROM RELATIONAL DATABASE",2022,"Journal of Theoretical and Applied Information Technology","100","1",,"61","69",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124493780&partnerID=40&md5=874fe8a346f639394ed57060e6e73323","In recent times, the world has become highly dependent on computer science which has become extends to various vital and secondary areas of life, such as social communication, security, commerce, marketing, training, and other fields, as many institutions have become dealing with a huge amount of information to Storage and use for their benefit, in order to manage the present and the future well. This situation made many institutions need several means to keep pace with this great development of information in terms of quantity and quality, without losing what they could possess of previous information stored within databases, often relational system which provide easy ways of dealing with the information, also provides the possibility of applying several complex and important calculations and elicit. Developers in recent days have came out with what's called a NoSQL database, that surpasses its previous predecessor, these developers have also made many programs and tools that works with this new structure, that's characterized in 4 different categories, each one differs from the other in its fundamentals, that’s why finding a way to migrate Data Base with all of data and effects from a relational system to this new structure became a must. In this paper, we will study in-depth and comprehensively the different categories of the NoSQL system, by following the comparative approach using WSM method and depending on a set of characteristics and features, in order to determine the optimal category that enables us to completely migrate all data from a relational system with all its details, and capabilities towards a NoSQL system, in order to put the first step to start studying the migration of data from a relational system to a NoSQL system © 2021 Little Lion Scientific","Big Data; Migration; NoSQL; Relational Database; WSM",,Article,Scopus,2-s2.0-85124493780
"Adil B., Abdelhadi F.","57215669187;55935734000;","A BIG DATA ANALYTICS FRAMEWORK FOR COMPETITIVE INTELLIGENCE SYSTEMS",2022,"Journal of Theoretical and Applied Information Technology","100","1",,"149","169",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124492743&partnerID=40&md5=16221110efb119b3d4a492fabd4011cb","Nowadays, companies are facing a lot of challenges due to the volume and velocity of data available online, the nature of this data which comes in different formats structured, semi-structured or unstructured, forces the adoption of new tools and techniques to process and transform data to knowledge, competitive intelligence systems aims at setting-up tools and software to handle this stream of data from data collection, data analysis, data visualization to the results dissemination for stakeholders to enhance the decision making process of companies. In this paper, we present a big data analytics layer/framework for competitive intelligence systems and we implement it in the case of XEW 2.0 system relying on Apache Spark capabilities and big data analytics technologies, we validate the proposed framework with a case study about Research in Morocco in order to achieve a technological surveillance, the framework shows promising results in providing analysts with a toolbox to extract strategic information. © 2021 Little Lion Scientific","Apache Spark; Big Data; Big Data Analytics; Competitive Intelligence; Data Mining",,Article,Scopus,2-s2.0-85124492743
"Yergaliyeva B.B., Seitkulov Y.N., Satybaldina D.Z.H.","57191241749;24280172200;57193740669;","METHODS FOR THE SECURE USE OF EXTERNAL SERVERS TO SOLVE COMPUTATIONALLY-COMPLEX PROBLEMS WITH SECRET PARAMETERS",2022,"Journal of Theoretical and Applied Information Technology","100","1",,"235","245",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124483425&partnerID=40&md5=0ba9249c5f6121ef5fe41d804cb2b594","In this work, we study methods for the secure use of external insecure computers (servers) when solving computationally-complex problems with secret parameters. This problem is one of the important scientific directions in the field of information security of cloud computing. The article presents methods for secure outsourcing of the problem of finding the extremum of a function, as well as one method for finding the value of an analytical (holomorphic) function on a secret argument. Note that our main goal is to demonstrate new methods of secure outsourcing of scientific computing, so we model classes of problems in such a way as to clearly show the essence of these methods. © 2021 Little Lion Scientific","Big Data; Client-Server Interactions; Cloud Computing; Computationally-Complex Problem; Information Security; Secure Outsourcing",,Article,Scopus,2-s2.0-85124483425
"Sasaki Y.","57362485400;","A Survey on IoT Big Data Analytic Systems: Current and Future",2022,"IEEE Internet of Things Journal","9","2",,"1024","1036",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120572480&doi=10.1109%2fJIOT.2021.3131724&partnerID=40&md5=95c2ac30dd4ae77431762644c60160ba","The Internet of Things (IoT) has become widespread around the world. Since a large number of diverse devices, such as vehicles, household electrical appliances, smart phones, and environmental sensors are connected to the Internet, we can obtain a large volume of diverse IoT data, known as IoT big data. The generation of IoT big data means that efficient analytic systems are needed for many application scenarios, for example, to optimize urban planning, solve air pollution problems, and improve business decisions. In this survey, we review current systems that can efficiently analyze IoT data. Existing systems can be categorized into batch and stream processing systems. We explore Hadoop-and Spark-based batch processing systems for spatiotemporal and trajectory data. We also review fog-and edge-aware stream processing systems. Although many existing systems can efficiently and effectively analyze specific data and tasks, no system exists that can handle all characteristics of IoT big data: volume, velocity, variety, veracity, and variability. We present some open issues and discuss the future of IoT big data analytic systems. This survey aims to help researchers and practitioners better understand current systems and develop new IoT big data analytic systems. © 2014 IEEE.","Distributed data processing; edge computing; smart city; spatiotemporal data; streaming processing; trajectory data","Batch data processing; Data handling; Edge computing; Internet of things; Smart city; Smartphones; Surveys; Analytics systems; Batch production; Batch production system; Distributed data processing; Edge computing; Production system; Spatio-temporal data; Streaming processing; Trajectories datum; Trajectory data.; Big data",Article,Scopus,2-s2.0-85120572480
"Li K., Bai K., Li Z., Guo J., Chang N.-B.","57192156636;36676931900;13407966600;57343811500;7202467963;","Synergistic data fusion of multimodal AOD and air quality data for near real-time full coverage air pollution assessment",2022,"Journal of Environmental Management","302",,"114121","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119266636&doi=10.1016%2fj.jenvman.2021.114121&partnerID=40&md5=b81fddcd6eb00f2d61680288623fae37","Data gaps in satellite aerosol optical depth (AOD) retrievals pose a huge challenge in near real-time air quality assessment. Here, we present a multimodal aerosol data fusion approach to integrate multisource AOD and air quality data for the generation of full coverage AOD maps at hourly resolution. Specifically, data gaps in each Himawari-8 AOD snapshot were partially filled by merging all available daytime AOD snapshots, and these partially gap-filled AOD maps were then fused with coarse yet spatially complete numerical AOD simulations to generate full coverage AOD imageries. Ground-based air quality measurements, including concentrations of PM2.5, PM10, NO2, and SO2, were simultaneously assimilated into gridded AOD fields to enhance the overall data accuracy. A practical implementation of the proposed method was illustrated by generating hourly full-coverage AOD maps in China from 2015 to 2020, and the validation results indicate this new AOD dataset agreed well with ground-based AOD measurements (R = 0.83), from which a ubiquitous AOD decreasing trend was revealed, especially during the noontime. Moreover, the hourly resolution and full-coverage advantages of this AOD dataset allow us to better assess spatiotemporal variations of PM10 and PM2.5 pollution that occurred in China. Overall, the proposed method paves a new way as big data analytics to advance regional air pollution assessment given the full coverage capacity and enhanced accuracy of the resulting AOD and PM concentration data. © 2021 Elsevier Ltd","Air quality management; AOD; Data fusion; Haze pollution; Himawari; Optimal interpolation","nitrogen dioxide; sulfur dioxide; aerosol; aerosol property; air quality; atmospheric pollution; environmental assessment; optical depth; real time; satellite altimetry; spatial resolution; aerosol; air monitoring; air quality; Article; big data; circadian rhythm; cloud; computer simulation; controlled study; data accuracy; data completeness; data integration; data validity; deep belief network; dust storm; geographic mapping; haze; intermethod comparison; kernel method; measurement accuracy; nitrogen concentration; optical depth; particulate matter 10; particulate matter 2.5; predictive validity; satellite imagery; seasonal variation; snow cover; spatiotemporal analysis; time factor; aerosol; air pollutant; air pollution; environmental monitoring; particulate matter; China; Aerosols; Air Pollutants; Air Pollution; Data Accuracy; Environmental Monitoring; Particulate Matter",Article,Scopus,2-s2.0-85119266636
"Zhang X., He A., Guo R., Zhao Y., Yang L., Morita S., Xu Y., Noda I., Ozaki Y.","57204941099;42361166700;56032993700;57262711700;35406561000;57263611700;7406452235;57262533300;57220598317;","A new approach to removing interference of moisture from FTIR spectrum",2022,"Spectrochimica Acta - Part A: Molecular and Biomolecular Spectroscopy","265",,"120373","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115167760&doi=10.1016%2fj.saa.2021.120373&partnerID=40&md5=940777467973246fb9167bed23b0b2c6","An approach is developed to remove the interference of moisture from FTIR spectra. The interference arises from two aspects: the fluctuation on the temperature of the HeNe laser and the fluctuation on the transient concentration of moisture in the light - path of an FTIR spectrometer. The temperature fluctuation on the HeNe laser produces a systematic spectral shift between single-beam sample and background spectra, which often makes spectral subtraction method invalid in removing the interference of moisture. Herein, the Carbo similarity metric (the CAB value) is used to reflect the subtle spectral shift. A database of single-beam background spectra is established based on the concept of big-data and the pigeon-hole theory. The spectral shift is corrected by selecting suitable single-beam background spectra from the database to match with the given single-beam sample spectrum according to the CAB value. The interference caused by the fluctuation of the transient concentration of moisture is removed using a comprehensive 2D-COS method. We apply the approach on two polymeric samples to retrieve high-quality spectra and reliable second derivative spectra without the interference of moisture. The present work provides a new opportunity of obtaining the reliable second derivative spectra in the spectral region masked by moisture. © 2021 Elsevier B.V.","2D-COS; Big-data; FTIR; SACPs; Water vapor","Big data; Fourier transform infrared spectroscopy; %moisture; 2d-COS; Background spectrum; FTIR; He-Ne lasers; SACP; Single-beam; Spectra's; Spectral shift; Water vapour; Moisture",Article,Scopus,2-s2.0-85115167760
"Xiong Y.-J., Cheng S.-Y., Chen B.","57188725178;57219471237;57190186839;","Mitigating Lifetime-Energy-Makespan Issues in Reliability-Aware Workflow Scheduling for Big Data",2022,"Journal of Circuits, Systems and Computers","31","1","2250012","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111427663&doi=10.1142%2fS0218126622500128&partnerID=40&md5=d1ed9b04a3bcccf2404ae8a47cbb9253","The emergence of cloud computing in big data era has exerted a substantial impact on our daily lives. The conventional reliability-aware workflow scheduling (RWS) is capable of improving or maintaining system reliability by fault tolerance techniques such as replication and checkpointing based recovery. However, the fault tolerant techniques used in RWS would inevitably result in higher system energy consumption, longer execution time, and worse thermal profiles that would in turn lead to a decreased hardware lifespan. To mitigate the lifetime-energy-makespan issues of RWS in cloud computing systems for big data, we propose a novel methodology that decomposes the complicated studied problem. In this methodology, we provide three procedures to solve the energy consumption, execution makespan, and hardware lifespan issues in cloud systems executing real-time workflow applications. We implement numerous simulation experiments to validate the proposed methodology for RWS. Simulation results clearly show that the proposed RWS strategies outperform comparative approaches in reducing energy consumption, shortening execution makespan, and prolonging system lifespan while maintaining high reliability. The improvements on energy saving, reduction on makespan, and increase in lifespan can be up to 23.8%, 18.6%, and 69.2%, respectively. Results also show the potentiality of the proposed method to develop a distributed analysis system for big data that serves satellite signal processing, earthquake early warning, and so on. © 2022 World Scientific Publishing Company","Big Data; Cloud computing; energy; lifespan; makespan; reliability-aware workflow scheduling; replication","Big data; Cloud computing; Computer aided software engineering; Energy conservation; Energy utilization; Fault tolerant computer systems; Green computing; Real time systems; Scheduling; Signal processing; Comparative approach; Distributed analysis system; Earthquake early warning; Fault tolerance techniques; Fault tolerant technique; Reducing energy consumption; System energy consumption; Workflow applications; Fault tolerance",Article,Scopus,2-s2.0-85111427663
"Akshay Kumaar M., Samiayya D., Vincent P.M.D.R., Srinivasan K., Chang C.-Y., Ganesh H.","57426156200;57194221849;55808710700;57192191217;8076196000;57425130900;","A Hybrid Framework for Intrusion Detection in Healthcare Systems Using Deep Learning",2022,"Frontiers in Public Health","9",,"824898","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123421217&doi=10.3389%2ffpubh.2021.824898&partnerID=40&md5=d91f824975212d654a4ab6a7dc87cd10","The unbounded increase in network traffic and user data has made it difficult for network intrusion detection systems to be abreast and perform well. Intrusion Systems are crucial in e-healthcare since the patients' medical records should be kept highly secure, confidential, and accurate. Any change in the actual patient data can lead to errors in the diagnosis and treatment. Most of the existing artificial intelligence-based systems are trained on outdated intrusion detection repositories, which can produce more false positives and require retraining the algorithm from scratch to support new attacks. These processes also make it challenging to secure patient records in medical systems as the intrusion detection mechanisms can become frequently obsolete. This paper proposes a hybrid framework using Deep Learning named “ImmuneNet” to recognize the latest intrusion attacks and defend healthcare data. The proposed framework uses multiple feature engineering processes, oversampling methods to improve class balance, and hyper-parameter optimization techniques to achieve high accuracy and performance. The architecture contains <1 million parameters, making it lightweight, fast, and IoT-friendly, suitable for deploying the IDS on medical devices and healthcare systems. The performance of ImmuneNet was benchmarked against several other machine learning algorithms on the Canadian Institute for Cybersecurity's Intrusion Detection System 2017, 2018, and Bell DNS 2021 datasets which contain extensive real-time and latest cyber attack data. Out of all the experiments, ImmuneNet performed the best on the CIC Bell DNS 2021 dataset with about 99.19% accuracy, 99.22% precision, 99.19% recall, and 99.2% ROC-AUC scores, which are comparatively better and up-to-date than other existing approaches in classifying between requests that are normal, intrusion, and other cyber attacks. Copyright © 2022 Akshay Kumaar, Samiayya, Vincent, Srinivasan, Chang and Ganesh.","artificial intelligence; big data; deep learning; healthcare; network security; neural networks",,Article,Scopus,2-s2.0-85123421217
"Guo Y., Wang T., Chen W., Kaptchuk T.J., Li X., Gao X., Yao J., Tang X., Xu Z.","57204419336;57437439600;55716044700;26643513100;57437873400;57212969386;57202690493;57437731400;57438019300;","Acceptability of Traditional Chinese Medicine in Chinese People Based on 10-Year's Real World Study With Mutiple Big Data Mining",2022,"Frontiers in Public Health","9",,"811730","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123920893&doi=10.3389%2ffpubh.2021.811730&partnerID=40&md5=7de0d59d053f7bd0bf40694fa70ee2af","In the past decades, numerous clinical researches have been conducted to illuminate the effects of traditional Chinese medicine for better inheritance and promotion of it, which are mostly clinical trials designed from the doctor's point of view. This large-scale data mining study was conducted from real-world point of view in up to 10 years' big data sets of Traditional Chinese Medicine (TCM) in China, including both medical visits to hospital and cyberspace and contemporaneous social survey data. Finally, some important and interesting findings appear: (1) More Criticisms vs. More Visits. The intensity of criticism increased by 2.33 times over the past 10 years, while the actual number of visits increased by 2.41 times. (2) The people of younger age, highly educated and from economically developed areas have become the primary population for utilizing TCM, which is contrary to common opinions on the characteristics of TCM users. The discovery of this phenomenon indicates that TCM deserves further study on how it treats illness and maintains health. Copyright © 2022 Guo, Wang, Chen, Kaptchuk, Li, Gao, Yao, Tang and Xu.","big data; China; data mining; social review; TCM; traditional Chinese medicine","article; big data; China; Chinese; Chinese medicine; data mining; human; mining",Article,Scopus,2-s2.0-85123920893
"Ek H.M., Nair V., Douglas C.M., Lieuwen T.C., Emerson B.L.","57191570076;57195617383;57193870900;7004824356;55345338500;","Permuted proper orthogonal decomposition for analysis of advecting structures",2022,"Journal of Fluid Mechanics","930",,"2021908","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120082523&doi=10.1017%2fjfm.2021.908&partnerID=40&md5=dcd795fff55932ccf45a6cd23e254007","Flow data are often decomposed using proper orthogonal decomposition (POD) of the space-time separated form, which targets spatially correlated flow structures in an optimal manner. This paper analyses permuted POD (PPOD), which decomposes data as, where is a general spatial coordinate system, is the coordinate along the bulk advection direction and are along mutually orthogonal directions normal to the advection characteristic. This separation of variables is associated with a fundamentally different inner product space for which PPOD is optimal and targets correlations in space. This paper presents mathematical features of PPOD, followed by analysis of three experimental datasets from high-Reynolds-number, turbulent shear flows: a wake, a swirling annular jet and a jet in cross-flow. In the wake and swirling jet cases, the leading PPOD and space-only POD modes focus on similar features but differ in convergence rates and fidelity in capturing spatial and temporal information. In contrast, the leading PPOD and space-only POD modes for the jet in cross-flow capture completely different features - advecting shear layer structures and flapping of the jet column, respectively. This example demonstrates how the different inner product spaces, which order the PPOD and space-only POD modes according to different measures of variance, provide unique 'lenses' into features of advection-dominated flows, allowing complementary insights. © 2022 Cambridge University Press. All rights reserved.","big data; shear layers; turbulent reacting flows","Advection; Big data; Principal component analysis; Reynolds number; Wakes; Co-ordinate system; Decomposition modes; Flow data; Inner product space; Orthogonal directions; Paper analysis; Shear layer; Spacetime; Spatial coordinates; Turbulent reacting flows; Shear flow; advection; correlation; data set; decomposition analysis; flow structure; fluid mechanics; jet flow; Reynolds number; shear flow; turbulent flow",Article,Scopus,2-s2.0-85120082523
"Guo H., Li J., Gao H., Zhang K.","57189589481;57211142666;34769881200;57188876928;","PSATop-k: Approximate range top-k computation on big data",2022,"Knowledge-Based Systems","235",,"107614","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118490305&doi=10.1016%2fj.knosys.2021.107614&partnerID=40&md5=61f69116aa02c689e22e04b5beb8fe0d","Approximate top-k query returns a list of k tuples that have approximate largest scores with respect to the user given query. However, existing algorithms cannot effectively process the approximate top-k queries on big data, because they either restrict the class of ranking functions, or fail to take selection conditions into consideration. In this paper, a novel algorithm PSATop-k, which combines partitioning and sampling techniques, is proposed to answer approximate range top-k query efficiently. PSATop-k is suitable for queries with selection conditions and arbitrary ranking functions. PSATop-k first determines the sampling size that meets the accuracy requirement, then draws sufficient random tuples to return result by accessing a subset of the partitioned data. The experimental results on the real-life and synthetic datasets demonstrate that PSATop-k performs much better than the existing algorithms. Specially, as result set size varies from 10 to 80, PSATop-k runs 12.22 to 14.30 times faster than TA-based and Coreset-based methods on average. The speedup ratios are 11.43 to 23.34 and 55.86 to 649.06 in the experiments of error bound and tuple number, respectively. © 2021 Elsevier B.V.","Approximate range top-k; Big data; Partitioning; Sampling","Big data; Data handling; Query processing; Approximate range top-k; Condition; Novel algorithm; Partitioning; Partitioning techniques; Range top-k query; Ranking functions; Real life datasets; Sampling technique; Top-k query; Information retrieval",Article,Scopus,2-s2.0-85118490305
"Nimmy S.F., Hussain O.K., Chakrabortty R.K., Hussain F.K., Saberi M.","55220096200;8618708200;55538597700;7101795409;24723128200;","Explainability in supply chain operational risk management: A systematic literature review",2022,"Knowledge-Based Systems","235",,"107587","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118479625&doi=10.1016%2fj.knosys.2021.107587&partnerID=40&md5=efc370aba1ebb26a0156209feb8a2745","It is important to manage operational disruptions to ensure the success of supply chain operations. To achieve this aim, researchers have developed techniques that determine the occurrence of operational risk events which assists supply chain operational risk managers develop plans to manage them by detection/monitoring, mitigation/management, or optimization techniques. Various artificial intelligence (AI) approaches have been used to develop such techniques in the broad activities of operational risk management. However, all of these techniques are black box in their working nature. This means that the chosen technique cannot explain why it has given that output and whether it is correct and free from bias. To address this, researchers argue the need for supply chain management professionals to move towards using explainable AI methods for operational risk management. In this paper, we conduct a systematic literature review on the techniques used to determine operational risks and analyse whether they satisfy the requirement of them being explainable. The findings highlight the shortcomings and inspires directions for future research. From a managerial perspective, the paper encourages risk managers to choose techniques for supply chain operational risk management that can be auditable as this will ensure that the risk managers know why they should take a particular risk management action rather than just what they should do to manage the operational risks. © 2021 Elsevier B.V.","Artificial Intelligence (AI); Big data; Explainable AI (XAI); Supply chain operational risk management (SCORM)","Artificial intelligence; Managers; Risk management; Supply chain management; Artificial intelligence; Explainable artificial intelligence (XAI); Management techniques; Operational risk managements; Operational risks; Optimization techniques; Risk manager; Supply chain operation; Supply chain operational risk management; Systematic literature review; Big data",Article,Scopus,2-s2.0-85118479625
"Atharvan G., Koolikkara Madom Krishnamoorthy S., Dua A., Gupta S.","57306596700;57306124900;7004844676;56925715300;","A way forward towards a technology-driven development of industry 4.0 using big data analytics in 5G-enabled IIoT",2022,"International Journal of Communication Systems","35","1","e5014","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117701493&doi=10.1002%2fdac.5014&partnerID=40&md5=acece3b3fbf721c07dd0590ebd2ea7f3","The evolution of Internet of Things (IoT) has led to the development of Industrial Internet of Things (IIoT). IIoT is one the widely applied areas to facilitate people in the manufacturing world. The adoption of IIoT automates sensing, capturing, communicating, and processing in real time. To understand how rapidly IoT and IIoT are growing, this article examines the emergence of 5G-enabled IIoT, current research trends in IIoT, key milestones achieved in IIoT, and IoT applications specific to 5G-enabled IIoT. The paper presents the state-of-the-art in networking layered framework of IIoT and comparing relationships of technologies of cloud computing as well as edge computing paradigms. We also explored the type of security attacks and their preventive measures in an IIoT-driven 5G technology. We have also highlighted the revolution of IIoT-driven 5G framework which satisfies the demands of IIoT applications. © 2021 John Wiley & Sons Ltd.","5G technology; big data analytics; cloud computing; edge computing; fog computing; IIoT","5G mobile communication systems; Advanced Analytics; Big data; Data Analytics; Industry 4.0; Internet of things; Network layers; 'current; Application specific; Cloud-computing; Computing paradigm; Edge computing; Preventive measures; Real- time; Research trends; Security attacks; State of the art; Edge computing",Article,Scopus,2-s2.0-85117701493
"Fan Z.","57222043050;","Fault detection of energy-aware grid systems in big data environment",2022,"International Journal of Communication Systems","35","1","e4755","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101154231&doi=10.1002%2fdac.4755&partnerID=40&md5=d018307ea5aedd6bedb5efa499f74abe","Grid technology based on parallel theory is used to study a large amount of data processing. This research mainly discusses the fault detection of grid dynamic system in a big data environment. In order to verify the communication efficiency between different security nodes of the ZSS system under different operating system conditions, this study will use four physical computers and VM Ware software to establish a simulated communication environment, using GlobusToolkit (GT)'s existing deployment mechanism to realize the overall grid dynamic deployment architecture. Using Web service technology remotely calls the monitoring module interface of resource nodes in the domain and uses standard protocols to send monitoring data. At the same time, in order for the management node to save the monitoring data in the local standardized form of XML, the administrator can use the saved historical data to process system failures, analyze, and predict system performance. In order to solve the problem of information explosion, a fault detector is used to precisely control the cost of message detection and the state distribution of the detection result and the time of message delay. Under special circumstances, it takes 60 min to execute a 30-min job, and the fault detection point consumes an average of 54.33 min. The research results show that the fault-tolerant strategy can improve the performance of resource consumption in the fault-tolerant tracking system. Adopting the method of dynamically adjustable detection interval can effectively reduce the overhead of the grid dynamic system. The detection method used in this study can meet the application requirements of the grid. © 2021 John Wiley & Sons Ltd.","big data environment; dynamic deployment; fault detection; grid dynamic system; grid service","Big data; Computer operating systems; Data handling; Dynamics; Fault detection; Fault tolerance; Fault tolerant computer systems; Monitoring; Power management; Web services; Application requirements; Communication efficiency; Communication environments; Deployment mechanisms; Fault-tolerant strategy; Information explosion; Resource consumption; Web service technology; Information management",Article,Scopus,2-s2.0-85101154231
"AlNuaimi N., Masud M.M., Serhani M.A., Zaki N.","57205576975;14037743900;15925757100;16044448200;","Streaming feature selection algorithms for big data: A survey",2022,"Applied Computing and Informatics","18","1-2",,"113","135",,6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060576571&doi=10.1016%2fj.aci.2019.01.001&partnerID=40&md5=6495e6c16a556e422e27ee378ae4ba7a","Organizations in many domains generate a considerable amount of heterogeneous data every day. Such data can be processed to enhance these organizations’ decisions in real time. However, storing and processing large and varied datasets (known as big data) is challenging to do in real time. In machine learning, streaming feature selection has always been considered a superior technique for selecting the relevant subset features from highly dimensional data and thus reducing learning complexity. In the relevant literature, streaming feature selection refers to the features that arrive consecutively over time; despite a lack of exact figure on the number of features, numbers of instances are well-established. Many scholars in the field have proposed streaming-feature-selection algorithms in attempts to find the proper solution to this problem. This paper presents an exhaustive and methodological introduction of these techniques. This study provides a review of the traditional feature-selection algorithms and then scrutinizes the current algorithms that use streaming feature selection to determine their strengths and weaknesses. The survey also sheds light on the ongoing challenges in big-data research. © 2019, Noura AlNuaimi, Mohammad Mehedy Masud, Mohamed Adel Serhani and Nazar Zaki.","Big data; Redundant features; Relevant features; Streaming feature grouping; Streaming feature selection",,Article,Scopus,2-s2.0-85060576571
"Meena K., Tayal D.K., Castillo O., Jain A.","57220388830;16640428000;7007101709;55576323800;","Handling data-skewness in character based string similarity join using Hadoop",2022,"Applied Computing and Informatics","18","1-2",,"22","44",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056827023&doi=10.1016%2fj.aci.2018.11.001&partnerID=40&md5=6d44667080cc85225698944646185410","The scalability of similarity joins is threatened by the unexpected data characteristic of data skewness. This is a pervasive problem in scientific data. Due to skewness, the uneven distribution of attributes occurs, and it can cause a severe load imbalance problem. When database join operations are applied to these datasets, skewness occurs exponentially. All the algorithms developed to date for the implementation of database joins are highly skew sensitive. This paper presents a new approach for handling data-skewness in a character- based string similarity join using the MapReduce framework. In the literature, no such work exists to handle data skewness in character-based string similarity join, although work for set based string similarity joins exists. Proposed work has been divided into three stages, and every stage is further divided into mapper and reducer phases, which are dedicated to a specific task. The first stage is dedicated to finding the length of strings from a dataset. For valid candidate pair generation, MR-Pass Join framework has been suggested in the second stage. MRFA concepts are incorporated for string similarity join, which is named as “MRFA-SSJ” (MapReduce Frequency Adaptive – String Similarity Join) in the third stage which is further divided into four MapReduce phases. Hence, MRFA-SSJ has been proposed to handle skewness in the string similarity join. The experiments have been implemented on three different datasets namely: DBLP, Query log and a real dataset of IP addresses & Cookies by deploying Hadoop framework. The proposed algorithm has been compared with three known algorithms and it has been noticed that all these algorithms fail when data is highly skewed, whereas our proposed method handles highly skewed data without any problem. A set-up of the 15-node cluster has been used in this experiment, and we are following the Zipf distribution law for the analysis of skewness factor. Also, a comparison among existing and proposed techniques has been shown. Existing techniques survived till Zipf factor 0.5 whereas the proposed algorithm survives up to Zipf factor 1. Hence the proposed algorithm is skew insensitive and ensures scalability with a reasonable query processing time for string similarity database join. It also ensures the even distribution of attributes. © 2018, Kanak Meena, Devendra K. Tayal, Oscar Castillo and Amita Jain.","Big data; Data skewness; Hadoop; MapReduce; Similarity join",,Article,Scopus,2-s2.0-85056827023
"Chen T., Ma J., Liu Y., Chen Z., Xiao N., Lu Y., Fu Y., Yang C., Li M., Wu S., Wang X., Li D., He F., Hermjakob H., Zhu Y.","56555417200;57339535000;57424730100;57424765200;57424659800;57424659900;57225870147;55660348300;57424709400;12239213700;57424660000;57424730200;7201951930;6701613156;57418257400;","iProX in 2021: connecting proteomics data sharing with big data",2022,"Nucleic acids research","50","D1",,"D1522","D1527",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123388199&doi=10.1093%2fnar%2fgkab1081&partnerID=40&md5=d437ce42ad422bdcfe2a89a4d9565a56","The rapid development of proteomics studies has resulted in large volumes of experimental data. The emergence of big data platform provides the opportunity to handle these large amounts of data. The integrated proteome resource, iProX (https://www.iprox.cn), which was initiated in 2017, has been greatly improved with an up-to-date big data platform implemented in 2021. Here, we describe the main iProX developments since its first publication in Nucleic Acids Research in 2019. First, a hyper-converged architecture with high scalability supports the submission process. A hadoop cluster can store large amounts of proteomics datasets, and a distributed, RESTful-styled Elastic Search engine can query millions of records within one second. Also, several new features, including the Universal Spectrum Identifier (USI) mechanism proposed by ProteomeXchange, RESTful Web Service API, and a high-efficiency reanalysis pipeline, have been added to iProX for better open data sharing. By the end of August 2021, 1526 datasets had been submitted to iProX, reaching a total data volume of 92.42TB. With the implementation of the big data platform, iProX can support PB-level data storage, hundreds of billions of spectra records, and second-level latency service capabilities that meet the requirements of the fast growing field of proteomics. © The Author(s) 2021. Published by Oxford University Press on behalf of Nucleic Acids Research.",,"proteome; biology; genetics; information dissemination; protein database; proteomics; software; Big Data; Computational Biology; Databases, Protein; Information Dissemination; Proteome; Proteomics; Software",Article,Scopus,2-s2.0-85123388199
"Liu D., Wang W., Zhao Y.","56068235400;57222151776;56447412100;","Effect of weather on online food ordering",2022,"Kybernetes","51","1",,"165","209",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101724896&doi=10.1108%2fK-05-2020-0322&partnerID=40&md5=c4f9f53de36fd58c32a3441110df2a71","Purpose: Weather affects consumer decision-making. However, academic research on how weather factors affect specific takeaway foods is limited. This paper aims to fill in the gap and therefore to contribute to online marketing and operation. Design/methodology/approach: Web crawler techniques were first exploited to collect takeaway food ordering data from Meituan, the world’s largest GMV platform. Then statistics models and a time series regression model were selected to study the weather impact on online orders. Findings: The findings highlight that certain weather factors, such as temperature, air quality and rainfall have clear effects on most category takeaway orders. Originality/value: Quantitative analysis of weather impacts on the takeaway ordering business will help to guide the online service platforms for marketing promotion and the settled businesses to make reasonable arrangements for inventory and marketing tactics. © 2021, Emerald Publishing Limited.","Big data; M-R model; Takeaway ordering; Weather influence; Web crawler","Air quality; Decision making; Marketing; Regression analysis; Time series; Academic research; Consumer decision making; Design/methodology/approach; On-line service; Online marketing; Statistics model; Time-series regression; Weather factors; Web crawler",Article,Scopus,2-s2.0-85101724896
"Mariani M., Baggio R.","36817705700;23476323900;","Big data and analytics in hospitality and tourism: a systematic literature review",2022,"International Journal of Contemporary Hospitality Management","34","1",,"231","278",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118988974&doi=10.1108%2fIJCHM-03-2021-0301&partnerID=40&md5=bd24aa88337744c43b8411f62eaaf6af","Purpose: The purpose of this work is to survey the body of research revolving around big data (BD) and analytics in hospitality and tourism, by detecting macro topical areas, research streams and gaps and to develop an agenda for future research. Design/methodology/approach: This research is based on a systematic literature review of academic papers indexed in the Scopus and Web of Science databases published up to 31 December 2020. The outputs were analyzed using bibliometric techniques, network analysis and topic modeling. Findings: The number of scientific outputs in research with hospitality and tourism settings has been expanding over the period 2015–2020, with a substantial stability of the areas examined. The vast majority are published in academic journals where the main reference area is neither hospitality nor tourism. The body of research is rather fragmented and studies on relevant aspects, such as BD analytics capabilities, are virtually missing. Most of the outputs are empirical. Moreover, many of the articles collected relatively small quantities of records and, regardless of the time period considered, only a handful of articles mix a number of different techniques. Originality/value: This work sheds new light on the emergence of a body of research at the intersection of hospitality and tourism management and data science. It enriches and complements extant literature reviews on BD and analytics, combining these two interconnected topics. © 2021, Emerald Publishing Limited.","Analytics; Big data; Hospitality; Latent Dirichlet allocation; Systematic literature review; Tourism",,Article,Scopus,2-s2.0-85118988974
"Yan H.","57465097500;","News and Public Opinion Multioutput IoT Intelligent Modeling and Popularity Big Data Analysis and Prediction",2022,"Computational Intelligence and Neuroscience","2022",,"3567697","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125201855&doi=10.1155%2f2022%2f3567697&partnerID=40&md5=6a7b56dc8dd6e5b82a4671825cde989d","Based on the news and public opinion multioutput Internet of Things architecture, this article analyzes and predicts its popularity with big data. Firstly, the model adopts a three-tier architecture, in which the bottom layer is the data layer. It is mainly responsible for the collection of the terminal sensor data of the Internet of Things, and it uses intelligent big data as the data warehouse. Secondly, the computing layer on the data layer mainly provides the computing framework. Using the open-source SQL query engine, a cluster environment based on memory computing is constructed to realize the parallelization of data computing. It is used for interactive operations between the system and users. It receives and forwards the query requests submitted by the client browser, transmits them to the server cluster for execution, and displays the results in the browser. The end is displayed to the user. After that, combined with the needs of the development and application of news and public opinion big data, the data collection process was analyzed and designed, and the distributed data collection architecture was built. The intelligent Internet of Things was adopted for data storage, the data storage structure was analyzed and designed, and the data storage structure was designed to avoid catching. The repeat check algorithm is used to repeatedlystore the obtained page data. At the same time, according to the analysis of the business needs of the news and public opinion information platform, the overall functional structure of the platform was designed. The database and platform interface were designed in detail. The simulation results show that the model realizes the statistical query of the collected sensor alarm information and historical data on the user system, combines the historical operating data to analyze the relationship between the supply/return water temperature of the heat exchange station and the outdoor temperature, and realizes chart visualization of data analysis. © 2022 Hindawi Limited. All rights reserved.",,"Cluster computing; Data acquisition; Data handling; Data visualization; Data warehouses; Digital storage; Internet of things; Open systems; Query processing; Search engines; Social aspects; Temperature; Bottom layers; Data layer; Data storage; Intelligent models; Internet of things architectures; Multi-output; Public opinions; Sensors data; Storage structures; Three-tier architecture; Big data; data analysis; human; public opinion; software; Big Data; Data Analysis; Humans; Internet of Things; Public Opinion; Software",Article,Scopus,2-s2.0-85125201855
"Xun Z.","57463672600;","Monitoring and Analysis of English Classroom Teaching Quality Based on Big Data",2022,"Security and Communication Networks","2022",,"5365807","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125124040&doi=10.1155%2f2022%2f5365807&partnerID=40&md5=1d20256bd745d813eb2922f4a3795da7","In order to deal with the problems of low evaluation accuracy and long evaluation time in the traditional English classroom teaching quality monitoring and evaluation methods, this study proposes an English classroom teaching quality monitoring and evaluation method based on big data. According to the selection principle of English classroom teaching quality monitoring factors, this study analyzes the English classroom teaching quality monitoring factors, constructs the English classroom teaching quality monitoring index system, obtains the monitoring index data, filters and compresses the monitoring data, and takes the preprocessed English classroom teaching quality monitoring data as the evaluation index. This study uses the principal component analysis method to calculate the weight of the English classroom teaching quality evaluation index and constructs an English classroom teaching quality evaluation model. The simulation results show that the proposed method has high accuracy and short evaluation time. © 2022 Zhang Xun.",,"Monitoring; Principal component analysis; Quality control; Classroom teaching qualities; Evaluation accuracy; Evaluation methods; Monitoring and analysis; Monitoring and evaluations; Monitoring index; Monitoring methods; Quality evaluation; Quality monitoring; Selection principles; Big data",Article,Scopus,2-s2.0-85125124040
"Lv Z.","57463619000;","Research on Optimization and Application of University Student Development and Management Strategy Driven by Multidimensional Big Data",2022,"Scientific Programming","2022",,"6538069","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125115993&doi=10.1155%2f2022%2f6538069&partnerID=40&md5=d0403fc2c2760fae2ce30cdc921ab9a8","The purpose of education is to enable students to develop fully, freely, comprehensively, and harmoniously. The development of college students is a social problem that must be focused on in higher education under the background of China's powerful human resources. It is an important subject that higher education must attach great importance to and solve to find out the way and method to solve and avoid the development of college students to help them face risks correctly, meet challenges, and grow healthily. Under the background in the era of big data, Internet and mobile intelligent terminal has great popularity in colleges and universities, especially in the digital library, the official platform construction, and is widely used in Internet multimedia technology in college classroom, although for college students and the teacher provides vast amounts of information data, exploring the college students' horizons, enriching the students' knowledge structure. But the explosive growth of data and information security threat to the development of college students management work still bring the severe test of this; education workers in colleges and universities should comply with the development of The Times, the big data organic blend in the current education system, and the specific practice of education, for modern education practice and college students' all-round development's age characteristic new development mode and effective way. In order to adapt to the new form of the development of The Times, the management of colleges and universities should keep pace with The Times, and integrate the concept of big data and advanced technology into the education management work, so as to realize the national deployment of the education system innovation development strategy, explore teaching rules, students' growth and development tasks, and other realistic needs of the times. In addition, making big data the most powerful internal driving force for educational development helps colleges and universities to realize the frontier, timeliness, interactivity, and individuation of educational management. Colleges and universities should actively apply the idea of big data, improve the data resource integration and use ability of the staff in the field of education management through diversified means, improve the management level in multiple dimensions, and promote the updating and upgrading of the management structure of higher education and teaching. By analyzing the background of big data era, this paper analyzes the challenges of college students' management in the era of big data and puts forward the strategies of college students' management in the era of big data, so as to improve the quality and efficiency of college students' management. © 2022 Zhimei Lv.",,"Digital libraries; Human resource management; Information management; Multimedia systems; Security of data; Strategic planning; Students; College students; Colleges and universities; Development strategies; Education management; Education systems; High educations; Optimisations; Student development; Student management; University students; Big data",Article,Scopus,2-s2.0-85125115993
"Zhou W.","57215270218;","Big Data-Driven Hierarchical Local Area Network Security Risk Event Prediction Algorithm",2022,"Scientific Programming","2022",,"4960360","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125114984&doi=10.1155%2f2022%2f4960360&partnerID=40&md5=46532c06ab65a4af0570f5426b87ae9d","Big data processing technology has attracted a lot of attention due to its forecasting and warning of Internet security situation. The current risk assessment system still has problems such as high false alarm rate and excessive reliance on expert knowledge in the security defense system. Based on the big data-driven principle, this paper constructs a hierarchical local area network security risk event prediction model and proposes a predictive complex event processing method. The model building process is evolved and improved on the basis of the scoring function. The establishment method of vulnerability database and vulnerability association database is introduced in detail. At the same time, the problem of the difference between the structure and identification method of the information in the information database and the vulnerability database is solved, and the effect of timely modification when the data do not match is realized. Experimental results show that the algorithm has an accuracy of 98.75% and a fault tolerance rate of 0.0035, which promotes the accuracy of the network risk assessment results based on multistage network attacks. © 2022 Wei Zhou.",,"Big data; Database systems; Fault tolerance; Forecasting; Local area networks; Network security; Data driven; Data processing technologies; Event prediction; Internet security; Networks security; Prediction algorithms; Risks assessments; Security risks; Security situation; Vulnerability database; Risk assessment",Article,Scopus,2-s2.0-85125114984
"Zhang W.","57463894900;","Quality Improvement of College Students' Innovation and Entrepreneurship Education Based on Big Data Analysis under the Background of Cloud Computing",2022,"Scientific Programming","2022",,"8734474","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125113348&doi=10.1155%2f2022%2f8734474&partnerID=40&md5=b9834814ede5e4f9e43641f2cdcae48d","China is in a critical period of national rejuvenation and national prosperity. Innovation is the soul of a nation and an inexhaustible driving force for national development. Youth are the hope and future of a country. In order to enable young people to have the entrepreneurial ability and innovative spirit of the development of the new era, it has become a very important link for college students to better enter the society and realize their own value. It has become a key link in the development of individuals and China, and has attracted the attention of the whole society. With the continuous development of Internet technology and the deepening of application exploration, in the context of cloud computing, the improvement of the quality of college students' innovation and entrepreneurship education based on big data analysis has also become the focus of universities, society, and the country. The core of the system is the discrete dynamic modeling technology of complex systems. This paper will discuss ways to improve the quality of college students' innovation and entrepreneurship education based on big data under the background of cloud computing by introducing the development and principle of complex system discrete dynamic modeling technology and analyzing the deficiencies and problems of college students' innovation and entrepreneurship quality in many aspects. © 2022 Wenhui Zhang.",,"Big data; Data handling; Information analysis; Quality control; Students; Cloud-computing; College students; Critical periods; Discrete dynamic models; Driving forces; Entrepreneurship education; Modeling technology; National development; Quality improvement; Young peoples; Cloud computing",Article,Scopus,2-s2.0-85125113348
"Bradley J.R.","7402239078;","Joint Bayesian Analysis of Multiple Response-Types Using the Hierarchical Generalized Transformation Model",2022,"Bayesian Analysis","17","1",,"127","164",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125100673&doi=10.1214%2f20-BA1246&partnerID=40&md5=7f4ad8209de0ff82910b36c4a7fbf65b","Consider the situation where an analyst has a Bayesian statistical model that performs well for continuous data. However, suppose the observed dataset consists of multiple response-types (e.g., continuous, count-valued, Bernoulli trials, etc.), which are distributed from more than one class of distributions. We refer to these types of data as “multiple response-type” datasets. The goal of this article is to introduce a reasonable easy-to-implement all-purpose method that “converts” a Bayesian statistical model for continuous responses (call this the preferred model) into a Bayesian model for multiple response-type datasets. To do this, we consider a transformation of the multiple response-type data, such that the transformed data can be reasonably modeled using the preferred model. What is unique with our strategy is that we treat the transformations as unknown and use a Bayesian approach to model this uncertainty. The implementation of our Bayesian approach to unknown transformations is straightforward, and involves two steps. The first step produces posterior replicates of the transformed multiple responsetype data from a latent conjugate multivariate (LCM) model. The second step involves generating values from the posterior distribution implied by the preferred model. We demonstrate the flexibility of our model through an application to Bayesian additive regression trees (BART) and a spatio-temporal mixed effects (SME) model.We provide a thorough joint multiple response-type spatio-temporal analysis of coronavirus disease 2019 (COVID-19) cases, the adjusted closing price of the Dow Jones Industrial (DJI), and Google Trends data. © 2022 International Society for Bayesian Analysis","Bayesian hierarchical model; Big data; Gibbs sampler; Log-linear models.; Markov chain monte carlo; Multiple response-types; Non-gaussian; Nonlinear",,Article,Scopus,2-s2.0-85125100673
"Ren Y., Feng X., Li L.","57459821300;57462994700;57460210800;","Integrated Development Technology of Artificial Intelligence, Big Data and Cloud Computing",2022,"Forest Chemicals Review","2022","January-February",,"197","206",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125050574&doi=10.17762%2fjfcr.vi.200&partnerID=40&md5=4ca1460231b68d82630641c93372b15e","In terms of the current development status and future development direction of artificial intelligence, big data and cloud computing, the relationship between the three is inseparable. The essence of the Trinity era is that the development of the future era will focus on the three technologies of artificial intelligence (AI), big data and cloud computing. At the same time, big data has gradually entered people's vision. After the information revolution, the connection between people's daily life and data is becoming more and more close. Many enterprises begin to use the mining and analysis of big data to mine the development trend and business model of enterprises, so as to help enterprises obtain higher operation efficiency and stronger competitive advantage. Starting from reality and combined with the author's work experience, this paper discusses the combination of artificial intelligence, big data and cloud computing. © 2022 Kriedt Enterprises Ltd. All Rights Reserved.","Artificial intelligence; Big data; Cloud computing; Information revolution","Artificial intelligence; Cloud computing; Competition; 'current; Cloud-computing; Daily datum; Daily lives; Development directions; Development status; Development technology; Development trends; Information revolution; Integrated development; Big data; Artificial Intelligence; Competition; Computation; Data; Development; Efficiency; Life; Technology",Article,Scopus,2-s2.0-85125050574
"Chi H., Chi Y.","57462032100;57461941100;","Smart Home Control and Management Based on Big Data Analysis",2022,"Computational Intelligence and Neuroscience","2022",,"3784756","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125020627&doi=10.1155%2f2022%2f3784756&partnerID=40&md5=34757e180af3161c2a02d7aada560409","In order to improve the effect of smart home control and management, a new smart home control and management method based on big data analysis is designed. The basic hardware of smart home control and management is designed, including smoke sensor hardware, temperature and humidity sensor hardware, and infrared sensor hardware, so as to collect smart home data and realize data visualization and buzzer alarm. The collected data are transmitted through the indoor wireless network of smart home gateway equipment, and the data distributed cache architecture based on big data analysis is used to store smart home data. Based on the relevant data, the hybrid particle swarm optimization algorithm is used to schedule the control and management tasks of smart home to complete the control and management of smart home. The experimental results show that the device control and scenario management effect of this method is better, and the communication performance is superior and has high practical application value. © 2022 Hindawi Limited. All rights reserved.",,"Automation; Big data; Cost effectiveness; Data handling; Data visualization; Gateways (computer networks); Humidity control; Information management; Intelligent buildings; Microwave sensors; Scheduling algorithms; Smoke; Wireless sensor networks; Control and management; Control methods; Hardware sensors; Infrared sensor; Management IS; Management method; Sensor hardware; Smart homes; Smoke sensors; Temperature and humidity sensor; Particle swarm optimization (PSO); algorithm; data analysis; health care delivery; wireless communication; Algorithms; Big Data; Data Analysis; Delivery of Health Care; Wireless Technology",Article,Scopus,2-s2.0-85125020627
"Zhang Y.","57462015300;","Intelligent Recommendation Model of Contemporary Pop Music Based on Knowledge Map",2022,"Computational Intelligence and Neuroscience","2022",,"1756585","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125020155&doi=10.1155%2f2022%2f1756585&partnerID=40&md5=5622200c8e2ff361f523fbc05c8981e7","With the advent of the era of big data, the rise of Web2.0 completely subverts the traditional Internet model and becomes the trend of today's information age. Simultaneously, massive amounts of data and information have infiltrated various Internet companies, resulting in an increase in the problem of information overload. In the online world, learning how to quickly and accurately select the parts we are interested in from a variety of data has become a hot topic. Intelligent music recommendation has become a current research hotspot in music services as a viable solution to the problem of information overload in the digital music field. On the basis of precedents, this paper examines the characteristics of music in a comprehensive and detailed manner. A knowledge graph-based intelligent recommendation algorithm for contemporary popular music is proposed. User-defined tags are described as the free genes of music in this paper, making it easier to analyze user behavior and tap into user interests. It has been confirmed that this algorithm's recommendation quality is relatively high, and it offers a new development path for improving the speed of searching for health information services. © 2022 Hindawi Limited. All rights reserved.",,"Behavioral research; Information services; 'current; Data and information; Hot topics; Hotspots; Information age; Information overloads; Internet models; Knowledge map; Music recommendation; Viable solutions; Graphic methods; algorithm; intelligence; knowledge; music; Algorithms; Big Data; Intelligence; Knowledge; Music",Article,Scopus,2-s2.0-85125020155
"Bashir M.O.I.","57461888400;","Application of Artificial Intelligence (AI) in Dredging Efficiency in Bangladesh",2022,"Annals of Emerging Technologies in Computing","6","1",,"74","88",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125015754&doi=10.33166%2fAETiC.2022.01.005&partnerID=40&md5=b86309cfb2727d9de91b3a8d9634c7b2","The integration of Artificial Intelligence (AI) into the dredging systems and dredging machinery used in ""capital"" and ""maintenance"" dredging in Bangladesh can enhance the efficiency of the machines and dredging process, enabling the operators to perform regular and repetitive dredging tasks safely in the rivers, ports, and estuaries all over the country. AI, including Big Data, Machine Learning, Internet of Thing, Blockchain and Sensors and Simulators with their catalytic potentials, can systematically compile and evaluate specific data collected from different sources, develop applications or simulators, connect the stakeholders on a virtual platform, store lakes of information without compromising their intellectual rights, predicting models to harness the challenges, minimise the cost of dredging, identify possible threats and help protect the already dredged areas by giving timely signals for further maintenance. Furthermore, the application of AI modulated dredging devices and machinery can play a significant role when monitoring aspects becomes crucial, keeping environmental impacts mitigated without affecting the quality of the human environment. This study includes the evaluation of the application of AI – its prospect and challenges in the existing dredging systems in Bangladesh against the backdrop of the challenges faced in capital and maintenance dredging in the major rivers – and assess whether such inclusion of AI is likely to minimise the cost of dredging in the rivers of Bangladesh and facilitate the materialisation of the objectives of Bangladesh Delta Plan 2100.This paper studies the organisation's infrastructural requirement for the integration of AI into dredging systems, using benchmarking such as 1-""Understanding AI Ready Approach"", 2-""Strategies for Implementing AI"", 3-""Data Management"", 4-""Creating AI Literate Workforce and Upskilling"", and 5-""Identifying Threats"" concerning the management and dredging operations of Bangladesh Inland Water Transport Authority (BIWTA), under Bangladesh Ministry of Shipping and Bangladesh Water Development Board (BWDB). The paper also uses several case studies such as channel dredging to show that the use of AI can bring a significant change in the dredging operations both in reducing the cost of dredging and in terms of harnessing the barriers in adaptive management and environmental impacts. © 2022 by the author(s).","AI ready approach; Artificial intelligence; Bangladesh delta plan 2100; Big Data; Blockchain; Capital and maintenance dredging; Harnessing the challenges; Implementing AI; IoT; Reducing cost; Simulators; Upskilling","Artificial intelligence; Big data; Cost reduction; Distributed ledger; Efficiency; Information management; Internet of things; Laws and legislation; Maintenance; Simulators; Artificial intelligence ready approach; Bangladesh; Bangladesh delta plan 2100; Block-chain; Capital and maintenance dredging; Dredging operations; Harnessing the challenge; Implementing artificial intelligence; Reducing costs; Upskilling; Blockchain",Article,Scopus,2-s2.0-85125015754
"Yan M., Yan M.","57460645700;57460684500;","Monitoring and Early Warning Analysis of the Epidemic Situation of Escherichia coli Based on Big Data Technology and Cloud Computing",2022,"Journal of Healthcare Engineering","2022",,"8739447","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125005095&doi=10.1155%2f2022%2f8739447&partnerID=40&md5=33024e8ae98e19ef821eebf1524f89ee","The purpose of this study is to analyze the molecular epidemiological characteristics and resistance mechanisms of Escherichia coli. The study established a big data cloud computing prediction model for the epidemic mechanism of the pathogen. The study establishes the early warning, control parameters, and mathematical model of Escherichia coli infectious disease and monitors the molecular sequence of the pathogen based on discrete indicators. A nonlinear mathematical model equation was used to establish the epidemic trend model of Escherichia coli. The study shows that the use of the model can control the relative error at about 5%. The experiment proves the effectiveness of the combined model. © 2022 Meishu Yan and Meizi Yan.",,"Big data; Cloud computing; Disease control; Nonlinear equations; Cloud-computing; Control parameters; Data clouds; Data technologies; Early warning; Early warning analysis; Infectious disease; Nonlinear mathematical model; Prediction modelling; Resistance mechanisms; Escherichia coli",Article,Scopus,2-s2.0-85125005095
"Qing Y.","57459817900;","Research on Integrated Development Technology of Artificial Intelligence, Big Data and Cloud Computing",2022,"Forest Chemicals Review","2022","January-February",,"349","356",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124976031&doi=10.17762%2fjfcr.vi.218&partnerID=40&md5=e41c7dfe5f52157ba74f1555246558f1","Big data is profoundly changing our society and our way of production, life and thinking. At the same time, the development of big data continues to promote the innovation and breakthrough of artificial intelligence. Artificial intelligence is the focus of current research. All countries also raise artificial intelligence to the national strategic level and seize the commanding height of artificial intelligence. This paper analyzes the strategic characteristics of the development of artificial intelligence in the United States, Britain and Japan from the two dimensions of technology deployment and system guarantee. This paper studies the artificial intelligence technology based on big data and the development strategy of artificial intelligence, so as to provide a strategic idea for the development of artificial intelligence in China. The idea has a certain reference value for the research on the integrated development technology of artificial intelligence, big data and cloud computing. © 2022 Forest Chemicals Review. All rights reserved.","Artificial intelligence; Big data; Integrated development; Technology deployment","Artificial intelligence; Cloud computing; 'current; Britain; Cloud-computing; Development technology; Integrated development; Paper analysis; Production life; Strategic level; Technology deployment; Two-dimensions; Big data; Artificial Intelligence; Computation; Data; Development; Paper; Research; Technology; United Kingdom",Article,Scopus,2-s2.0-85124976031
"Li L., Mao Z., Ren Y.","57460210800;57460081200;57459821300;","E-commerce Precision Marketing Based on the Advantages of Big Data Technology",2022,"Forest Chemicals Review","2022","January-February",,"70","79",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124944414&doi=10.17762%2fjfcr.vi.186&partnerID=40&md5=fc70b7593bb25afa1456ee6bc189ede4","The core of big data marketing is to enable online advertising to be delivered to the right people through the right carrier at the right time. This can improve the conversion rate and achieve the effect of precision marketing. Combined with the mature cases involving big data precision marketing of relevant enterprises, this paper analyzes the necessity of big data precision marketing by taking enterprise big data precision marketing as an example. This paper also puts forward the specific implementation ways and methods of enterprises in big data precision marketing. The experimental results show that the method proposed in this paper can unify and integrate multiple sets of user data of different systems by opening up the data islands of various systems within the enterprise. This method can effectively analyze and mine data and label users. This method can carry out precision marketing activities for users through data modeling. © 2022 Kriedt Enterprises Ltd. All right reserved.","Big data marketing; Data modeling; Online advertising; Precision marketing","Commerce; Information analysis; Marketing; Big data marketing; Conversion rates; Data marketing; Data technologies; Marketing IS; Multiple set; Online advertizing; Paper analysis; Precision marketings; User data; Big data; Commerce; Cores; Data; Islands; Marketing; Paper; Precision; Systems",Article,Scopus,2-s2.0-85124944414
"Izhar A., Rastogi A., Ali S.S., Quadri S.M.K., Rizvi S.A.M.","57221958683;57197832541;57194419777;23397581400;55601665100;","Feature-driven label generation for congestion detection in smart cities under big data",2022,"International Journal of Advanced Technology and Engineering Exploration","9","86",,"94","110",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124895785&doi=10.19101%2fIJATEE.2021.874739&partnerID=40&md5=6399d8b80581a5f0f9a83ae3a67075ff","Due to rapid urbanization and the emergence of smart cities, the problem of traffic congestion has materialized into a major issue for smart city planners. Therefore, traffic congestion prediction is needed to effectively reduce traffic congestion and enhance the road capacity. There have been various studies which have tried to solve the problem of traffic congestion. However, it is difficult to properly judge the effectiveness of such studies given the absence of properly labeled datasets. Additionally, current studies use datasets with relatively lesser number of data instances, which does not correctly reflect the big data nature of the traffic data. Motivated by these problems and challenges, in this paper, we aim to study the problem of traffic congestion with respect to effective label-generation under big data perspective. Essentially, we provide two sound and intuitive techniques for label generation which help in the correct annotation of unlabeled data. One of the techniques is based on the number of vehicles plying on the road and the other is based on the amalgamation of average speed and number of vehicles. For this purpose, we consider a publicly available CityPulse traffic dataset with 13.5 million data instances. Using our techniques, we generate “congested” and “not-congested” labels depicting whether there is congestion on the road or not. To tackle the class imbalance problem, besides using random undersampling and oversampling techniques, we also introduce a mixture of the two techniques to negate any bias inherent to two individual sampling techniques. To test the effectiveness of our label generation approaches, we make the extensive use of various machine learning techniques and for performance evaluation we use all the standard classification evaluation metrics. Finally, we compare our techniques with a previous work which only considered average speed for label generation. Our results demonstrate the effectiveness of the proposed approaches against the comparing method. For example, in random undersampling the F1-score of every classifier under the proposed techniques is close to 1, whereas that under the comparing method, F1-score is as low as 0.70 in multinomial naïve Bayes (MNB) classifier and 0.88 in support vector machine (SVM). Similarly, in oversampling, our approaches have a close F1-score of 1 across all the classifiers, whereas the comparing method gets as low as 0.70 in MNB. The same trend can be seen in the mixture of both the sampling techniques. © 2022 Aamish Izhar et al.","Big data; Classification; Label generation; Smart cities; Traffic congestion",,Article,Scopus,2-s2.0-85124895785
"Mohamed W., Abdel-Fattah M.A.","57225108217;55545556200;","A proposed hybrid algorithm for mining frequent patterns on Spark",2022,"International Journal of Business Intelligence and Data Mining","20","2",,"146","169",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124800195&doi=10.1504%2fIJBIDM.2022.120833&partnerID=40&md5=99dd1c62d10c4acd44627c40e055d58c","Frequent itemset mining is one of the most important data mining techniques applied to discover frequent itemset, interesting information, and correlation from data. Many algorithms such as Apriori, Fp-growth and Eclat have been adjusted and implemented to deal with big data. Those algorithms are implemented on big data processing engines such as MapReduce and Spark. However, the existing implementations have limitations. Consequently, this paper proposes a hybrid algorithm to mine frequent patterns on sparse big dataset over Spark platform. The proposed hybrid algorithm uses Apriori in the first few levels then switches to use Eclat for the rest of levels. The proposed hybrid algorithm consists of four phases. Experiments for testing the performance of the proposed algorithm are conducted, and the elapsed time of the proposed hybrid algorithm is compared with parallel fp-growth, YAFIM and Eclat-Spark. The proposed algorithm outperforms YAFIM, Eclat, and fp-growth with a high degree of minimum support. © 2022 Inderscience Enterprises Ltd.","Apriori; Big data; Eclat; Frequent pattern mining; Spark","Big data; Data handling; Apriori; Data-mining techniques; Eclat; FP growths; Frequent itemset; Frequent itemset mining; Frequent patterns minings; Hybrid algorithms; Interesting information; Processing engine; Data mining",Article,Scopus,2-s2.0-85124800195
"Joseph N., Lindblad I., Zaker S., Elfversson S., Albinzon M., Ødegård Ø., Hantler L., Hellström P.M.","57456475600;57456560000;57456603500;57456560100;57456344100;57456344200;57456603600;7101653405;","Automated data extraction of electronic medical records: Validity of data mining to construct research databases for eligibility in gastroenterological clinical trials",2022,"Upsala Journal of Medical Sciences","127",,"e8260","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124775827&doi=10.48101%2fUJMS.V127.8260&partnerID=40&md5=a6197d6ccf019bd875b635b0c2fda675","Background: Electronic medical records (EMRs) are adopted for storing patient-related healthcare information. Using data mining techniques, it is possible to make use of and derive benefit from this massive amount of data effectively. We aimed to evaluate validity of data extracted by the Customized eXtraction Program (CXP). Methods: The CXP extracts and structures data in rapid standardised processes. The CXP was programmed to extract TNFα-native active ulcerative colitis (UC) patients from EMRs using defined International Classification of Disease-10 (ICD-10) codes. Extracted data were read in parallel with manual assessment of the EMR to compare with CXP-extracted data. Results: From the complete EMR set, 2,802 patients with code K51 (UC) were extracted. Then, CXP extracted 332 patients according to inclusion and exclusion criteria. Of these, 97.5% were correctly identified, resulting in a final set of 320 cases eligible for the study. When comparing CXP-extracted data against manually assessed EMRs, the recovery rate was 95.6–101.1% over the years with 96.1% weighted average sensitivity. Conclusion: Utilisation of the CXP software can be considered as an effective way to extract relevant EMR data without significant errors. Hence, by extracting from EMRs, CXP accurately identifies patients and has the capacity to facilitate research studies and clinical trials by finding patients with the requested code as well as funnel down itemised individuals according to specified inclusion and exclusion criteria. Beyond this, medical procedures and laboratory data can rapidly be retrieved from the EMRs to create tailored databases of extracted material for immediate use in clinical trials. © 2022 The Author(s).","Big data; Data analytics; Data extraction; Data mining; Electronic medical records","data mining; electronic health record; factual database; human; International Classification of Diseases; procedures; Data Mining; Databases, Factual; Electronic Health Records; Humans; International Classification of Diseases",Article,Scopus,2-s2.0-85124775827
"Al-Ateeq B., Sawan N., Al-Hajaya K., Altarawneh M., Al-Makhadmeh A.","57456259100;56051422600;57455162400;56771026800;57201615832;","BIG DATA ANALYTICS IN AUDITING AND THE CONSEQUENCES FOR AUDIT QUALITY: A STUDY USING THE TECHNOLOGY ACCEPTANCE MODEL (TAM)",2022,"Corporate Governance and Organizational Behavior Review","6","1",,"64","78",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124717857&doi=10.22495%2fcgobrv6i1p5&partnerID=40&md5=fc25e24a2a598d65a485793a9e9b9308","The study examines the impacts of using two dimensions of the technology acceptance model (TAM), perceived usefulness and perceived ease of use, on the adoption of big data analytics in auditing, and the subsequent impact on audit quality. Five hypotheses were developed. A questionnaire survey was undertaken with external affiliated audit companies and offices in Jordan. Eventually, 130 usable questionnaires were collected, representing a 72.22% response rate. Structural equation modelling (SEM) was employed for diagnosing the measurement model, and to test the hypotheses of the study. The study finds that perceived usefulness and perceived ease of use have a direct effect on audit quality, without mediating the actual use of data analytics. However, the use of big data analytics is shown to moderate the relationship between perceived usefulness and audit quality, but not between the perceived ease of use and audit quality. The study is one of the first to examine auditors’ acceptance of big data analytics in their work and the impact of this acceptance and actual use on audit quality. It contributes to the existing literature in auditing through its application of SEM to examine the impact of big data analytics usage on audit quality by using the TAM. © 2022 The Authors.","Audit Quality; Big Data; Big Data Analytics; Technology Acceptance Model",,Article,Scopus,2-s2.0-85124717857
"Liu Z., Gao P., Li W.","57454649400;57454961500;57454857600;","Research on Big Data-Driven Rural Revitalization Sharing Cogovernance Mechanism Based on Cloud Computing Technology",2022,"Wireless Communications and Mobile Computing","2022",,"2163126","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124706583&doi=10.1155%2f2022%2f2163126&partnerID=40&md5=75cb1e654bfe7710738a691f465c2d7f","The political arrangement and strategic action to realize socialist sharing in the new era is the rural revitalization strategy, which is a development strategy to complete the historical task of farmers living a better life and solve the problem of insufficient rural development. Data sharing and driving, on the other hand, remain the weak links in China's digital village construction. This paper proposes to develop a BD (big data)-driven rural revitalization governance, sharing, and monitoring platform based on CC (cloud computing) technology to promote the reshaping of rural governance patterns and to provide systematic support for the implementation of rural revitalization strategies. Knowledge governance rules and processes for rural community public affairs are created in a BD environment. Clustering in feature space is considered to address the low efficiency caused by clustering algorithms in high-dimensional data. The method proposed in this paper is perfectly matched in both the preprocessing and clustering steps, which not only ensures the algorithm's accuracy but also significantly improves its efficiency. © 2022 Zhuan Liu et al.",,"Big data; Cloud computing; Efficiency; Planning; Regional planning; Cloud computing technologies; Clusterings; Data driven; Data Sharing; Development strategies; Mechanism-based; Rural development; Sharing platforms; Villages' constructions; Weakest links; Clustering algorithms",Article,Scopus,2-s2.0-85124706583
"Liang J., Sun J., Wei W., Laghari R.A.","35797222900;57454730800;57454937800;57203970466;","Dynamic constitutive analysis of aluminum alloy materials commonly used in railway vehicles big data and its application in LS-DYNA",2022,"EAI Endorsed Transactions on Scalable Information Systems","9","34","e8","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124701829&doi=10.4108%2feai.28-9-2021.171169&partnerID=40&md5=b2bc943c563f73f0bae4fa8fcb55e653","The mechanical test for the three commonly used aluminum alloy materials (6005A-T6, 6008-T6 and 6082-T6) for rail vehicle big data are carried at different temperatures and different strain rates in this study. It can be seen that there are significant difference mechanical properties for these aluminum alloy. The main differences between them are the sensitivity of the strain rate and temperature. 6008-T6 and 6005A-T6 alloys have lower sensitivity to strain rate, while 6082-T6 alloy has higher sensitivity to strain rate and temperature. Based on the experimental data, the two commonly used dynamic material models of LS-DYNA, the Cowper-Symonds constitutive model and the Johnson-Cook constitutive model, are fitted. And then the two constitutive models are verified and compared by the dropping hammer impact test of the anti-climbing device. The results show that the Cowper-Symonds constitutive model has higher accuracy. © 2021 Juxing Liang et al., licensed to EAI. This is an open access article distributed under the terms of the Creative Commons Attribution license, which permits unlimited use, distribution and reproduction in any medium so long as the original work is properly cited","Aluminum alloy for vehicle; Anti-climbing device dropping hammer impact test; Big data; Dynamic constitutive model","Aluminum alloys; Big data; Constitutive models; Hammers; Strain rate; Aluminum alloy for vehicle; Aluminum alloy materials; Anti-climbing device dropping hammer impact test; Big data applications; Constitutive analysis; Dynamic constitutive model; LS-DYNA; Railway vehicles; Strain-rates; Strain-temperature; Vehicles",Article,Scopus,2-s2.0-85124701829
"Tang J., Li C., Fu Y., Li C.","57218303641;57209478353;57218304915;57454804400;","The Borderless Integration of Financial Management Innovation Using Big Data Analysis of Social Media",2022,"Wireless Communications and Mobile Computing","2022",,"4711617","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124690865&doi=10.1155%2f2022%2f4711617&partnerID=40&md5=698782a151572df0358d337ce4dc7999","This article mainly studies the innovation of borderless integrated financial management in big data social media in the era of wireless communication networks. For evaluating company A's financial sharing capabilities, five first-level indicators are divided into four dimensions: quality, efficiency, cost, and safety, and are allowed to score the financial sharing center capabilities. First of all, system integration takes performance appraisal as the core, and through the establishment of a reasonable appraisal system, performance appraisal for the members of the financial team is carried out. Secondly, team members need to accept two-way leadership, two-way assessment, and two-way incentives from the finance and business departments. The experimental factors were analyzed, and the results showed that the cumulative variance contribution rate with common factors reached 89.339%. The importance of the first-level indicators is in order of process reengineering (27.08%), operation management (22.33%), information system (20.24%), personnel management (17.178%), and strategic planning (12.46%). © 2022 Jinghui Tang et al.",,"Big data; Human resource management; Information management; Reengineering; Social networking (online); Financial managements; Four dimensions; Management innovation; Performance appraisal; Social media; System integration; Systems performance; Team members; Two ways; Wireless communications networks; Finance",Article,Scopus,2-s2.0-85124690865
"Jiang C.","57454801600;","Network Security and Ideological Security Based on Wireless Communication and Big Data Analysis",2022,"Wireless Communications and Mobile Computing","2022",,"1159978","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124686962&doi=10.1155%2f2022%2f1159978&partnerID=40&md5=328ac31f8e7ff3bbca7d20940460a89b","The rapid development of information technology has caused unprecedented changes in society and life. People's dependence on information terminals represented by mobile communication devices has greatly increased. The average time spent on information terminals is more than three hours per day, especially for the younger generation. This figure will be even more exaggerated. The West takes the network as an important position for cultural penetration into China and pushes the cultural products of good and bad to China in large quantities. In this context, the study of the relationship between network culture and ideology has become an important node in the construction of ideological security defense. This study first makes a detailed analysis of the concept and research status of network ideology, then discusses the harm of Western ideological network penetration, and puts forward the corresponding countermeasures. According to the characteristics of the current stage, a network ideology big data platform is designed and the detection ability and running time of the platform are analyzed. The results show that the overall performance of the platform is very good and it is more suitable to work in parallel mode. © 2022 Chunlan Jiang.",,"Network security; Ideological networks; Information terminal; Mobile communication devices; Network penetrations; Networks security; Research status; Security defense; Time-spent; Wireless communications; Younger generations; Big data",Article,Scopus,2-s2.0-85124686962
"Aristodimou A., Diavastos A., Pattichis C.S.","26654079800;55204811800;35495139000;","A fast supervised density-based discretization algorithm for classification tasks in the medical domain",2022,"Health informatics journal","28","1",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124680482&doi=10.1177%2f14604582211065397&partnerID=40&md5=fad5c124155840dfd12f941eeb5bc0b8","Discretization is a preprocessing technique used for converting continuous features into categorical. This step is essential for processing algorithms that cannot handle continuous data as input. In addition, in the big data era, it is important for a discretizer to be able to efficiently discretize data. In this paper, a new supervised density-based discretization (DBAD) algorithm is proposed, which satisfies these requirements. For the evaluation of the algorithm, 11 datasets that cover a wide range of datasets in the medical domain were used. The proposed algorithm was tested against three state-of-the art discretizers using three classifiers with different characteristics. A parallel version of the algorithm was evaluated using two synthetic big datasets. In the majority of the performed tests, the algorithm was found performing statistically similar or better than the other three discretization algorithms it was compared to. Additionally, the algorithm was faster than the other discretizers in all of the performed tests. Finally, the parallel version of DBAD shows almost linear speedup for a Message Passing Interface (MPI) implementation (9.64× for 10 nodes), while a hybrid MPI/OpenMP implementation improves execution time by 35.3× for 10 nodes and 6 threads per node.","big data; classification; density estimation; density-based discretization; supervised discretization","algorithm; article; big data; classifier; discretization",Article,Scopus,2-s2.0-85124680482
"Luo W., Xu J., Zhou Z.","57259636300;57259636200;57259740500;","Design of Data Classification and Classification Management System for Big Data of Hydropower Enterprises Based on Data Standards",2022,"Mobile Information Systems","2022",,"8103897","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124670178&doi=10.1155%2f2022%2f8103897&partnerID=40&md5=907984807d84260db79e8b8a0fce81b9","The advent of the era of big data has had a great impact on traditional management methods, and companies have also begun to make changes. The management approach has changed from initially focusing on business development to now focusing on user experience and putting people first. The data standard classification management system is a system for management and analysis based on the database. Therefore, this article is based on data standards, taking hydropower companies as an example, to design and research the data classification management system to promote the operation and safety of hydropower companies. This article mainly uses the experimental method, data collection method, and algorithm analysis method to thoroughly understand and explore the content of this article. The experimental results show that the testability of this article can basically reach the general level, and the delay time of the system does not exceed 10 seconds, which can be applied to the company. © 2022 Wei Luo et al.",,"Classification (of information); Hydroelectric power; Information management; Business development; Classification managements; Data classification; Data standards; Experimental methods; Hydropower companies; Management method; Management systems; Traditional management; Users' experiences; Big data",Article,Scopus,2-s2.0-85124670178
"Bhatia S., Alojail M.","56655934600;55749882700;","A Novel Approach for Deciphering Big Data Value Using Dark Data",2022,"Intelligent Automation and Soft Computing","33","2",,"1261","1271",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124667703&doi=10.32604%2fiasc.2022.023501&partnerID=40&md5=7cfc5079215101a66140fb78aa5f98f7","The last decade has seen a rapid increase in big data, which has led to a need for more tools that can help organizations in their data management and decision making. Business intelligence tools have removed many of the obstacles to data visibility, and numerous data mining technologies are playing an essential role in this visibility. However, the increase in big data has also led to an increase in ‘dark data’, data that does not have any predefined structure and is not generated intentionally. In this paper, we show how dark data can be mined for practical purposes and utilized to gain business insight. The most common type of dark data is a log file generated on a web server. Using the example of log files generated by e-commerce transactions, this paper shows how residual data and data trails can prove to be valuable when an actual dataset is inaccessible, and explains the usage of residual data for modeling purposes. The work uses a system identification approach, based on natural language processing for log file tokenization and feature extraction. The features are then embedded into the next step, which uses a deep neural network to identify customers for targeted advertising. The results achieve a significant accuracy and show how dark data has the potential to deliver value for business. Locating, organizing, and understanding dark data can unlock its relevance, usefulness, and potential monetization, but it is important to act when the benefits of use outweigh the costs of access and analysis. © 2022, Tech Science Press. All rights reserved.","Big data; Dark data; Deep learning; Enterprise resource planning; Information storage; Internet of things; NLP",,Article,Scopus,2-s2.0-85124667703
"Mutasher W.G., Aljuboori A.F.","57453938500;57204174463;","New and Existing Approaches Reviewing of Big Data Analysis with Hadoop Tools",2022,"Baghdad Science Journal","19","4",,"887","898",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124666331&doi=10.21123%2fbsj.2022.19.4.0887&partnerID=40&md5=a383c4754dc7c55f754061ca33e664ab","Everybody is connected with social media like (Facebook, Twitter, LinkedIn, Instagram etc.) that generate a large quantity of data and which traditional applications are inadequate to process. Social media are regarded as an important platform for sharing information, opinion, and knowledge of many subscribers. These basic media attribute Big data also to many issues, such as data collection, storage, moving, updating, reviewing, posting, scanning, visualization, Data protection, etc. To deal with all these problems, this is a need for an adequate system that not just prepares the details, but also provides meaningful analysis to take advantage of the difficult situations, relevant to business, proper decision, Health, social media, science, telecommunications, the environment, etc. Authors notice through reading of previous studies that there are different analyzes through HADOOP and its various tools such as the sentiment in real-time and others. However, dealing with this Big data is a challenging task. Therefore, such type of analysis is more efficiently possible only through the Hadoop Ecosystem. The purpose of this paper is to analyze literature related analysis of big data of social media using the Hadoop framework for knowing almost analysis tools existing in the world under the Hadoop umbrella and its orientations in addition to difficulties and modern methods of them to overcome challenges of big data in offline and real -time processing. Real-time Analytics accelerates decision-making along with providing access to business metrics and reporting. Comparison between Hadoop and spark has been also illustrated. © 2022 University of Baghdad. All rights reserved.","Apache-Spark; Big Data; Hadoop; IOT; Social Media",,Article,Scopus,2-s2.0-85124666331
"Shahzadi S., Yasir M., Aftab B., Babar S., Hassan M.","57202292482;57452136800;57452443100;57452288100;56179916100;","Exploration of Protein Aggregations in Parkinson's Disease Through Computational Approaches and Big Data Analytics",2022,"Methods in molecular biology (Clifton, N.J.)","2340",,,"449","467",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124616667&doi=10.1007%2f978-1-0716-1546-1_19&partnerID=40&md5=3452e7227b77e12f8d9117e09c8e0580","Protein aggregation has been implicated in numerous neurodegenerative disorders whose etiologies are poorly understood, and for which there are no effective treatments. Here we show that the computational approaches may help us to better understand the basics of Parkinson's disease (PD). The high-resolution structural, dynamical, and mechanistic insights delivered by computational studies of protein aggregation have a unique potential to enable the rational manipulation of oligomer formation. Additionally, big data and machine learning methods may provide valuable insights to better understand the nature of proteins involved in PD and their aggregative behavior for the betterment of PD treatment. © 2022. Springer Science+Business Media, LLC, part of Springer Nature.","Big data; Parkinson’s disease; Protein aggregation","protein aggregate; human; machine learning; Parkinson disease; Big Data; Data Science; Humans; Machine Learning; Parkinson Disease; Protein Aggregates",Article,Scopus,2-s2.0-85124616667
"Muritala B.A., Hernández-Lara A.-B., Sánchez-Rebull M.-V., Perera-Lluna A.","57220190936;8681953800;54406113300;57219366583;","#CoronavirusCruise: Impact and implications of the COVID-19 outbreaks on the perception of cruise tourism",2022,"Tourism Management Perspectives","41",,"100948","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124605216&doi=10.1016%2fj.tmp.2022.100948&partnerID=40&md5=91b4bc257688e1235c833ca3a1892a4a","Early in the COVID-19 pandemic, the Diamond Princess became the center of the largest outbreak outside the original epicenter in China. This outbreak which left 712 passengers infected and 14 dead, followed by subsequent outbreaks affecting over one-third of the active ships in the cruise industry's global fleet, quickly became a crisis that captured public attention and dominated mainstream news and social media. This study investigates the perception of cruising during these outbreaks by analyzing the tweets on cruising using Natural Language Processing (NLP). The findings show a prevalent negative sentiment in most of the analyzed tweets, while the criticisms directed at the cruise industry were based on perceptions and stereotypes of the industry before the pandemic. The study provides insight into the concerns raised in these conversations and highlights the need for new business models outside the pre-pandemic mass-market model and to genuinely make cruising more environmentally friendly. © 2022 The Authors","Big data; Coronavirus; Crisis communication; eWOM; Information integration theory; Risk perception; SARF; Sentiment analysis",,Article,Scopus,2-s2.0-85124605216
"Alyoubi K.H.","57144527700;","An Efficient Kernel Density Based Algorithm of Big Data in Cybersecurity for Enhancing Smart City",2022,"International journal of online and biomedical engineering","18","1",,"52","64",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124568968&doi=10.3991%2fijoe.v18i01.27875&partnerID=40&md5=7e29ded57cf1c7ac39b367db4196d1a3","Smart cities are attracting much interest in terms of future development. As new technologies come on stream, ordinary towns are reshaping themselves as smart cities, where technology is used to improve connections between all elements of the town. The technology can be embedded everywhere and can harvest data for dedicated smart city applications. Smart cities will have a huge number of different devices running these applications. There will be a substantial amount of data associated with these devices. In the interlinked smart city environment, many different messages could be shared between them. Such devices will be associated with many security risks and privacy issues, as many of the shared statistics could also hold personal data. A substantial review of research has been recently undertaken to ensure that data will be safe in the smart city environment. This review has included all the latest research in the area and is intended to ensure that all the data required to run green smart cities and the devices required for them will remain secure and confidential. © 2022. International journal of online and biomedical engineering. All Rights Reserved.","Auditing; Big data; Cybersecurity; Kde; Smart cities; Svm algorithm",,Article,Scopus,2-s2.0-85124568968
"Allam T.M.","56545593500;","Estimate the Performance of Cloudera Decision Support Queries",2022,"International journal of online and biomedical engineering","18","1",,"127","138",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124502200&doi=10.3991%2fijoe.v18i01.27877&partnerID=40&md5=aef2db02b64316906e58414570bde132","Hive and Impala queries are used to process a big amount of data. The overwriting amount of information requires an efficient data processing system. When we deal with a long-term batch query and analysis Hive will be more suitable for this query. Impala is the most powerful system suitable for real-time interactive Structured Query Language (SQL) query which are added a massive parallel processing to Hadoop distributed cluster. The data growth makes a problem with SQL Cluster because the execution processing time is increased. In this paper, a comparison is demonstrated between the performance time of Hive, Impala and SQL on two different data models with different queries chosen to test the performance. The results demonstrate that Impala outperforms Hive and SQL cluster when it comes to analyze data and processing tasks. Using two benchmark datasets, TPC-H and statistical computing, we compare the performance of Hive, Impala, and SQL clusters 2009 Statistical Graphics Data Expo. © 2022. International journal of online and biomedical engineering.All Rights Reserved","Big data; Graphics data expo 2009; Hadoop; Hive; Impala; Massive parallel processing; Tpch",,Article,Scopus,2-s2.0-85124502200
"Ahasan R., Alam M.S., Chakraborty T., Hossain M.M.","57220811460;55731178500;57226308988;57207311791;","Applications of GIS and geospatial analyses in COVID-19 research: A systematic review",2022,"F1000Research","9",,"1379","","",,4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124466556&doi=10.12688%2ff1000research.27544.2&partnerID=40&md5=f3e1a722181d06f98917b0486bb696ef","Background: Geographic information science (GIS) has established itself as a distinct domain and incredibly useful whenever the research is related to geography, space, and other spatio-temporal dimensions. However, the scientific landscape on the integration of GIS in COVID-related studies is largely unknown. In this systematic review, we assessed the current evidence on the implementation of GIS and other geospatial tools in the COVID-19 pandemic. Methods: We systematically retrieved and reviewed 79 research articles that either directly used GIS or other geospatial tools as part of their analysis. We grouped the identified papers under six broader thematic groups based on the objectives and research questions of the study- environmental, socio-economic, and cultural, public health, spatial transmission, computer-aided modeling, and data mining. Results: The interdisciplinary nature of how geographic and spatial analysis was used in COVID-19 research was notable among the reviewed papers. Geospatial techniques, especially WebGIS, have even been widely used to visualize the data on a map and were critical to informing the public regarding the spread of the virus, especially during the early days of the pandemic. This review not only provided an overarching view on how GIS has been used in COVID-19 research so far but also concluded that geospatial analysis and technologies could be used in future public health emergencies along with statistical and other socio-economic modeling techniques. Our review also highlighted how scientific communities and policymakers could leverage GIS to extract useful information to make an informed decision in the future. Conclusions: Despite the limited applications of GIS in identifying the nature and spatio-temporal pattern of this raging pandemic, there are opportunities to utilize these techniques in handling the pandemic. The use of spatial analysis and GIS could significantly improve how we understand the pandemic as well as address the underserviced demographic groups and communities. © 2022 Ahasan R et al.","Coronavirus; COVID-19; Evidence-based practice; GIS; Spatial analysis; Systematic review","Article; big data; computer analysis; coronavirus disease 2019; cultural factor; data mining; environmental factor; epidemiological surveillance; geographic distribution; geographic information system; geographic mapping; human; medical information; medical research; pandemic; public health; remote sensing; social media; socioeconomics; spatial analysis; statistical analysis; systematic review; virus transmission",Article,Scopus,2-s2.0-85124466556
"Pan Q., Yang G.","57449634800;57449738100;","Application of mining algorithm in personalized Internet marketing strategy in massive data environment",2022,"Journal of Intelligent Systems","31","1",,"237","244",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124466014&doi=10.1515%2fjisys-2022-0014&partnerID=40&md5=94a4f0f1f5510b7a36175f65b93882ca","Internet marketing requires a personalized marketing strategy. In this study, the application of data mining in personalized Internet marketing was studied. Based on the mining algorithm, a personalized marketing method was designed. Through the calculation of frequent closed item sets and support counts of positive and negative samples, the interval with a high success rate for marketing was obtained. With performance analysis, it was found that the success rate of the marketing method proposed in this study improved 8% compared with the traditional marketing method and had a better performance under the smaller interval number and smaller minimum success number. After applying the designed method in telecommunication enterprise A, it was found that after adopting the marketing method of this study, the marketing success rate of enterprise A increased from 2.72 to 6.31%, which indicated the effectiveness of the method. The research results of this study verify the role of data mining algorithms in Internet marketing, which is conducive to the further application of mining algorithms in personalized marketing and innovation of business modes. © 2022 Qianqian Pan and Gang Yang, published by De Gruyter.","big data; data mining; internet marketing; personalized marketing","Big data; Commerce; Marketing; Strategic planning; Data environment; Internet marketing; Internet marketing strategies; Itemset; Marketing IS; Marketing strategy; Massive data; Mining algorithms; Personalized marketings; Support count; Data mining",Article,Scopus,2-s2.0-85124466014
"Wang Z., Wan Y., Liang H.","57289499100;57226279801;55612923500;","The Impact of Cloud Computing-Based Big Data Platform on IE Education",2022,"Wireless Communications and Mobile Computing","2022",,"9740407","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124417763&doi=10.1155%2f2022%2f9740407&partnerID=40&md5=88bb6d2858c970d649cf6889837713a6","""Mass Entrepreneurship, Mass Innovation""is for the benign, rapid, and better development of our economy and society. In the 21st century, innovation and entrepreneurship (IE) under the background of technological development must take the masses as the main body to stimulate the vitality of mass IE. Compared with traditional IE education, the impact of big data platforms on education cannot be underestimated. Therefore, this article aims to research the influence of cloud computing-based big data platforms on IE education. This article proposes to use the cloud computing-based big data platform as a carrier to carry out IE education to change the drawbacks of traditional IE education. This research uses the literature analysis method and questionnaire survey method. It takes college students as the survey subjects of IE education, who are also the main object, and fully collects data. The research shows that cloud computing-based big data platforms have multiple impacts on IE education. During the epidemic, noncontact education is closely related to network technology. © 2022 Ziyan Wang et al.",,"Cloud computing; Students; Surveys; Cloud-computing; Data platform; Economy and society; Entrepreneurship education; Literature analysis method; Main bodies; Questionnaire surveys; Research use; Survey methods; Technological development; Big data",Article,Scopus,2-s2.0-85124417763
"Shen T.","57447700300;","Evaluation of urban and rural population flow and spatial planning effect under the background of big data",2022,"Journal of Computational Methods in Sciences and Engineering","22","1",,"321","332",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124409844&doi=10.3233%2fJCM-215675&partnerID=40&md5=dd821055644e18520b8cac20a3de8039","In order to improve the effect of urban and rural regional spatial planning and resource allocation, we need to focus on the factor of urban and rural population flow. Therefore, this paper evaluates the effect of urban and rural population mobility and spatial planning under the background of big data. Based on the evaluation of the effect of urban-rural population migration, according to the evaluation principle of the effect of urban-rural population migration after the reform and opening up, the evaluation index system is constructed, the dimensionless processing of the evaluation index system is carried out, the evaluation index weight is calculated, and the evaluation index is quantified, so as to complete the effect evaluation of spatial planning. Finally, taking Tianjin as an example, the application effect of this design method is verified. © 2022 - IOS Press. All rights reserved.","Big data background; effect of spatial planning; Evaluation method; urban and rural population flow","Population dynamics; Population statistics; Big data background; Data backgrounds; Effect of spatial planning; Evaluation methods; Population flow; Rural population; Spatial planning; Urban and rural; Urban and rural population flow; Urban population; Big data",Article,Scopus,2-s2.0-85124409844
"Gallego I., Font X., González-Rodríguez M.R.","24449064400;55657225927;24288899800;","The impact of COVID-19 on European tourists' attitudes to air travel and the consequences for tourist destination evoked set formation",2022,"Tourism Management Perspectives","41",,"100945","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124404703&doi=10.1016%2fj.tmp.2022.100945&partnerID=40&md5=f128d6cd48f883b9a070bb9cf12a31aa","We study how risk conditions derived from the COVID-19 pandemic may impact on both the desire to travel and intention to visit of tourists and, therefore, on different stages of the destination choice process. We analyse 5134 million flight searches and 379 million flight picks during 2020 for the 17 largest European tourism source markets. An unweighted index number is employed to measure the average variation for searches and picks, for the year 2020, in relation to the reference base period (year 2019). This is done for air travel in general and to Spain specifically. The study then proceeds to conduct an analysis of 17 international travel destinations that are in the evoked sets of the two largest outbound markets in Europe (Germany and UK). We also identify which markets are most favourable to Spain. The research design can inform cost-efficient marketing decisions in a situation of high uncertainty. © 2022","Anxiety; Big data; COVID-19; Europe; Evoked set; Flights; Risk",,Article,Scopus,2-s2.0-85124404703
"Wu C., Li Y.","57192542565;57422261000;","FLOM: Toward Efficient Task Processing in Big Data with Federated Learning",2022,"Security and Communication Networks","2022",,"5277362","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124399408&doi=10.1155%2f2022%2f5277362&partnerID=40&md5=c64a945471da42f098ede82bd22b9e58","With the diversification and individuation of user requirements as well as the rapid development of computing technology, the large-scale tasks processing for big data in edge computing environment has become a research focus nowadays. Many recent efforts for task processing are designed and implemented based on some traditional protocols and optimization methods. Therefore, it is more difficult to explore the task allocation strategy that maximizes the overall system revenue from the perspective of global load balancing. In order to overcome this problem, a large-scale tasks processing approach called Federated Learning based Optimization Methodology (FLOM) for large-scale tasks processing was presented to achieve accurate task classification and overall load balancing while satisfying task allocation requirements. FLOM performs the data aggregation and establishes the personalized models by federated learning. The deep network model is designed for deep feature learning of task requests and hosts in the substrate network. The experimental results show the capability of FLOM in terms of large-scale task classification as well as allocation. © 2022 Chunyi Wu and Ya Li.",,"Balancing; Data handling; Deep learning; Computing environments; Computing technology; Edge computing; Large-scales; Load-Balancing; Optimization methodology; Task allocation; Task classification; Task-processing; User requirements; Big data",Article,Scopus,2-s2.0-85124399408
"Xiong X.","57447044100;","Analyzing the Influencing Factors of Economic Fluctuations in the Era of Big Data",2022,"Scientific Programming","2022",,"9374025","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124392535&doi=10.1155%2f2022%2f9374025&partnerID=40&md5=159e63d45d54076dcf4de5960110807d","In view of the problems of poor correlation and poor calculation accuracy of the existing factors affecting economic fluctuations, this paper designs a new method for analyzing the factors affecting economic fluctuations based on the characteristics of the era of big data. First, after determining the factors influencing the economic fluctuations in the era of big data, the calculation method of key impact indicators is clarified. Then, the entropy law method is used to assign a weight value to the index, and the set is formed after the index is gathered. Then, the EM algorithm is used to assign the expected value to the data in the set. Finally, the set of influencing factors of the economic fluctuations is taken as input information, and the correlation evaluation model of influencing factors of economic fluctuations is constructed by the Monte Carlo method to complete the analysis of the influencing factors. The experimental results show that the correlation coefficient of influencing factors of economic fluctuations in the era of big data analyzed in this paper is high and the calculation accuracy of influencing factors is high, which proves the feasibility of this method. © 2022 Xiaoliang Xiong.",,"Economic and social effects; Monte Carlo methods; Calculation accuracy; Correlation coefficient; Correlation evaluations; Economic fluctuations; EM algorithms; Evaluation models; Expected values; Impact indicators; MonteCarlo methods; Weight values; Big data",Article,Scopus,2-s2.0-85124392535
"Zhang Q.","57206521392;","A Big Data-Driven Approach to Analyze the Influencing Factors of Enterprise's Technological Innovation",2022,"Computational Intelligence and Neuroscience","2022",,"3785685","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124389438&doi=10.1155%2f2022%2f3785685&partnerID=40&md5=e3e640369f15b37d9a1c8565e66758bf","A data-driven intelligent analysis method is proposed in this paper to explore and identify the enterprise's technological innovation influencing factors. Questionnaire surveys or expert interviews are usually adopted by the traditional evaluation methods for indicators of technological innovation selection. However, it inevitably involves human factors and experts' subjective judgments, which may affect the result of enterprises evaluation. The research presents an improved text clustering method based on a semantic concept model to explore and analyze the key influencing factors of enterprise's technological innovation. The study collects textual data from 400 enterprises in Beijing and smart analyzes the critical influencing factors of enterprise's technological innovation by using the proposed method. The influencing factors can be divided into seven categories. In addition, compared with the traditional K-means clustering method, the proposed method has a good effect. We proposed a methodology to conduct an intelligent analysis for enterprise's technological innovation under the data-driven. It can provide more objective and auxiliary suggestions for the evaluation of the enterprise's technology innovation. © 2022 Qianqian Zhang.",,"Big data; K-means clustering; Semantics; Surveys; Analysis method; Clustering methods; Data driven; Data-driven approach; Evaluation methods; Intelligent analysis; Questionnaire surveys; Subjective judgement; Technological innovation; Text Clustering; Cluster analysis; China; human; invention; theoretical model; Beijing; Big Data; Humans; Inventions; Models, Theoretical",Article,Scopus,2-s2.0-85124389438
"Shi L., Zhou J., Zhou X.","57191337183;57221050250;57209219034;","Evaluation of Regional Ecological Efficiency and Intelligent Decision Support for Sustainable Development Based on Environmental Big Data",2022,"Computational Intelligence and Neuroscience","2022",,"2820426","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124383641&doi=10.1155%2f2022%2f2820426&partnerID=40&md5=f14312c1a95fc7649a3980c5a1715b47","To promote urbanization in the next stage, it is of great significance to explore the ecological efficiency of green ecological regions and advance the sustainable development of a social economic system. However, spatial heterogeneity has not been fully considered in the existing evaluation models or methods for regional ecological efficiency (REE), and the corresponding decisions on sustainable development are not the optimal solutions. To solve the problems, this paper explores the evaluation of REE and intelligent decision support for sustainable development by analyzing environmental big data. Firstly, the spatiotemporal evolution of REE was examined based on environmental big data to clarify the spatial layout of REE and the sources of the spatial differences. Next, a multiobjective optimal decision-making model was established for the sustainable development of a regional ecosystem, and the solving method was presented for the model. The proposed model was proved valid through experiments. © 2022 Lelai Shi et al.",,"Decision making; Decision support systems; Ecology; Efficiency; Planning; Sustainable development; Ecological efficiency; Ecological regions; Evaluation methods; Evaluation models; Intelligent decision support; Optimal solutions; Social-economic systems; Spatial heterogeneity; Spatial layout; Spatiotemporal evolution; Big data; China; economic development; ecosystem; sustainable development; urbanization; Big Data; China; Economic Development; Ecosystem; Sustainable Development; Urbanization",Article,Scopus,2-s2.0-85124383641
"Huang X.","57447062600;","Personalized Travel Route Recommendation Model of Intelligent Service Robot Using Deep Learning in Big Data Environment",2022,"Journal of Robotics","2022",,"7778592","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124376924&doi=10.1155%2f2022%2f7778592&partnerID=40&md5=64d14e43c0f5b39fc32456ed19950c33","Aiming at the problems that the traditional model is difficult to extract information features, difficult to learn deep knowledge, and cannot automatically and effectively obtain features, which leads to the problem of low recommendation accuracy, this paper proposes a personalized tourism route recommendation model of intelligent service robot using deep learning in a big data environment. Firstly, by crawling the relevant website data, obtain the basic information data and comment the text data of tourism service items, as well as the basic information data, and comment the text data of users and preprocess them, such as data cleaning. Then, a neural network model based on the self-attention mechanism is proposed, in which the data features are obtained by the Gaussian kernel function and node2vec model, and the self-attention mechanism is used to capture the long-term and short-term preferences of users. Finally, the processed data is input into the trained recommendation model to generate a personalized tourism route recommendation scheme. The experimental analysis of the proposed model based on Pytorch deep learning framework shows that its Pre@10, Rec@10 values are 88% and 83%, respectively, and the mean square error is 1.537, which are better than other comparison models and closer to the real tourist route of the tourists. © 2022 Xiang Huang.",,"Big data; Deep learning; Intelligent robots; Mean square error; Attention mechanisms; Data environment; Extract informations; Information data; Intelligent Service robots; Model-based OPC; Personalized tourisms; Text data; Traditional models; Travel routes; Recommender systems",Article,Scopus,2-s2.0-85124376924
"Wei Z., Yan L.","57445124600;57445773900;","Construction of an Intelligent Evaluation Model of Mental Health Based on Big Data",2022,"Journal of Sensors","2022",,"4378718","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124326262&doi=10.1155%2f2022%2f4378718&partnerID=40&md5=d25f9be77b9934c6e04d2ef4ba560e00","In this paper, mental health data were used to evaluate the educational effects, in which the high and low scorers of three emotions, autism, positivity, and anxiety, are compared separately to explore the subtle differences in the long-term trends of the sensing traits of people with opposite characteristics. Based on the fusion of multiple kinds of sensing traits, the differences in physical and mental health assessment of positive and negative emotions by different fusion trait approaches are explored, and speech and behavioural traits are fused to build a physical and mental health assessment system for positive and negative emotions. Energy gravity uses physical distance to estimate the residual energy of nodes and considers the energy distribution of downstream nodes. The main work is to combine the data of mental health of higher education students using data mining techniques, to analyze the feasibility study of mental health education of college students. Relevant definitions, classifications, tasks, processes, and application areas of data mining techniques are introduced, and the basic principles of data mining are analyzed in detail. Taking the mental health assessment data of new students as the research object, the decision tree algorithm is used to construct a decision tree model for students with depressive symptoms, and an association rule algorithm is used to data mine the relationship between factors of psychological dimensions. Finally, it can find out the hidden laws and knowledge behind the data information and analyze the relationship that exists between psychological problems and students. © 2022 Zhang Wei and Liang Yan.",,"Big data; Decision trees; Education computing; Health; Students; Behavioral traits; Data-mining techniques; Evaluation models; Health assessments; Health data; Intelligent evaluation; Long-term trend; Mental health; Physical health; Positive and negative emotions; Data mining",Article,Scopus,2-s2.0-85124326262
"Liang J.","57445990100;","""oBE"" Concept for New Training Mode of Electronic Information Science and Technology Professionals under Big Data Analysis",2022,"Computational Intelligence and Neuroscience","2022",,"8075708","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124270355&doi=10.1155%2f2022%2f8075708&partnerID=40&md5=38dba4a509e61f0d074daae5beb7cb0b","As an educational concept based on learning output, OBE (Outcome-Based Education) is student-centered and emphasizes students' personal progress and learning achievement. Based on BD (big data) analysis, this paper proposes a new talent training mode for the electronic information science and technology specialty. This model employs BD analysis technology to examine the correlation index between social demand and talent cultivation, based on the employment situation of students in the country in previous years. The teaching reform was carried out under the OBE concept, and a new training scheme was formed. Training objectives, graduation requirements, curriculum system, and continuous improvement mechanism were all determined. This paper proposes an algorithm for determining the degree of similarity between the knowledge required by the organization and the knowledge held by its employees. The person with the highest similarity is identified as the training candidate by the algorithm, and the training candidate is then trained according to the knowledge that the organization requires. Talent training in the field of electronic information science and technology has yielded positive results. © 2022 Jifeng Liang.",,"Big data; E-learning; Education computing; Knowledge management; Concept-based; Correlation index; Electronic information; Learning achievement; Outcome-based education; Science and Technology; Student-centred; Talent cultivations; Talent trainings; Training mode; Students; curriculum; data analysis; electronics; human; technology; Big Data; Curriculum; Data Analysis; Electronics; Humans; Technology",Article,Scopus,2-s2.0-85124270355
"Li-Yun Z., Cheng-Ke W., Qiang Z.","57444682000;57444682100;57445549200;","The Construction of Folk Sports Featured Towns Based on Intelligent Building Technology Based on the Internet of Things",2022,"Applied Bionics and Biomechanics","2022",,"4541533","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124260878&doi=10.1155%2f2022%2f4541533&partnerID=40&md5=f057200dcbe507ffda69dffe7fc45f09","With the emergence of the Internet of Things, technology and Internet thinking have entered traditional communities, and combined with traditional technologies, many new and better management methods and solutions have been born. Among them, the concept of intelligent buildings is also known to people. Based on big data technology, cloud computing technology, and Internet of Things technology, smart buildings provide smart and convenient devices and services for smart device users. The Internet of Things technology is entering our lives at an unimaginable speed. It has been applied in many fields. Smart home, smart transportation, smart medical, smart agriculture, and smart grid are widely used in the Internet of Things technology. The application of Internet of Things technology to the construction of folk sports characteristic towns is of great significance. The construction of folk sports characteristic towns and the protection of intangible cultural heritage have the same purpose and interoperability of elements as the development of traditional cities. From the perspective of protecting folk culture and intangible cultural heritage, it is effective to promote the development of small towns with folk custom characteristics. Based on the research on the construction of folk-custom sports towns, this paper proposes a series of data model analysis and analyzes the proportion of sports preferences in the survey of volunteers in the folk-custom sports towns. The final result of the research shows that the ball games sports personnel accounted for the largest proportion, with 156 people accounting for 48.15%. This shows that about half of the people like ball sports, which proves that ball sports should be the mainstay of folk sports towns, and other sports should be supplemented by other sports. © 2022 Zhou Li-yun et al.",,"Automation; Intelligent buildings; Internet of things; Building technologies; Cloud computing technologies; Data technologies; Folk customs; Intangible cultural heritages; Internet of things technologies; Management method; Modeling analyzes; Smart devices; Technology-based; Sports; adult; agriculture; article; ball sport; big data; city; cloud computing; female; human; human experiment; inheritance; internet of things; male; thinking; velocity",Article,Scopus,2-s2.0-85124260878
"Ruan H., Zhang X.","57446196800;57446196900;","Optimization of a Wireless Sensor-Based Tennis Motion Pattern Recognition System",2022,"Journal of Sensors","2022",,"6232267","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124256588&doi=10.1155%2f2022%2f6232267&partnerID=40&md5=0bb8bd900c297b0e21c9e4a0d0df6b25","With the rapid development of information technology in today's era, the application of the Internet, big data, and smart bracelet information technology in the field of sports has enhanced the intelligence of sports and plays an important role in promoting sports performance. This paper focuses on the application of wireless sensors in the field of tennis, using research methods such as literature research, video analysis, comparative research, and mathematical statistics, to explore and analyze the application of wireless sensors in the field of tennis big data, tennis robotics, and the implementation of tennis teaching and training, to provide a theoretical basis for promoting the application of wireless sensors in the field of tennis and also for the broader application of wireless sensors in sports to provide a theoretical reference. For the problem of multiple scales of motion targets in action videos, two video action recognition methods based on high- and low-level feature fusion are proposed, which are the video action recognition methods based on top-down feature fusion and the video action recognition methods based on bottom-up feature fusion. The multipowered mobile anchor nodes are allowed to move along a prescribed route and broadcast multiple power signals, and then, the location of the unknown node is estimated using a four-ball intersection weight center-of-mass algorithm. Simulations show experimentally that the algorithm reduces the average localization error and requires fewer anchor nodes. © 2022 Haoxuan Ruan and Xinchen Zhang.",,"Big data; Motion estimation; Pattern recognition; Statistics; Action recognition; Features fusions; Literature researches; Motion pattern; Optimisations; Recognition methods; Research method; Sport performance; Video analysis; Wireless sensor; Tennis",Article,Scopus,2-s2.0-85124256588
"Guo J., Bai Y., Ding M., Song L., Yu G., Liang Y., Fan Z.","57443414700;57204591321;57443414800;57444187500;57443227700;57443997900;57443607400;","Analysis of Carotid Ultrasound Screening of High-Risk Groups of Stroke Based on Big Data Technology",2022,"Journal of Healthcare Engineering","2022",,"6363691","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124241825&doi=10.1155%2f2022%2f6363691&partnerID=40&md5=089d94f1eafeb631f46a86f146b095c1","In order to understand detection of carotid atherosclerosis in the screening of high-risk stroke populations in a certain area of China, we have analyzed related risk factors of CAS. In accordance with the requirements of the ""2015 Technical Plan for the Screening and Intervention Projects for High-Risk Stroke Populations,""a cluster sampling method was used to select 4532 (number of screened persons from 2015 to 2021) permanent residents over 41 years old () in Shaheying Town, Liulin Town, Chenggu County, Hanzhong City, Shaanxi Province, and Da'an Town, Ningqiang County, and nearby communities are selected as the screening targets. We screened out high-risk groups of stroke based on big data technology and understood the detection of CAS. According to the screening results of big data technology, it was divided into two groups: CAS group and non-CAS group. The basic information, medical history, personal lifestyle, physical examination, and laboratory examination results of the two groups were classified and counted. The measurement data such as age and waist circumference of the two groups were tested by two independent samples, and the count data of gender, stroke history, hypertension, and other data were tested by the χ2 test of the four-table data, and the logistic regression model was used to analyze the risk factors for CAS of population at high risk of stroke. The results proved the following: (1) Among the 4532 screeners, 865 cases were screened out of the high-risk population of stroke, with an average age of (58.5 ± 8.3) years, mainly 59 to 68 years old, accounting for 43.8%, and the male-to-female ratio was 1.6: 1. (2) The detection rates of CAS, intimal thickening, plaque formation, and stenosis among high-risk groups of stroke were 55.5%, 10.2%, 52.2%, and 32.6%, respectively. (3) Among the high-risk groups of stroke, CAS patients have a history of stroke, the proportion of hypertension, age, total cholesterol, and low-density lipoprotein cholesterol levels that are higher than those in the non-CAS group, and the difference is statistically significant. (4) Logistic regression analysis shows that age, diabetes, and low-density lipoprotein cholesterol are independent risk factors for CAS in the high-risk population of stroke in this area. © 2022 Jiankang Guo et al.",,"Big data; Cholesterol; Diagnosis; Lipoproteins; Logistic regression; Population statistics; Ultrasonic applications; Carotid ultrasounds; Cluster sampling; Data technologies; Lipoprotein cholesterol; Low density lipoproteins; Related risk; Risk factors; Risk groups; Sampling method; Ultrasound screenings; Risk assessment; cholesterol; high density lipoprotein cholesterol; low density lipoprotein cholesterol; adult; aged; artery intima proliferation; Article; big data; body mass; carotid artery; carotid ultrasound measurement; cerebrovascular accident; cholesterol blood level; color Doppler flowmetry; common carotid artery; comparative study; controlled study; diabetes mellitus; dyslipidemia; echography; exercise; female; high density lipoprotein cholesterol level; high risk population; human; hypertension; internal carotid artery; laboratory test; lifestyle; low density lipoprotein cholesterol level; major clinical study; male; middle aged; neck circumference; obesity; physical examination; quality control; risk factor; sex ratio; smoking; subclavian artery; vertebral artery; waist circumference",Article,Scopus,2-s2.0-85124241825
"Peruzzi M., Dunson D.B.","57219631599;57208455661;","Spatial Multivariate Trees for Big Data Bayesian Regression",2022,"Journal of Machine Learning Research","23",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124235967&partnerID=40&md5=321a2ad6921d0b40d418a30eae8623da","High resolution geospatial data are challenging because standard geostatistical models based on Gaussian processes are known to not scale to large data sizes. While progress has been made towards methods that can be computed more efficiently, considerably less attention has been devoted to methods for large scale data that allow the description of complex relationships between several outcomes recorded at high resolutions by different sensors. Our Bayesian multivariate regression models based on spatial multivariate trees (SpamTrees) achieve scalability via conditional independence assumptions on latent random effects following a treed directed acyclic graph. Information-theoretic arguments and considerations on computational efficiency guide the construction of the tree and the related efficient sampling algorithms in imbalanced multivariate settings. In addition to simulated data examples, we illustrate SpamTrees using a large climate data set which combines satellite data with land-based station data. Software and source code are available on CRAN at https://CRAN.R-project.org/package=spamtree. © 2022 Michele Peruzzi and David B. Dunson.","Directed acyclic graph; Gaussian process; Geostatistics; Markov chain Monte Carlo; Multiscale/multiresolution; Multivariate regression","Big data; Computational efficiency; Forestry; Gaussian distribution; Gaussian noise (electronic); Markov processes; Regression analysis; Trees (mathematics); Bayesian regression; Gaussian Processes; Geo-spatial data; Geo-statistics; High resolution; Markov chain Monte Carlo; Markov Chain Monte-Carlo; Model-based OPC; Multiscale-multiresolution; Multivariate regression; Directed graphs",Article,Scopus,2-s2.0-85124235967
"Lv S., Lian H.","36961359100;23972818200;","Debiased Distributed Learning for Sparse Partial Linear Models in High Dimensions",2022,"Journal of Machine Learning Research","23",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124227641&partnerID=40&md5=4d51637ab1daec4666a9802d0bf6b343","Although various distributed machine learning schemes have been proposed recently for purely linear models and fully nonparametric models, little attention has been paid to distributed optimization for semi-parametric models with multiple structures (e.g. sparsity, linearity and nonlinearity). To address these issues, the current paper proposes a new communication-efficient distributed learning algorithm for sparse partially linear models with an increasing number of features. The proposed method is based on the classical divide and conquer strategy for handling big data and the computation on each subsample consists of a debiased estimation of the doubly regularized least squares approach. With the proposed method, we theoretically prove that our global parametric estimator can achieve the optimal parametric rate in our semi-parametric model given an appropriate partition on the total data. Speciffically, the choice of data partition relies on the underlying smoothness of the nonparametric component, and it is adaptive to the sparsity parameter. Finally, some simulated experiments are carried out to illustrate the empirical performances of our debiased technique under the distributed setting. © 2022 Microtome Publishing. All rights reserved.","Big data; Distributed learning; High dimensions; Reproducing kernel Hilbert space (RKHS); Semi-parametric models","Data handling; Learning algorithms; Learning systems; Least squares approximations; Optimization; Parameter estimation; Distributed learning; Distributed machine learning; Higher dimensions; Learning schemes; Linear modeling; Non-parametric model; Partial linear models; Reproducing kernel hilbert space; Reproducing Kernel Hilbert spaces; Semi-parametric modeling; Big data",Article,Scopus,2-s2.0-85124227641
"Hassan C.A.U., Hammad M., Uddin M., Iqbal J., Sahi J., Hussain S., Ullah S.S.","57443559400;57428634200;42062605100;57212516963;57203578880;57217510430;57217100958;","Optimizing the Performance of Data Warehouse by Query Cache Mechanism",2022,"IEEE Access","10",,,"13472","13480",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124221217&doi=10.1109%2fACCESS.2022.3148131&partnerID=40&md5=b0645365531fb02a91962a2318785d1a","Fast access of data from Data Warehouse (DW) is a need for today's Business Intelligence (BI). In the era of Big Data, the cache is regarded as one of the most effective techniques to improve the performance of accessing data. DW has been widely used by several organizations to manage data and use it for Decision Support System (DSS). Many methods have been used to optimize the performance of fetching data from DW. Query cache method is one of those methods that play an effective role in optimization. The proposed work is based on a cache-based mechanism that helps DW in two aspects: the first one is to reduce the execution time by directly accessing records from cache memory, and the second is to save cache memory space by eliminating non-frequent data. Our target is to fill the cache memory with the most used data. To achieve this goal aging-based Least Frequently Used (LFU) algorithm is used by considering the size and frequency of data simultaneously. The priority and expiry age of the data in the cache memory is managed by dealing with both the size and frequency of data. LFU sets priorities and counts the age of data placed in cache memory. The entry with the lowest age count and priority is eliminated first from the cache block. Ultimately, the proposed cache mechanism efficiently utilized cache memory and fills a large performance gap between the main DW and the business user query. © 2013 IEEE.","big data; Data warehouse; optimization; query optimization","Artificial intelligence; Big data; Data warehouses; Decision support systems; Information management; Accessing data; Business-intelligence; Cache mechanism; Data caches; Least frequently used; Optimisations; Performance; Queries optimization; Time factors; Cache memory",Article,Scopus,2-s2.0-85124221217
"Eshragh A., Roosta F., Nazari A., Mahoney M.W.","26537402800;57208899069;56218303900;7202006961;","LSAR: Efficient Leverage Score Sampling Algorithm for the Analysis of Big Time Series Data",2022,"Journal of Machine Learning Research","23",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124200550&partnerID=40&md5=7829ae5cc075cf5a7dd576e29acbe98e","We apply methods from randomized numerical linear algebra (RandNLA) to develop improved algorithms for the analysis of large-scale time series data. We first develop a new fast algorithm to estimate the leverage scores of an autoregressive (AR) model in big data regimes. We show that the accuracy of approximations lies within (1 + O(ε)) of the true leverage scores with high probability. These theoretical results are subsequently exploited to develop an efficient algorithm, called LSAR, for fitting an appropriate AR model to big time series data. Our proposed algorithm is guaranteed, with high probability, to find the maximum likelihood estimates of the parameters of the underlying true AR model and has a worst case running time that significantly improves those of the state-of-the-art alternatives in big data regimes. Empirical results on large-scale synthetic as well as real data highly support the theoretical results and reveal the efficacy of this new approach. © 2022 Ali Eshragh and Fred Roosta and Asef Nazari and Michael W. Mahoney.","Autoregressive model; Big data regime; Maximum likelihood estimation; Randomized numerical linear algebra; Sampling","Big data; Linear algebra; Maximum likelihood estimation; Time series; Time series analysis; Autoregressive modelling; Big data regime; High probability; Improved * algorithm; Large-scale time series; Maximum-likelihood estimation; Numerical Linear Algebra; Randomized numerical linear algebra; Sampling algorithm; Time-series data; Numerical methods",Article,Scopus,2-s2.0-85124200550
"Lehrer S.F., Xie T.","16180629000;56583433100;","The Bigger Picture: Combining Econometrics with Analytics Improves Forecasts of Movie Success",2022,"Management Science","68","1",,"189","210",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124196652&doi=10.1287%2fmnsc.2020.3911&partnerID=40&md5=72cd55b32403a837baf5e24fd1410d25","There exists significant hype regarding how much machine learning and incorporating social media data can improve forecast accuracy in commercial applications. To assess if the hype is warranted, we use data from the film industry in simulation experiments that contrast econometric approaches with tools from the predictive analytics literature. Further, we propose new strategies that combine elements from each literature in a bid to capture richer patterns of heterogeneity in the underlying relationship governing revenue. Our results demonstrate the importance of social media data and value from hybrid strategies that combine econometrics and machine learning when conducting forecasts with new big data sources. Specifically, although both least squares support vector regression and recursive partitioning strategies greatly outperform dimension reduction strategies and traditional econometrics approaches in forecast accuracy, there are further significant gains from using hybrid approaches. Further, Monte Carlo experiments demonstrate that these benefits arise from the significant heterogeneity in how social media measures and other film characteristics influence box office outcomes. © 2021 INFORMS","Big data; Heteroskedasticity; Machine learning; Model specification; Movies; Social media","Economics; Forecasting; Machine learning; Predictive analytics; Social networking (online); Statistics; Commercial applications; Data-source; Film industry; Forecast accuracy; Heteroskedasticity; Hybrid strategies; Least squares support vector regression; Model specifications; Social media; Social media datum; Big data",Article,Scopus,2-s2.0-85124196652
"Florencias-Oliveros O., Gonzalez-De-La-Rosa J.-J., Sierra-Fernandez J.-M., Aguera-Perez A., Espinosa-Gavira M.-J., Palomares-Salas J.-C.","57194267398;7004978019;55051057200;57206479960;57203206740;35976517000;","Site Characterization Index for Continuous Power Quality Monitoring Based on Higher-order Statistics",2022,"Journal of Modern Power Systems and Clean Energy","10","1",,"222","231",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124145727&doi=10.35833%2fMPCE.2020.000041&partnerID=40&md5=88f315dc7e98d285b6c0384a1f3b9541","The high penetration of distributed generation (DG) has set up a challenge for energy management and consequently for the monitoring and assessment of power quality (PQ). Besides, there are new types of disturbances owing to the uncontrolled connections of non-linear loads. The stochastic behaviour triggers the need for new holistic indicators which also deal with big data of PQ in terms of compression and scalability so as to extract the useful information regarding different network states and the prevailing PQ disturbances for future risk assessment and energy management systems. Permanent and continuous monitoring would guarantee the report to claim for damages and to assess the risk of PQ distortions. In this context, we propose a measurement method that postulates the use of two-dimensional (2D) diagrams based on higher-order statistics (HOSs) and a previous voltage quality index that assesses the voltage supply waveform in a continous monitoring campaign. Being suitable for both PQ and reliability applications, the results conclude that the inclusion of HOS measurements in the industrial metrological reports helps characterize the deviations of the voltage supply waveform, extracting the individual customers' pattern fingerprint, and compressing the data from both time and spatial aspects. The method allows a continuous and robust performance needed in the SG framework. Consequently, the method can be used by an average consumer as a probabilistic method to assess the risk of PQ deviations in site characterization. © 2013 State Grid Electric Power Research Institute.","big data; Continuous statistical monitoring; data compression; higher-order statistics (HOSs); power quality (PQ)","Big data; Data compression; Energy management; Information management; Monitoring; Power quality; Stochastic systems; Continuous monitoring; Continuous statistical monitoring; High order statistics; High-order statistic; Higherorder statistic (HOS); Power quality; Site characterization; Statistical monitoring; Voltage supply; Waveforms; Higher order statistics",Article,Scopus,2-s2.0-85124145727
"Hou W.","57440817700;","Analysis of Key Indicators in English Teaching Evaluation Based on Big Data Model",2022,"Scientific Programming","2022",,"1231700","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124102394&doi=10.1155%2f2022%2f1231700&partnerID=40&md5=52e4c153737adfaa473b69b94a1935ae","With the advent of the era of big data, the traditional English teaching methods in the past can no longer accurately assess the comprehensive level of English teachers and classrooms because of various factors. In order to reexamine and plan English teaching content, based on the big data model, we will carefully analyze the key indicators in English teaching evaluation using computer technologies such as particle swarm optimization and support vector machine, hoping to dig out the characteristics of English education in a deeper way, so as to make a series of index adjustments to English classroom and improve English teaching level. The results of this study show the following: (1) The average accuracy of the evaluation index of the model designed in this study is as high as 96.56%; after 20 tests, the test time of this model method is the least, and the test time can be as low as 13.32 ms. (2) For eight first-class indexes of A, B, C, D, E, F, G, and H and 29 second-class indexes, the expert scores are all greater than 3.66, and the standard deviation is all less than 1, which accords with the standard of reaching common opinions. The key index test system is reasonable. (3) We find that the weights of A2, D1, H1, and H2 are all higher than 0.5, the weights of A1, B3, C5, E5, F4, and G4 are all higher than 0.3, and the weights of other indexes are all less than 0.3. This shows that each index has a different weight and emphasis on English teaching evaluation. (4) Taking a certain teacher as an example to assess English proficiency can effectively analyze the key indicators of English teachers and enable the teacher to make corresponding improvements and formulate strategies. On the whole, the teacher has strong writing ability and listening ability; the ability of speaking and translating is slightly weak, both of which are about 0.8; for listening analysis, idiom and sentence ability are generally to be enhanced, about 0.8. (5) The comprehensive scoring of English teaching is carried out, large difference in scoring values is avoided, and fairer test results are given. It is found that after big data analysis, the key indicators of English are analyzed accurately, the classroom teaching is diversified, and the students' final classroom evaluation reflects well, so this method has obvious advantages. © 2022 Weili Hou.",,"C (programming language); Education computing; Particle swarm optimization (PSO); Petroleum reservoir evaluation; Students; Support vector machines; Well testing; Computer technology; Content-based; English teaching; Key indicator; Support vectors machine; Teachers'; Teaching contents; Teaching evaluation; Teaching methods; Test time; Big data",Article,Scopus,2-s2.0-85124102394
"Meng X., Huang W.","57219112647;57416094100;","Analysis of Key Factors Affecting Undergraduate Entrepreneurship Ability from a Big Data Perspective",2022,"Wireless Communications and Mobile Computing","2022",,"2198948","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124101433&doi=10.1155%2f2022%2f2198948&partnerID=40&md5=5b193f5ec3d404cd13ae826680634bad","To achieve rapid social and economic growth, society has raised expectations for the overall quality and competence of today's college students in the environment of big data. College students must have a strong spirit of creation and entrepreneurship, as well as a precise level of professional knowledge and practical talents. Based on this backdrop, a weighted K-means clustering method (WK-means) is utilized to investigate the key determinants of college students' entrepreneurial ability and their interactions with important text content, such as the college student innovation and entrepreneurship competition. To begin, extract a number of characteristics from the sample of entrepreneurial college students, particularly the entrepreneurial ability components. The sample is then divided into two groups. Entrepreneurs who succeed are categorized as positive, while those who fail are classified as negative. The clustering result yields the feature weight of each sample. The weight of each attribute is proportionate to its importance. Second, using the multivariate relationship analysis approach, the correlation between various factors is determined, and the impact of various factors on college students' entrepreneurial capacity is further investigated. The results of the experiment demonstrate that adaptability and learning ability are the most important characteristics influencing college students' entrepreneurial aptitude, whereas self-control has little impact. © 2022 Xiangmin Meng and Wenjun Huang.",,"Economics; Factor analysis; K-means clustering; Multivariant analysis; Students; College students; Economic growths; K-means clustering method; Key determinants; Key factors; Overall quality; Precise levels; Professional knowledge; Social growth; Text content; Big data",Article,Scopus,2-s2.0-85124101433
"Philip N.Y., Razaak M., Chang J., Suchetha M.S., Okane M., Pierscionek B.K.","56030280400;55639468300;57441178800;55781001400;57191521185;7004160267;","A Data Analytics Suite for Exploratory Predictive, and Visual Analysis of Type 2 Diabetes",2022,"IEEE Access","10",,,"13460","13471",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124101388&doi=10.1109%2fACCESS.2022.3146884&partnerID=40&md5=cef58c2d198cdc8d92aa91ae8367546d","Long-term management of chronic disorders such as Type 2 Diabetes (T2D) requires personalised care for patients due to variation in patient characteristics and their response to a specific line of treatment. The availability of large volumes of electronic records of T2D patient data provides opportunities for application of big data analysis to gain insights into the disease manifestation and its impact on patients. Data science in healthcare has the potential to identify hidden knowledge from the database, re- confirm existing knowledge, and aid in personalising treatment. In this paper, we present a suite of data analytics for T2D disease management that allows clinicians and researchers to identify associations between different patient biological markers and T2D related complications. The analytics suite consists of exploratory, predictive, and visual analytics with capabilities including multi-tier classification of T2D patient profiles that associate them to specific conditions, T2D related complication risk prediction, and prediction of patient response to a particular line of treatment. The analytics presented in this paper explore advanced data analysis techniques, which are potential tools for clinicians in decision-making that can contribute to better management of T2D. © 2013 IEEE.","Big data for healthcare; data analytics; healthcare data visualisation; personalised care; prediction analytics; risk prediction; T2D","Big data; Data visualization; Decision making; Diseases; Hospital data processing; Patient treatment; Predictive analytics; Risk assessment; Visualization; Big data for healthcare; Data analytics; Diabetes patients; Diabetes-related complications; Healthcare data visualization; Medical services; Personalized care; Prediction analytic; Risk predictions; Type-2 diabetes; Forecasting",Article,Scopus,2-s2.0-85124101388
"Wu S., Dong Z.","57441119400;57441319700;","An Auxiliary Decision-Making System for Electric Power Intelligent Customer Service Based on Hadoop",2022,"Scientific Programming","2022",,"5165718","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124091285&doi=10.1155%2f2022%2f5165718&partnerID=40&md5=fa90ec23b2421304da7445e99e3bf113","Aiming at the problems of low security, high occupancy rate, and long response time in the current power intelligent customer service assistant decision-making system, a power intelligent customer service assistant decision-making system based on the Hadoop big data framework is designed. By analyzing the Hadoop big data framework, according to the characteristics and core elements of the HDFS distributed file system, the MapReduce programming model, and the data mining algorithm, the basic process of power intelligent customer service assistance decision-making is established. We analyze the overall and functional requirements of the system, design the overall architecture and application architecture of the system, design the E-R diagram and table structure of the database according to the database design principle, and realize the design of power intelligent customer service auxiliary decision-making system based on Hadoop big data framework. The test results show that the proposed method has high system security and low system occupancy and can effectively shorten the system response time. The systems run more flawlessly as compared to the existing methods and give impressing results with lesser CPU utilization. The response time was recorded to be about 12.2 seconds for 1000 power intelligent customer servers, which is much lower than that of the competitors. © 2022 Shisong Wu and Zhaojie Dong.",,"Big data; Data mining; Decision making; Distributed computer systems; Distributed database systems; Response time (computer systems); Sales; Assistant decision-making system; Characteristic elements; Current power; Customer-service; Data framework; Decision-making systems; Electric power; Occupancy rate; Power; Service-based; File organization",Article,Scopus,2-s2.0-85124091285
"Ali T.A.L., Khafagy M.H., Farrag M.H.","57441261300;36617440800;57440859300;","Special Negative Database (SNDB) for Protecting Privacy in Big Data",2022,"International Journal of Advanced Computer Science and Applications","13","1",,"79","91",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124090860&doi=10.14569%2fIJACSA.2022.0130111&partnerID=40&md5=4da7a389b63b7192bcbbf57345a4cb10","Despite the importance of big data, it faces many challenges. The most important big data challenges are data storage, heterogeneity, inconsistency, timeliness, security, scalability, visualization, fault tolerance, and privacy. This paper concentrates on privacy which is one of the most pressing issues with big data. As mentioned in the Literature Review below there are numerous methods for safeguarding privacy with big data. This paper introduces an efficient technique called Specialized Negative Database (SNDB) for protecting privacy in big data. SNDB is proposed to avoid the drawbacks of all previous techniques. SNDB is based on deceiving bad users and hackers by replacing only sensitive attribute with its complement. Bad user cannot differentiate between the original data and the data after applying this technique. © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.","Big data; Big data challenges; Data integrity; Privacy violations; Privacy-preserving techniques; Special negative database","Data visualization; Database systems; Digital storage; Fault tolerance; Personal computing; Privacy-preserving techniques; Big data challenge; Data challenges; Data integrity; Data storage; Literature reviews; Negative database; Pressung; Privacy violation; Sensitive attribute; Special negative database; Big data",Article,Scopus,2-s2.0-85124090860
"Liu N., Jiang R., Tai X.","57440913000;57440719300;57440913100;","Dialectical Analysis of Comparative Pedagogy Based on Multiple Intelligences Evaluation",2022,"Scientific Programming","2022",,"5031639","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124088772&doi=10.1155%2f2022%2f5031639&partnerID=40&md5=90a227ad5741bc2b48dfd86ef22c9d96","As a common and mature algorithm, the neural network algorithm has been widely used in many industries throughout the country. The traditional dialectical analysis method for multiple intelligences evaluation in comparative education cannot meet the dialectical needs with different characteristics, the information big data model of multiple intelligences evaluation based on neural network algorithm has been gradually applied to several evaluation systems of comparative education. This paper studies the application of neural network algorithms in the dialectical analysis of comparative education in China and puts forward multiple intelligences evaluation model based on neural network algorithm, which can realize the intelligent evaluation of comparative education according to the characteristics of teaching behavior. At the same time, the idea of random big data acquisition is combined with digital feature analysis based on neural network algorithm and particle swarm optimization algorithm. Finally, the experimental results show that the dialectical analysis model of comparative education based on multiple intelligences evaluation of neural network algorithm can efficiently process the education data with tracking intelligence, which achieves a new breakthrough in the multiple intelligences evaluation of comparative education in China and saves a lot of time for the dialectical analysis process. © 2022 Ningning Liu et al.",,"Data acquisition; Particle swarm optimization (PSO); Analysis method; Analysis models; Digital features; Evaluation models; Feature analysis; Intelligent evaluation; Model-based OPC; Multiple intelligences; Neural networks algorithms; Particle swarm optimization algorithm; Big data",Article,Scopus,2-s2.0-85124088772
"Lian S., Mi R., Tang R.","57440720200;57441321600;57441321700;","Journalists' Response and Reporting of Public Emergencies in the Era of Artificial Intelligence",2022,"Applied Bionics and Biomechanics","2022",,"6574365","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124085621&doi=10.1155%2f2022%2f6574365&partnerID=40&md5=cb0bdc4d39c076c6dea5ab3cd49f6f0b","In order to improve the response ability of journalists to public emergencies, this study uses the computer simulation research method, based on Shannon's information theory and the basic theory of journalism and communication, constructs the general model of public emergencies based on the statistical results of big data, and constructs the irrational natural person game model of four types of news communication participants. In the simulation analysis, it is found that in the face of public emergencies, journalists, on the one hand, firmly implement the publicity caliber, strengthen the amount of creation, and improve the forwarding amount and approval amount of manuscripts under the artificial intelligence news system. On the other hand, they should achieve effective interaction with nonofficial news media such as we media, and the subjective news department should provide effective guidance and guidance to the media. © 2022 Shaoying Lian et al.",,"Computer games; Game theory; Information theory; Basic theory; Computer simulation research; Effective interactions; Game models; General model; Public emergencies; Research method; Response ability; Shannon's Information Theory; Simulation analysis; Artificial intelligence; adult; article; artificial intelligence; big data; case report; clinical article; computer simulation; female; game; human; information science; journalism; male; mass medium; simulation; theoretical study",Article,Scopus,2-s2.0-85124085621
"Peng Z., Liang F., Mu L.","57440919200;57218822159;57218825850;","Big Data-Based Access Control System in Educational Information Security Assurance",2022,"Wireless Communications and Mobile Computing","2022",,"2853821","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124080984&doi=10.1155%2f2022%2f2853821&partnerID=40&md5=207de01218b066ad5ad694fb77fb9e8d","Access control is a key strategy to prevent and protect the network. The most important task is to ensure that visitors will not use network resources and will not conduct unauthorized access to them. This paper studies a number of information security issues in the process of education informatization, aimed at studying the application of access control systems based on big data in education information security. This article elaborates on the related concepts of access control and proposes an attribute-based encryption scheme. Based on this, an information system security system model with security strategy as the core content is constructed. The experimental results in this paper show that the improved access control system has the more efficient guarantee ability in education information security, which is about 47.3% higher than the traditional access control system. © 2022 Zhong Peng et al.",,"Big data; Control systems; Cryptography; Access control systems; Attribute-based encryption schemes; Education informations; Information systems security; Informatization; Network resource; Security assurance; Security issues; System models; Unauthorized access; Access control",Article,Scopus,2-s2.0-85124080984
"Deng Y., Zheng H., Yan J.","57441021300;57441021400;57441021500;","Applications of Big Data in Economic Information Analysis and Decision-Making under the Background of Wireless Communication Networks",2022,"Wireless Communications and Mobile Computing","2022",,"7084969","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124074214&doi=10.1155%2f2022%2f7084969&partnerID=40&md5=31624cf9b306db4cb787e99abe89e301","Owing to the growing volumes of mobile telecommunications customers, Internet websites, and digital services, there are more and more big data styles and types around the world. With the help of big data technology with high semantic information, this paper focuses on exploring the value and corresponding application of big data in finance. By comparing with the existing methods in terms of search speed and data volume, we can effectively see the effectiveness and superiority of the algorithm proposed in this paper. Furthermore, the algorithm proposed in this paper can provide some reference ideas for the follow-up-related research work. © 2022 Yaotian Deng et al.",,"Decision making; Information services; Semantics; Data technologies; Data volume; Decisions makings; Digital services; Economic information; Mobile telecommunications; Search speed; Semantics Information; Speed-volume; Wireless communications networks; Big data",Article,Scopus,2-s2.0-85124074214
"Feng H., Chen G.","57440719100;57440912400;","A Novel Data Visualization Model Based on Autoencoder Using Big Data Analysis and Distributed Processing Technology",2022,"Scientific Programming","2022",,"7698174","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124067373&doi=10.1155%2f2022%2f7698174&partnerID=40&md5=4c8865f5ac55a654ac333c700522d78a","From the standpoint of visual elements, this article investigates the use of visual information technology in visual communication design. At this time, information visualization and data visualization are widely used to display visual form, which greatly facilitates people's use, provides a solid application foundation for visual communication design, and promotes its development. The image presentation of data is a common encoding process, and the reading of image content is the corresponding decoding process from the perspective of encoding and decoding. The combined efficacy of data encoding and image decoding determines the effectiveness of data visualization. It is worth noting that when it comes to ""encoding and decoding,""it has been established that the design mode of data visualization and visual communication is not a process of copying images but rather an external form of human thought. Then, there is the unmistakable presence of something unseen in the encoding and decoding processes. It also serves as the encoding and decoding key in the human brain. The image is as follows. From the standpoint of encoding and decoding, this article employs the data visualization self-encoder method to obtain visual data. Design pattern representation for perceptual communication can effectively support users' rapid motion analysis during the browsing process. © 2022 Hui Feng and Guozhen Chen.",,"Big data; Decoding; Encoding (symbols); Information systems; Information use; Signal encoding; Visual communication; Visualization; Auto encoders; Decoding process; Distributed processing technologies; Encoding and decoding; Encoding process; Model-based OPC; Visual communication designs; Visual elements; Visual information; Visualization modeling; Data visualization",Article,Scopus,2-s2.0-85124067373
"Khan F., Urooj A., Khan S.A., Khosa S.K., Muhammadullah S., Almaspoor Z.","57396631200;57401835200;57199395513;57208567640;57402261400;57193736894;","Evaluating the Performance of Feature Selection Methods Using Huge Big Data: A Monte Carlo Simulation Approach",2022,"Mathematical Problems in Engineering","2022",,"6607330","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124033087&doi=10.1155%2f2022%2f6607330&partnerID=40&md5=fd79376ff891236a3ee0099883c99c12","In this article, we compare autometrics and machine learning techniques including Minimax Concave Penalty (MCP), Elastic Smoothly Clipped Absolute Deviation (E-SCAD), and Adaptive Elastic Net (AEnet). For simulation experiments, three kinds of scenarios are considered by allowing the multicollinearity, heteroscedasticity, and autocorrelation conditions with varying sample sizes and the varied number of covariates. We found that all methods show improved their performance for a large sample size. In the presence of low and moderate multicollinearity and low and moderate autocorrelation, the considered methods retain all relevant variables. However, for low and moderate multicollinearity, excluding AEnet, all methods keep many irrelevant predictors as well. In contrast, under low and moderate autocorrelation, along with AEnet, the Autometrics retain less irrelevant predictors. Considering the case of extreme multicollinearity, AEnet retains more than 93 percent correct variables with an outstanding gauge (zero percent). However, the potency of remaining techniques, specifically MCP and E-SCAD, tends towards unity with augmenting sample size but capturing massive irrelevant predictors. Similarly, in case of high autocorrelation, E-SCAD has shown good performance in the selection of relevant variables for a small sample, while in gauge, Autometrics and AEnet are performed better and often retained less than 5 percent irrelevant variables. In the presence of heteroscedasticity, all techniques often hold all relevant variables but also suffer from overspecification problems except AEnet and Autometrics which circumvent the irrelevant predictors and establish the true model precisely. For an empirical application, we take into account the workers' remittance data for Pakistan along its twenty-seven determinants spanning from 1972 to 2020 for Pakistan. The AEnet selected thirteen relevant covariates of workers' remittance while E-SCAD and MCP suffered from an overspecification problem. Hence, the policymakers and practitioners should focus on the relevant variables selected by AEnet to improve workers' remittance in the case of Pakistan. In this regard, the Pakistan government has devised policies that make it easy to transfer remittances legally and mitigate the cost of transferring remittances from abroad. The AEnet approach can help policymakers arrive at relevant variables in the presence of a huge set of covariates, which in turn produce accurate predictions. © 2022 Faridoon Khan et al.",,"Big data; Gages; Intelligent systems; Learning systems; Monte Carlo methods; Regression analysis; Sampling; Auto correlation; Covariates; Elastic net; Minimax; Multicollinearity; Pakistan; Performance; Sample sizes; Smoothly clipped absolute deviation; Workers'; Autocorrelation",Article,Scopus,2-s2.0-85124033087
"Selim A.Z., Hanafy I.M., El-Attar N.E., Awad W.A.","57219964744;6507470979;57189095951;54408044300;","Balanced Schedule on Storm for Performance Enhancement",2022,"International Journal of Advanced Computer Science and Applications","13","1",,"622","632",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124019988&doi=10.14569%2fIJACSA.2022.0130175&partnerID=40&md5=ad8c3e9df3e3dd7100441e657822b579","In recent years, real-time and big data aroused and received a lot of attention due to the spread of embedded systems in almost everything in life. This has led to many challenges that need to be solved to enhance and improve systems that work on big real-time data. Apache Storm is a system used for computing and analyzing big real-time data of distributed systems. This paper aims to develop a scheduler to improve the scheduling of the applications represented by topologies on the Storm cluster. The proposed scheduler is hybridization between the scheduling algorithms of A3 Storm and the Workload scheduler. Its objective is to minimize the communication between tasks while balancing the workload on all cluster machines. The proposed scheduler is compared with the A3 Storm and Fischer and Bernstein’s scheduling algorithm. The comparison has been made using four different topologies. The experimental results show that our proposed scheduler outperforms the two other schedulers in throughput and complete latency © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.","Apache storm; Big data; Real-time; Scheduling","Balancing; Big data; Embedded systems; Real time systems; Scheduling algorithms; Storms; Topology; Apache storm; Cluster machines; Embedded-system; Hybridisation; Performance enhancements; Real- time; Real-time data; Scheduling",Article,Scopus,2-s2.0-85124019988
"Abouzahir M., Elmansouri K., Latif R., Ramzi M.","54791972200;56091352900;7004392558;55676214900;","Towards a Low-Cost FPGA Micro-Server for Big Data Processing",2022,"International Journal of Advanced Computer Science and Applications","13","1",,"874","884",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124016490&doi=10.14569%2fIJACSA.2022.01301101&partnerID=40&md5=14ce1c0382f9da04e7a08f0c9d04c1d6","The development of big data in the era of data explosion and the growing demand for micro-servers in place of traditional servers to adapt to lightweight tasks in recent years has put into question how to integrate and make use of these two important domains. During the same era, CPU performance growth has reached a certain maturity. In order to surpass these issues and to reach high performances computing, a new trend now is to use multiple processing units or heterogeneous components in micro-servers to reduce computational complexity. The implementation of Big Data processing algorithms using embedded heterogeneous architectures rises a new challenges due to constraints of the used architecture-based system on chip which require a special attention and imposed new demands to our works. In this article, we focus on using embedded FPGA accelerator to give a solution to this problem. Precisely, we will attempt to prototype a micro-server for the processing of big data on FPGA and compare its performances with a high-end GPGPU using existing benchmarks. The implementation on the FPGA is done using a High-Level Synthesis based-OpenCL (HLS) instead of the traditional description language. The obtained results shows that FPGA is an interesting alternative and can be a promising platform to design a micro-server when it comes to process a hug amount of data, in particular with the emerging technologies for FPGA programming using HLS approach and by adopting the OpenCL optimization strategies. © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.","(hls) high-level synthesis; Arria 10 fpga (field programmable gate arrays); Big data; Gpgpu (general purpose graphics processing unit); Parallel computing","Big data; Computer graphics; Computer graphics equipment; Costs; Data handling; Graphics processing unit; High level synthesis; Program processors; System-on-chip; (hls) high-level synthesis; Arria 10 field programmable gate array; Data explosion; General purpose graphic processing unit; Growing demand; High-level synthesis; Low-costs; Micro-servers; Parallel com- puting; Performance; Field programmable gate arrays (FPGA)",Article,Scopus,2-s2.0-85124016490
"Du J., Pi Y.","57439774700;57439252700;","Research on Privacy Protection Technology of Mobile Social Network Based on Data Mining under Big Data",2022,"Security and Communication Networks","2022",,"3826126","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123981401&doi=10.1155%2f2022%2f3826126&partnerID=40&md5=c2328e13fcc024c0168ee32676c6a813","With the advent of the era of big data, people's lives have undergone earth-shaking changes, not only getting rid of the cumbersome traditional data collection but also collecting and sorting information directly from people's footprints on social networks. This paper explores and analyzes the privacy issues in current social networks and puts forward the protection strategies of users' privacy data based on data mining algorithms so as to truly ensure that users' privacy in social networks will not be illegally infringed in the era of big data. The data mining algorithm proposed in this paper can protect the user's identity from being identified and the user's private information from being leaked. Using differential privacy protection methods in social networks can effectively protect users' privacy information in data publishing and data mining. Therefore, it is of great significance to study data publishing, data mining methods based on differential privacy protection, and their application in social networks. © 2022 Jiawen Du and Yong Pi.",,"Big data; Data mining; Social networking (online); Social sciences computing; Data collection; Data mining algorithm; Data publishing; Differential privacies; Mobile social networks; Network-based; Privacy issue; Privacy protection; Protection technologies; User privacy; Data privacy",Article,Scopus,2-s2.0-85123981401
"Hou T.","57438095400;","Research on Management Efficiency and Dynamic Relationship in Intelligent Management of Tourism Engineering Based on Industry 4.0",2022,"Computational Intelligence and Neuroscience","2022",,"5831062","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123963533&doi=10.1155%2f2022%2f5831062&partnerID=40&md5=1fef75a7f60701dbccefe98b64313b30","The digital age of artificial intelligence marks the rapid development of tourism engineering and the gradual improvement of intelligent management theory. This study aims to solve the problems of low efficiency of dynamic relationship analysis and low data utilization in traditional intelligent management methods of tourism engineering. This work studies the dynamic optimization model of tourism engineering management theory based on the artificial intelligence data analysis model and designs the dynamic analysis model of tourism engineering management data based on the convolution neural network. The model can collect dynamic data information of tourism management from many aspects and can also be used to study and analyze human behavior patterns based on the convolutional neural network algorithm. According to the human behavior data analysis model and convolution neural network algorithm, this study formulates the real-time management data scheme of tourism engineering and better extracts the characteristic information of the dynamic data of tourism engineering management. The results show that the topology optimization model of tourism intelligent management based on the convolutional neural network achieves high feasibility, high data accuracy, and high response speed. It can improve the collaborative coupling relationship between management efficiency and dynamic data in tourism engineering management based on big data analysis technology. It realizes the effective combination of tourism management, digital management, and artificial intelligence algorithm. © 2022 Tianchen Hou.",,"Behavioral research; Convolutional neural networks; Data handling; Efficiency; Industry 4.0; Information management; Optimization; Topology; Convolution neural network; Convolutional neural network; Data analysis models; Dynamic data; Engineering management; Intelligent management; Management data; Management efficiency; Management theory; Tourism management; Convolution; article; artificial intelligence; big data; convolutional neural network; data accuracy; data analysis; feasibility study; management theory; time management; tourism; velocity; Algorithms; Artificial Intelligence; Humans; Intelligence; Neural Networks, Computer; Tourism",Article,Scopus,2-s2.0-85123963533
"Shubinsky I., Zamyshlaev A., Bochkov A.","6507778765;57403792300;57213287436;","APPLICATION OF ARTIFICIAL INTELLIGENCE IN RUSSIA’S RAILWAY NETWORK ASSET MANAGEMENT",2022,"Reliability: Theory and Applications","17",,,"42","48",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123832794&doi=10.24412%2f1932-2321-2022-366-42-48&partnerID=40&md5=09a37cd5a9eb708cf57d3cab360c4d9b","The article presents general information on the system and methodology of asset management and Big Data methods (EKP URRAN) used on Russia’s railway network. The relevance of the publication is defined by the requirement of rational management of available resources amidst the stagnation of the global economy. That applies fully to the railway industry, where it is required to ensure an acceptable level of dependability of facilities and processes, while maintaining the traffic safety risks at an acceptable level. The architecture of EKP URRAN is presented. The system’s future outlooks are examined, most importantly in terms of application of artificial intelligence in predicting hazardous events in the operation of railway transportation. © 2022 The authors. All right reserved.","Artificial intelligence; Asset management; Big Data; Dependability; Risk; Safety",,Article,Scopus,2-s2.0-85123832794
"Cao Z., Wu Z., Guo G., Ma W., Wang H.","57192945308;55608513400;52463618100;57225905362;57433404000;","Quantifying spatial associations between effective green spaces and cardiovascular and cerebrovascular diseases by applying volunteered geo-referenced data",2022,"Environmental Research Letters","17","1","014055","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123805029&doi=10.1088%2f1748-9326%2fac40b3&partnerID=40&md5=46dc8cd5fe4badbff4446f2a1c7eec8e","Among the top public health risks, cardiovascular and cerebrovascular diseases cause more than 1 million deaths annually globally. Due to the calming effect of green spaces and their ability to trap air pollutants, urban green spaces are considered have close associations with cardiovascular and cerebrovascular diseases. However, ignoring the spatial heterogeneity of different urban green space types and considering only the configuration or compositions of urban green spaces has resulted in inconsistent and contradictory conclusions. Therefore, by introducing Tencent urban density data, four effective green spaces (EGSs) were categorized. Category 1 EGSs, which exhibit a high increasing of visitors and areas, accounted for the smallest areal percentage (0.81%). Category 2 EGSs, which exhibit a low increasing of visiting and high increasing of areas, accounted for the highest areal percentage (42.51%). Category 3 EGSs, which exhibit a high increasing of visiting and low increasing of areas, accounted for 13.70% of the total EGS areas. Category 4 EGSs, which exhibit a low increasing of visiting and areas, accounted for 3.75% of the total EGS areas. Using a geographically weighted regression model, spatial associations between EGS and cardiovascular and cerebrovascular diseases were quantified. Consequently, these spatial associations varied among EGS types and seasons. EGS configurations (perimeters of vegetation and areas of vegetation) have a more significant association with cardiovascular and cerebrovascular diseases than the composition (normalized difference vegetation index) of EGS. Spatial associations implying stronger relationships were observed in EGS1. The strongest association was found in summer. Enlarge the coverage of evergreen vegetation in all EGS is first considered to enhance the negative association between EGS and chronic diseases. A methodology framework was provided to classify urban green space types using multi-source data. Suggestions for how to plan different urban green spaces for developing sustainable cities have been provided in this study, which offer scientific support to urban managers and planners for effective decision making. © 2022 The Author(s). Published by IOP Publishing Ltd.","cardiovascular and cerebrovascular diseases; effective urban green space; social media big data; urban green space composition; urban green space configuration","Big data; Decision making; Health risks; Regression analysis; Social networking (online); Cardiovascular disease; Cerebrovascular disease; Effective urban green space; Social media; Social medium big data; Space compositions; Space configuration; Urban green space composition; Urban green space configuration; Urban green spaces; Vegetation; cardiovascular disease; data set; greenspace; health risk; heterogeneity; mortality risk; NDVI; public health; quantitative analysis; risk factor; seasonal variation",Article,Scopus,2-s2.0-85123805029
"Zhu M.","57432860300;","The Construction of University Network Education System Based on Mobile Edge Computing in the Era of Big Data",2022,"Wireless Communications and Mobile Computing","2022",,"1368841","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123801904&doi=10.1155%2f2022%2f1368841&partnerID=40&md5=61ed0b1398de637656f864ed3272ea89","This article first established a university network education system model based on physical failure repair behavior at the big data infrastructure layer and then examined in depth the complex common causes of multiple data failures in the big data environment caused by a single physical machine failure, all based on the principle of mobile edge computing. At the application service layer, a performance model based on queuing theory is first established, with the amount of available resources as a conditional parameter. The model examines important events in mobile edge computing, such as queue overflow and timeout failure. The impact of failure repair behavior on the random change of system dynamic energy consumption is thoroughly investigated, and a system energy consumption model is developed as a result. The network education system in colleges and universities includes a user login module, teaching resource management module, student and teacher management module, online teaching management module, student achievement management module, student homework management module, system data management module, and other business functions. Later, the theory of mobile edge computing proposed a set of comprehensive evaluation indicators that characterize the relevance, such as expected performance and expected energy consumption. Based on these evaluation indicators, a new indicator was proposed to quantify the complex constraint relationship. Finally, a functional use case test was conducted, focusing on testing the query function of online education information; a performance test was conducted in the software operating environment, following the development of the test scenario, and the server's CPU utilization rate was tested while the software was running. The results show that the designed network education platform is relatively stable and can withstand user access pressure. The performance ratio indicator can effectively assist the cloud computing system in selecting a more appropriate option for the migrated traditional service system. © 2022 Min Zhu.",,"Big data; Complex networks; Computation theory; Education computing; Energy utilization; Function evaluation; Information management; Mobile edge computing; Repair; Software testing; Students; Application services; Data environment; Data infrastructure; Evaluation indicators; Failure repairs; Machine failure; Model-based OPC; Multiple data; Network education systems; System models; Queueing theory",Article,Scopus,2-s2.0-85123801904
"Li Y.","57433149500;","Reflections on the Innovation of University Scientific Research Management in the Era of Big Data",2022,"Scientific Programming","2022",,"7674486","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123801472&doi=10.1155%2f2022%2f7674486&partnerID=40&md5=e8620ce2b7b0a2d1fc535f66ce233278","In China, universities are important centers for SR (scientific research) and innovation, and the quality of SR management has a significant impact on university innovation. The informatization of SR management is a critical component of university development in the big data environment. As a result, it is crucial to figure out how to improve SR management. As a result, this paper builds a four-tier B/W/D/C (Browser/Web/Database/Client) university SR management innovation information system based on big data technology and thoroughly examines the system's hardware and software configuration. The SVM-WNB (Support Vector Machine-Weighted NB) classification algorithm is proposed, and the improved algorithm runs in parallel on the Hadoop cloud computing platform, allowing the algorithm to process large amounts of data efficiently. The optimization strategy proposed in this paper can effectively optimize the execution of scientific big data applications according to a large number of simulation experiments and real-world multidata center environment experiments. © 2022 Yiming Li.",,"Information management; Support vector machines; Critical component; Data environment; Data technologies; Informatization; Management innovation; Management IS; Research management; Scientific researches; System hardware; Web database; Big data",Article,Scopus,2-s2.0-85123801472
"Zhang W., Liu B., Tsai S.-B.","57433586900;57433295500;55910203400;","Analysis and Research on Digital Reading Platform of Multimedia Library by Big Data Computing in Internet Era",2022,"Wireless Communications and Mobile Computing","2022",,"5939138","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123781765&doi=10.1155%2f2022%2f5939138&partnerID=40&md5=2011fd90aa948c8f165d81d0d0340060","Digital reading promotion service is a service way for libraries to provide readers with a series of digital resources, enjoy the service functions, and share the experience of using them in various digital reading platforms, which is to meet the reading interests and reading needs of more readers, and is also the focus of the current library work. In the era of new media, the characteristics of digital reading are subtly changing the readers' needs for reading environment, reading content, and reading style. Libraries should keep pace with the development of the times and provide readers with diversified, intelligent, and targeted digital reading platforms. The digital reading platform should continuously improve the digital reading service functions, broaden the service scope and dissemination channels, and finally realize the diversification, interest, and intelligence of digital reading service. This paper takes the digital reading platform of libraries in the region as the research theme and carries out research work on libraries. The province is divided into three regions according to the geographical map: southeastern region, central region, and northwestern region. The digital reading platforms of 14 prefecture-level public libraries and 58 libraries of higher education institutions in each region were accessed. Firstly, we check the construction of digital resources within the library websites, secondly, we count the opening of digital reading platform functions, and finally, we check the opening of digital reading platforms. Through the research, it is found that there are problems of unbalanced distribution of digital reading resources in regional libraries; unattractive design of readers' interface and inadequate reading service functions; lack of continuous training of readers' guidance; insufficient publicity and promotion; low efficiency of staff in responding to consultation; and low degree of platform openness and weak awareness of sharing. Finally, the problems found in the research are summarized, and the solution measures for the regional digital reading platform are proposed. Libraries in the digital era should give priority to systems that can manage all library resources comprehensively and effectively, adapt to more flexible library workflows, and enable libraries to provide better services to users. © 2022 Wanxia Zhang et al.",,"Big data; Geographical regions; Personnel training; 'current; Digital reading; Digital resources; Geographical maps; Higher education institutions; Multimedia libraries; New media; Public library; Service functions; Service scope; Digital libraries",Article,Scopus,2-s2.0-85123781765
"Li W.","57432723900;","Big Data Precision Marketing Approach under IoT Cloud Platform Information Mining",2022,"Computational Intelligence and Neuroscience","2022",,"4828108","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123780233&doi=10.1155%2f2022%2f4828108&partnerID=40&md5=c05f80c45eaa306ecc509fdbd62a3fc1","In this article, an in-depth study and analysis of the precision marketing approach are carried out by building an IoT cloud platform and then using the technology of big data information mining. The cloud platform uses the MySQL database combined with the MongoDB database to store the cloud platform data to ensure the correct storage of data as well as to improve the access speed of data. The storage method of IoT temporal data is optimized, and the way of storing data in time slots is used to improve the efficiency of reading large amounts of data. For the scalability of the IoT data storage system, a MongoDB database clustering scheme is designed to ensure the scalability of data storage and disaster recovery capability. The relevant theories of big data marketing are reviewed and analyzed; secondly, based on the relevant theories, combined with the author's work experience and relevant information, a comprehensive analysis and research on the current situation of big data marketing are conducted, focusing on its macro-, micro-, and industry environment. The service model combines the types of user needs, encapsulates the resources obtained by the alliance through data mining for service products, and publishes and delivers them in the form of data products. From the perspective of the development of the telecommunications industry, in terms of technology, the telecommunications industry has seen the development trend of mobile replacing fixed networks and triple play. The development of emerging technologies represented by the Internet of Things and cloud computing has also led to technological changes in the telecommunications industry. Operators are facing new development opportunities and challenges. It also divides the service mode into self-service and consulting service mode according to the different degrees of users' cognition and understanding of the service, as well as proposes standardized data mining service guarantee from two aspects: after-sales service and operation supervision. A customized data mining service is a kind of data mining service for users' personalized needs. And the intelligent data mining service guarantee is proposed from two aspects of multicase experience integration and group intelligence. In the empirical research part, the big data alliance in Big Data Industry Alliance, which provides data mining service as the main business, is selected as the research object, and the data mining service model of the big data alliance proposed in this article is applied to the actual alliance to verify the scientific and rationality of the data mining service model and improve the data mining service model management system. © 2022 Wang Li.",,"Big data; Commerce; Computation theory; Data mining; Database systems; Digital storage; Information management; Information services; Marketing; Scalability; Telecommunication industry; Cloud platforms; Data marketing; Data-mining services; Information mining; Marketing IS; MongoDB; Precision marketings; Service mode; Service modeling; Telecommunications industry; Internet of things; cloud computing; data mining; marketing; technology; Big Data; Cloud Computing; Data Mining; Marketing; Technology",Article,Scopus,2-s2.0-85123780233
"Hu C., Sun Y.","57433586000;57432723200;","The Influence of Football Training Based on Big Data on Physical Function and Football Skills",2022,"Mobile Information Systems","2022",,"1735022","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123767146&doi=10.1155%2f2022%2f1735022&partnerID=40&md5=55dc9cb79c7fdcd7d61f5fecb5e0829e","In order to actively respond to the government's call to scientifically create campus football culture, combine the characteristics of football sports, and improve people's understanding of the mental and intellectual functions of football, this article focuses on the impact of football training on physical function and football technology. Based on the understanding of related theories, the experiment on the impact of football training on physical function and football technology was carried out. The experimental results showed that the weight, height, and BMI increased significantly during the period of football training (P<0.05). The independent sample T test showed that there were no significant differences in height, weight, and BMI between the two groups before and after training; the standing long jump performance of the control group after training showed an upward trend, but the significance level was not statistically significant. Three months later, the time for the experimental team to complete the eight-character dribble test in football training was reduced from 20.51 seconds to 15.57 seconds. The independent sample T test found that there was no significant difference in the physical fitness of the two groups before training and the changes in football skills of the subjects before and after training. Then, the clustering algorithm in the big data was used to analyze the data of the experimental group. The standing long jump has the highest performance; the second category belongs to the third level, and the third category belongs to the second level. © 2022 Changjun Hu and Yang Sun.",,"Clustering algorithms; Sports; Control groups; Independent samples; Intellectual functions; Performance; Physical fitness; Physical function; Significance levels; Standing long jumps; T-tests; Upward trend; Big data",Article,Scopus,2-s2.0-85123767146
"Wang Y., Peng C., Liu D., Wang N., Gao X.","57433156900;56313611100;57192557912;55694111900;57222996912;","ForgeryNIR: Deep Face Forgery and Detection in Near-Infrared Scenario",2022,"IEEE Transactions on Information Forensics and Security","17",,,"500","515",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123763801&doi=10.1109%2fTIFS.2022.3146766&partnerID=40&md5=b5828d4797c63f013adc85aefb4299f8","Deep face forgery and detection is an emerging topic due to the development of GANs. Face forgery detection relies greatly on existing databases for evaluation and adequate training examples for data-hungry machine learning algorithms. However, considering the wide application of face recognition in near-infrared scenarios, there is no publicly available face forgery database that includes near-infrared modality currently. In this paper, we present an attempt at constructing a large-scale dataset for face forgery detection in the near-infrared modality and propose a new forgery detection method based on knowledge distillation named cross-modality knowledge distillation aiming to use a teacher model which is pre-trained on the visible light-based (VIS) big data to guide the student model with a small amount of near-infrared (NIR) data. The proposed near-infrared face forgery dataset, named ForgeryNIR, contains a total of over 50,000 real and fake identities. A number of perturbations are applied to help simulate real-world scenarios. All source images in ForgeryNIR are collected from CASIA NIR-VIS 2.0, and fake images are generated via multiple GAN techniques. The proposed dataset fills the gap of face forgery detection research in the near-infrared modality. A comprehensive study on six representative detection baselines is conducted to evaluate the performance of face forgery detection algorithms in the NIR domain. We further construct a hard testing set, named ForgeryNIR+, which contains forged images that have bypassed existing face forgery detection methods. The proposed datasets will be publicly available and aim to help boost further research on face forgery detection, as well as NIR face detection and recognition. © 2005-2012 IEEE.","deepfake; face forgery detection; Near-infrared face","Big data; Database systems; Distillation; Fake detection; Infrared devices; Learning algorithms; Learning systems; Perturbation techniques; Teaching; Deepfake; Face; Face forgery detection; Forgery; Forgery detections; Near Infrared; Near-infrared; Near-infrared face; Perturbation method; Video; Face recognition",Article,Scopus,2-s2.0-85123763801
"Manivannan P., Prabha D., Balasubramanian K.","57210769681;57224577264;57203368750;","Artificial intelligence databases: Turn-on big data of the SMBs",2022,"International Journal of Business Information Systems","39","1",,"1","16",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123753558&doi=10.1504%2fIJBIS.2022.120367&partnerID=40&md5=aa8f05a02009d9c6be5b9bdc8099e70c","The small and medium businesses are working hard to make sense on the information data that has been collected from network sources and to translate it into tangible results. In fact, the major data growth trends and shifts in information. Big data have coined to generate and extremely more complex to associate in business databases. Most researcher work focuses on the relational database that requires lots of data processing. That's the reason, artificial intelligence (AI) can achieve input and ability to extend NoSQL document database depending on data type. This research recognises documented MongoDB as real-time access to data stored on various storage platform for all sizes of business. This paper proposed NoSQL-MongoDB model with data shared process embedded with AI and machine learning at the system-level by virtue datasets from the big data analytics. This methodology contributes a narrow view of database management turns on big data challenges for SMBs. © 2022 Inderscience Enterprises Ltd.. All rights reserved.","Big data; Database management system; DBMS; Non-relational; NoSQL-MongoDB; Small and medium business; SMB",,Article,Scopus,2-s2.0-85123753558
"Al-Bana M.R., Farhan M.S., Othman N.A.","57433039900;56081890800;35776730400;","An Efficient Spark-Based Hybrid Frequent Itemset Mining Algorithm for Big Data",2022,"Data","7","1","11","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123750919&doi=10.3390%2fdata7010011&partnerID=40&md5=5070a874b63414e7c9dcb46b4f40db0a","Frequent itemset mining (FIM) is a common approach for discovering hidden frequent patterns from transactional databases used in prediction, association rules, classification, etc. Apriori is an FIM elementary algorithm with iterative nature used to find the frequent itemsets. Apriori is used to scan the dataset multiple times to generate big frequent itemsets with different cardinalities. Apriori performance descends when data gets bigger due to the multiple dataset scan to extract the frequent itemsets. Eclat is a scalable version of the Apriori algorithm that utilizes a vertical layout. The vertical layout has many advantages; it helps to solve the problem of multiple datasets scanning and has information that helps to find each itemset support. In a vertical layout, itemset support can be achieved by intersecting transaction ids (tidset/tids) and pruning irrelevant itemsets. However, when tids become too big for memory, it affects algorithms efficiency. In this paper, we introduce SHFIM (spark-based hybrid frequent itemset mining), which is a three-phase algorithm that utilizes both horizontal and vertical layout diffset instead of tidset to keep track of the differences between transaction ids rather than the intersections. Moreover, some improvements are developed to decrease the number of candidate itemsets. SHFIM is implemented and tested over the Spark framework, which utilizes the RDD (resilient distributed datasets) concept and in-memory processing that tackles MapReduce framework problem. We compared the SHFIM performance with Spark-based Eclat and dEclat algorithms for the four benchmark datasets. Experimental results proved that SHFIM outperforms Eclat and dEclat Spark-based algorithms in both dense and sparse datasets in terms of execution time. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; Diffset; Frequent pattern mining; Horizontal layout; Spark; Vertical layout",,Article,Scopus,2-s2.0-85123750919
"Krishnamoorthi R., Joshi S., Almarzouki H.Z., Shukla P.K., Rizwan A., Kalpana C., Tiwari B.","57201530772;57212646143;57336347000;56599752300;24779840400;57432628500;36934359400;","A Novel Diabetes Healthcare Disease Prediction Framework Using Machine Learning Techniques",2022,"Journal of Healthcare Engineering","2022",,"1684017","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123745975&doi=10.1155%2f2022%2f1684017&partnerID=40&md5=9872492cfca1549485d72a8c5cfc4706","Diabetes is a chronic disease that continues to be a significant and global concern since it affects the entire population's health. It is a metabolic disorder that leads to high blood sugar levels and many other problems such as stroke, kidney failure, and heart and nerve problems. Several researchers have attempted to construct an accurate diabetes prediction model over the years. However, this subject still faces significant open research issues due to a lack of appropriate data sets and prediction approaches, which pushes researchers to use big data analytics and machine learning (ML)-based methods. Applying four different machine learning methods, the research tries to overcome the problems and investigate healthcare predictive analytics. The study's primary goal was to see how big data analytics and machine learning-based techniques may be used in diabetes. The examination of the results shows that the suggested ML-based framework may achieve a score of 86. Health experts and other stakeholders are working to develop categorization models that will aid in the prediction of diabetes and the formulation of preventative initiatives. The authors perform a review of the literature on machine models and suggest an intelligent framework for diabetes prediction based on their findings. Machine learning models are critically examined, and an intelligent machine learning-based architecture for diabetes prediction is proposed and evaluated by the authors. In this study, the authors utilize our framework to develop and assess decision tree (DT)-based random forest (RF) and support vector machine (SVM) learning models for diabetes prediction, which are the most widely used techniques in the literature at the time of writing. It is proposed in this study that a unique intelligent diabetes mellitus prediction framework (IDMPF) is developed using machine learning. According to the framework, it was developed after conducting a rigorous review of existing prediction models in the literature and examining their applicability to diabetes. Using the framework, the authors describe the training procedures, model assessment strategies, and issues associated with diabetes prediction, as well as solutions they provide. The findings of this study may be utilized by health professionals, stakeholders, students, and researchers who are involved in diabetes prediction research and development. The proposed work gives 83% accuracy with the minimum error rate. © 2022 Raja Krishnamoorthi et al.",,"Big data; Data Analytics; Forecasting; Health care; Predictive analytics; Support vector machines; Blood sugar levels; Chronic disease; Data prediction; Data set; Machine learning models; Machine learning techniques; Metabolic disorders; Population health; Prediction modelling; Research issues; Decision trees; adult; Article; big data; classification algorithm; computer prediction; controlled study; data analysis; decision tree; diabetes mellitus; diagnostic accuracy; diagnostic test accuracy study; female; health data; human; k nearest neighbor; logistic regression analysis; machine learning; major clinical study; predictive value; pregnancy diabetes mellitus; random forest; receiver operating characteristic; sensitivity and specificity; stakeholder engagement; support vector machine",Article,Scopus,2-s2.0-85123745975
"Huang J., Dang F.","57432661000;57432612700;","Analysis of Inducing Factors of Chronic Pulmonary Heart Disease Caused by Chronic Obstructive Pulmonary Disease at High Altitude through Epidemiological Investigation under Intelligent Medicine and Big Data",2022,"Journal of Healthcare Engineering","2022",,"2612074","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123744082&doi=10.1155%2f2022%2f2612074&partnerID=40&md5=d9bf00e46d8073921029120f893ce115","This study explores the risk factors of chronic pulmonary heart disease (CPHD) induced by plateau chronic obstructive pulmonary disease (COPD) based on intelligent medical treatment and big data of electrocardiogram (ECG) signal. Based on GPU, a wavelet algorithm is introduced to extract features of ECG signal, and it was combined with generalized regression neural network (GRNN) to improve classification accuracy. From June 2018 to December 2020, 10,185 patients diagnosed with COPD in the plateau area by pulmonary function testing, ECG, and chest X-ray at X Hospital are taken as the research objects to evaluate the distribution of CPHD incidence at different ages and altitudes. The running time of GTX780Ti is about 15 times shorter than that of CPU. The accuracy of N detection based on the GPU-accelerated neural network model reached 98.06%. Accuracy (Acc), sensitivity (Se), specificity (Sp), and positive rate (PR) of V were 99.03%, 89.17%, 98.92%, and 93.18%, respectively. The Acc, Se, Sp, and PR of S were 99.54%, 86.22%, 99.74%, and 92.56%, respectively. The GRNN classification accuracy was up to 98%. 19% of COPD patients were diagnosed with CPHD, including 1,409 males (72.82%) and 526 females (36.24%). The highest prevalence of CPHD was 64.60% when the altitude was 1,900-2,499 m, and the prevalence was only 2.43% when the altitude was ≥3,500 m. The highest prevalence of CPHD was 63.77% at the age of 61-70 years, and the lowest prevalence at the age of 15∼20 years was only 0.26%. Therefore, the GPU-based neural network model improved the classification accuracy of ECG signals. Age and altitude were risk factors for CPHD induced by high-altitude COPD, which provided a reference for the prevention, diagnosis, and treatment of CPHD in high-altitude areas. © 2022 Jiong Huang and Fulin Dang.",,"Big data; Biomedical signal processing; Cardiology; Heart; Neural networks; Patient monitoring; Pulmonary diseases; Chronic obstructive pulmonary disease; Classification accuracy; Electrocardiogram signal; Generalized regression; Heart disease; Inducing factors; Medical treatment; Neural network model; Regression neural networks; Risk factors; Electrocardiography",Article,Scopus,2-s2.0-85123744082
"Chapelle G., Eymeoud J.B.","57142295900;57226251729;","Can big data increase our knowledge of local rental markets? A dataset on the rental sector in France",2022,"PLoS ONE","17","1 January","e0260405","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123737995&doi=10.1371%2fjournal.pone.0260405&partnerID=40&md5=9f66afac728081d2041ba0aa7f5a2119","Social Scientists and policy makers need precise data on market rents. Yet, while housing prices are systematically recorded, few accurate data sets on rents are available. In this paper, we present a new data set describing local rental markets in France based on online ads collected through to webscraping. Comparison with alternate sources reveals that online ads provide a non biased picture of rental markets and allow coverage of the whole territory. We then estimate hedonic models for prices and rents and document the spatial variations in rent-price ratios. We show that rents do not increase as much as prices in the tightest housing markets. We use our dataset to estimate the market rent of each transaction and of social dwellings. In the latter case,this allows us to estimate the in-kind benefit received by social tenants which is mainly driven by the level of private rent in their municipality. © 2022 Chapelle, Eyméoud. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"article; big data; France; human; price; real estate market; administrative personnel; advertising; economics; France; housing; legislation and jurisprudence; sociology; urban population; Administrative Personnel; Advertising; Big Data; France; Housing; Humans; Social Sciences; Urban Population",Article,Scopus,2-s2.0-85123737995
"Kowalczyk P., Komorkiewicz M., Skruch P., Szelest M.","57282596300;55053601200;26538956900;51764550000;","Efficient Characterization Method for Big Automotive Datasets Used for Perception System Development and Verification",2022,"IEEE Access","10",,,"12629","12643",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123695806&doi=10.1109%2fACCESS.2022.3145192&partnerID=40&md5=6e806c7a5f725b4cd1c5a11557ee710c","The paper proposes a formal approach for describing and evaluating the datasets that are used in automotive applications for machine learning, testing, and validation purposes. Proper, that is, qualitative and quantitative characterization of the datasets can simplify the analysis, evaluation, and comparison of perception-based algorithms designed for highly automated vehicles. Such formalism is also needed to achieve compliance with the automotive industry safety standards that have been recently introduced. Characterization in the form of size or type of raw data, number of recognized and classified objects, and environmental parameters is not perfectly suitable for describing both the static and dynamic aspects of automotive datasets; therefore, another approach is required. In this paper, an efficient method based on an object tracking mechanism, grid representation of the sensor field of view, heatmap concept, and Wasserstein metric is proposed. The efficiency of the method is demonstrated by its ability to handle both the size, properties, and diversity of the dataset, including static and time-varying aspects. The presented description can also be used to compare different datasets and to define the amount of data to be collected. © 2013 IEEE.","Autonomous vehicles; big data; perception; trajectory","Accident prevention; Automotive industry; Big data; Learning systems; Regulatory compliance; Automotive applications; Automotives; Autonomous Vehicles; Characterization methods; Correlation; Formal approach; Heating system; Perception systems; System development; System verifications; Data visualization",Article,Scopus,2-s2.0-85123695806
"Cabrera-Álvarez P.","57212673401;","Survey Research in Times of Big Data [Investigación con encuestas en los tiempos del big data]",2022,"Empiria",,"53",,"31","51",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123627098&doi=10.5944%2fEMPIRIA.53.2022.32611&partnerID=40&md5=849d1671e31b9c132b950dee5c773ca4","Although surveys still dominate the research landscape in social sciences, alternative data sources such as social media posts or GPS data open a whole range of opportunities for researchers. In this scenario, some voices advocate for a progressive substitution of survey data. They anticipate that big data, which is cheaper and faster than surveys, will be enough to answer relevant research questions. However, this optimism contrasts with all the quality and accessibility issues associated with big data such as the lack of coverage or data ownership and restricted accessibility. The aim of this paper is to explore how, nowadays, the combination of big data and surveys results in significant improvements in data quality and survey costs. © 2022 Universidad Nacional de Educacion a Distancia. All rights reserved.","Administrative data; Big data; Data linkage; Social media data; Survey methodology",,Article,Scopus,2-s2.0-85123627098
"Li G.","57427941000;","Analysis of Matching of Corpus Input and English Proficiency Based on the Big Data Neural Network Model",2022,"Advances in Multimedia","2022",,"2190873","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123533985&doi=10.1155%2f2022%2f2190873&partnerID=40&md5=9c6e3ac881eb21521cb3926d96e8982e","In the era of ""Internet +""big data, the theory and technology of English corpus are becoming more and more mature. Corpus is an important method to reflect some language characteristics and clarify some language phenomena. In terms of cultural exchanges, Chinese students majoring in English have obvious cultural differences at home and abroad and lack the atmosphere and context for cultural exchanges. In addition, students have problems such as insufficient cultural communication skills. The big data neural network model is adopted in this paper to compare and analyze the intermediary sentences in the corpus to explore the development trend of English proficiency. Through the analysis of typical cases, it explores the weak links in the corpus teaching process and summarizes a method focusing on the combination of use of corpus and English teaching. © 2022 Guangli Li.",,"Students; Chinese students; Communication skills; Compare and analyze; Cultural difference; Data neural networks; Development trends; Matchings; Neural network model; Teaching process; Weakest links; Big data",Article,Scopus,2-s2.0-85123533985
"Lee K.H., Urtnasan E., Hwang S., Lee H.Y., Lee J.H., Koh S.B., Youk H.","57426398300;56786372500;57426350700;57426517100;56785374200;57397881700;57003214900;","Concept and Proof of the Lifelog Bigdata Platform for Digital Healthcare and Precision Medicine on the Cloud",2022,"Yonsei medical journal","63",,,"S84","S92",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123460542&doi=10.3349%2fymj.2022.63.S84&partnerID=40&md5=471a02eaf6ee44821ee7a73c5bb466d5","PURPOSE: We propose the Lifelog Bigdata Platform as a sustainable digital healthcare system based on individual-centric lifelog datasets and describe the standardization of lifelog and clinical data in its full-cycle management system. MATERIALS AND METHODS: The Lifelog Bigdata Platform was developed by Yonsei Wonju Health System on the cloud to support digital healthcare and precision medicine. It consists of five core components: data acquisition system, de-identification of individual information, lifelog integration, analyzer, and service. We designed a gathering system into a dedicated virtual machine to save lifelog or clinical outcomes and established standard guidelines for maintaining the quality of gathering procedures. We used standard integration keys to integrate the lifelog and clinical data. Metadata were generated from the data warehouse after loading combined or fragmented data on it. We analyzed the de-identified lifelog and clinical data using the lifelog analyzer to prevent and manage acute and chronic diseases through providing results of statistics on analysis. RESULTS: The big data centers were built in four hospitals and seven companies for integrating lifelog and clinical data to develop the Lifelog Bigdata Platform. We integrated and loaded lifelog big data and clinical data for 3 years. In the first year, we uploaded 94 types of data on the platform with a total capacity of 221 GB. CONCLUSION: The Lifelog Bigdata Platform is the first to combine lifelog and clinical data. The proposed standardization guidelines can be used for future platforms to achieve a virtuous cycle structure of lifelogging big data and an industrial ecosystem. © Copyright: Yonsei University College of Medicine 2022.","big data; digital health; Lifelog; precision medicine","chronic disease; ecosystem; health care delivery; hospital; human; personalized medicine; Chronic Disease; Delivery of Health Care; Ecosystem; Hospitals; Humans; Precision Medicine",Article,Scopus,2-s2.0-85123460542
"Zou J., Gong W., Huang G., Hu G., Gong W.","57219529583;57280189300;57280312700;57280440200;57280819900;","Research on the Improvement of Big Data Feature Investment Analysis Algorithm for Abnormal Trading in the Financial Securities Market",2022,"International Journal of Circuits, Systems and Signal Processing","16",,"50","406","412",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123454064&doi=10.46300%2f9106.2022.16.50&partnerID=40&md5=a9dbdb24d98237e131eca40a0d4fbeb1","—Traditional investment analysis algorithms usually only analyze the similarity between financial time series and financial data, which leads to inaccurate and inefficient analysis of investment characteristics. In addition, the trading volume of financial securities market is huge, the amount of investment data is also very large, and the detection of abnormal transactions is difficult. The aim of feature extraction is to obtain mathematical features that can be recognized by machine. Different from the traditional methods, this paper studies and improves the big data investment analysis algorithm of abnormal transactions in financial securities market. After processing the captured trading data of financial securities market, the big data feature of abnormal trading is extracted. Combined with the abnormal trading and the financial securities market, the investment strategy is determined. The optimization objective function is set and the genetic algorithm is used to improve the investment analysis algorithm. The simulation experiment verifies the improved investment analysis algorithm, and the average Accuracy of investment analysis is increased by at least 11.24%, the ROI is significantly improved, and the efficiency is higher, which indicates that the proposed algorithm has ideal application performance. © 2022, North Atlantic University Union NAUN. All rights reserved.","Abnormal trading; Algorithm improvement; Big data feature; Investment analysis algorithm; —financial securities market","Big data; Commerce; Data mining; Financial markets; Genetic algorithms; Market Research; Time series analysis; Abnormal trading; Algorithm improvements; Analysis algorithms; Big data feature; Data feature; Financial security; Investment analyse algorithm; Investment analysis; Securities market; —financial security market; Investments",Article,Scopus,2-s2.0-85123454064
"Meng J., Chen Y.","57426259300;57225201620;","A study on predictive modeling of users’ parasocial relationship types based on social media text big data",2022,"International Journal of Circuits, Systems and Signal Processing","16",,,"171","180",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123453833&doi=10.46300%2f9106.2022.16.21&partnerID=40&md5=2d0911b83b1ac35c8b02d75ef60cc5ee","The traditional parasocial relationship type prediction model obtains prediction results by analyzing and clustering the direct data. The prediction results are easily disturbed by noisy data, and the problems of low processing efficiency and accuracy of the traditional prediction model gradually appear as the amount of user data increases. To address the above problems, the research constructs a prediction model of user parasocial relationship type based on social media text big data. After pre-processing the collected social media text big data, the interference data that affect the accuracy of non-model prediction are removed. The interaction information in the text data is mined based on the principle of similarity calculation, and semantic analysis and sentiment annotation are performed on the information content. On the basis of BP neural network, we construct a prediction model of user’s parasocial relationship type. The performance test data of the model shows that the average prediction accuracy of the constructed model is 89.84%, and the model has low time complexity and higher processing efficiency, which is better than other traditional models. © 2022, North Atlantic University Union NAUN. All rights reserved.","Neural network; Parasocial relationship; Prediction model; Relationship type prediction; Social media; Textual big data","Big data; Efficiency; Forecasting; Semantics; Social networking (online); User profile; Clusterings; Neural-networks; Noisy data; Parasocial relationship; Prediction modelling; Predictive models; Relationship type prediction; Social media; Textual big data; Type predictions; Neural networks",Article,Scopus,2-s2.0-85123453833
"Alhazmi H.E., Eassa F.E., Sandokji S.M.","55320340600;6506548846;57189387913;","Towards Big Data Security Framework by Leveraging Fragmentation and Blockchain Technology",2022,"IEEE Access","10",,,"10768","10782",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123348752&doi=10.1109%2fACCESS.2022.3144632&partnerID=40&md5=c1ea501537d5241d46e985732d8c83ae","The world is facing a growth in the amount and variety of data generated by both users and machines. Despite the exponential increases, the tools and technologies developed to manage these data volumes are not intended to meet security and data protection requirements. Additionally, most of the current big data security systems are offered by a centralized third party, which is vulnerable to many security threats. Blockchain technology plays a significant role by addressing modern technology concerns such as decentralization, non-tampering, trust, data ownership, and traceability, making it great potential to protect personal information. This research presents a new big data security solution empowered by blockchain technology and incorporates fragmentation, encryption, and access control techniques. Our proposed fragmentation algorithm takes into account the data owner's demand for encryption to be added to the fragmentation process. Furthermore, data fragments will be stored in the distributed manner offered by the big data environment, resulting in an additional layer of data protection. In order to achieve an optimal security solution, we aim to enhance big data security with acceptable overhead and avoid the encryption overhead for non-sensitive and low-sensitive data portions. We present the results of our implemented techniques to highlight that the overheads (in terms of computation time) introduced by our solution are negligible relative to its security and privacy gains. © 2022 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Big Data; Blockchains; Encryption; Peer-to-peer computing; Security; Social networking (online); Streaming media","Access control; Big data; Blockchain; Distributed computer systems; Media streaming; Peer to peer networks; Security systems; Social sciences computing; Big data security; Block-chain; Fragmentation; Peer-to-peer computing; Security; Security frameworks; Security solutions; Social networking (online); Streaming medium; Cryptography",Article,Scopus,2-s2.0-85123348752
"Guo J.","56927101200;","Deep learning approach to text analysis for human emotion detection from big data",2022,"Journal of Intelligent Systems","31","1",,"113","126",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123314663&doi=10.1515%2fjisys-2022-0001&partnerID=40&md5=564dc721f3f0d87b0e492dc2012c2650","Emotional recognition has arisen as an essential field of study that can expose a variety of valuable inputs. Emotion can be articulated in several means that can be seen, like speech and facial expressions, written text, and gestures. Emotion recognition in a text document is fundamentally a content-based classification issue, including notions from natural language processing (NLP) and deep learning fields. Hence, in this study, deep learning assisted semantic text analysis (DLSTA) has been proposed for human emotion detection using big data. Emotion detection from textual sources can be done utilizing notions of Natural Language Processing. Word embeddings are extensively utilized for several NLP tasks, like machine translation, sentiment analysis, and question answering. NLP techniques improve the performance of learning-based methods by incorporating the semantic and syntactic features of the text. The numerical outcomes demonstrate that the suggested method achieves an expressively superior quality of human emotion detection rate of 97.22% and the classification accuracy rate of 98.02% with different state-of-the-art methods and can be enhanced by other emotional word embeddings. © 2022 Jia Guo, published by De Gruyter.","deep learning; human emotion detection; NLP; text analysis","Big data; Character recognition; Deep learning; Information retrieval systems; Numerical methods; Semantics; Sentiment analysis; Deep learning; Embeddings; Emotion detection; Emotion recognition; Emotional recognition; Facial Expressions; Human emotion; Human emotion detection; Learning approach; Written texts; Embeddings",Article,Scopus,2-s2.0-85123314663
"Alemazkoor N., Tootkaboni M., Nateghi R., Louhghalam A.","56124213800;29967492200;36677968200;33568004900;","Smart-Meter Big Data for Load Forecasting: An Alternative Approach to Clustering",2022,"IEEE Access","10",,,"8377","8387",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123292234&doi=10.1109%2fACCESS.2022.3142680&partnerID=40&md5=eff1453f20dee163deb89309fc8b8abb","Accurate forecasting of electricity demand is vital to the resilient management of energy systems. Recent efforts in harnessing smart-meter data to improve forecasting accuracy have primarily centered around cluster-based approaches (CBAs), where smart-meter data are grouped into a small number of clusters and separate prediction models are developed for each cluster. The cluster-based predictions are then aggregated to compute the total demand. CBAs have provided promising results compared to conventional approaches that are generally not conducive to integrating smart-meter data. However, CBAs are computationally costly and suffer from the curse of dimensionality, especially under scenarios involving smart-meter data from millions of customers. In this work, we propose an efficient reduced model approach (RMA) that leverages a novel hierarchical dimension reduction algorithm to enable the integration of fine-resolution high-dimensional smart-meter data for millions of customers in load prediction. We demonstrate the applicability of our proposed approach by using data from a utility company, based in Illinois, United States, with more than 3.7 million customers and present model performance in-terms of forecast accuracy. The proposed hierarchical dimension reduction approach enables utilizing the high-resolution data from smart-meters in a scalable manner that is not exploitable otherwise. The results shows significant improvements in forecast accuracy compared to the available approaches that either do not harness fine-resolution data or are not scalable to large-scale smart-meter big data. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/","Big Data; Clustering algorithms; Data models; Load forecasting; Load modeling; Prediction algorithms; Predictive models","Big data; Cluster analysis; Data integration; Data structures; Electric power plant loads; Forecasting; Information management; Reduction; Sales; Smart meters; Cluster based approach; Dimension reduction; Fine resolution; Hierarchical dimension reduction; Load forecasting; Load modeling; Prediction algorithms; Predictive models; Short term load forecasting; Smart-meter data; Clustering algorithms",Article,Scopus,2-s2.0-85123292234
"Hassanat A.B., Ali H.N., Tarawneh A.S., Alrashidi M., Alghamdi M., Altarawneh G.A., Abbadi M.A.","24343672100;57225688780;57190020156;56405416200;57213626210;56512419600;57194447491;","Magnetic Force Classifier: A Novel Method for Big Data Classification",2022,"IEEE Access","10",,,"12592","12606",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123283345&doi=10.1109%2fACCESS.2022.3142888&partnerID=40&md5=879261452c1ad5a0e0a119368cdfb6f5","There are a plethora of invented classifiers in Machine learning literature, however, there is no optimal classifier in terms of accuracy and time taken to build the trained model, especially with the tremendous development and growth of Big data. Hence, there is still room for improvement. In this paper, we propose a new classification method that is based on the well-known magnetic force. Based on the number of points belonging to a specific class/magnet, the proposed magnetic force (MF) classifier calculates the magnetic force at each discrete point in the feature space. Unknown examples are classified using the magnetic forces recorded in the trained model by various magnets/classes. When compared to existing classifiers, the proposed MF classifier achieves comparable classification accuracy, according to the experimental results utilizing 28 different datasets. More importantly, we found that the proposed MF classifier is significantly faster than all other classifiers tested, particularly when applied to Big datasets and hence could be a viable option for structured Big data classification with some optimization. © 2013 IEEE.","Artificial intelligence; classification algorithms; data mining; machine learning; supervised learning","Big data; Classification (of information); Iron; Magnetism; Supervised learning; Classification algorithm; Classification methods; Data classification; Force; Machine learning literature; Magnetic force; Novel methods; Optimal classifiers; Specific class; Training data; Data mining",Article,Scopus,2-s2.0-85123283345
"Domingues M., Rocha Silva R., Bernardino J.","57422934800;57441484000;8847095400;","3iCubing: An Interval Inverted Index Approach to Data Cubes",2022,"IEEE Access","10",,,"8449","8461",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123273562&doi=10.1109%2fACCESS.2022.3142449&partnerID=40&md5=cafe740e544d35411409575da424e6c4","The increase in the amounts of information used to analyze data is problematic since the memory necessary to store and process it is getting quite big. The interval inverted index representation was developed to reduce the required memory to store data, and Frag-Cubing is one of the most popular algorithms. In this paper, we propose two new data cubing algorithms: 3iCubing and M3iCubing. 3iCubing is a Frag-Cubing-based algorithm that uses the interval inverted index representation, while M3iCubing uses both a normal and interval inverted index data representation. The algorithms were compared using synthetic and real data sets in indexation and querying operations, both runtime and memory-wise. The experimental evaluation shows that 3iCubing can considerably reduce the memory needed to index a data set, reducing around 25% of the memory used by Frag-Cubing. Moreover, the results show that the interval inverted index representation is dependent on the data skewness to reduce the memory consumption, having positive results with highly skewed and real-world data sets. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/","Arrays; Complexity theory; Indexes; Memory management; Periodic structures; Proposals; Runtime","Data reduction; Indexing (of information); Information management; Array; Complexity theory; Data cube; Data set; Index; Inverted indices; Memory-management; OLAP; Proposal; Runtimes; Big data",Article,Scopus,2-s2.0-85123273562
"Herrero González A.","57222353719;","The value of data and its applicability in the Health Sector [El valor de los datos y su aplicabilidad en el Sector Sanitario]",2022,"Revista Espanola de Medicina Nuclear e Imagen Molecular","41","1",,"39","42",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123271428&doi=10.1016%2fj.remn.2021.11.002&partnerID=40&md5=24bddd425f6448aa4759052785104083","Currently news and/or articles on the use of Artificial Intelligence and the Big Data are flooding us and this situation has worsened with the pandemic, where great importance has been given to its use and the various applications in all sectors. Some areas of technology and opportunities that are increasingly are more present in our day to day. The sector that has experienced the most growth during this time of pandemic is, without a doubt, the Health sector. The imperative need has fostered and expedited the use of these technologies. The use of data to be able to undertake treatments in a short time, see the evolutions of the different diseases and predict their state is what has driven its use and where due to the situation any help was and is little. From this article we intend to give an explanation of the benefits of using the Artificial Intelligence and the different Big Data techniques, both in the study and evolution of diseases as in their prevention, detection, monitoring and treatment. © 2021 Sociedad Española de Medicina Nuclear e Imagen Molecular","Artificial intelligence; Big Data; Deep Learning; Machine Learning; Natural Language Processing; Radiomics","Article; artificial intelligence; big data; deep learning; health care; natural language processing; pandemic; radiomics; Artificial Intelligence; Big Data; Pandemics",Article,Scopus,2-s2.0-85123271428
"Sylvestre E., Joachim C., Cécilia-Joseph E., Bouzillé G., Campillo-Gimenez B., Cuggia M., Cabié A.","26121411100;57421881100;56491188200;37010975900;51565776900;6507457425;7003925228;","Data-driven methods for dengue prediction and surveillance using real-world and Big Data: A systematic review",2022,"PLoS neglected tropical diseases","16","1",,"e0010056","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123269709&doi=10.1371%2fjournal.pntd.0010056&partnerID=40&md5=61f41e53df3b7b9568a90af8b26068bf","BACKGROUND: Traditionally, dengue surveillance is based on case reporting to a central health agency. However, the delay between a case and its notification can limit the system responsiveness. Machine learning methods have been developed to reduce the reporting delays and to predict outbreaks, based on non-traditional and non-clinical data sources. The aim of this systematic review was to identify studies that used real-world data, Big Data and/or machine learning methods to monitor and predict dengue-related outcomes. METHODOLOGY/PRINCIPAL FINDINGS: We performed a search in PubMed, Scopus, Web of Science and grey literature between January 1, 2000 and August 31, 2020. The review (ID: CRD42020172472) focused on data-driven studies. Reviews, randomized control trials and descriptive studies were not included. Among the 119 studies included, 67% were published between 2016 and 2020, and 39% used at least one novel data stream. The aim of the included studies was to predict a dengue-related outcome (55%), assess the validity of data sources for dengue surveillance (23%), or both (22%). Most studies (60%) used a machine learning approach. Studies on dengue prediction compared different prediction models, or identified significant predictors among several covariates in a model. The most significant predictors were rainfall (43%), temperature (41%), and humidity (25%). The two models with the highest performances were Neural Networks and Decision Trees (52%), followed by Support Vector Machine (17%). We cannot rule out a selection bias in our study because of our two main limitations: we did not include preprints and could not obtain the opinion of other international experts. CONCLUSIONS/SIGNIFICANCE: Combining real-world data and Big Data with machine learning methods is a promising approach to improve dengue prediction and monitoring. Future studies should focus on how to better integrate all available data sources and methods to improve the response and dengue management by stakeholders.",,"dengue; forecasting; human; Big Data; Dengue; Forecasting; Humans",Article,Scopus,2-s2.0-85123269709
"Xue L., Yang C.","57421677500;57421136100;","Copying and Recreation Methods of Painting Works Relying on Mobile Digital Multimedia Big Data Analysis",2022,"Computational Intelligence and Neuroscience","2022",,"7734506","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123250012&doi=10.1155%2f2022%2f7734506&partnerID=40&md5=b82f497ac8fca6b998e48c34792180ab","In order to improve the effect of copying and recreation of painting works, this paper combines mobile digital multimedia big data technology to improve the image coding algorithm, identify the characteristics of existing works, apply the algorithm to the detailed analysis of painting works, and construct the main functional structure modules of the system. Moreover, this paper combines the existing hardware equipment to construct the painting works' recreation system and obtains the image processing module. After the system is constructed, the effect of copying and recreating painting works is analyzed through the mobile digital multimedia big data analysis technology. Finally, this paper constructs the system of this paper through simulation methods and uses experiments to calculate the feature recognition effect and copy effect of the painting works of the system. Through experimental analysis, it can be known that the copying and recreation system of painting works based on mobile digital multimedia big data analysis proposed in this paper can help painters effectively improve the effect of recreation. © 2022 Li Xue and Chuangjian Yang.",,"Big data; Image coding; Image enhancement; Information analysis; Painting; Data technologies; Digital multimedia; Experimental analysis; Features recognition; Functional structure; Image coding algorithms; Image Processing Module; Data handling; algorithm; data analysis; multimedia; recreation; Algorithms; Big Data; Data Analysis; Multimedia; Recreation",Article,Scopus,2-s2.0-85123250012
"Zhang Y.","57421027200;","Development and Application of Artificial Intelligence Multimedia Technology Based on Big Data",2022,"Mobile Information Systems","2022",,"2073091","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123233943&doi=10.1155%2f2022%2f2073091&partnerID=40&md5=36a33356a1ef928bed6a529d845c8cda","Artificial intelligence is an innovative enterprise theme that combines computer science, physiology, science, and other disciplines, including expert programs, technical experiments, games, native language understanding, cognitive rehabilitation, robotics, engineering, recognition, and other fields. The breakthrough growth made the arrival of the era of big data have a fundamental impact on many traditional industries and has also promoted the transformation of teaching methods and methods in the field of education. In the traditional classroom, due to the limitations of teaching resources, teaching time, and location, students are basically passively accepted by the teacher, and the teaching form is single. This will improve in the age of big data with well-developed network message. Modern distance education uses advanced technology such as computer networks, multimedia, and artificial intelligence to build a virtual teaching environment based on the network. It breaks through the limitations of teachers, teaching materials, experimental equipment, and other resources that exist under the traditional teaching model. Using educational resources of the same scale can expand educational capabilities exponentially and at the same time overcomes the unified progress of traditional teaching methods, single teaching methods, and students' dissatisfaction. The shortcomings of differences and personalities that are not reflected are conducive to the improvement of the quality of education. It is the original intention of education to offer students special study resources and to stimulate students' enthusiasm for learning. To enhance the current learning environment, the article builds a personalized multimedia network teaching system based on big data and streaming media technology. By recording the teacher's class process as a video and uploading it to the multimedia network teaching system, students can watch and download related videos online for learning and also communicate online. The system can also provide personalized and intelligent recommendations for students based on student learning habits. The system is developed based on intelligent multimedia method and has the characteristics of security and stability and has done some help for network teaching of educational institutions. © 2022 Yezi Zhang.",,"Artificial intelligence; Big data; Computer aided instruction; E-learning; Education computing; Engineering education; Learning systems; Media streaming; Metadata; Multimedia systems; Teaching; Development and applications; Innovative enterprise; Multimedia networks; Multimedia technologies; Native language; Network teaching; Teachers'; Teaching methods; Teaching systems; Technology-based; Students",Article,Scopus,2-s2.0-85123233943
"Zhao H., Lyu F., Luo Y.","57420282900;57419228500;57419228600;","Research on the Effect of Online Marketing Based on Multimodel Fusion and Artificial Intelligence in the Context of Big Data",2022,"Security and Communication Networks","2022",,"1516543","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123216642&doi=10.1155%2f2022%2f1516543&partnerID=40&md5=4ffdfaa5ebfb29163401985a07151869","Traditional online marketing methods use a single model to predict the advertising conversion rate, but the prediction results are not accurate, and users are not satisfied with the recommendation results. Therefore, this paper proposes an online marketing method based on multimodel fusion and artificial intelligence algorithms under the background of big data. First, it introduces big data technology and analyzes the characteristics of network advertising marketing model (RTB). Second, combined with multitask learning and fusion technology to improve the single model in advertising conversion rate prediction effect, prediction results to further improve the accuracy of results. Then, tF-IDF technology in artificial intelligence algorithm is used to measure the importance of advertising words in online marketing and calculate the contribution degree. Finally, according to XGBoost technology, the multitask fusion model of online marketing effect is classified. Experiments are used to analyze the effect of online marketing. Experimental results show that the proposed method can improve the accuracy of advertising conversion rate prediction and online sales of goods. © 2022 Hongyu Zhao et al.",,"Artificial intelligence; Electronic commerce; Forecasting; Marketing; Advertizing; Artificial intelligence algorithms; Conversion rates; Data technologies; Fusion technology; Marketing models; Multi-model fusion; Online marketing; Rate predictions; Single models; Big data",Article,Scopus,2-s2.0-85123216642
"Zhang Y., Wei J., Wang Y., Tsai S.-B.","57420091400;57419568900;57419748100;55910203400;","An Empirical Study on the Growth of Agricultural Green Total Factor Productivity in the Huanghuai River Economic Zone by Big Data Computing",2022,"Mathematical Problems in Engineering","2022",,"1775027","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123215965&doi=10.1155%2f2022%2f1775027&partnerID=40&md5=06aa9e3ec262266f564efbd74dbcac43","Facing the new form and situation of the Huaihe Economic Zone, it is of great significance to analyze the sources of growth and the intrinsic mechanism of the green total factor productivity of its economic-ecological system, to grasp the spatial and temporal characteristics of green total factor productivity, and to study the influence of each factor on green total factor productivity to achieve sustainable economic development in the Huaihe Economic Zone. Based on the clarification of economic growth theory, green economy theory, carbon cycle theory, and green total factor productivity theory, this paper identifies and discusses the limitation that the existing research literature often ignores the endogenous role of carbon sinks when measuring green total factor productivity. Then, the green total factor productivity of Huaihe Economic Zone based on carbon cycle from 2004 to 2017 is measured using the superefficient nonradial SBM model. Combined with the GML productivity index, it is decomposed into technical progress and technical efficiency and analyzed in comparison with the green total factor productivity without considering ecological purification capacity (carbon sink) from the perspective of time and space. Finally, the spatial Durbin model is used to analyze the effects of seven variables, including the level of economic development, environmental regulation, R&D level, and openness to the outside world, on green total factor productivity in the Huaihe Economic Zone, and to analyze the direct and indirect effects of each variable on green total factor productivity. TFP based on expected output carbon sink and GDP overall outperforms TFP based on expected output GDP only, mainly because the growth of technical efficiency is underestimated when carbon sink is not considered. Technical efficiency and technological progress are equally important for the growth of TFP in an eco-economic perspective. It is of great practical significance for both the comprehensive understanding of the green total factor productivity level and the improvement path of the ecosystem and the coordinated and sustainable development of the Huaihe Economic Zone. © 2022 Yanan Zhang et al.",,"Big data; Carbon; Computation theory; Economic analysis; Economic and social effects; Efficiency; Productivity; Sustainable development; Carbon cycles; Carbon sink; Ecological systems; Economic zones; Empirical studies; Intrinsic mechanisms; New forms; Spatial characteristics; Technical efficiency; Total factor productivity; Environmental regulations",Article,Scopus,2-s2.0-85123215965
"Guo W., Yao K.","57419401900;57419219700;","Supply Chain Governance of Agricultural Products under Big Data Platform Based on Blockchain Technology",2022,"Scientific Programming","2022",,"4456150","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123214647&doi=10.1155%2f2022%2f4456150&partnerID=40&md5=acb26dcf004253fa526d9ecb3e5122f2","The present work serves to improve the stable cooperation relationship among subjects of supply chain such as enterprises, farmers, intermediary organizations, and retailers and enhance the governance and optimization of agricultural product supply chain, thus strengthening the competitiveness of China's agricultural industry. The supply chain governance of agricultural products is taken as the research object. Initially, the stabilities of two supply chain organization modes, ""company and farmer""and ""company, intermediary organization and farmer,""are analyzed by static game analysis. Then, based on the above analysis and the characteristics of blockchain institutional technology, a detailed analyzation is made on the mechanism of supply chain of agricultural products governance based on blockchain technology. Finally, the functional framework of agricultural supply chain governance is designed based on the basic framework of blockchain technology, and analyzation is made on the trust mechanism and contract mechanism of agricultural supply chain governance based on blockchain technology. The research results show that problems such as information and cognitive constraints in agricultural supply chain governance cannot be completely solved only through the evolution of blockchain organizational structure and the supply of governance mechanism, and speculative behavior will still appear. Optimizing the governance of supply chain of agricultural products based on blockchain technology can realize the transformation of its governance scenario. Meanwhile, the blockchain technologies such as deintermediation, demistrust, and intelligent contract play an important role in the process of agricultural supply chain governance, which can make it change in many aspects such as organization mode, application operation, and governance mechanism. The rapid development of new generation information technologies such as blockchain, the Internet of Things, and computer technology makes it possible to comprehensively digitize economic activities such as production and transaction in the supply chain of agricultural products. The present work combines the technical logic of blockchain digital governance with the institutional logic of agricultural product supply chain governance and tries to solve the instability problems caused by imperfect organization, lack of trust, and incomplete contract in agricultural product supply chain governance with the characteristics of blockchain such as deintermediation, demistrust, and intelligent contract. © 2022 Wei Guo and Kai Yao.",,"Big data; Blockchain; Competition; Supply chains; Agricultural industries; Agricultural supply chains; Block-chain; Cooperation relationships; Data platform; Governance mechanisms; Optimisations; Product supply chains; Research object; Supply chain governances; Agricultural products",Article,Scopus,2-s2.0-85123214647
"Liu Y., Tsai S.-B.","57420262200;55910203400;","Dynamic Evolution of Service Trade Network Structure and Influence Mechanism in Countries along the ""belt and Road"" with Big Data Analysis",2022,"Mathematical Problems in Engineering","2022",,"8378137","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123207192&doi=10.1155%2f2022%2f8378137&partnerID=40&md5=90daf9a0f1198552078c82b1adddcbdd","With the accelerating process of economic globalization, the deep adjustment of the international industrial division of labor, and the continuous upgrading of the global value chain, the development of the service industry has entered into a new historical period. This study analyzes the dynamic evolution of trade network structure at three levels: macro, mesa, and micro, and empirically explores its influencing factors. Taking national service trade as the research object, this study analyzes the topological structural characteristics of the food trade network of ""One Belt and One Road""countries by selecting indicators such as network point degree, average path length, trade intensity, intermediation degree, and aggregation coefficient, and further explores the characteristics of association network structure in national service trade network using block model. The topological structural characteristics of the ""Belt and Road""trade network are analyzed from three aspects: overall, local, and individual. Then, we construct a regression model to analyze the factors influencing the dynamic evolution of the service trade network structure of the countries along the ""Belt and Road""through the QAP method. Finally, based on the summary of the research findings, this study puts forward the corresponding considerations and policy recommendations from the perspectives of regional, directional, and reciprocal investment, to benefit more efficient and strategic national service trade under the framework of ""Belt and Road.""Based on the results of this study, this study proposes the following policy recommendations: firstly, actively promote the service trade cooperation with other countries to optimize the market layout; secondly, promote the service industry reform to extend the length of the value chain and improve its position in the value chain; and thirdly, jointly promote the infrastructure construction of ""Belt and Road""with other countries, promote trade liberalization, and reduce the trade volume. Secondly, we should promote the reform of the service industry, extend the length of the value chain, and improve our position in the value chain. Finally, we should vigorously promote the development of innovation, promote the two-way opening of new service industries, adhere to the ""going out""strategy, and cultivate high-quality and excellent technical talents, to promote the overall improvement of the innovation-driven capacity of the service industry with a people-oriented approach. © 2022 Yang Liu and Sang-Bing Tsai.",,"Big data; Dynamics; International trade; Regression analysis; Roads and streets; Topology; Accelerating process; Dynamic evolution; Economic globalization; Influence mechanism; Network influences; Network structures; Policy recommendations; Service industries; Structural characteristics; Value chains; Investments",Article,Scopus,2-s2.0-85123207192
"Wang H., Li Q., Tsai S.-B.","57419391100;57419391200;55910203400;","Network Design Algorithm Implementation for Resilient Transportation System under Continuous Risk Perturbation with Big Data Analysis",2022,"Mathematical Problems in Engineering","2022",,"6032899","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123206322&doi=10.1155%2f2022%2f6032899&partnerID=40&md5=bdf0132885509044b6a1bd9ab7b976e9","With the rapid economic development and urbanization process accelerating, motor vehicle ownership in large cities is increasing year by year; urban traffic congestion, parking difficulties, and other problems are becoming increasingly serious; in ordinary daily life, continuous risk of disturbance, having a flexible transportation system network is more able to alleviate daily congestion in the city, and the main thing about flexible transportation network is its algorithm. It is worth noting that congestion in many cities is generally reflected in the main roads, while many secondary roads and branch roads are underutilized, and the limited road resources in cities are not fully utilized. As an economic and effective road traffic management measure, one-way traffic can balance the spatial and temporal distribution of traffic pressure within the road network, make full use of the existing urban road network capacity, and solve the traffic congestion problem. Therefore, it is of great theoretical and practical significance to develop a reasonable and scientific one-way traffic scheme according to the characteristics of traffic operation in different regions. Based on the fixed demand model, the influence of traffic demand changes is further considered, the lower-level model is designed as an elastic demand traffic distribution model, the excess demand method is used to transform the elastic demand problem into an equivalent fixed demand problem based on the extended network, and the artificial bee colony algorithm based on risk perturbation is designed to solve the two-level planning model. The case study gives a one-way traffic organization optimization scheme that integrates three factors, namely, the average load degree overload limit of arterial roads, the detour coefficient, and the number of on-street parking spaces on feeder roads, and performs sensitivity analysis on the demand scaling factor. © 2022 Hongxiao Wang et al.",,"Big data; Economic and social effects; Highway administration; Highway planning; Motor transportation; Optimization; Risk assessment; Roads and streets; Street traffic control; Traffic congestion; Urban transportation; Algorithm implementation; Economic development; Elastic demand; Large cities; Motor vehicle; Network design; One way traffic; Transportation system; Urban traffic congestion; Vehicle ownership; Sensitivity analysis",Article,Scopus,2-s2.0-85123206322
"Yu J.","57420283200;","Big Data Analytics and Discrete Choice Model for Enterprise Credit Risk Early Warning Algorithm",2022,"Security and Communication Networks","2022",,"3272603","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123203530&doi=10.1155%2f2022%2f3272603&partnerID=40&md5=19c2e892ac443b43424380777e88ca54","A business credit risk early warning algorithm based on big data analysis and discrete selection model is presented to address the issues of poor sample fitting performance, long warning time, and low warning accuracy that plague the traditional enterprise credit risk early warning algorithm. A-share listed enterprises in China were chosen as the credit data source for screening the samples based on big data analysis. After screening, financial failure firms were coupled, and paired samples were created. The credit risk variables, which included financial and corporate governance characteristics, were chosen based on the created samples. The enterprise financial risk submodel and the nonfinancial risk submodel were built based on the enterprise credit risk variables, and the financial and nonfinancial index scores of enterprise customers were evaluated separately to develop a discrete choice model of enterprise credit risk. The algorithm's sample fitting performance was employed to achieve early warning of corporate credit risk. The algorithm based on big data analytics and discrete choice model is compared to the traditional method in order to verify its validity. The findings of the experiment reveal that the algorithm's sample fitting performance is superior to the traditional one, making it more suitable for enterprise credit risk early warning. The proposed model depicts 85% accuracy. © 2022 Jiangbo Yu.",,"Advanced Analytics; Big data; Data Analytics; Data handling; Finance; Screening; Credit data; Credit risk early warnings; Credit risks; Discrete choice models; Performance; Risk variables; Selection model; Submodels; Warning algorithms; Warning time; Risk assessment",Article,Scopus,2-s2.0-85123203530
"Pérez L.F.-R., Blasco Á.R.","57213909356;57420178800;","A Data Science Approach to Cost Estimation Decision Making - Big Data and Machine Learning [Un enfoque de ciencia de datos para la toma de decisiones en la estimación de costes - Big Data y aprendizaje automático]",2022,"Revista de Contabilidad-Spanish Accounting Review","25","1",,"45","57",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123188337&doi=10.6018%2fRCSAR.401331&partnerID=40&md5=71b034505df9c2c723cebdf659c0dd07","Cost estimation may become increasingly difficult, slow, and resource-consuming when it cannot be performed analytically. If traditional cost estimation techniques are usable at all under those circumstances, they have important limitations. This article analyses the potential applications of data science to management accounting, through the case of a cost estimation task posted on Kaggle, a Google data science and machine learning website. When extensive data exist, machine learning techniques can overcome some of those limitations. Applying machine learning to the data reveals non-obvious patterns and relationships that can be used to predict costs of new assemblies with acceptable accuracy. This article discusses the advantages and limitations of this approach and its potential to transform cost estimation, and more widely management accounting. The multinational company Caterpillar posted a contest on Kaggle to estimate the price that a supplier would quote for manufacturing a number of industrial assemblies, given historical quotes for similar assemblies. Hitherto, this problem would have required reverse-engineering the supplier’s accounting structure to establish the cost structure of each assembly, identifying non-obvious relationships among variables. This complex and tedious task is usually performed by human experts, adding subjectivity to the process. ©2022 ASEPUC. Published by EDITUM - Universidad de Murcia. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).","Big Data; Cost Estimation; Data Science; Machine Learning",,Article,Scopus,2-s2.0-85123188337
"Li Z.","57420272800;","Accurate Digital Marketing Communication Based on Intelligent Data Analysis",2022,"Scientific Programming","2022",,"8294891","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123187951&doi=10.1155%2f2022%2f8294891&partnerID=40&md5=338eaec4c88721835ee2b4a3e99a3fb0","In digital marketing, the core advantages of scientific and technological means such as artificial intelligence and big data analysis gradually appear and pay attention to them. This paper studies the accuracy of digital marketing and proposes an intelligent algorithm based on data analysis, which improves the effect of marketing communication. Through the combination of intelligent algorithms and big data analysis, the data are convincing. Through the comparison and improvement of intelligent algorithm logistic regression and XGBoost, this paper puts forward an improved algorithm of XGBoost based on Bayesian optimization parameters, which can improve the efficiency of digital marketing communication and enhance the social influence of digital marketing. © 2022 ZhuoJun Li.",,"Big data; Commerce; Information analysis; Marketing; Bayesian optimization; Digital marketing; Improved * algorithm; Intelligent Algorithms; Intelligent data analysis; Logistics regressions; Marketing communications; Optimization parameter; Social influence; Data handling",Article,Scopus,2-s2.0-85123187951
"Lu M.","56340982400;","Big Data Optimization and Applications in Running Efficiency of Higher Education",2022,"Scientific Programming","2022",,"1044800","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123183922&doi=10.1155%2f2022%2f1044800&partnerID=40&md5=b61656b1d3caf6ef7dfeb5e224ec8519","The world is undergoing great changes that have not been seen at present; colleges and universities can only adapt to social development with a more active and open attitude. Meanwhile, colleges and universities strive to obtain more social resources during the process of gradually embedding social funds into the operation system of universities. In such a backdrop, we establish an optimization model for university running efficiency under limited funds, where the objective function is quadratic and restraint condition is linear. With the help of optimization theory, we have obtained the optimal solution of this optimization model and put forward corresponding suggestions to improve the running efficiency of higher education. © 2022 Mingxia Lu.",,"Efficiency; Optimization; Site selection; Colleges and universities; Data application; Data optimization; Embeddings; High educations; Operation system; Optimization models; Running efficiency; Social development; Social resources; Big data",Article,Scopus,2-s2.0-85123183922
"Jafari N., Besharati M.R., Izadi M., Talebpour A.","57222956512;57215843593;56781128700;56632208600;","COVID and nutrition: A machine learning perspective",2022,"Informatics in Medicine Unlocked","28",,"100857","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123180118&doi=10.1016%2fj.imu.2022.100857&partnerID=40&md5=275f866f511c9e8b6a8e5b37f735e4db","A self-report questionnaire survey was conducted online to collect big data from over 16000 Iranian families (who were the residents of 1000 urban and rural areas of Iran). The resulting data storage contained over 1 M records of data and over 1G records of automatically inferred information. Based on this data storage, a series of machine learning experiments was conducted to investigate the relationship between nutrition and the risk of contracting COVID-19. With highly accurate scores, the findings strongly suggest that foods and water sources containing certain natural bioactive and phytochemical agents may help to reduce the risk of apparent COVID-19 infection. © 2022 The Author(s)","Big data; COVID-19; Diet; Machine learning; Multilayer perceptron; Nutrition; Random forest",,Article,Scopus,2-s2.0-85123180118
"Yu T.","55560614800;","Unbalanced Big Data-Compatible Cloud Storage Method Based on Redundancy Elimination Technology",2022,"Scientific Programming","2022",,"1371778","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123178022&doi=10.1155%2f2022%2f1371778&partnerID=40&md5=bd2c296331e82190282fcb86932ca48a","In order to meet the requirements of users in terms of speed, capacity, storage efficiency, and security, with the goal of improving data redundancy and reducing data storage space, an unbalanced big data compatible cloud storage method based on redundancy elimination technology is proposed. A new big data acquisition platform is designed based on Hadoop and NoSQL technologies. Through this platform, efficient unbalanced data acquisition is realized. The collected data are classified and processed by classifier. The classified unbalanced big data are compressed by Huffman algorithm, and the data security is improved by data encryption. Based on the data processing results, the big data redundancy processing is carried out by using the data deduplication algorithm. The cloud platform is designed to store redundant data in the cloud. The results show that the method in this paper has high data deduplication rate and data deduplication speed rate and low data storage space and effectively reduces the burden of data storage. © 2022 Tingting Yu.",,"Cryptography; Data acquisition; Data reduction; Digital storage; Redundancy; Classifieds; Cloud storages; Data de duplications; Data storage; Data-redundancy; Elimination technology; Redundancy elimination; Storage efficiency; Storage security; Storage spaces; Big data",Article,Scopus,2-s2.0-85123178022
"Hu S., Wang J.","57419758700;57420276600;","Evaluation Algorithm of Ideological and Political Assistant Teaching Effect in Colleges and Universities under Network Information Dissemination",2022,"Scientific Programming","2022",,"3589456","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123176665&doi=10.1155%2f2022%2f3589456&partnerID=40&md5=bfd75ef659385a9ea795f518a62b5508","Ideological and political course is a key course to implement the fundamental task of building morality and cultivating people. Teaching evaluation is an important part of the construction of ideological and political courses. Constructing a perfect teaching evaluation index system is an urgent need to further deepen the teaching reform of ideological and political courses and improve the teaching quality of ideological and political courses. In order to improve the practical application effect of mixed teaching mode, an online and offline mixed teaching effect evaluation method based on big data analysis is proposed. Firstly, the big data in the process of mixed teaching are collected by using big data technology, and the evaluation index system is constructed from three dimensions. The required data are extracted according to the index, and then the association rules between the relevant data of the evaluation index are established, the phase space distribution of the data is obtained. Finally, the constraint parameter analysis method is used to fuse the control variables and explanatory variables of the index-related data to realize the online and offline mixed teaching effect evaluation. The application analysis results show that the method in this paper obtains ideal evaluation results of online and offline mixed teaching effects, which is conducive to improving teaching quality. © 2022 Siyuan Hu and Jingsheng Wang.",,"Information dissemination; Phase space methods; Quality control; Teaching; Colleges and universities; Effect evaluation; Evaluation algorithm; Evaluation indices system; Network information; Offline; Teaching effects; Teaching evaluation; Teaching quality; Teaching reforms; Big data",Article,Scopus,2-s2.0-85123176665
"Lai M.","57419925600;","Analysis of Financial Risk Early Warning Systems of High-Tech Enterprises under Big Data Framework",2022,"Scientific Programming","2022",,"9055294","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123175993&doi=10.1155%2f2022%2f9055294&partnerID=40&md5=7479593b1ab5fdc99d10bea05001514e","With the further development of China's market economy, the competition faced by companies in the market has become more intense, and many companies have difficulty facing pressure and risks. Among the many types of enterprises, high-tech enterprises are the riskiest. The emergence of big data technologies and concepts in recent years has provided new opportunities for financial crisis early warning. Through in-depth study of the theoretical feasibility and practical value of big data indicators, the use of big data indicators to develop an early warning system for financial crises has important theoretical value for breaking through the stagnant predicament of financial crisis early warning. As a result of the preceding context, this research focuses on the influence of big data on the financial crisis early warning model, selects and quantifies the big data indicators and financial indicators, designs the financial crisis early warning model, and verifies its accuracy. The specific research design ideas include the following: (1) We make preliminary preparations for model construction. Preliminary determination and screening of training samples and early warning indicators are carried out, the samples needed to build the model and the early warning indicator system are determined, and the principles of the model methods used are briefly described. First, we perform a significant analysis of financial indicators and screen out early warning indicators that can clearly distinguish between financial crisis companies and nonfinancial crisis companies. (2) We analyze the sentiment tendency of the stock bar comment data to obtain big data indicators. Then, we establish a logistic model based on pure financial indicators and a logistic model that introduces big data indicators. Finally, the two models are tested and compared, the changes in the model's early warning effect before and after the introduction of big data indicators are analyzed, and the optimization effect of big data indicators on financial crisis early warning is tested. © 2022 Maotao Lai.",,"Commerce; Finance; Risk assessment; Screening; Crisis early warnings; Early warning indicators; Early Warning System; Early-warning models; Financial crisis; Financial indicator; Financial risks; High tech; Logistics model; Risk early warning; Big data",Article,Scopus,2-s2.0-85123175993
"Chang Y., Meng S., Chao H.","57419405500;57419405600;57419761800;","Innovative Research on Teaching Method of Taekwondo in College Elective Courses under the Background of Big Data",2022,"Applied Bionics and Biomechanics","2022",,"2329952","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123162354&doi=10.1155%2f2022%2f2329952&partnerID=40&md5=f4f0f5da1f23e249d09e77eeae086b84","In the new stage of the new century, a new technological revolution is coming quietly. This revolution is represented by ""data.""The application of ""big data (B D A)""technology is causing changes in all walks of life, and the use of ""B D A research methods""in the education field will inevitably become a trend. The purpose of this article is an innovative research on the teaching methods of Taekwondo based on the background of B D A in a college elective course. This paper first introduces the core technology of the database by summarizing the basic theory of the database. Based on the current situation of elective Taekwondo teaching in contemporary universities, analyze the current problems and deficiencies and conduct innovative research on college elective Taekwondo teaching methods combined with Beidou technology. This paper systematically expounds the practical connection, method innovation, and implementation path between BDA technology and college elective Taekwondo teaching methods and compares the traditional Taekwondo teaching methods based on BDA technology. Experimental research shows that compared with traditional Taekwondo teaching methods, the performance of university Taekwondo teaching based on data mining (D M I) in the context of B D A is more than 20% higher, which fully reflects its feasibility and the innovation of traditional Taekwondo teaching methods needs to be solved urgently. © 2022 Yahui Chang et al.",,"Big data; Data mining; Engineering education; Radio navigation; Basic theory; Core technology; Data technologies; Education field; Elective course; Innovative research; Research method; Taekwondo; Teaching methods; Technological revolution; Teaching; article; big data; data mining; feasibility study; human; human experiment; taekwondo; teaching",Article,Scopus,2-s2.0-85123162354
"Wang Y.","57419581300;","Animation Character Detection Algorithm Based on Clustering and Cascaded SSD",2022,"Scientific Programming","2022",,"4223295","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123160849&doi=10.1155%2f2022%2f4223295&partnerID=40&md5=db31fb3e2954be2c76117fe992c11fd0","With the evolution of the Internet and information technology, the era of big data is a new digital one. Accordingly, animation IP has been more and more widely welcomed and concerned with the continuous development of the domestic and international animation industry. Hence, animation video analysis will be a good landing application for computers. This paper proposes an algorithm based on clustering and cascaded SSD for object detection of animation characters in the big data environment. In the training process, the improved classification Loss function based on Focal Loss and Truncated Gradient was used to enhance the initial detection effect. In the detection phase, this algorithm designs a small target enhanced detection module cascaded with an SSD network. In this way, the high-level features corresponding to the small target region can be extracted separately to detect small targets, which can effectively enhance the detection effect of small targets. In order to further improve the effect of small target detection, the regional candidate box is reconstructed by a k-means clustering algorithm to improve the detection accuracy of the algorithm. Experimental results demonstrate that this method can effectively detect animation characters, and performance indicators are better than other existing algorithms. © 2022 Yuan Wang.",,"Big data; K-means clustering; Object detection; Signal detection; Clusterings; Continuous development; Data environment; Detection algorithm; Detection effect; Detection phase; Loss functions; Small targets; Training process; Video analysis; Animation",Article,Scopus,2-s2.0-85123160849
"Li L., Tsai S.-B.","57420089900;55910203400;","An Empirical Study on the Precise Employment Situation-Oriented Analysis of Digital-Driven Talents with Big Data Analysis",2022,"Mathematical Problems in Engineering","2022",,"8758898","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123160593&doi=10.1155%2f2022%2f8758898&partnerID=40&md5=e334de9f758951d03479c2d546ab1b91","This paper conducts an in-depth research analysis on the precise employment of college graduates in the context of big data using a number-driven approach. The textual information of the study is obtained by using in-depth interviews, and the evaluation index system of college students' employment quality is constructed by combining the step-by-step coding method with rooting theory. The research on the current situation of employment recommendation platform research and the application status of big data in the employment recommendation platform is explored by using a bibliometric approach. And the innovative use of web crawler technology is used to comprehensively understand the recommendation function and status quo of the same type of recommendation platform, which provides a reference for the research of this platform. Based on the preliminary analysis of platform requirements and overall design, the overall design and functional implementation of the big data employment recommendation platform are carried out by using big data crawler technology, big data architecture technology, text mining technology, database technology, etc. The construction of a recommendation module based on user history information, a recommendation based on real-time user online behavior data, and hybrid recommendation carried out on the recommendation module to grasp all-round the platform is built based on a stakeholder perspective. Based on the platform construction, the initial platform operation and maintenance management mechanism was established from the stakeholder's perspective. The Pearson correlation coefficient is used to objectively evaluate the current situation of talent supply in universities and talent demand in enterprises from the perspective of image and data. In the research on the development status of the big data education industry, the Lorenz curve and Gini coefficient are used to match the status of new big data majors with their college construction volume in each province and provide data support for the reasonable adjustment of majors setting in each province according to the education level. © 2022 Lin Li and Sang-Bing Tsai.",,"Behavioral research; Big data; Correlation methods; Employment; Engineering education; Students; Web crawler; College graduates; Current situation; Empirical studies; Employment recommendations; Employment situation; Evaluation indices system; In-depth interviews; Overall design; Research analysis; Textual information; Quality control",Article,Scopus,2-s2.0-85123160593
"Khan W., Kong L., Brekhna B., Wang S., Yan H.","57218510689;55796966500;57195519483;57418455800;57418644300;","Online Streaming Features Selection via Markov Blanket",2022,"Symmetry","14","1","149","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123071296&doi=10.3390%2fsym14010149&partnerID=40&md5=7f688a7aa0a98e30ab4a74ad303a73e3","Streaming feature selection has always been an excellent method for selecting the relevant subset of features from high-dimensional data and overcoming learning complexity. However, little attention is paid to online feature selection through the Markov Blanket (MB). Several studies based on traditional MB learning presented low prediction accuracy and used fewer datasets as the number of conditional independence tests is high and consumes more time. This paper presents a novel algorithm called Online Feature Selection Via Markov Blanket (OFSVMB) based on a statistical conditional independence test offering high accuracy and less computation time. It reduces the number of conditional independence tests and incorporates the online relevance and redundant analysis to check the relevancy between the upcoming feature and target variable T, discard the redundant features from Parents-Child (PC) and Spouses (SP) online, and find PC and SP simultaneously. The performance OFSVMB is compared with traditional MB learning algorithms including IAMB, STMB, HITON-MB, BAMB, and EEMB, and Streaming feature selection algorithms including OSFS, Alpha-investing, and SAOLA on 9 benchmark Bayesian Network (BN) datasets and 14 real-world datasets. For the performance evaluation, F1, precision, and recall measures are used with a significant level of 0.01 and 0.05 on benchmark BN and real-world datasets, including 12 classifiers keeping a significant level of 0.01. On benchmark BN datasets with 500 and 5000 sample sizes, OFSVMB achieved significant accuracy than IAMB, STMB, HITON-MB, BAMB, and EEMB in terms of F1, precision, recall, and running faster. It finds more accurate MB regardless of the size of the features set. In contrast, OFSVMB offers substantial improvements based on mean prediction accuracy regarding 12 classifiers with small and large sample sizes on real-world datasets than OSFS, Alpha-investing, and SAOLA but slower than OSFS, Alpha-investing, and SAOLA because these algorithms only find the PC set but not SP. Furthermore, the sensitivity analysis shows that OFSVMB is more accurate in selecting the optimal features. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Bayesian network; Big data; Conditional independence test; Feature selection; Markov blanket; Sensitivity analysis; Streaming feature",,Article,Scopus,2-s2.0-85123071296
"Zhou H., Ren H., Royer P., Hou H., Yu X.-Y.","57203850566;36698273600;55964324300;56403939400;35286248800;","Big Data Analytics for Long-Term Meteorological Observations at Hanford Site",2022,"Atmosphere","13","1","136","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123055580&doi=10.3390%2fatmos13010136&partnerID=40&md5=ad44e05b07b3176889d84c0898243af6","A growing number of physical objects with embedded sensors with typically high volume and frequently updated data sets has accentuated the need to develop methodologies to extract useful information from big data for supporting decision making. This study applies a suite of data analytics and core principles of data science to characterize near real-time meteorological data with a focus on extreme weather events. To highlight the applicability of this work and make it more accessible from a risk management perspective, a foundation for a software platform with an intuitive Graphical User Interface (GUI) was developed to access and analyze data from a decommissioned nuclear production complex operated by the U.S. Department of Energy (DOE, Richland, USA). Exploratory data analysis (EDA), involving classical non-parametric statistics, and machine learning (ML) techniques, were used to develop statistical summaries and learn characteristic features of key weather patterns and signatures. The new approach and GUI provide key insights into using big data and ML to assist site operation related to safety management strategies for extreme weather events. Specifically, this work offers a practical guide to analyzing long-term meteorological data and highlights the integration of ML and classical statistics to applied risk and decision science. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Classification; Exploratory data analysis; Graphical User Interface (GUI); Hanford site; Heatwave; High wind; Machine learning; Meteorological data; Random forest","Big data; Data Analytics; Data handling; Decision trees; Graphical user interfaces; Meteorology; Risk management; Weather forecasting; Weather information services; Exploratory data analysis; Extreme weather events; Graphical user interface; Hanford site; Heatwaves; High winds; Meteorological data; Meteorological observation; Physical objects; Random forests; Machine learning; decision making; extreme event; heat wave; instrumentation; machine learning; meteorology; real time; risk assessment; weather forecasting; wind field; Hanford; United States; Washington [United States]",Article,Scopus,2-s2.0-85123055580
"Li W., Zhang Y., Wang J., Li Q., Zhao D., Tang B., Wang S., Shao H.","57389085900;57417240900;55915366000;57221243386;57212589664;57202319910;57417241000;57202324229;","MicroRNA-489 Promotes the Apoptosis of Cardiac Muscle Cells in Myocardial Ischemia-Reperfusion Based on Smart Healthcare",2022,"Journal of Healthcare Engineering","2022",,"2538769","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123052684&doi=10.1155%2f2022%2f2538769&partnerID=40&md5=a3c92b1ba099fd55ca8914dc46a4f810","With the development of information technology, the concept of smart healthcare has gradually come to the fore. Smart healthcare uses a new generation of information technologies, such as the Internet of Things (loT), big data, cloud computing, and artificial intelligence, to transform the traditional medical system in an all-around way, making healthcare more efficient, more convenient, and more personalized. miRNAs can regulate the proliferation, differentiation, and apoptosis of human cells. Relevant studies have also shown that miRNAs may play a key role in the occurrence and development of myocardial ischemia-reperfusion injury (MIRI). This study aims to explore the effects of miR-489 in MIRI. In this study, miR-489 expression in a myocardial ischemia-reperfusion animal model and H9C2 cells induced by H/R was detected by qRT-PCR. The release of lactate dehydrogenase (LDH) and the activity of creatine kinase (CK) was detected after miR-489 knockdown in H9C2 cells induced by H/R. The apoptosis of H9C2 cells and animal models were determined by ELISA. The relationship between miR-489 and SPIN1 was verified by a double fluorescence reporter enzyme assay. The expression of the PI3K/AKT pathway-related proteins was detected by Western blot. Experimental results showed that miR-489 was highly expressed in cardiac muscle cells of the animal model and in H9C2 cells induced by H/R of the myocardial infarction group, which was positively associated with the apoptosis of cardiac muscle cells with ischemia-reperfusion. miR-489 knockdown can reduce the apoptosis of cardiac muscle cells caused by ischemia-reperfusion. In downstream targeting studies, it was found that miR-489 promotes the apoptosis of cardiac muscle cells after ischemia-reperfusion by targeting the inhibition of the SPIN1-mediated PI3K/AKT pathway. In conclusion, high expression of miR-489 is associated with increased apoptosis of cardiac muscle cells after ischemia-reperfusion, which can promote the apoptosis after ischemia-reperfusion by targeting the inhibition of the SPIN1-mediated PI3K/AKT pathway. Therefore, miR-489 can be one of the potential therapeutic targets for reducing the apoptosis of cardiac muscle cells after ischemia-reperfusion. © 2022 Wenhua Li et al.",,"Animals; Cell death; Enzymes; Health care; Heart; Muscle; Animal model; Cardiac muscles; Cloud-computing; Data clouds; H9c2 cells; Ischaemia-reperfusion; Medical systems; Muscle cell; Myocardial ischemia-reperfusion; Myocardial ischemia-reperfusion injuries; RNA; biological marker; creatine kinase; cyt c protein; glycogen synthase kinase 3beta; lactate dehydrogenase; microRNA; microRNA 489; phosphatidylinositol 3 kinase; protein; protein Bax; protein kinase B; unclassified drug; animal cell; animal experiment; animal model; animal tissue; apoptosis; Article; artificial intelligence; big data; cardiac muscle cell; cloud computing; controlled study; creatine kinase blood level; double fluorescence reporter enzyme assay; enzyme activity; enzyme assay; enzyme linked immunosorbent assay; gene amplification; gene control; gene knockdown; genetic transfection; H9c2(2-1) cell line; health care system; heart tissue; internet of things; lactate dehydrogenase blood level; medical technology; myocardial ischemia reperfusion injury; nonhuman; pathogenesis; Pi3K/Akt signaling; protein expression; rat; real time polymerase chain reaction; Western blotting",Article,Scopus,2-s2.0-85123052684
"Lin C., Zheng Y., Xiao X., Lin J.","57221245214;57417330200;55455278500;57192252622;","CXR-RefineDet: Single-Shot Refinement Neural Network for Chest X-Ray Radiograph Based on Multiple Lesions Detection",2022,"Journal of Healthcare Engineering","2022",,"4182191","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123051892&doi=10.1155%2f2022%2f4182191&partnerID=40&md5=c0001d39f42868241c4cacd45f0a87ab","The workload of radiologists has dramatically increased in the context of the COVID-19 pandemic, causing misdiagnosis and missed diagnosis of diseases. The use of artificial intelligence technology can assist doctors in locating and identifying lesions in medical images. In order to improve the accuracy of disease diagnosis in medical imaging, we propose a lung disease detection neural network that is superior to the current mainstream object detection model in this paper. By combining the advantages of RepVGG block and Resblock in information fusion and information extraction, we design a backbone RRNet with few parameters and strong feature extraction capabilities. After that, we propose a structure called Information Reuse, which can solve the problem of low utilization of the original network output features by connecting the normalized features back to the network. Combining the network of RRNet and the improved RefineDet, we propose the overall network which was called CXR-RefineDet. Through a large number of experiments on the largest public lung chest radiograph detection dataset VinDr-CXR, it is found that the detection accuracy and inference speed of CXR-RefineDet have reached 0.1686 mAP and 6.8 fps, respectively, which is better than the two-stage object detection algorithm using a strong backbone like ResNet-50 and ResNet-101. In addition, the fast reasoning speed of CXR-RefineDet also provides the possibility for the actual implementation of the computer-aided diagnosis system. © 2022 Cong Lin et al.",,"Biological organs; Computer aided diagnosis; Inference engines; Large dataset; Medical imaging; Object detection; Object recognition; 'current; Artificial intelligence technologies; Diagnoses of disease; Disease detection; Disease diagnosis; Extraction capability; Features extraction; Lesion detection; Neural-networks; Single-shot; Radiography; aortic disease; Article; atelectasis; big data; cardiomegaly; comparative study; computer assisted diagnosis; deep neural network; detection algorithm; diagnostic accuracy; diagnostic radiologist; feature extraction; ground glass opacity; human; interstitial lung disease; lung calcification; lung consolidation; lung disease; lung fibrosis; lung infiltrate; lung nodule; pleura effusion; pleura thickening; pneumothorax; residual neural network; thorax radiography; x-ray computed tomography; artificial intelligence; pandemic; X ray; Artificial Intelligence; COVID-19; Humans; Neural Networks, Computer; Pandemics; SARS-CoV-2; Tomography, X-Ray Computed; X-Rays",Article,Scopus,2-s2.0-85123051892
"Xia Y., Wang X., Wu W., Shi H.","57210470116;57210474909;57417094900;57210461712;","Rehabilitation of Sepsis Patients with Acute Kidney Injury Based on Intelligent Medical Big Data",2022,"Journal of Healthcare Engineering","2022",,"8414135","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123049307&doi=10.1155%2f2022%2f8414135&partnerID=40&md5=43d90afacd825a4c5fa2eded38861b7e","The objective of this study was to explore rehabilitation of patients with acute kidney injury (AKI) treated with Xuebijing injection by using intelligent medical big data analysis system. Based on Hadoop distributed processing technology, this study designed a medical big data analysis system and tested its performance. Then, this analysis system was used to systematically analyze rehabilitation of sepsis patients with AKI treated with Xuebijing injection. It is found that the computing time of this system does not increase obviously with the increase of cases. The results of systematic analysis showed that the glomerular filtration rate (59.31 ± 3.87% vs 44.53 ± 3.53%) in the experimental group was obviously superior than that in the controls after one week of treatment. The levels of urea nitrogen (9.32 ± 2.21 mmol/L vs. 14.32 ± 0.98 mmol/L), cystatin C (1.65 ± 0.22 mg/L vs. 2.02 ± 0.13 mg/L), renal function recovery time (6.12 ± 1.66 days vs. 8.66 ± 1.17 days), acute physiology and chronic health evaluation system score (8.98 ± 2.12 points vs. 12.45 ± 2.56 points), sequential organ failure score (7.22 ± 0.86 points vs. 8.61 ± 0.97 points), traditional Chinese medicine (TCM) syndrome score (6.89 ± 1.11 points vs. 11.33 ± 1.23 points), and ICU time (16.43 ± 2.37 days vs. 12.15 ± 2.56 days) in the experimental group were obviously lower than those in the controls, and the distinctions had statistical significance (P<0.05). The significant efficiency (37.19% vs. 25.31%) and total effective rate (89.06% vs. 79.06%) in the experimental group were obviously superior than those in the controls, and distinction had statistical significance (P<0.05). In summary, the medical big data analysis system constructed in this study has high efficiency. Xuebijing injection can improve the renal function of sepsis patients with kidney injury, and its therapeutic effect is obviously better than that of Western medicine, and it has clinical application and promotion value. © 2022 Yanmei Xia et al.",,"Data handling; Efficiency; Information analysis; Patient rehabilitation; Patient treatment; Urea; Computing time; Data analysis system; Distributed processing technologies; Experimental groups; Glomerular filtration rate; Performance; Renal functions; Statistical significance; Systematic analysis; Big data; cystatin C; glucose; xuebijing; acute kidney failure; adjuvant therapy; adult; aged; algorithm; APACHE; Article; big data; bladder obstruction; blood oxygen tension; Chinese medicine; comparative study; controlled study; diagnostic imaging; female; glomerulus filtration rate; human; hydronephrosis; intensive care unit; kidney function; major clinical study; male; medical decision making; organ dysfunction score; oxygen saturation; prognosis; resuscitation; sepsis; Sequential Organ Failure Assessment Score; statistical significance; therapy effect; western medicine",Article,Scopus,2-s2.0-85123049307
"He P., Zhang B., Shen S.","57417155500;57417382500;57417110300;","Effects of Out-of-Hospital Continuous Nursing on Postoperative Breast Cancer Patients by Medical Big Data",2022,"Journal of Healthcare Engineering","2022",,"9506915","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123045463&doi=10.1155%2f2022%2f9506915&partnerID=40&md5=e96857f6a3c0dd3b277ac4e60f2f1d19","This study aimed to explore the application value of the intelligent medical communication system based on the Apriori algorithm and cloud follow-up platform in out-of-hospital continuous nursing of breast cancer patients. In this study, the Apriori algorithm is optimized by Amazon Web Services (AWS) and graphics processing unit (GPU) to improve its data mining speed. At the same time, a cloud follow-up platform-based intelligent mobile medical communication system is established, which includes the log-in, my workstation, patient records, follow-up center, satisfaction management, propaganda and education center, SMS platform, and appointment management module. The subjects are divided into the control group (routine telephone follow-up, 163) and the intervention group (continuous nursing intervention, 216) according to different nursing methods. The cloud follow-up platform-based intelligent medical communication system is used to analyze patients' compliance, quality of life before and after nursing, function limitation of affected limb, and nursing satisfaction under different nursing methods. The running time of Apriori algorithm is proportional to the data amount and inversely proportional to the number of nodes in the cluster. Compared with the control group, there are statistical differences in the proportion of complete compliance data, the proportion of poor compliance data, and the proportion of total compliance in the intervention group (P<0.05). After the intervention, the scores of the quality of life in the two groups are statistically different from those before treatment (P<0.05), and the scores of the quality of life in the intervention group were higher than those in the control group (P<0.05). The proportion of patients with limited and severely limited functional activity of the affected limb in the intervention group is significantly lower than that in the control group (P<0.05). The satisfaction rate of postoperative nursing in the intervention group is significantly higher than that in the control group (P<0.001), and the proportion of basically satisfied and dissatisfied patients in the control group was higher than that in the intervention group (P<0.05). © 2022 Peijuan He et al.",,"Big data; Computer graphics; Computer graphics equipment; Data mining; Diseases; Hospitals; Learning algorithms; Nursing; Patient treatment; Program processors; Quality control; Web services; Amazon web services; Apriori algorithms; Breast Cancer; Cancer patients; Communications systems; Control groups; Follow up; Follow-up platform; Medical communication; Quality of life; Graphics processing unit; Amazon Web Services; Apriori algorithm; Article; big data; breast cancer; cancer patient; comparative study; data mining; follow up; human; medical record; mobile application; nursing intervention; patient satisfaction; quality of life",Article,Scopus,2-s2.0-85123045463
"Gakii C., Mireji P.O., Rimiru R.","57205392670;16304767000;55027374700;","Graph Based Feature Selection for Reduction of Dimensionality in Next-Generation RNA Sequencing Datasets",2022,"Algorithms","15","1","21","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123044683&doi=10.3390%2fa15010021&partnerID=40&md5=336c8ac0b4733c92cce3b24365b9aa78","Analysis of high-dimensional data, with more features (p) than observations (N) (p > N), places significant demand in cost and memory computational usage attributes. Feature selection can be used to reduce the dimensionality of the data. We used a graph-based approach, principal component analysis (PCA) and recursive feature elimination to select features for classification from RNAseq datasets from two lung cancer datasets. The selected features were discretized for association rule mining where support and lift were used to generate informative rules. Our results show that the graph-based feature selection improved the performance of sequential minimal optimization (SMO) and multilayer perceptron classifiers (MLP) in both datasets. In association rule mining, features selected using the graph-based approach outperformed the other two feature-selection techniques at a support of 0.5 and lift of 2. The non-redundant rules reflect the inherent relationships between features. Biological features are usually related to functions in living systems, a relationship that cannot be deduced by feature selection and classification alone. Therefore, the graph-based featureselection approach combined with rule mining is a suitable way of selecting and finding associations between features in high-dimensional RNAseq data. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Association rule mining; Big data; Classification; Discretization; Feature selection","Association rules; Big data; Clustering algorithms; Data mining; Feature extraction; Graphic methods; Optimization; Principal component analysis; Discretizations; Features selection; Graph-based; Graph-based features; High dimensional data; Lung Cancer; Principal-component analysis; Recursive feature elimination; Reduction of dimensionality; Rule mining; Classification (of information)",Article,Scopus,2-s2.0-85123044683
"Santonen T., Petsani D., Julin M., Garschall M., Kropf J., Van Der Auwera V., Bernaerts S., Losada R., Almeida R., Garatea J., Muñoz I., Nagy E., Kehayia E., De Guise E., Nadeau S., Azevedo N., Segkouli S., Lazarou I., Petronikolou V., Bamidis P., Konstantinidis E.","15835910300;57214018150;57212065682;55659107800;35746570800;57257693400;57190213784;55906657500;57210795054;57415726300;57415979800;57415726400;6701586207;8567811200;57370023300;8856578500;56347914200;57188568223;57224206537;6603398831;24587110400;","Cocreating a Harmonized Living Lab for Big Data Driven Hybrid Persona Development: Protocol for Cocreating, Testing, and Seeking Consensus",2022,"JMIR Research Protocols","11","1","e34567","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123018867&doi=10.2196%2f34567&partnerID=40&md5=c0f56ce2e8bd29dd5be705abe37c9323","Background: Living Labs are user-centered, open innovation ecosystems based on a systematic user cocreation approach, which integrates research and innovation processes in real-life communities and settings. The Horizon 2020 Project VITALISE (Virtual Health and Wellbeing Living Lab Infrastructure) unites 19 partners across 11 countries. The project aims to harmonize Living Lab procedures and enable effective and convenient transnational and virtual access to key European health and well-being research infrastructures, which are governed by Living Labs. The VITALISE consortium will conduct joint research activities in the fields included in the care pathway of patients: rehabilitation, transitional care, and everyday living environments for older adults. This protocol focuses on health and well-being research in everyday living environments. Objective: The main aim of this study is to cocreate and test a harmonized research protocol for developing big data driven hybrid persona, which are hypothetical user archetypes created to represent a user community. In addition, the use and applicability of innovative technologies will be investigated in the context of various everyday living and Living Lab environments. Methods: In phase 1, surveys and structured interviews will be used to identify the most suitable Living Lab methods, tools, and instruments for health-related research among VITALISE project Living Labs (N=10). A series of web-based cocreation workshops and iterative cowriting processes will be applied to define the initial protocols. In phase 2, five small-scale case studies will be conducted to test the cocreated research protocols in various real-life everyday living settings and Living Lab infrastructures. In phase 3, a cross-case analysis grounded on semistructured interviews will be conducted to identify the challenges and benefits of using the proposed research protocols. Furthermore, a series of cocreation workshops and the consensus seeking Delphi study process will be conducted in parallel to cocreate and validate the acceptance of the defined harmonized research protocols among wider Living Lab communities. Results: As of September 30, 2021, project deliverables Ethics and safety manual and Living lab standard version 1 have been submitted to the European Commission review process. The study will be finished by March 2024. Conclusions: The outcome of this research will lead to harmonized procedures and protocols in the context of big data driven hybrid persona development among health and well-being Living Labs in Europe and beyond. Harmonized protocols enable Living Labs to exploit similar research protocols, devices, hardware, and software for interventions and complex data collection purposes. Economies of scale and improved use of resources will speed up and improve research quality and offer novel possibilities for open data sharing, multidisciplinary research, and comparative studies beyond current practices. Case studies will also provide novel insights for implementing innovative technologies in the context of everyday Living Lab research. © 2022 JMIR Publications. All rights reserved.","Big data; Everyday living; Harmonization; Living Lab; Mobile phone; Personas; Small-scale real-life testing; Technology",,Article,Scopus,2-s2.0-85123018867
"Minh D.H.T., Ngo Y.-N.","56421859800;57191269245;","Compressed SAR Interferometry in the Big Data Era",2022,"Remote Sensing","14","2","390","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123009090&doi=10.3390%2frs14020390&partnerID=40&md5=8d17b4f641559fa6c4673a00df35acca","Modern Synthetic Aperture Radar (SAR) missions provide an unprecedented massive interferometric SAR (InSAR) time series. The processing of the Big InSAR Data is challenging for long-term monitoring. Indeed, as most deformation phenomena develop slowly, a strategy of a processing scheme can be worked on reduced volume data sets. This paper introduces a novel ComSAR algorithm based on a compression technique for reducing computational efforts while maintaining the performance robustly. The algorithm divides the massive data into many mini-stacks and then compresses them. The compressed estimator is close to the theoretical Cramer–Rao lower bound under a realistic C-band Sentinel-1 decorrelation scenario. Both persistent and distributed scatterers (PSDS) are exploited in the ComSAR algorithm. The ComSAR performance is validated via simulation and application to Sentinel-1 data to map land subsidence of the salt mine Vauvert area, France. The proposed ComSAR yields consistently better performance when compared with the state-of-the-art PSDS technique. We make our PSDS and ComSAR algorithms as an open-source TomoSAR package. To make it more practical, we exploit other open-source projects so that people can apply our PSDS and ComSAR methods for an end-to-end processing chain. To our knowledge, TomoSAR is the first public domain tool available to jointly handle PS and DS targets. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","ComSAR; InSAR; PSDS; PSI; Subsidence; TomoSAR; Vauvert","Big data; Interferometry; Subsidence; ComSAR; Distributed scatterers; Interferometric SAR; Performance; Persistent scatterers; PSI; Sentinel-1; Synthetic aperture radar interferometry; TomoSAR; Vauvert; Synthetic aperture radar",Article,Scopus,2-s2.0-85123009090
"Roznik M., Boyd M., Porth L.","57207956093;8121405900;55932820900;","Improving crop yield estimation by applying higher resolution satellite NDVI imagery and high-resolution cropland masks",2022,"Remote Sensing Applications: Society and Environment","25",,"100693","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122985668&doi=10.1016%2fj.rsase.2022.100693&partnerID=40&md5=e18090e5b37a345d28a567a478d49a7d","This research contributes to the literature by investigating if and by how much higher resolution satellite imagery improves crop yield estimation accuracy at the county level when paired with a high-resolution cropland mask. Satellite imagery is an interesting big data source that has potential applications in agriculture. When applying satellite imagery for crop yield estimation, practitioners choose which resolution (i.e., grid size) of images to use. Processing higher resolution images requires greater computing resources compared to lower resolution images. Practitioners may choose to use lower resolution images, but there may be a loss in crop yield model estimation accuracy. The cost of computation has decreased significantly with the advent of cloud computing and open access computing portals such as Google Earth Engine. These technologies have made satellite image processing more economical. The objective of this research is to quantify the crop yield estimation accuracy improvement that could be achieved by using higher resolution normalized difference vegetation index (NDVI) with a cropland mask. NDVI (a measure of crop greenness) data was collected for 48 U.S. states for four crops over 11 years. The crops investigated were corn, soybeans, spring wheat, and winter wheat. Each crop yield regression model estimation showed improved accuracy (R2) as the satellite NDVI resolution increased. Results suggest that using higher resolution satellite NDVI provides more accurate crop yield estimation compared to lower resolution satellite NDVI. This study is believed to be the most comprehensive study to date using NDVI to estimate crop yield, analyzing 48 states in the U.S. and four crops over 11 years using three resolution levels. © 2022","Big data; Crop insurance; Crop yield estimation; NDVI; Remote sensing",,Article,Scopus,2-s2.0-85122985668
"Vovchenko N., Ivanova O., Kostoglodova E., Khapilin S., Sapegina K.","55978022400;56763611400;56764731500;57210812353;57415972300;","Improving the Customs Regulation Framework in the Eurasian Economic Union in the Context of Sustainable Economic Development",2022,"Sustainability (Switzerland)","14","2","755","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122981601&doi=10.3390%2fsu14020755&partnerID=40&md5=54e56a6af789648565800a5621bb9c0d","The formation of a customs administration framework based on the digital economy in the Eurasian Economic Union (EAEU) requires the application of fundamentally new technologies. The successful implementation of digital technologies in the information space of the EAEU presupposes the solution of a number of problems associated with the ensuring the implementation of the concept of sustainable development of the EEU member states in the new economic reality and transition to a new paradigm of customs administration based on the digitalization of the processes of regulation of foreign economic activity. Based on this paradigm, we set the following tasks: To identify trends and substantiate the need for digitalization of the customs administration mechanism in the Eurasian Economic Union based on the use of new technologies; to reveal the meaningful features of digital technologies that are promising for the development of the mechanism of customs administration of the EAEU; consider the applied aspects of the latest information technologies used in the course of EAEU customs administration system digitalization; and assess the prospects for their use, analyze the prospects of organizational, legal and managerial support of this process in the EAEU at the supranational and national levels. The article concludes that within the framework of the digital transformation of the EAEU, new opportunities are opening up for the customs regulation framework, based on the introduction of technologies for analyzing large amounts of data, immersive technologies, blockchain, the use of innovative methods for obtaining and processing customs information (satellite tracking, radio frequency identification), and the introduction of artificial intelligence technologies in customs control processes. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; Customs administration; Digital transformation; Eurasian Economic Union; F02; F15; G28; G38","administration; artificial intelligence; economic activity; economic development; innovation; sustainable development; transformation; Eurasia",Article,Scopus,2-s2.0-85122981601
"Belaud J.-P., Prioux N., Vialle C., Buche P., Destercke S., Barakat A., Sablayrolles C.","55962927100;57209261101;35222800700;55930483000;34569363500;12784105900;8602947100;","Intensive Data and Knowledge-Driven Approach for Sustainability Analysis: Application to Lignocellulosic Waste Valorization Processes",2022,"Waste and Biomass Valorization","13","1",,"583","598",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122973531&doi=10.1007%2fs12649-021-01509-8&partnerID=40&md5=7455ceaf4bfd111c9f413f82cd6218bc","Abstract: The use of circular economy is becoming more and more important, particularly in the field of agriculture, a major provider of waste. In particular, a lot of researches are being done to transform the lignocellulosic waste from agriculture through desired ""sustainable"" processes. Sustainable processes mean economically viable, socially accepted, and environmentally responsible processes. Thanks to the ""life cycle thinking"", it is possible to assess such potential environmental impacts. However, these environmental analyzes require a lot of specific data, whose collection can be long and tedious, or simply impossible in practice. On the other hand, the huge amount of scientific articles describing the processes of valorization of co-products of agriculture constitutes a great, largely under-exploited source of data. Knowledge engineering (KE) tools can be used to compile processes and analyze them. In this paper, we propose an innovative approach, based on intensive data and KE methods, to help a decision maker to choose between different pretreatment processes and different biomasses. The main goal is to develop an intensive, semi-automated data collection approach and an associated tool for assistance with choices in a circular economy context. It is defined by five steps: (1) goal and scope, (2) intensive data and knowledge and the allocation of flows and releasesstructuration and integration, (3) life cycle inventory (LCI), (4) sustainability assessment and (5) analysis and ranking. The study of 13 pretreatment processes of rice straw and corn stover validate our proposal. Graphic abstract: [Figure not available: see fulltext.]. © 2021, The Author(s), under exclusive licence to Springer Nature B.V.","Agricultural waste; Big data; Circular economy; Knowledge engineering; Life cycle assessment; Lignocellulosic biomass","Agricultural robots; Agriculture; Data acquisition; Decision making; Economics; Environmental impact; Life cycle; Automated data collection; Environmentally responsible; Innovative approaches; Life cycle inventories; Lignocellulosic wastes; Pretreatment process; Sustainability analysis; Sustainability assessment; Sustainable development",Article,Scopus,2-s2.0-85122973531
"Ben-Hakoun E., Van De Voorde E., Shiftan Y.","57222661048;6602230250;6603900912;","Trends in Emission Inventory of Marine Traffic for Port of Haifa",2022,"Sustainability (Switzerland)","14","2","908","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122959235&doi=10.3390%2fsu14020908&partnerID=40&md5=4d73d836a45921e4e0a1d580b0eccc88","Located in the Middle East, Haifa Port serves both local and international trade interests (from Asia, Europe, America, Africa, etc.). Due to its strategic location, the port is part of the Belt and Road initiative. This research investigates Haifa Port’s emissions contribution to the existing daily emission inventory level in the area. This research is based on a developed full bottom-up model framework that looks at the single vessel daily voyage through its port call stages. The main data sources for vessel movements used in this research are the Israel Navy’s movements log and the Israel Administration of Shipping and Ports’ (ASP) operational vessel movements and cargo log. The Fuel Consumption (FC) data and Sulfur Content (SC) levels are based on official Israel ASP survey data. The observation years in this research are 2010–2018, with a focus on the Ocean-Going Vessel (OGV) type only. The results show that the vessel fleet calling at Israel ports mainly comprises vessels that have a lower engine tier grade (i.e., Tier 0 and 1), which is considered a heavy contributor to nitrogen oxide (NOx) pollution. The study recommends an additional cost charged (selective tariff) to reflect the external social cost linked to the single vessel air pollution combined with supportive technological infrastructure and economic incentive tools (e.g., electric subsidy) to attract or influence vessel owners to assign vessels equipped with new engine tier grades for calls at Israeli ports. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; Haifa Port Emission Inventory; Selective tariff; The daily port emission inventory","emission inventory; international trade; inventory; Africa; Asia; Europe; Haifa; Israel; Middle East",Article,Scopus,2-s2.0-85122959235
"Koylu C., Kasakoff A.","55770873500;6507029235;","Measuring and mapping long-term changes in migration flows using population-scale family tree data",2022,"Cartography and Geographic Information Science","49","2",,"154","170",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122946537&doi=10.1080%2f15230406.2021.2011419&partnerID=40&md5=e2a815ba6eeb95967f2c41d0a42615f2","Studying migration over a long period is challenging due to lack of data, uneven data quality, and the methodological challenges that arise when analyzing migration over large geographic areas and long time spans with constantly changing political boundaries. Crowd-sourced family tree data are an untapped source of volunteered geographic information generated by millions of users. These trees contain information on individuals such as birth and death places and years, and kinship ties, and have the potential to support analysis of population dynamics and migration over many generations and far into the past. In this article, we introduce a methodology to measure and map long-term changes in migration flows using a population-scale family-tree data set. Our methodology includes many steps such as extracting migration events, temporal periodization, gravity normalization, and producing time-series flow maps. We study internal migration in the continental United States between 1789 and 1924 using birthplaces and birthyears of children from a cleaned, geocoded, and connected set of family trees from Rootsweb.com. To the best of our knowledge, the results are the first migration flow maps that show how the internal migration flows within the U.S. changed over such a long period of time (i.e. 135 years). © 2022 Cartography and Geographic Information Society.","family trees; flow mapping; Migration; U.S. migration history; user-generated big data","Forestry; Mapping; Population statistics; Trees (mathematics); Family tree; Flow mapping; Long-term changes; Migration; Migration flows; Migration history; Tree data; US migration history; User-generated; User-generated big data; Big data; data set; GIS; internal migration; long-term change; migration; population dynamics; United States",Article,Scopus,2-s2.0-85122946537
"Stevens M., Wehrens R., Kostenzer J., Weggelaar-Jansen A.M., de Bont A.","57211482088;36195780700;57194339984;56074136100;15764712900;","Why Personal Dreams Matter: How professionals affectively engage with the promises surrounding data-driven healthcare in Europe",2022,"Big Data and Society","9","1",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122917858&doi=10.1177%2f20539517211070698&partnerID=40&md5=05849065d70d4626aa0151930b596b09","Recent buzzes around big data, data science and artificial intelligence portray a data-driven future for healthcare. As a response, Europe's key players have stimulated the use of big data technologies to make healthcare more efficient and effective. Critical Data Studies and Science and Technology Studies have developed many concepts to reflect on such overly positive narratives and conduct critical policy evaluations. In this study, we argue that there is also much to be learned from studying how professionals in the healthcare field affectively engage with this strong European narrative in concrete big data projects. We followed twelve hospital-based big data pilots in eight European countries and interviewed 145 professionals (including legal, governance and ethical experts, healthcare staff and data scientists) between 2018 and 2020. In this study, we introduce the metaphor of dreams to describe how professionals link the big data promises to their own frustrations, ideas, values and experiences with healthcare. Our research answers the question: how do professionals in concrete data-driven initiatives affectively engage with European Union's data hopes in their ‘dreams’ – and with what consequences? We describe the dreams of being seen, of timeliness, of connectedness and of being in control. Each of these dreams emphasizes certain aspects of the grand narrative of big data in Europe, makes particular assumptions and has different consequences. We argue that including attention to these dreams in our work could help shine an additional critical light on the big data developments and stimulate the development of responsible data-driven healthcare. © The Author(s) 2022.","big data; Dreams; Europe; expectations; healthcare; sociotechnical imaginaries",,Article,Scopus,2-s2.0-85122917858
"Than M., Richardson S., Pickering J.","9740810600;7202589042;57191067456;","Emergency department frequent attenders: big data insights for a big and complex problem",2022,"Emergency medicine journal : EMJ","39","1",,"2","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122903206&doi=10.1136%2femermed-2021-211560&partnerID=40&md5=be3bc9d536a8555acd94c6a90a18ea08",[No abstract available],"emergency department","hospital emergency service; human; retrospective study; Big Data; Emergency Service, Hospital; Humans; Retrospective Studies",Article,Scopus,2-s2.0-85122903206
"Silva D.H., Maziero E.G., Saadi M., Rosa R.L., Silva J.C., Rodriguez D.Z., Igorevich K.K.","57222469382;26025713700;55638443900;36242302100;57223273783;36242276900;57393192800;","Big data analytics for critical information classification in online social networks using classifier chains",2022,"Peer-to-Peer Networking and Applications","15","1",,"626","641",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122895297&doi=10.1007%2fs12083-021-01269-1&partnerID=40&md5=7cab7b2fc59acf4a8287d28b056adf72","Industrial and academic organizations are using online social network (OSN) for different purposes, such as social and economic aspects. Now, OSN is a new mean of obtaining information from people about their preferences, and interests. Due to the large volume of user-generated content, researchers use various techniques, such as sentiment analysis or data mining to evaluate this information automatically. However, the sentiment analysis of OSN content is performed by different methods, but there are some problems to obtain highly reliable results, mainly because of the lack of user profile information, such as gender and age. In this work, a novel dataset is built, which contains the writing characteristics of 160,000 users of the Twitter OSN. Before creating classification models with Machine Learning (ML) techniques, feature transformation and feature selection methods are applied to determine the most relevant set of characteristics. To create the models, the Classifier Chain (CC) transformation technique and different machine learning algorithms are applied to the training set. Simulation results show that the Random Forest, XGBoost and Decision Tree algorithms obtain the best performance results. In the testing phase, these algorithms reached Hamming Loss values of 0.033, 0.033, and 0.034, respectively, and all of them reached the same F1 micro-average value equal to 0.976. Therefore, our proposal based on a multidimensional learning technique using CC transformation overcomes other similar proposals. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Age-group classifier; Big data; Feature selection; Feature transformation; Gender classifier; Multi-label classification","Big data; Classification (of information); Data Analytics; Data mining; Decision trees; Learning algorithms; Machine learning; Metadata; Sentiment analysis; Social networking (online); User profile; Age groups; Age-group classifier; Classifier chains; Economic aspects; Feature transformations; Features selection; Gender classifier; Information classification; Large volumes; Sentiment analysis; Feature extraction",Article,Scopus,2-s2.0-85122895297
"Gomez B., Soar P.J., Downs P.W.","8903060600;57199841988;7003518660;","Good vibrations: Big data impact bedload research",2022,"Earth Surface Processes and Landforms","47","1",,"129","142",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122885459&doi=10.1002%2fesp.5304&partnerID=40&md5=403bcd1511cb148dfc68156d37007361","The fact that bedload transport is an inherently time-variant and location-sensitive fluvial process was revealed by systematic sampling, nine decades ago. Subsequent stream-wide measurements, that frequently incorporated lengthy collection periods, as well as the adoption of standardized sampling procedures, averaged out some temporal and spatial variability. However, continuous, highly resolved, long-period records of transport activity generated by active and passive bedload monitoring on diverse rivers have recently brought this variability into sharp focus. A defining characteristic of these ‘big data’ is that there are many possible bedload transport rates for each discharge and a wide range of discharges associated with each transport rate. Crucially, this incoherent scatter, which is generated by the various factors that affect bedload transport, can no longer be viewed as ‘noise’ that can be averaged out. We demonstrate that, even for small datasets, different methods of reporting and analysing bedload transport records provide different perspectives on the bedload transport rate–flow relation. The inclusion/exclusion of zero values, present in all data that capture the intermittent nature of bedload transport, also affects the relation. To unlock the potential of big data and facilitate the development of bedload transport–flow relations from field measurements, it is essential to employ modes of analysis that are robust to outliers and do not assume that associations between the independent and dependent variables are the same at all levels. © 2021 John Wiley & Sons Ltd.",,"Bed load; Bed-load transport; Bed-load transport rate; Data impact; Flow relations; Fluvial process; Sampling procedures; Systematic sampling; Time variant; Wide measurement; Big data; bedload; data set; fluvial process; sampling; spatiotemporal analysis; vibration",Article,Scopus,2-s2.0-85122885459
"Wang Y.","57411444300;","Big Data Mining Method of Marketing Management Based on Deep Trust Network Model",2022,"International Journal of Circuits, Systems and Signal Processing","16",,,"578","584",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122827508&doi=10.46300%2f9106.2022.16.72&partnerID=40&md5=5a07385ad91fbcf83f734e991bc379d9","Through big data mining, enterprises can deeply understand the consumer preferences, behavior characteristics, market demand and other derived data of customers, so as to provide the basis for formulating accurate marketing strategies. Therefore, this paper proposes a marketing management big date mining method based on deep trust network model. This method first preprocesses the big data of marketing management, including data cleaning, data integration, data transformation and data reduction, and then establishes a big data mining model by using deep trust network to realize the research on the classification of marketing management data. Experimental results show that the proposed method has 99.08% accuracy, the capture rate reaches 88.11%, and the harmonic average between the accuracy and the recall rate is 89.27%, allowing for accurate marketing strategies. © 2022, North Atlantic University Union NAUN. All rights reserved.","Big date mining method; Deep trust network model; Marketing management","Commerce; Consumer behavior; Data integration; Data mining; Information management; Metadata; Strategic planning; Big date mining method; Consumers' preferences; Data mining methods; Deep trust network model; Marketing management; Marketing strategy; Mining enterprise; Mining methods; Network models; Trust networks; Big data",Article,Scopus,2-s2.0-85122827508
"Sadok H., Sakka F., El Maknouzi M.E.H.","55030069000;57221234630;57205599237;","Artificial intelligence and bank credit analysis: A review",2022,"Cogent Economics and Finance","10","1","2023262","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122824000&doi=10.1080%2f23322039.2021.2023262&partnerID=40&md5=8ff4c67c767582c82edbc2bb6763a72c","This article teases out the ramifications of artificial intelligence (AI) use in the credit analysis process by banks and other financing institutions. The unique features of AI models, coupled with the expansion of computing power, make new sources of information (big data) available for creditworthiness assessments. Combined, the use of AI and big data can capture weak signals, whether in the form of interactions or non-linearities between explanatory variables that appear to yield prediction improvements over conventional measures of creditworthiness. At the macroeconomic level, this translates into positive estimates for economic growth. On a micro scale, instead, the use of AI in credit analysis improves financial inclusion and access to credit for traditionally underserved borrowers. However, AI-based credit analysis processes raise enduring concerns due to potential biases and ethical, legal, and regulatory problems. These limits call for the establishment of a new generation of financial regulation introducing the certification of AI algorithms and of data used by banks. © 2022 The Author(s). This open access article is distributed under a Creative Commons Attribution (CC-BY) 4.0 license.","Artificial intelligence; big data; credit analysis; credit scoring; regulation",,Article,Scopus,2-s2.0-85122824000
"Chuah M.-H., Thurusamry R.","36727110400;57234402100;","The relationship between architecture, social, law and market in determine challenges of big data analysis for Malaysia SMEs",2022,"Cogent Business and Management","9","1","2021835","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122817654&doi=10.1080%2f23311975.2021.2021835&partnerID=40&md5=c9482b94f62a82f7faaa919de628c8d2","The study aims to review the challenges of implementing big data by Malaysian small- and medium-size enterprises (SMEs)as well as to figure out the relationship between architecture, social, law and market. Currently, there is limited research and literature examining the challenges of big data analysis in Malaysia SMEs. This paper has opted for a thorough analysis of the complexities of implementing big data at SMEs in Malaysia by applying a comprehensive literature search from different databases, journals, Google scholar, IEEE and content review. This research offers practical insights into the complexities of taking Big Data in SMEs in Malaysia. It is proposed that law, architecture, social and market can be applied by Malaysian SMEs to adopt big data analytic. The authors used quantitative survey. This is done by soliciting criteria from general managers/managing directors/CEOs or data analysts from LinkedIn group. Data wereanalysed by partial least square. This research makes contribution to academics and practitioners to the emerge of predictive model to suit the challenges of big data adoption in Malaysia SMES. © 2022 The Author(s). This open access article is distributed under a Creative Commons Attribution (CC-BY) 4.0 license.","big data; business analysis; Malaysia; partial least square; SMEs",,Article,Scopus,2-s2.0-85122817654
"Chartier C., Lee J.C., Borschel G., Chandawarkar A.","57222278446;56029000100;6603445987;14044666200;","Using Big Data to Assess Legitimacy of Plastic Surgery Information on Social Media",2022,"Aesthetic surgery journal","42","1",,"NP38","NP40",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122770991&doi=10.1093%2fasj%2fsjab253&partnerID=40&md5=2ceaae9b3f37671214cfbd578e4504cf","BACKGROUND: The proliferation of social media in plastic surgery poses significant difficulties for the public in determining legitimacy of information. This work proposes a system based on social network analysis (SNA) to assess the legitimacy of information contributors within a plastic surgery community. OBJECTIVES: The aim of this study was to quantify the centrality of individual or group accounts on plastic surgery social media by means of a model based on academic plastic surgery and a single social media outlet. METHODS: To develop the model, a high-fidelity, active, and legitimate source account in academic plastic surgery (@psrc1955, Plastic Surgery Research Council) appearing only on Instagram (Facebook, Menlo Park, CA) was chosen. All follower-followed relationships were then recorded, and Gephi (https://gephi.org/) was used to compute 5 different centrality metrics for each contributor within the network. RESULTS: In total, 64,737 unique users and 116,439 unique follower-followed relationships were identified within the academic plastic surgery community. Among the metrics assessed, the in-degree centrality metric is the gold standard for SNA, hence this metric was designated as the centrality factor. Stratification of 1000 accounts by centrality factor demonstrated that all of the top 40 accounts were affiliated with a plastic surgery residency program, a board-certified academic plastic surgeon, a professional society, or a peer-reviewed journal. None of the accounts in the top decile belonged to a non-plastic surgeon or non-physician; however, this increased significantly beyond the 50th percentile. CONCLUSIONS: A data-driven approach was able to identify and successfully vet a core group of interconnected accounts within a single plastic surgery subcommunity for the purposes of determining legitimate sources of information. © 2021 The Aesthetic Society. Reprints and permission: journals.permissions@oup.com.",,"human; plastic surgery; reconstructive surgery; social media; surgeon; Big Data; Humans; Reconstructive Surgical Procedures; Social Media; Surgeons; Surgery, Plastic",Article,Scopus,2-s2.0-85122770991
"Ivanov R., Kazantsev F., Zavarzin E., Klimenko A., Milakhina N., Matushkin Y.G., Savostyanov A., Lashin S.","57210985609;55544764800;57222715022;56988827400;57219702610;6603547993;6506101859;23489648100;","ICBrainDB: An Integrated Database for Finding Associations between Genetic Factors and EEG Markers of Depressive Disorders",2022,"Journal of Personalized Medicine","12","1","53","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122758658&doi=10.3390%2fjpm12010053&partnerID=40&md5=149673d0439702467db7c004a66db130","In this study, we collected and systemized diverse information related to depressive and anxiety disorders as the first step on the way to investigate the associations between molecular genetics, electrophysiological, behavioral, and psychological characteristics of people. Keeping that in mind, we developed an internet resource including a database and tools for primary presentation of the collected data of genetic factors, the results of electroencephalography (EEG) tests, and psychological questionnaires. The sample of our study was 1010 people from different regions of Russia. We created the integrated ICBrainDB database that enables users to easily access, download, and further process information about individual behavioral characteristics and psychophysiological responses along with inherited trait data. The data obtained can be useful in training neural networks and in machine learning construction processes in Big Data analysis. We believe that the existence of such a resource will play an important role in the further search for associations of genetic factors and EEG markers of depression. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Database; Depression; EEG; Questionnaires; SNP","adolescent; adult; aged; Article; artificial neural network; Beck Depression Inventory; behavior; big data; brain; depression; DNA isolation; electroencephalogram; electroencephalography; electrophysiology; female; functional magnetic resonance imaging; genetic analysis; genetic marker; genetic susceptibility; Hamilton Depression Rating Scale; human; Internet; major clinical study; male; molecular genetics; phenotype; questionnaire; Russian Federation; State Trait Anxiety Inventory",Article,Scopus,2-s2.0-85122758658
"Yigitcanlar T., Regona M., Kankanamge N., Mehmood R., D’costa J., Lindsay S., Nelson S., Brhane A.","6505536041;57221192835;57207298965;25643246000;57410482400;57410922300;57410041400;57410922400;","Detecting Natural Hazard-Related Disaster Impacts with Social Media Analytics: The Case of Australian States and Territories",2022,"Sustainability (Switzerland)","14","2","810","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122728322&doi=10.3390%2fsu14020810&partnerID=40&md5=8f95db262d91a162f4053020c35cda7c","Natural hazard-related disasters are disruptive events with significant impact on people, communities, buildings, infrastructure, animals, agriculture, and environmental assets. The expo-nentially increasing anthropogenic activities on the planet have aggregated the climate change and consequently increased the frequency and severity of these natural hazard-related disasters, and consequential damages in cities. The digital technological advancements, such as monitoring systems based on fusion of sensors and machine learning, in early detection, warning and disaster response systems are being implemented as part of the disaster management practice in many countries and presented useful results. Along with these promising technologies, crowdsourced social media disaster big data analytics has also started to be utilized. This study aims to form an understanding of how social media analytics can be utilized to assist government authorities in estimating the damages linked to natural hazard-related disaster impacts on urban centers in the age of climate change. To this end, this study analyzes crowdsourced disaster big data from Twitter users in the testbed case study of Australian states and territories. The methodological approach of this study employs the social media analytics method and conducts sentiment and content analyses of location-based Twitter messages (n = 131,673) from Australia. The study informs authorities on an innovative way to analyze the geographic distribution, occurrence frequency of various disasters and their damages based on the geo-tweets analysis. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Australia; Big data; Climate change; Data analytics; Disaster damage; Disaster impact; Natural hazard-related disaster; Social media; Twitter; Urbanization","analytical method; climate change; detection method; disaster management; management practice; natural hazard; social media; urbanization; Australia",Article,Scopus,2-s2.0-85122728322
"Remondino M., Zanin A.","24723327400;57410503600;","Logistics and Agri‐Food: Digitization to Increase Competitive Advantage and Sustainability. Literature Review and the Case of Italy",2022,"Sustainability (Switzerland)","14","2","787","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122726503&doi=10.3390%2fsu14020787&partnerID=40&md5=4c1e27cef1e6775d389f86e887ef7135","This paper examines the current challenges faced by logistics with a focus on the agri‐food sector. After outlining the context, a review of the literature on the relationship between logistics and strategic management in gaining and increasing competitiveness in the agri‐food sector is con-ducted. In particular, the flow of the paper is as follows: after examining the aforementioned managerial problem and its broader repercussions, the paper proceeds to address two main research questions. First, how and by which tools can digitization contribute to improving supply chain management and sustainability in logistics? Second, what are the main managerial and strategic implications and consequences of this for the agri‐food sector in terms of efficiency, effectiveness, cost reduction, and supply chain optimization? Finally, the paper presents Italy as a case study, chosen both for its peculiar internal differences in logistical infrastructures and entrepreneurial management between Northern and Southern regions (which could be at least partially overcome with the use of new technologies and frameworks) and for the importance of the agri‐food sector for the domestic economy (accounting about 25% of the country’s GDP), on which digitization should have positive effects in terms of value creation and sustainability. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Agriculture; Agri‐food; Agri‐tech; Artificial intelligence; Big Data; Blockchain; Digitization; IoT; Logistics; Strategic management","competitiveness; entrepreneur; literature review; supply chain management; sustainability; Italy",Article,Scopus,2-s2.0-85122726503
"Chui C.K., Lin S.-B., Zhang B., Zhou D.-X.","24297847200;55764763400;57221235814;57406593000;","Realization of Spatial Sparseness by Deep ReLU Nets with Massive Data",2022,"IEEE Transactions on Neural Networks and Learning Systems","33","1",,"229","243",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122571686&doi=10.1109%2fTNNLS.2020.3027613&partnerID=40&md5=baf1f885a095109ae11d036062ace4fd","The great success of deep learning poses urgent challenges for understanding its working mechanism and rationality. The depth, structure, and massive size of the data are recognized to be three key ingredients for deep learning. Most of the recent theoretical studies for deep learning focus on the necessity and advantages of depth and structures of neural networks. In this article, we aim at rigorous verification of the importance of massive data in embodying the outperformance of deep learning. In particular, we prove that the massiveness of data is necessary for realizing the spatial sparseness, and deep nets are crucial tools to make full use of massive data in such an application. All these findings present the reasons why deep learning achieves great success in the era of big data though deep nets and numerous network structures have been proposed at least 20 years ago. © 2012 IEEE.","Deep nets; learning theory; massive data; spatial sparseness","Deep learning; Deep net; Depth structure; Learning Theory; Massive data; Network structures; Neural-networks; Spatial sparseness; Theoretical study; Working mechanisms; Metadata; article; big data; deep learning",Article,Scopus,2-s2.0-85122571686
"Sun C., Hou Z., Shen D., Nie T.","55484791300;15622832600;57221404147;8655865200;","Progressive Entity Matching via Cost Benefit Analysis",2022,"IEEE Access","10",,,"3979","3989",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122561072&doi=10.1109%2fACCESS.2021.3139987&partnerID=40&md5=066af15b9d3fb28c15a0c97e59b52f4d","Entity matching (EM) is a fundamental problem in data preprocessing, and is a long running topic in big data analytics and mining communities. In big data era, (nearly) real-time data applications become popular, and call for progressive EM, which produces as many match pairs as possible in very limited time. Previous progressive EM focus on memory based solutions, but disk based solutions are necessary when dirty datasets cannot be fully loaded into memory. To this end, we propose a cost benefit analysis based progressive EM approach, which partitions data according to coarse clustering results and then iteratively schedules data partitions in a greedy way for high progressive resolution. At first, based on estimated record pair similarities, records are fast coarsely clustered; then, record clusters with near average similarities are greedily distributed to the same partitions, and data partitions are cached. After that, cost model is defined with time and space constrains, and benefit model is defined with expected resolution results. On the basis of the cost benefit model, a greedy approximate method is proposed to effectively schedule data for high progressiveness of EM. Finally, we implement extensive experiments over several datasets to evaluate our approach, and show its advantages over existing works. © 2013 IEEE.","Cost benefit model; Data integration; Data partitioning; Entity matching; Progressive","Big data; Clustering algorithms; Costs; Data integration; Data mining; Iterative methods; Cost-benefit models; Cost-benefits analysis; Data application; Data partition; Data partitioning; Data preprocessing; Entity matching; Mining communities; Progressive; Real-time data; Cost benefit analysis",Article,Scopus,2-s2.0-85122561072
"Lee J.-S., Cho I.-S.","57216938525;57216933171;","Extracting the Maritime Traffic Route in Korea Based on Probabilistic Approach Using Automatic Identification System Big Data",2022,"Applied Sciences (Switzerland)","12","2","635","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122545729&doi=10.3390%2fapp12020635&partnerID=40&md5=dca1753e2690409ac4ce239d5ad5efab","To protect the environment around the world, we are actively developing ecofriendly energy. Offshore wind farm generation installed in the sea is extremely large among various energies, and friction with ships occurs regularly. Other than the traffic designated area and the traffic separate scheme, traffic routes in other sea areas are not protected in Korea. Furthermore, due to increased cargo volume and ship size, there is a risk of collisions with marine facilities and marine pollution. In this study, maritime safety traffic routes that must be preserved are created to ensure the safety of maritime traffic and to prevent accidents with ecofriendly energy projects. To construct maritime traffic routes, the analysis area is divided, and ships are classified using big data. These data are used to estimate density, and 50% maritime traffic is chosen. This result is obtained by categorizing the main route, inner branch route, and outer branch route. The Korean maritime traffic route is constructed, and the width of the route is indicated. Furthermore, this route can be applied as a navigation route for maritime autonomous surface ships. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; Density estimation; Maritime safety; Maritime traffic route; MASS",,Article,Scopus,2-s2.0-85122545729
"Kochunov P., Ma Y., Hatch K.S., Schmaal L., Jahanshad N., Thompson P.M., Adhikari B.M., Bruce H., Chiappelli J., Van der Vaart A., Goldwaser E.L., Sotiras A., Ma T., Chen S., Nichols T.E., Hong L.E.","55894435200;57223404578;57216971908;57226219359;8517650500;57217465353;35725098200;57188934170;6506973168;57316699800;56660553700;57317283400;57324547500;57324547400;7102408893;57405055700;","Separating Clinical and Subclinical Depression by Big Data Informed Structural Vulnerability Index and Its impact on Cognition: ENIGMA Dot Product",2022,"Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing","27",,,"133","143",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122545344&partnerID=40&md5=dd90906966d38c8e410188d0dbe17688","Big Data neuroimaging collaborations including Enhancing Neuro Imaging Genetics through Meta-Analysis (ENIGMA) integrated worldwide data to identify regional brain deficits in major depressive disorder (MDD). We evaluated the sensitivity of translating ENIGMA-defined MDD deficit patterns to the individual level. We treated ENIGMA MDD deficit patterns as a vector to gauge the similarity between individual and MDD patterns by calculating ENIGMA dot product (EDP). We analyzed the sensitivity and specificity of EDP in separating subjects with (1) subclinical depressive symptoms without a diagnosis of MDD, (2) single episode MDD, (3) recurrent MDD, and (4) controls free of neuropsychiatric disorders. We compared EDP to the Quantile Regression Index (QRI; a linear alternative to the brain age metric) and the global gray matter thickness and subcortical volumes and fractional anisotropy (FA) of water diffusion. We performed this analysis in a large epidemiological sample of UK Biobank (UKBB) participants (N=17,053/19,265 M/F). Group-average increases in depressive symptoms from controls to recurrent MDD was mirrored by EDP (r2=0.85), followed by FA (r2=0.81) and QRI (r2=0.56). Subjects with MDD showed worse performance on cognitive tests than controls with deficits observed for 3 out of 9 cognitive tests administered by the UKBB. We calculated correlations of EDP and other brain indices with measures of cognitive performance in controls. The correlation pattern between EDP and cognition in controls was similar (r2=0.75) to the pattern of cognitive differences in MDD. This suggests that the elevation in EDP, even in controls, is associated with cognitive performance - specifically in the MDD-affected domains. That specificity was missing for QRI, FA or other brain imaging indices. In summary, translating anatomically informed meta-analytic indices of similarity using a linear vector approach led to better sensitivity to depressive symptoms and cognitive patterns than whole-brain imaging measurements or an index of accelerated aging.",,"biology; brain; cognition; depression; diagnostic imaging; human; major depression; meta analysis; nuclear magnetic resonance imaging; Big Data; Brain; Cognition; Computational Biology; Depression; Depressive Disorder, Major; Humans; Magnetic Resonance Imaging",Article,Scopus,2-s2.0-85122545344
"Qiang Y., Tao X., Gou X., Lang Z., Liu H.","57368385600;57405329000;57272757100;57219605077;56862382200;","Towards a Bibliometric Mapping of Network Public Opinion Studies",2022,"Information (Switzerland)","13","1","17","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122513415&doi=10.3390%2finfo13010017&partnerID=40&md5=3ed54085ba98b0cb5905bc9edf65c4c3","To grasp the current status of network public opinion (NPO) research and explore the knowledge base and hot trends from a quantitative perspective, we retrieved 1385 related papers and conducted a bibliometric mapping analysis on them. Co-occurrence analysis, cluster analysis, co-citation analysis and keyword burst analysis were performed using VOSviewer and CiteSpace software. The results show that the NPO is mainly distributed in the disciplinary fields associated with journalism and communication and public management. There are four main hotspots: analysis of public opinion, analysis of communication channels, technical means and challenges faced. The knowledge base in the field of NPO research includes social media, user influence, and user influence related to opinion dynamic modeling and sentiment analysis. With the advent of the era of big data, big data technology has been widely used in various fields and to some extent can be said to be the research frontier in the field. Transforming big data public opinion into early warning, realizing in-depth analysis and accurate prediction of public opinion as well as improving decision-making ability of public opinion are the future research directions of NPO. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Bibliometrics; Knowledge base; Network public opinion; Research hotspots","Big data; Cluster analysis; Decision making; Mapping; Metadata; Sentiment analysis; Social aspects; Bibliometric; Co-Citation Analysis; Co-occurrence analysis; Current status; Hotspots; Mapping analysis; Network public opinions; Public opinions; Research hotspot; User influences; Knowledge based systems",Article,Scopus,2-s2.0-85122513415
"Cui J., Li X., Wang Y.","57215858748;57225179914;57244767200;","Design of E-commerce Data Scalable Storage System Based on Mobile Internet Communication Technology",2022,"International Journal of Circuits, Systems and Signal Processing","16",,,"595","602",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122508117&doi=10.46300%2f9106.2022.16.74&partnerID=40&md5=40e836ee255a23f4dea5ba55ae4b0db3","The traditional encrypted storage system is inefficient when it encrypts the data of the Internet of Things, and there are few IOT data nodes that can be encrypted in a short time. In order to solve the above problems, a new Internet of Things data effective information encryption storage system is proposed. The hardware and software of the system are mainly designed. The chip selected for the collector is TTSAD251, which can expand the collection range. The processor is set with multiple cores to reduce the system power consumption. The memory uses SPRTAN-2 chip as the structure chip. The software work consists of three parts: collecting effective information of Internet of Things big data, establishing encrypted documents and storing effective information of big data of Internet of Things. In order to detect the working effect of the system, the experimental comparison with the traditional system shows that the proposed encryption storage system can improve the storage range of big data effective information of the Internet of Things by 20.58%, and the work efficiency by 5.64%. Compared with the traditional system, the designed system also has obvious advantages in the number of big data node secrets. In different files, the average number of big data information node encryption in this system is about 166,700. The experimental data show that the designed system has ideal application performance and provides a reliable basis for related fields. © 2022, North Atlantic University Union NAUN. All rights reserved.","Big data expansion; Communications technology; Electronic Commerce; Mobile Internet; Storage system","Big data; Cryptography; Digital storage; Mobile commerce; Big data expansion; Communicationtechnology; Data expansion; Encrypted storages; Hardware and software; Internet communication; Mobile Internet; Scalable storage systems; Storage systems; Traditional systems; Internet of things",Article,Scopus,2-s2.0-85122508117
"Kochunov P., Shen L., van Horn J.D., Thompson P.M.","55894435200;57405303700;7004432310;57217465353;","Session Introduction: Big Data Imaging Genomics",2022,"Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing","27",,,"68","72",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122502347&partnerID=40&md5=d8c75ef8aaf7781ae83e5905abc1812d","This PSB 2022 session addresses challenges and solutions in translating Big Data Imaging Genomics research towards personalized medicine and guiding individual clinical decisions. We will focus on Big Data analyses, pattern recognition, machine learning and AI, electronic health records, guiding diagnostic and treatment decisions and reports of state-of-the-art findings from large and diverse imaging, genomics, and other biomedical datasets.",,"biology; human; machine learning; personalized medicine; Big Data; Computational Biology; Humans; Imaging Genomics; Machine Learning; Precision Medicine",Article,Scopus,2-s2.0-85122502347
"Bobak C.A., Muse M., Giffin K.A., Williamson D.A., Greene C.S., Moore J.H., Wall D.P.","57203320469;57404576900;57159713700;57404907500;12141512300;7405241093;57222475933;","Human Intrigue: Meta-analysis approaches for big questions with big data while shaking up the peer review process",2022,"Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing","27",,,"156","162",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122498354&partnerID=40&md5=ae2e1314405cb456a017cf1776d14c16","Scientific innovation has long been heralded the collaborative effort of many people, groups, and studies to drive forward research. However, the traditional peer review process relies on reviewers acting in a silo to critically judge research. As research becomes more cross-disciplinary, finding reviewers with appropriate expertise to provide feedback on an entire paper is increasingly difficult. We sought to pilot a crowd peer review process that allowed reviewers to interact with one another in the spirit of collaborative science. We focused this session on manuscripts using meta-analysis, to fully embrace the importance of collaborative and open scientific research in the field of biocomputing. Our pilot study found that researchers enjoy a more collaborative peer review process and felt that the process led to higher quality feedback for submitting authors than traditional review offers.",,"biology; human; meta analysis; peer review; pilot study; Big Data; Computational Biology; Humans; Peer Review, Research; Pilot Projects",Article,Scopus,2-s2.0-85122498354
"Shen J., Xu C., Ying Y.","57404788500;57405442900;57404788600;","Construction of intelligent supply chain system of agricultural products based on big data",2022,"Acta Agriculturae Scandinavica Section B: Soil and Plant Science","72","1",,"375","385",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122494335&doi=10.1080%2f09064710.2021.2005131&partnerID=40&md5=6511328c2abdfbeca6b68e2935d49fc4","The core of agricultural product supply chain management is to emphasise the use of integrated thoughts and concepts to guide the management practices of enterprises. That is, the operation of the entire supply chain is guided by consumer demand rather than the individual management of links, and the entire supply chain is strictly controlled as a system. Based on big data technology, this paper constructs a high-performance agricultural product recommendation algorithm. Aiming at the lagging construction of my country’s agricultural product supply chain network facilities, low integration of various operating links, high logistics costs, imbalances in supply and demand, and serious agricultural product quality and safety issues, this paper combines the big data technology to study the agricultural product intelligent supply chain system, and uses the network equilibrium method to construct the agricultural product supply chain network model that considers the effort level of multiple producers and retailers. Finally, this paper proves the reliability of the system model in this paper through experimental research. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","agricultural products; Big data; intelligent supply chain; system construction",,Article,Scopus,2-s2.0-85122494335
"Parthasarathy R., Ayyappan P., Loong S.S., Hussin N., Riaz Ahamed S.S.","57402230500;57209771618;57401807700;44862106900;36677866500;","An Industry 4.0 Vision With An Artificial Intelligence Techniques And Methods",2022,"International Journal of Mechanical Engineering","7","1",,"1314","1322",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122485070&partnerID=40&md5=a7042d6e9f8bcac9ebf7bf6c3c885bce","The current industrial activity is rigid and difficult to change; innovations can hardly be afforded, reduction of raw material prices generally lowers quality and can increase process costs and, in consequence, profit margins decline constantly. All these inconvenient points to the need for the next great revolution in industrial manufacturing, which will lead to several enhance attributes in comparison with actual production activities for this new revolutionary industrial age an attribute that is directly linked with artificial intelligence (AI). In fact, the defined cyber physical systems, connected by the internet of things, take all the attention when referring to the new industry 4.0. But, nevertheless, the new industrial environment will benefit from several tools and applications that complement the real formation of a smart, embedded system that is able to perform the autonomous tasks and most of these revolutionary concepts rest in the same background theory as artificial intelligence does, where the analysis and filtration of huge amounts of incoming information from different types of sensors, assist to the interpretation and suggestion of the most recommended course of action. For that reason, artificial intelligence science suit perfectly with the challenges that arise in the consolidation of the fourth industrial revolution. The aim of this paper is to present and facilitate the proposed Industrial AI ecosystem, which defines a sequential thinking strategy for needs, challenges, technologies and methodologies for developing transformative AI systems for industry. © Kalahari Journals.","Autonomous robots; Big data; Cyber physical systems (CPS); Embedded systems; Industry 4.0. artificial intelligence; Internet of things (IoT); Radio frequency identification (RFID); Wireless sensors networks (WSN)",,Article,Scopus,2-s2.0-85122485070
"Park J.K., Park E.Y.","57404186700;57211751343;","Search performance evaluation of JSON datasets in a big data environment",2022,"International Journal of Mechanical Engineering","7","1",,"198","203",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122477254&partnerID=40&md5=38aa904d1e194c10bc01b2cc5c1016da","BACKGROUND/OBJECTIVES: In recent years, IoT is rapidly increasing, and as smart devices become active in large quantities, the amount of data produced is rapidly increasing. METHODS/STATISTICAL ANALYSIS: In addition, the generated data is a mixture of structured and unstructured data. Big data means processing and analyzing such large amounts of data. XML is widely used for data storage and retrieval. However, the XML method has a problem of inefficiency that takes a lot of time when searching for a large amount of data. FINDINGS: In this paper, we analyzed the relational data model that is widely used in databases, the XML model and JSON model that are widely used for data management. In addition, a JSON model was presented to enable quick search in a big data environment. To analyze the performance of the proposed JSON modeling, a comparison experiment with XML was performed. By performing an experiment using data having 50,000 records, it was found that faster search was performed with the proposed JSON method. In addition, it was verified that the search speed linearly increases even when the data increases. In contrast, the XML model search was slow and unstable. Through experiments, it was found that JSON modeling performed faster search in a big data environment. Improvements/Applications: In the future, we plan to improve the model proposed through database experiments in larger quantities. © Kalahari Journals.","Big data; Evaluation; JSON; Relational database; XML",,Article,Scopus,2-s2.0-85122477254
"Zhu F., Song Z.","57403666100;57404011900;","Systematic Regulation of Personal Information Rights in the Era of Big Data",2022,"SAGE Open","12","1",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122442919&doi=10.1177%2f21582440211067529&partnerID=40&md5=70738ee786bb00ad60b3aedf11a4defb","Big data has an important impact on people’s production and life. The existing legal and judicial protection, sanctions, and mechanisms for the enforcement of information rights have proved insufficient to stem the serious consequences of rampant leakage and illegal activity. Based on Information Full Life Cycle Theory, this article combines qualitative analysis with quantitative analysis, uses data from the Survey Report on App Personal Information Leakage released by China Consumers Association as an example, and finds that illegal access, illegal provisions, and illegal transactions have become important sources of personal information leakage. The main reasons for this problem include limitations of the technologies used, the falsification of informed consent, the lag of legislative protections, and a lack of administrative supervision. Systematic regulation of the right to protect personal information should include a variety of initiatives. First, it should be used to identify who to protect and how to protect them. Second, there needs to be a shift from identifiable subject regulations to risk control. Third, legislation needs to be comprehensive, entailing a shift from fragmented to systemic reforms. Fourth, protection efforts should include supervision, self-regulation, and management. Finally, the jurisdiction of legislation should extend across cyberspace and physical reality as a means to achieve a balance between effective protection and the reasonable use of personal information. © The Author(s) 2022.","big data; personal information rights; systematic regulation",,Article,Scopus,2-s2.0-85122442919
"Hu X.","57403234000;","Fuzzy-TOPSIS Evaluation of Power Product-Service System: A Framework Driven by Big Data",2022,"Energy Engineering: Journal of the Association of Energy Engineering","119","1",,"301","314",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122426049&doi=10.32604%2fEE.2022.014877&partnerID=40&md5=56f21dd79a6383951bc01ca7a9abfea4","Power product-service system (power PSS), which combines industrial electric products with electric energy ser-vices, is an effective solution for power enterprises under the background of the rapid development of power sys-tems. In the life cycle of power PSS, evaluation decision of power PSS alternatives is of great significance for subsequent implementation. To address the power PSS alternative evaluation problem, a power PSS evaluation framework is explored driven by the big data of stakeholder comments. Based on the multi-stakeholder comments of power PSS evaluation decision’s influence factors, the index system is constructed through analyzing and sum-marizing the co-occurrence matrix and semantic network diagram of high-frequency words. To determine the fuzzy index value of power PSS alternative, the stakeholders’ vague opinions expressed by trapezoidal fuzzy number are integrated by group decision method. Fuzzy concept is introduced into the classical Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS) method and fuzzy-TOPSIS method is put forward by using the fuzzy index value. The improved TOPSIS is adopted to sequence the power PSS alternatives. The case of power PSS evaluation of six alternatives for a power enterprise shows that the explored framework is effective and can provide a feasible solution for power PSS alternative evaluation. © 2022, Tech Science Press. All rights reserved.","Big data; Fuzzy-TOPSIS; Power product-service system; Trapezoidal fuzzy number","Fuzzy rules; Life cycle; Semantics; Fuzzy indexes; Fuzzy techniques; Fuzzy-technique for order preference by similarity to an ideal solution; Ideal solutions; Power enterprise; Power product-service system; Power products; Product-service systems; System evaluation; Trapezoidal fuzzy numbers; Big data",Article,Scopus,2-s2.0-85122426049
"Patrucco A., Moretto A., Trabucchi D., Golini R.","57189071886;39863383400;56786351500;26658320200;","How Do Industry 4.0 Technologies Boost Collaborations in Buyer-Supplier Relationships?",2022,"Research Technology Management","65","1",,"48","58",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122356952&doi=10.1080%2f08956308.2021.1999131&partnerID=40&md5=993311893ef80a4baea9a89236387f7c","Overview: Business leaders often consider digital technologies an enabler of new business models and market opportunities, but they often overlook their potential impact on the entire value chain. Considering three Industry 4.0 technologies—big data analytics and cloud computing, track and tracing, and simulation and modeling software—we identify the opportunities and challenges that emerge in the context of managing supply chain relationships. This study uses data from an international survey to test how these three Industry 4.0 technologies increase visibility and integration between buyers and suppliers and how they impact supply chain performance. Our results show mixed evidence: although all three technologies directly improve supply chain performance, big data analytics and cloud computing and simulation and modeling also fully support collaborative supply chain models, while track and tracing tools create more visible supply chains but are detrimental to obtaining higher process integration with suppliers. Surprisingly, buyer-supplier collaboration, in terms of visibility and integration, matters more than the technologies themselves. © Copyright © 2021, Innovation Research Interchange.","Buyer-supplier relationship; Digital technologies; Industry 4.0; Supply chain","Big data; Cloud computing; Computer software; Data Analytics; Industry 4.0; Sales; Visibility; Business leaders; Buyer-supplier integrations; Buyer-supplier relationships; Cloud-computing; Data clouds; Digital technologies; New business models; Simulation and modeling; Supply chain performance; Three industries; Supply chains",Article,Scopus,2-s2.0-85122356952
"Wang S., Liu R., Chu-Ren H.","56025113500;57398420600;57377713200;","Social changes through the lens of language: A big data study of Chinese modal verbs",2022,"PLoS ONE","17","1 January","e0260210","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122327458&doi=10.1371%2fjournal.pone.0260210&partnerID=40&md5=152df48dc1cfeb3a7ed9543656065328","Leech’s corpus-based comparison of English modal verbs from 1961 to 1992 showed the steep decline of all modal verbs together, which he ascribed to continuing changes towards a more equal and less authority-driven society. This study inspired many diachronic and synchronic studies, mostly on English modal verbs and largely assuming the correlation between the use of modal verbs and power relations. Yet, there are continuing debates on sampling design and the choices of corpora. In addition, this hypothesis has not been attested in any other language with comparable corpus size or examined with longitudinal studies. This study tracks the use of Chinese modal verbs from 1901 to 2009, covering the historical events of the New Culture Movement, the establishment of the PRC, the implementation of simplified characters and the completion and finalization of simplification of the Chinese writing system. We found that the usage of modal verbs did rise and fall during the last century, and for more complex reasons. We also demonstrated that our longitudinal end-to-end approach produces convincing analysis on English modal verbs that reconciles conflicting results in the literature adopting Leech’s point-to-point approach. © 2022 Wang et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"article; big data; human; human experiment; language; leech; longitudinal study; nonhuman; social change; writing system; China; Big Data; China; Language; Social Change",Article,Scopus,2-s2.0-85122327458
"Xu Y.","57398235400;","Research on the improvement of accounting work quality of new agricultural business entities under the background of big data",2022,"Acta Agriculturae Scandinavica Section B: Soil and Plant Science","72","1",,"440","453",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122314480&doi=10.1080%2f09064710.2021.2009553&partnerID=40&md5=e416a1d92bd27d2f1d21779fbd328bbe","The accounting of the performance of the new agricultural business entity and the choice of future strategic planning are all directly affected by the quality of its accounting work. Therefore, it is very important to strengthen the accounting supervision of the new agricultural business entity. This article combines big data technology to construct an accounting work quality improvement model of the new agricultural business entity based on big data, uses a clustering algorithm to process a large number of accounting work data of new agricultural business entities and reengineers the inherent expense reimbursement process. In addition, this article builds an expense control module and an image module in the system and integrates the general ledger module with multiple modules such as fund management and payable salary, so that the financial data information of the new agricultural business entities can be processed, summarised and analysed quickly and efficiently in the financial sharing service system. The experimental research results show that the new agricultural business entity accounting work system based on big data technology constructed in this article can effectively improve the accounting work quality of the new agricultural business entity. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","accounting; Big data; computerisation; new agricultural business entity; quality improvement",,Article,Scopus,2-s2.0-85122314480
"Bazmohammadi N., Madary A., Vasquez J.C., Mohammadi H.B., Khan B., Wu Y., Guerrero J.M.","55364701400;57218293139;57203104097;57433937900;55647924400;57397821800;35588010400;","Microgrid Digital Twins: Concepts, Applications, and Future Trends",2022,"IEEE Access","10",,,"2284","2302",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122288081&doi=10.1109%2fACCESS.2021.3138990&partnerID=40&md5=1f1c87a457205c94d8ba76c0329f0404","Following the fourth industrial revolution, and with the recent advances in information and communication technologies, the digital twinning concept is attracting the attention of both academia and industry worldwide. A microgrid digital twin (MGDT) refers to the digital representation of a microgrid (MG), which mirrors the behavior of its physical counterpart by using high-fidelity models and simulation platforms as well as real-time bi-directional data exchange with the real twin. With the massive deployment of sensor networks and IoT technologies in MGs, a huge volume of data is continuously generated, which contains valuable information to enhance the performance of MGs. MGDTs provide a powerful tool to manage the huge historical data and real-time data stream in an efficient and secure manner and support MGs' operation by assisting in their design, operation management, and maintenance. In this paper, the concept of the digital twin (DT) and its key characteristics are introduced. Moreover, a workflow for establishing MGDTs is presented. The goal is to explore different applications of DTs in MGs, namely in design, control, operator training, forecasting, fault diagnosis, expansion planning, and policy-making. Besides, an up-to-date overview of studies that applied the DT concept to power systems and specifically MGs is provided. Considering the significance of situational awareness, security, and resilient operation for MGs, their potential enhancement in light of digital twinning is thoroughly analyzed and a conceptual model for resilient operation management of MGs is presented. Finally, future trends in MGDTs are discussed. © 2013 IEEE.","Artificial intelligence; Automatic learning; Big data; Decision support system; Digital twin; Industry 4.0; Microgrids","Artificial intelligence; Big data; Decision support systems; E-learning; Electronic data interchange; Personnel training; Sensor networks; Simulation platform; Adaptation models; Automatic-learning; Computational modelling; Digital representations; Future trends; High-fidelity modeling; Information and Communication Technologies; Microgrid; Operation management; Industry 4.0",Article,Scopus,2-s2.0-85122288081
"Guo W., Zhang X., Kang L., Gao J., Liu Y.","57020407500;55308505700;57217044330;56151875600;57396466900;","Investigation of Flowback Behaviours in Hydraulically Fractured Shale Gas Well Based on Physical Driven Method",2022,"Energies","15","1","325","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122270992&doi=10.3390%2fen15010325&partnerID=40&md5=7e6a65b2f25ff1484503debb21a3a4af","Due to the complex microscope pore structure of shale, large-scale hydraulic fracturing is required to achieve effective development, resulting in a very complicated fracturing fluid flowback characteristics. The flowback volume is time-dependent, whereas other relevant parameters, such as the permeability, porosity, and fracture half-length, are static. Thus, it is very difficult to build an end-to-end model to predict the time-dependent flowback curves using static parameters from a machine learning perspective. In order to simplify the time-dependent flowback curve into simple parameters and serve as the target parameter of big data analysis and flowback influencing factor analysis, this paper abstracted the flowback curve into two characteristic parameters, the daily flowback volume coefficient and the flowback decreasing coefficient, based on the analytical solution of the seepage equation of multistage fractured horizontal Wells. Taking the dynamic flowback data of 214 shale gas horizontal wells in Weiyuan shale gas block as a study case, the characteristic parameters of the flowback curves were obtained by exponential curve fittings. The analysis results showed that there is a positive correlation between the characteristic parameters which present the characteristics of right-skewed distribution. The calculation formula of the characteristic flowback coefficient representing the flowback potential was established. The correlations between characteristic flowback coefficient and geological and engineering parameters of 214 horizontal wells were studied by spearman correlation coefficient analysis method. The results showed that the characteristic flowback coefficient has a negative correlation with the thickness × drilling length of the high-quality reservoir, the fracturing stage interval, the number of fracturing stages, and the brittle minerals content. Through the method established in this paper, the shale gas flowback curve containing complex flow mechanism can be abstracted into simple characteristic parameters and characteristic coefficients, and the relationship between static data and dynamic data is established, which can help to establish a machine learning method for predicting the flowback curve of shale gas horizontal wells. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big-data analysis; Flowback; Fracturing fluids; Horizontal well; Shale gas","Abstracting; Big data; Data Analytics; Data handling; Fracturing fluids; Gases; Hydraulic fracturing; Machine learning; Pore structure; Shale gas; Big-data analyse; Characteristics parameters; Flowback; Fracture half-length; Fracturing fluid flowback; Gas well; Large-scales; Pores structure; Simple++; Time dependent; Horizontal wells",Article,Scopus,2-s2.0-85122270992
"Sheng W., Xu A., Wu S.","57395368500;57395678200;57395875200;","Fast Access and Retrieval of Big Data Based on Unique Identification",2022,"Intelligent Automation and Soft Computing","32","3",,"1781","1795",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122205837&doi=10.32604%2fIASC.2022.022571&partnerID=40&md5=efc6bd6ce31461382538faf0f4a578e1","In big data applications, the data are usually stored in data files, whose data file structures, field structures, data types and lengths are not uniform. Therefore, if these data are stored in the traditional relational database, it is difficult to meet the requirements of fast storage and access. To solve this problem, we propose the mapping model between the source data file and the target HBase file. Our method solves the heterogeneity of the file object and the universality of the storage conversion. Firstly, based on the mapping model, we design “RowKey”, generation rules and algorithm. Then according to the mapping rules of data file fields with the HBase table column, the data in the data file are transformed into HBase. Finally, the retrieved keywords in “RowKey” are stored and used to achieve fast data retrieval by prefix matching or keyword matching method. Our method has been applied to different projects, which shows these results can be applied to the data conversion from regular row store data file to HBase distributed large data storage and has strong commonality. The method can be widely used in HBase big data storage applications. © 2022, Tech Science Press. All rights reserved.","Big data; Fast retrieval; HBase; Row store; RowKey",,Article,Scopus,2-s2.0-85122205837
"Yuan F., Yang Y., Li Q., Mostafavi A.","57200651504;57206628592;57209808281;57202661272;","Unraveling the Temporal Importance of Community-Scale Human Activity Features for Rapid Assessment of Flood Impacts",2022,"IEEE Access","10",,,"1138","1150",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122071480&doi=10.1109%2fACCESS.2021.3137651&partnerID=40&md5=272eef7d9cd93447fe9b1ab070bc1467","The objective of this research is to explore the temporal importance of community-scale human activity features for rapid assessment of flood impacts. Ultimate flood impact data, such as flood inundation maps and insurance claims, becomes available only weeks and months after the floods have receded. Crisis response managers, however, need near-real-time data to prioritize emergency response. This time lag creates a need for rapid flood impact assessment. Accordingly, community-scale big data (such as satellite imagery) has been utilized for the early estimation of end flood impacts. Some recent studies have shown promising results for using human activity fluctuations as indicators of flood impacts. Existing studies, however, used mainly a single community-scale activity feature for the estimation of flood impacts and have not investigated their temporal importance for indicating flood impacts. Hence, in this study, we examined the importance of heterogeneous human activity features (such as human mobility, visits to points-of-interest, and social media posts) in different flood event stages. Using four community-scale big data categories we derived ten features related to the variations in human activity (e.g., travel, credit card transactions, and online communications) and evaluated their temporal importance for rapid assessment of flood impacts. Using multiple random forest models, we examined the temporal importance of each feature in indicating the extent of flood impacts (measured by the flood insurance claims and flood inundations) in the context of the 2017 Hurricane Harvey in Harris County, Texas. Our findings reveal that 1) fluctuations in human activity index and percentage of congested roads are the most important indicators for rapid flood impact assessment during response and recovery stages; 2) variations in credit card transactions assumed a middle ranking in both response and recovery stages; and 3) patterns of geolocated social media posts (Twitter) were of low importance across flood stages. Insights derived from data analysis reveal the potential for harnessing community-scale data characterizing human activity fluctuations for rapid assessment of flood impacts. The results of this research could rapidly forge a multi-tool enabling crisis managers to identify hotspots with severe flood impacts at various stages then to plan and prioritize effective response strategies. © 2021 IEEE Access. All rights reserved.","Big Data; Codes; Credit cards; Floods; Hurricanes; Indexes; Social networking (online)","Big data; Decision trees; Emergency services; Floods; Insurance; Learning systems; Managers; Satellite imagery; Social networking (online); Code; Credit cards; Human activities; Impact assessments; Index; Rapid assessment; Rapid flood impact assessment; Smart resilience; Social networking (online); Urban floods; Hurricanes",Article,Scopus,2-s2.0-85122071480
"Essam A., Abdel-Fattah M.A., Abdelhamid L.","57223259870;55545556200;57393189300;","Towards Enhancing the Performance of Parallel FP-Growth on Spark",2022,"IEEE Access","10",,,"286","296",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122066878&doi=10.1109%2fACCESS.2021.3137789&partnerID=40&md5=001858d2da3089115bfa6abd0c26f261","Frequent itemset mining (FIM) is a crucial tool for identifying hidden patterns in information. FP-Growth is an FIM algorithm used to find associations. When the data size increases, the execution of FIM algorithms on a single machine suffers from computational problems, such as memory and time consumption. For these reasons, parallel and distributed processing on platforms such as Spark is essential. The parallel frequent pattern (PFP) is the implementation of FP-Growth in Spark. The main problem with PFP is that it does not consider the load balancing between cluster units. This research proposes an enhanced balanced parallel frequent pattern 'EBPFP' algorithm to enhance and balance the PFP. The proposed algorithm (EBPFP) proposes two ideas. First, a strategy for load balancing between groups is proposed to ensure that the items are evenly divided between the nodes, and the cluster resources are used more effectively. Second, the improved conditional pattern base (ICPB) method aims to remove infrequent items from the conditional pattern base before constructing local FP-Trees. The experimental results show that the proposed EBPFP algorithm outperforms PFP, and the difference in running time between EBPFP and PFP was 21.56% and 39.72%, respectively. © 2013 IEEE.","association rule analysis; Big data; data mining; frequent pattern growth algorithm; load balancing; spark","Big data; Clustering algorithms; Job analysis; Scheduling algorithms; Association rule analysis; FP growths; Frequent itemset mining; Frequent pattern growth; Frequent pattern growth algorithm; Growth algorithms; Itemset; Load-Balancing; Partitioning algorithms; Task analysis; Data mining",Article,Scopus,2-s2.0-85122066878
"Parsa K., Hassall M., Naderpour M.","57200191262;37074484100;54795635500;","Enhancing Alarm Prioritization in the Alarm Management Lifecycle",2022,"IEEE Access","10",,,"99","111",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122064446&doi=10.1109%2fACCESS.2021.3137865&partnerID=40&md5=729cc7a61b05e9a1c0fa439c7b0fdad0","Despite significant improvements being made in control and safety systems, near-miss incidents and adverse accidents continue to occur in the industry. Indeed, humans have a vital role in process control success or failure due to their responses to abnormal situations and alarms. A broad study on the alarm system performance shows that good rationalization and accurate prioritization of alarms should increase the efficacy of alarm systems and improve operator decision performances. This paper discusses current gaps in alarm prioritization approaches. It then proposes a method based on Graph theory and metrics capabilities to facilitate and improve the alarm prioritization process. The method is developed based on the causal and layer of protection modeling, followed by measuring the graph metrics for prioritization purposes. Finally, the proposed method is evaluated through implementation in a simulated case study. Results show that this approach facilitates similar achievement to the alarm workshop and produces more valuable data to the cascade of abnormal situations in a structured method and shorter time. © 2013 IEEE.","abnormal situation management; alarm flood; Alarm management; alarm prioritization; alarm rationalization; big data management; causal relation study; graph theory and graph metrics; modeling; safety systems","Accident prevention; Alarm systems; Big data; Floods; Graph theory; Process control; Abnormal situation managements; Alarm Floods; Alarm management; Alarm prioritization; Alarm rationalization; Big data management; Causal relation study; Causal relations; Graph metrics; Graph theory and graph metric; Modeling; Prioritization; Rationalisation; Information management",Article,Scopus,2-s2.0-85122064446
"Sharma P., Kurban H., Dalkilic M.","57223628425;56375007000;8876125600;","DCEM: An R package for clustering big data via data-centric modification of Expectation Maximization",2022,"SoftwareX","17",,"100944","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121962934&doi=10.1016%2fj.softx.2021.100944&partnerID=40&md5=1bf1c1ca0fd1ff5b446634c2b84252cb","Clustering is intractable, so techniques exist to give a best approximation. Expectation Maximization (EM), initially used to impute missing data, is among the most popular. Parameters of a fixed number of probability distributions (PDF) together with the probability of a datum belonging to each PDF are iteratively computed. EM does not scale with data size, and this has hampered its current use. Using a data-centric approach, we insert hierarchical structures within the algorithm to separate high expressive data (HE) from low expressive data (LE): the former greatly affects the objective function at some iteration i, while LE does not. By alternating using either HE or HE+LE, we significantly reduce run-time for EM. We call this new, data-centric EM, EM*. We have designed and developed an R package called DCEM (Data Clustering with Expectation Maximization) to emphasize that data is driving the algorithm. DCEM is superior to EM as we vary size, dimensions, and separability, independent of the scientific domain. DCEM is modular and can be used as either a stand-alone program or a pluggable component. DCEM includes our implementation of the original EM as well. To the best of our knowledge, there is no open source software that specifically focuses on improving EM clustering without explicit parallelization, modified seeding, or data reduction. DCEM is freely accessible on CRAN (Comprehensive R Archive Network). © 2021 The Author(s)","Big data; Data centric machine learning; Expectation Maximization; Open source software; Unsupervised clustering","Big data; Clustering algorithms; Image segmentation; Iterative methods; Machine learning; Maximum principle; Open systems; Probability distributions; Best approximations; Clusterings; Data centric; Data centric machine learning; Data clustering; Expectation Maximization; Fixed numbers; Missing data; Probability: distributions; Unsupervised clustering; Open source software",Article,Scopus,2-s2.0-85121962934
"Agrawal S., Sahu A., Kumar G.","56565994300;57193873763;57207952466;","A conceptual framework for the implementation of Industry 4.0 in legal informatics",2022,"Sustainable Computing: Informatics and Systems","33",,"100650","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121918847&doi=10.1016%2fj.suscom.2021.100650&partnerID=40&md5=2d539d804e44b58c22f409faaa1722e8","The growing number of applications of Industry 4.0 in the field of legal informatics offers huge opportunities for data scientists and academic researchers. The term Industry 4.0 is defined as “the fourth industrial revolution that connects embedded systems to Cyber-Physical-Systems”. It emphasizes the end-to-end digitalization of all physical resources and integrating digital environments with the value chain organizations. Industry 4.0 comprises a variety of technologies such as Cyber-Physical-Systems, Big Data, Internet of Things, Artificial Intelligence, Cloud Computing and Cybersecurity. It has been found that the implementation of these technologies may be useful in achieving the objective of legal informatics. It can help lawmakers to align jurisprudence by providing modern computational technologies to improve and advance the traditional legal justice system. Further, the integration of legal informatics with Industry 4.0 will be strengthening the legal justice system by providing decision-making support, data transparency, real-time monitoring, cost-effective solution, and triple-bottom-line performance in the future. Therefore, the article aims to determine the implementation patterns of Industry 4.0 technologies in legislative institutions and administrations. The study proposes a conceptual framework that integrates Industry 4.0 with legal informatics. The findings show that implementing Industry 4.0 technologies such as Artificial Intelligence, Big Data, and Cloud Computing plays a vital role for legal firms that are currently in the nascent stage of development. © 2021 Elsevier Inc.","Artificial intelligence; Industry 4.0; Legal informatics; Sustainable legal technology","Artificial intelligence; Big data; Cloud computing; Cost effectiveness; Cyber Physical System; Decision making; Embedded systems; Chain organizations; Cloud-computing; Conceptual frameworks; Digital environment; Embedded-system; End to end; Legal informatics; Physical resources; Sustainable legal technology; Value chains; Industry 4.0",Article,Scopus,2-s2.0-85121918847
"Kim M., You S., You J.-S., Kim S.-Y., Park J.H.","57387646600;57195493662;57387646700;57387646800;55388512600;","Income-related mortality inequalities and its social factors among middle-aged and older adults at the district level in aging seoul: An ecological study using administrative big data",2022,"International Journal of Environmental Research and Public Health","19","1","383","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121869285&doi=10.3390%2fijerph19010383&partnerID=40&md5=2ae511d7ee2330af193ce605889f94bb","This study investigated income-related health inequality at sub-national level, focusing on mortality inequality among middle-aged and older adults (MOAs). Specifically, we examined income-related mortality inequality and its social factors among MOAs across 25 districts in Seoul using administrative big data from the National Health Insurance Service (NHIS). We obtained access to the NHIS’s full-population micro-data on both incomes and demographic variables for the entire residents of Seoul. Slope Index of Inequality (SII) and Relative Index of Inequality (RII) were calculated. The effects of social attributes of districts on SIIs and RIIs were examined through ordinary least squares and spatial regressions. There were clear income-related mortality gradients. Cross-district variance of mortality rates was greater among the lowest income group. SIIs were smaller in wealthier districts. Weak spatial correlation was found in SIIs among men. Lower RIIs were linked to lower Gini coefficients of income for both genders. SIIs (men) were associated with higher proportions of special occupational pensioners and working population. Lower SIIs and RIIs (women) were associated with higher proportions of female household heads. The results suggest that increasing economic activities, targeting households with female heads, reforming public pensions, and reducing income inequality among MOAs can be good policy directions. © 2021 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/).","Administrative big data; Districts in Seoul; Ecological study; RII; SII; Socioeconomic characteristics of small area; Spatial analysis","administrative system; aging population; elderly population; mortality; social policy; spatial analysis; adult; aged; aging; article; big data; controlled study; demography; female; gender; Gini coefficient; household; human; income inequality; least square analysis; lowest income group; male; middle aged; mortality rate; national health insurance; pension; pensioner; resident; social aspect; South Korea; spatial regression; aging; epidemiology; health disparity; income; mortality; socioeconomics; South Korea; Seoul [South Korea]; South Korea; Aged; Aging; Big Data; Female; Health Status Disparities; Humans; Income; Male; Middle Aged; Mortality; Seoul; Social Factors; Socioeconomic Factors",Article,Scopus,2-s2.0-85121869285
"Gu Y., Han C., Chen Y., Xing W.W.","57197785183;57203288279;57387332900;55337378000;","Mission Replanning for Multiple Agile Earth Observation Satellites Based on Cloud Coverage Forecasting",2022,"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15",,,"594","608",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121842843&doi=10.1109%2fJSTARS.2021.3135529&partnerID=40&md5=b461ed4a45f32b6c834c758780ea079e","Recent decades have witnessed a tremendous growth in the number of Earth observation satellites (EOSs), which presents a huge challenge for mission planning. For the EOSs with optical sensors particularly, the observation mission is significantly influenced by the uncertainty of cloud coverage, which has been identified as the most dominant factor for the invalidation of remote sensing images. To overcome this uncertainty, uncertainty programming methods, namely, chance constraint programming (CCP), stochastic expectation model, and robust optimization, are put forth. Despite their success, these approaches are limited in that they simplified the complex cloud coverage uncertainty, which may be different from the true cloud conditions, and they did not take the true cloud information into consideration. Motivated by these recent trends toward Big Data of satellite cloud images and machine learning for spatiotemporal prediction, this article explores a dynamic replanning scheme for multiple EOSs based on cloud forecasting. Specifically, we propose a new approach mainly in the following three steps: first, proactive scheduling based on a CCP is implemented and uploaded via ground control; second, cloud forecasting can be continuously conducted relying on the predictive recurrent neural network and the latest satellite cloud image; and third, mission replanning can be conducted according to the initial schedule and relatively accurate cloud information. Simulation results show that the cloud forecasting method is effective, and the replanning approach presents highly efficient and accurate scheduling results. © 2008-2012 IEEE.","Agile Earth observation satellite (AEOS); artificial neural network; cloud forecasting; mission replanning; uncertainty programming","Big data; Computer programming; Constraint theory; Observatories; Optimization; Planning; Production control; Recurrent neural networks; Remote sensing; Rock mechanics; Satellites; Scheduling; Stochastic models; Stochastic systems; Agile earth observation satellite; Chance constraint programming; Cloud coverage; Cloud forecasting; Earth observation satellites; Earth observing systems; Mission re-planning; Re-planning; Uncertainty; Uncertainty programming; Forecasting; artificial neural network; EOS; forecasting method; satellite data; satellite imagery; spatiotemporal analysis; uncertainty analysis",Article,Scopus,2-s2.0-85121842843
"Milton C.L.","7006422102;","The Future of Nurse Scholarship: The Ethical Challenges",2022,"Nursing Science Quarterly","35","1",,"25","27",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121830863&doi=10.1177%2f08943184211051335&partnerID=40&md5=9b58628e5ace9d62e62e9fd5ec80df6a","The advancement of a healthcare discipline is reliant on the disciplines’ ability to produce rigorous scholarship activities and products. The healthcare disciplines, especially nursing, are facing ever-changing priorities as shortages loom and exhaustion permeates the climate. Empirical public health priorities during the pandemic have dominated professional healthcare literature and global health communications. This article shall offer ethical implications for the discipline of nursing as it seeks the advancement of scholarship. Topics include straight-thinking issues surrounding nursing and medicine national policy statements, the big data movement, and evolutionary return of competency-based nurse education. © The Author(s) 2021.","big data; competency-based nursing education; ethics; scholarship","global health; health care delivery; human; medical education; morality; Delivery of Health Care; Fellowships and Scholarships; Global Health; Humans; Morals",Article,Scopus,2-s2.0-85121830863
"Lim H.J., Ro Y.S., Kim K.H., Park J.H., Hong K.J., Song K.J., Shin S.D.","57210135040;57381915000;57194436697;57222635486;55433745900;57037616400;57361449300;","The ed-plann score: A simple risk stratification tool for out-of-hospital cardiac arrests derived from emergency departments in Korea",2022,"Journal of Clinical Medicine","11","1","174","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121772245&doi=10.3390%2fjcm11010174&partnerID=40&md5=14efb027523b3a09d70b6c69e78c3cf6","Early risk stratification of out-of-hospital cardiac arrest (OHCA) patients with insufficient information in emergency departments (ED) is difficult but critical in improving intensive care resource allocation. This study aimed to develop a simple risk stratification score using initial information in the ED. Adult patients who had OHCA with medical etiology from 2016 to 2020 were enrolled from the Korean Cardiac Arrest Research Consortium (KoCARC) database. To develop a scoring system, a backward logistic regression analysis was conducted. The developed scoring system was validated in both external dataset and internal bootstrap resampling. A total of 8240 patients were analyzed, including 4712 in the development cohort and 3528 in the external validation cohort. An ED-PLANN score (range 0–5) was developed incorporating 1 point for each: P for serum pH ≤ 7.1, L for serum lactate ≥ 10 mmol/L, A for age ≥ 70 years old, N for non-shockable rhythm, and N for no-prehospital return of spontaneous circulation. The area under the receiver operating characteristics curve (AUROC) for favorable neurological outcome was 0.93 (95% CI, 0.92–0.94) in the development cohort, 0.94 (95% CI, 0.92–0.95) in the validation cohort. Hosmer–Lemeshow goodness-of-fit tests also indicated good agreement. The ED-PLANN score is a practical and easily applicable clinical scoring system for predicting favorable neurological outcomes of OHCA patients. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; Cardiac arrest; Prediction model; Prognosis",,Article,Scopus,2-s2.0-85121772245
"Tang S., He B., Yu C., Li Y., Li K.","35194592300;7402047189;8126115700;36623945100;57193140976;","A Survey on Spark Ecosystem: Big Data Processing Infrastructure, Machine Learning, and Applications",2022,"IEEE Transactions on Knowledge and Data Engineering","34","1",,"71","91",,6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121746717&doi=10.1109%2fTKDE.2020.2975652&partnerID=40&md5=139e87d614a82b6129196fdcb654dede","With the explosive increase of big data in industry and academic fields, it is important to apply large-scale data processing systems to analyze Big Data. Arguably, Spark is the state-of-the-art in large-scale data computing systems nowadays, due to its good properties including generality, fault tolerance, high performance of in-memory data processing, and scalability. Spark adopts a flexible Resident Distributed Dataset (RDD) programming model with a set of provided transformation and action operators whose operating functions can be customized by users according to their applications. It is originally positioned as a fast and general data processing system. A large body of research efforts have been made to make it more efficient (faster) and general by considering various circumstances since its introduction. In this survey, we aim to have a thorough review of various kinds of optimization techniques on the generality and performance improvement of Spark. We introduce Spark programming model and computing system, discuss the pros and cons of Spark, and have an investigation and classification of various solving techniques in the literature. Moreover, we also introduce various data management and processing systems, machine learning algorithms and applications supported by Spark. Finally, we make a discussion on the open issues and challenges for large-scale in-memory data processing with Spark. © 1989-2012 IEEE.","In-memory data processing; RDD; Shark; Spark","Data handling; Fault tolerance; Information management; Learning algorithms; Machine learning; Surveys; Academic fields; Computing system; Data-processing system; In-memory data processing; Machine-learning; Performance; Processing infrastructures; Programming models; Resident distributed dataset; Shark; Big data",Article,Scopus,2-s2.0-85121746717
"Yu W., Sun H., Wu J., Lv Y., Shang X., Wang X.","57218823984;55794370100;56424134200;57338219500;57216127582;57384410800;","Mapping multimodal random accessibility using smart card data: a case study of bus and subway stations in Beijing",2022,"Transportation Planning and Technology","45","1",,"76","99",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121724584&doi=10.1080%2f03081060.2021.2017206&partnerID=40&md5=21ca99a186a6fa1a5730d331a9b4ef3d","Research on multimodal accessibility under uncertain travel time has become a significant issue. Existing studies on accessibility lack a direct integration of multi-source data accessibility evaluation methods. This paper develops a multimodal random accessibility model (MR model). Multi-source data is integrated with a built-in joint calculation method of walking time, waiting time, and transit time while considering the effects of both the travel time budget in the time dimension and distance friction parameter in the spatial dimension. Taking Beijing as an example, accessibility generally shows a downward trend from the center of the city to the suburbs, especially along the subway lines, and there is a positive correlation between traffic flow and accessibility. The low-accessible high-flow area is mainly distributed in areas away from the city center and at the end of subway lines. These results could help transport planners formulate more reasonable public transport planning policies. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","Accessibility; Beijing; Big data; multimodal; point of interest; public transport; random accessibility model; travel time budget","Budget control; Data integration; Smart cards; Subway stations; Travel time; Accessibility model; Beijing; Multi-modal; Multisource data; Point of interest; Public transport; Random accessibility; Random accessibility model; Subway lines; Travel time budgets; Big data",Article,Scopus,2-s2.0-85121724584
"Reed-Berendt R., Dove E.S., Pareek M., UK-REACH Study Collaborative Group","57222622024;49360947900;54936473500;","The Ethical Implications of Big Data Research in Public Health: “Big Data Ethics by Design” in the UK-REACH Study",2022,"Ethics and Human Research","44","1",,"2","17",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121671117&doi=10.1002%2feahr.500111&partnerID=40&md5=3005b41d67d908e4369fe1656001e2a5","In this article, we analyze legal and ethical issues raised in Big Data health research projects in the Covid-19 era and consider how these issues might be addressed in ways that advance positive values (e.g., furtherance of respect for persons and accordance with relevant legal frameworks) while mitigating or eliminating any negative aspects (e.g., exacerbation of social inequality and injustice). We apply this analysis specifically to UK-REACH (The United Kingdom Research Study into Ethnicity and Covid-19 Outcomes in Healthcare Workers), a project with which we are involved. We argue that Big Data projects like UK-REACH can be conducted in an ethically robust manner and that funders and sponsors ought to encourage similar projects to drive better evidence-based public policy in public health. As part of this, we advocate that a Big Data ethics-by-design approach be undertaken when such projects are constructed. This principle extends the work of those who advocate ethics by design by addressing prominent issues in Big Data health research projects; it holds that ethical values and principles in Big Data health research projects are best adhered to when they are already integrated into the project aims and methods at the design stage. In advocating this principle, we present a unique perspective regarding pressing ethical problems around large-scale, data-driven Covid-19 research, as well as legal issues associated with processing ostensibly anonymized health data. © 2021 The Authors. Ethics & Human Research published by Wiley Periodicals LLC on behalf of The Hastings Center.","Big Data; Big Data ethics; Big Data health research; Covid-19; Covid-19 research; data linkage; health data; human research ethics; UK-REACH study","health care personnel; human; public health; Big Data; COVID-19; Health Personnel; Humans; Public Health; SARS-CoV-2",Article,Scopus,2-s2.0-85121671117
"Punithavathi R., Kowsigan M., Shanthakumari R., Zivkovic M., Bacanin N., Sarac M.","26428382900;57194773637;57208394448;57208755936;37028223900;36462597700;","Protecting Data Mobility in Cloud Networks Using Metadata Security",2022,"Computer Systems Science and Engineering","42","1",,"105","120",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121658979&doi=10.32604%2fCSSE.2022.020486&partnerID=40&md5=dc534131c2c1ad6bd339318de5df1f5c","At present, health care applications, government services, and banking applications use big data with cloud storage to process and implement data. Data mobility in cloud environments uses protection protocols and algorithms to secure sensitive user data. Sometimes, data may have highly sensitive information, leading users to consider using big data and cloud processing regardless of whether they are secured are not. Threats to sensitive data in cloud systems produce high risks, and existing security methods do not provide enough security to sensitive user data in cloud and big data environments. At present, several security solutions support cloud systems. Some of them include Hadoop Distributed File System (HDFS) baseline Kerberos security, socket layer-based HDFS security, and hybrid security systems, which have time complexity in providing security interactions. Thus, mobile data security algorithms are necessary in cloud environments to avoid time risks in providing security. In our study, we propose a data mobility and security (DMoS) algorithm to provide security of data mobility in cloud environments. By analyzing metadata, data are classified as secured and open data based on their importance. Secured data are sensitive user data, whereas open data are open to the public. On the basis of data classification, secured data are applied to the DMoS algorithm to achieve high security in HDFS. The proposed approach is compared with the time complexity of three existing algorithms, and results are evaluated. © This work is licensed under a Creative Commons Attribution 4.0 International License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.","Big data; Cloud computing; Data mobility; Data security; DMoS algorithm","Big data; Cloud computing; Cloud data security; Complex networks; Digital storage; Metadata; Network security; Security systems; Cloud environments; Cloud systems; Cloud-computing; Data mobility; Data mobility and security algorithm; Distributed file systems; Open datum; Security algorithm; Time complexity; User data; File organization",Article,Scopus,2-s2.0-85121658979
"Yao Y., Lu Y., Guan Q., Wang R.","57191632843;57194620633;55838509944;57202886161;","Can parkland mitigate mental health burden imposed by the COVID-19? A national study in China",2022,"Urban Forestry and Urban Greening","67",,"127451","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121644338&doi=10.1016%2fj.ufug.2021.127451&partnerID=40&md5=30721ab2a1c621dae1a45d8a74b1fd67","The worldwide coronavirus disease 2019 (COVID-19) pandemic has seriously affected not only physical health but also mental wellbeing (i.e mental stress and suicide intention) of numerous urban inhabitants across the globe. While many studies have elucidated urban parkland enhances and mental wellbeing of urban residents, the potential for parkland to mitigate mental health burden imposed by the COVID-19 has received no attention. This nationwide study systematically explored the association between parkland, the COVID-19 pandemic situation and mental wellbeing from 296 cities in China. The study innovatively used big data from Baidu Search Engine to assess city-level mental wellbeing, thereby enabling comparisons among cities. The results show that the provision of parkland is positively associated with mental wellbeing during the COVID-19 epidemic. For COVID-19-related indicators, the geographical distance to Wuhan city, work resumption rate, and travel intensity within the city are also positively associated with mental wellbeing, while the number of COVID-19 infections and the proportion of migrants from Hubei Province for each city are negatively associated with mental wellbeing. Last, the most important finding is that parkland reduces the negative effect of COVID-19 on mental wellbeing during the COVID-19 epidemic. To achieve the goal of promoting mental wellbeing through urban planning and design during the future pandemics, policymakers and planners are advised to provide more well-maintained and accessible parkland and encourage residents to use them with proper precautions. © 2021 Elsevier GmbH","Big data; Buffer effect; COVID-19; Mental wellbeing; Parkland","COVID-19; mental health; policy making; spatiotemporal analysis; urban area; urban planning; China; Hubei; Wuhan",Article,Scopus,2-s2.0-85121644338
"Mills D., Pudney S., Pevcin P., Dvorak J.","57216146482;55537159500;55313613500;55599571600;","Evidence-based public policy decision-making in smart cities: Does extant theory support achievement of city sustainability objectives?",2022,"Sustainability (Switzerland)","14","1","3","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121632613&doi=10.3390%2fsu14010003&partnerID=40&md5=ad7351db2bdfa4adadbcbc4669870aa9","Evidence-based decision making is promoted as offering efficiency and effectiveness; however, its uptake has faced barriers such as underdeveloped supporting culture, limited access to evidence, and evidence that is not fully relevant. Smart city conceptualizations offer economic and environmental sustainability and better quality of life through evidence-based policy decision-making. We wondered whether smart city theory and practice has advanced the knowledge of evidence-based decision-making. We searched major databases for literature containing a mention of smart cities, decision-making, and policy. We identified relevant literature from a range of disciplines and supplemented these by following backwards and forwards citations. Evidence-based decision-making was found mostly in literature regarding the theory and practice of smart city operations, and, to lesser extents, the articles regarding policy decisions and tactical decisions. Better decision-making which supported the achievement of city sustainability objectives was reported in some articles; however, we found significant obstacles to the further achievement of city objectives in the areas of underachievement in collaborative decision-making, privileging of big data evidence, and artificial intelligence agents as decision-makers. We assembled a definition of smart city decision-making and developed an agenda of research which will support city governments, theorists, and practitioners in better achieving sustainability through improved decision-making. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Artificial intelligence; Big data; Collaboration; Decision-making; Evidence-based; Local government; Public administration policy; Smart city","artificial intelligence; decision making; quality of life; smart city; social policy; sustainability",Article,Scopus,2-s2.0-85121632613
"Manzi G., Miotti C., Mariani M.V., Papa S., Luongo F., Scoccia G., De Lazzari B., De Lazzari C., Benza R.L., Fedele F., Vizza C.D., Badagliacca R.","57189389386;57214838629;57204706732;44061627100;57205540266;57201898244;57216843879;6603866499;7004568891;7005613763;7003962691;7801601632;","Computational simulator models and invasive hemodynamic monitoring as tools for precision medicine in pulmonary arterial hypertension",2022,"Journal of Clinical Medicine","11","1","82","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121588376&doi=10.3390%2fjcm11010082&partnerID=40&md5=4b30cf2f3ed38163bf36200585ba8b5a","Precision medicine, providing the right therapeutic strategy for the right patient, could revolutionize management and prognosis of patients affected by cardiovascular diseases. Big data and artificial intelligence are pivotal for the realization of this ambitious design. In the setting of pulmonary arterial hypertension (PAH), the use of computational models and data derived from ambulatory implantable hemodynamic monitors could provide useful information for tailored treatment, as requested by precision medicine. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; Computational models; Implantable hemodynamic monitors; Precision medicine; Pulmonary arterial hypertension",,Article,Scopus,2-s2.0-85121588376
"Chang V., Li X., Zhang J., Xu Q., Valverde R.F.","56926234700;57381788100;57217048669;57381606900;57381256800;","Brand personality in cultural tourism and sustainable development by using big data analytics",2022,"International Journal of Business and Systems Research","16","1",,"125","139",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121519807&doi=10.1504%2fIJBSR.2022.119599&partnerID=40&md5=497cbde7c5cdaf2d457e1eb79506d3b3","The development of science and technology has entered the era of big data today. The method of big data has provided a new way of thinking and methods for analysing and solving problems in scientific projects. Many countries benefit from cultural tourism for economic development, but they are concerned about the sustainability of these cultural resources. The paper explores the opportunity of big data in cultural tourism and sustainable development as a tool that can help to understand the needs of tourists and their relationship to brand personality. Based on Rauschnabel et al.'s six university brand personality dimensions, this research aims to develop a model that could explain the brand personality that can support sustainable tourism by using questionnaires and statistical analysis. Data was collected through an online questionnaire survey with a convenience sample of 300 tourists in China. Results show that brand personality improves tourist satisfaction and tourism commitment. Meanwhile, tourist satisfaction is related to tourism commitment in terms of tourism affective commitment and tourism normative commitment. However, the constructs 'acceptable', 'productive', 'athletic' in Rauschnabel et al.'s university brand personality model are not suitable to describe tourism brand personality. Copyright © 2022 Inderscience Enterprises Ltd.","Big data; Brand personality; Cultural tourism; Sustainable development; Tourist satisfaction",,Article,Scopus,2-s2.0-85121519807
"Jiang C., Zhu S., Hu H., An S., Su W., Chen X., Li C., Zheng L.","39261748300;57375602700;57375753800;55900899500;57376509000;57209238820;57219130508;7403406292;","Deep learning model based on big data for water source discrimination in an underground multiaquifer coal mine",2022,"Bulletin of Engineering Geology and the Environment","81","1","26","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121379376&doi=10.1007%2fs10064-021-02535-5&partnerID=40&md5=c8b8e91845aa9a3e9993e9c9104ac279","In view of the difficulty and low efficiency of models established with big data, a deep feedforward network-based mine water inrush discriminant model is established. The stochastic gradient descent (SGD) algorithm is employed to optimize model parameter training. The cross-entropy loss and accuracy of the training and test samples are considered to characterize the model training effect. To prevent overfitting during model training, the dropout optimization method is incorporated into the hidden layers of the model. The four main water-filled aquifers in the Huainan Panxie mining area are chosen as discrimination objects, 1952 sets of water samples are applied as modeling data, and a total of nine indicators, including K+ + Na+, Ca2+, Mg2+, Cl−, SO42−, HCO3−, CO32−, pH, and total dissolved solids (TDS), are selected as discriminating factors. Through model training, the dropout rate, epoch number, batch size, and learning rate are set as 0.1, 90, 25, and 10−2, respectively, and the discriminant accuracy rates for the training and test samples are 93.34% and 96.68%, respectively. The trained model is applied to discriminate 30 groups of water samples, and the discrimination results for 28 groups of water samples are consistent with observations. The results indicate that the deep learning discriminant model based on big data achieves high accuracy, good applicability, and a suitable discrimination ability, and provides a certain guiding significance in the prevention and control of water inrush hazards and related field work. © 2021, Springer-Verlag GmbH Germany, part of Springer Nature.","Big data; Deep learning; Mine water inrush; Underground coal mine; Water sources","Aquifers; Coal mines; Deep learning; Gradient methods; Groundwater resources; Hydrogeology; Optimization; Stochastic models; Stochastic systems; Deep learning; Discriminant models; Mine water inrush; Model training; Model-based OPC; Test samples; Training sample; Underground coal mine; Water samples; Water source; Big data; accuracy assessment; algorithm; aquifer; coal mine; data set; hazard management; machine learning; water flow",Article,Scopus,2-s2.0-85121379376
"Ntlhakana L., Nelson G., Khoza-Shangase K., Dorkin E.","57200618674;9332733600;36005211600;57323672300;","Occupational hearing loss for platinum miners in South Africa: A case study of data sharing practices and ethical challenges in the mining industry",2022,"International Journal of Environmental Research and Public Health","19","1","1","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121377009&doi=10.3390%2fijerph19010001&partnerID=40&md5=953515520fa7bf8cc3fbde9cdf88bddc","Background: The relevant legislation ensures confidentiality and has paved the way for data handling and sharing. However, the industry remains uncertain regarding big data handling and sharing practices for improved healthcare delivery and medical research. Methods: A semi-qualitative cross-sectional study was used which entailed analysing miners’ personal health records from 2014 to 2018. Data were accessed from the audiometry medical surveillance database (n = 480), the hearing screening database (n = 24,321), and the occupational hygiene database (n = 15,769). Ethical principles were applied to demonstrate big data protection and sharing. Results: Some audiometry screening and occupational hygiene records were incomplete and/or inaccurate (N = 4675). The database containing medical disease and treatment records could not be accessed. Ethical challenges included a lack of clarity regarding permission rights when sharing big data, and no policy governing the divulgence of miners’ personal and medical records for research. Conclusion: This case study illustrates how research can be effectively, although not maliciously, obstructed by the strict protection of employee medical data. Clearly communicated company policies should be developed for the sharing of workers’ records in the mining industry to improve HCPs. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Audiometry; Electronic data; Ethical principles; Healthcare providers; Machine learning systems; Occupational exposures; Personal data","platinum; platinum; ethics; health care; machine learning; mining industry; occupational exposure; access to information; Article; audiometry; beneficence; big data; case study; confidentiality; cross-sectional study; data accuracy; data protection; environmental exposure; health care personnel; human; identifiable information; industrial hygiene; information processing; justice; machine learning; medical ethics; medical record; miner; mining; occupational deafness; occupational exposure; periodic medical examination; South Africa; workers rights; information dissemination; noise injury; occupational disease; South Africa; South Africa; Cross-Sectional Studies; Hearing Loss, Noise-Induced; Humans; Information Dissemination; Miners; Occupational Diseases; Platinum; South Africa",Article,Scopus,2-s2.0-85121377009
"De Micco F., De Benedictis A., Fineschi V., Frati P., Ciccozzi M., Pecchia L., Alloni R., Petrosillo N., Filippi S., Ghilardi G., Campanozzi L.L., Tambone V.","57192976476;8363389800;7003380926;6603736497;8791594200;35746897300;6603137437;7004966952;55386612400;56400602500;57196093949;6603144161;","From syndemic lesson after covid-19 pandemic to a “systemic clinical risk management” proposal in the perspective of the ethics of job well done",2022,"International Journal of Environmental Research and Public Health","19","1","15","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121350250&doi=10.3390%2fijerph19010015&partnerID=40&md5=5927f0d903b88e2b5d79ddc0134dd9f9","The syndemic framework proposed by the 2021–2030 World Health Organization (WHO) action plan for patient safety and the introduction of enabling technologies in health services involve a more effective interpretation of the data to understand causation. Based on the Systemic Theory, this communication proposes the “Systemic Clinical Risk Management” (SCRM) to improve the Quality of Care and Patient Safety. This is a new Clinical Risk Management model capable of developing the ability to observe and synthesize different elements in ways that lead to in-depth interventions to achieve solutions aligned with the sustainable development of health services. In order to avoid uncontrolled decision-making related to the use of enabling technologies, we devised an internal Learning Algorithm Risk Management (LARM) level based on a Bayesian approach. Moreover, according to the ethics of Job Well Done, the SCRM, instead of giving an opinion on events that have already occurred, proposes a bioethical co-working because it suggests the best way to act from a scientific point of view. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Bayesian network; Big data; Clinical risk management; Enabling technologies; Medical ethics; Patient safety; Quality of care; Support for policy making; Sustainability","Bayesian analysis; COVID-19; data set; ethics; health and safety; health education; medicine; policy making; risk assessment; sustainability; World Health Organization; accreditation; Article; artificial intelligence; Bayesian learning; clinical decision making; coronavirus disease 2019; cultural factor; diagnostic error; disability; disease severity; economic aspect; environmental parameters; Europe; falling; family counseling; global health; health care cost; health care facility; health care personnel; health care quality; health care system; high income country; hospital infection; hospital mortality; hospitalization; human; human rights; incidence; internet access; intersectoral collaboration; learning algorithm; lifestyle; low income country; medical education; medication error; middle income country; pandemic; patient education; patient engagement; patient harm; patient risk; patient safety; political system; public health service; radiological procedures; risk management; risk reduction; robotics; social aspect; socioeconomics; surgical risk; sustainable development; United States; Bayes theorem; pandemic; risk management; Bayes Theorem; COVID-19; Humans; Pandemics; Risk Management; SARS-CoV-2; Syndemic",Article,Scopus,2-s2.0-85121350250
"Cao Y., Liu J., Qi H., Gui J., Li K., Ye J., Liu C.","56710768700;57376459800;35727789500;25122127900;57204189178;57218448544;57221615369;","Scalable Distributed Hashing for Approximate Nearest Neighbor Search",2022,"IEEE Transactions on Image Processing","31",,,"472","484",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121334656&doi=10.1109%2fTIP.2021.3130528&partnerID=40&md5=b4446cf63265d8c0fd39f3906ac0ef24","Hashing has been widely applied to the large-scale approximate nearest neighbor search problem owing to its high efficiency and low storage requirement. Most investigations concentrate on learning hashing methods in a centralized setting. However, in existing big data systems, data is often stored across different nodes. In some situations, data is even collected in a distributed manner. A straightforward way to solve this problem is to aggregate all the data into the fusion center to obtain the search result (aggregating method). However, this strategy is not feasible because of the prohibitive communication cost. Although a few distributed hashing methods have been proposed to reduce this cost, they only focus on designing a distributed algorithm for a specific global optimization objective without considering scalability. Moreover, existing distributed hashing methods aim at finding a distributed solution to hashing, meanwhile avoiding accuracy loss, rather than improving accuracy. To address these challenges, we propose a Scalable Distributed Hashing (SDisH) model in which most existing hashing methods can be extended to process distributed data with no changes. Furthermore, to improve accuracy, we utilize the search radius as a global variable across different nodes to achieve a global optimum search result for every iteration. In addition, a voting algorithm is presented based on the results produced by multiple iterations to further reduce search errors. Theoretical analyses of communication, computation, and accuracy demonstrate the superiority of the proposed model. Numerical simulations on three large-scale and two relatively small benchmark datasets also show that the SDisH model achieves up to 44.75% and 10.23% accuracy gains compared to the aggregating method and state-of-the-art distributed hashing methods, respectively. © 1992-2012 IEEE.","approximate nearest neighbor search; distributed; Hashing","Big data; Costs; Digital storage; Global optimization; Hamming distance; Iterative methods; Nearest neighbor search; Numerical methods; Aggregating methods; Approximate Nearest Neighbor Search; Computational modelling; Distributed; Distributed database; Hashing; Hashing method; Higher efficiency; Large-scales; Search problem; Hash functions; algorithm; article; computer simulation; theoretical study",Article,Scopus,2-s2.0-85121334656
"Fei Z.","57371397000;","Research on rural land planning based on traditional farming culture",2022,"Acta Agriculturae Scandinavica Section B: Soil and Plant Science","72","1",,"248","259",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121130780&doi=10.1080%2f09064710.2021.2001041&partnerID=40&md5=3457dc4487f469fa557de3a700f58342","As the electronic process of land management is accelerating, the research on the construction of land use planning system tends to be intensified. This paper combines the concept of traditional farming culture to construct a rural land planning system, and proposes a stacked combination collaborative filtering recommendation algorithm to process traditional farming cultural data and promote the integration of traditional farming cultural data and rural land planning data. Moreover, starting from the existing problems in urban land planning at this stage, this paper proposes the rationality and importance of the land use planning system, and at the same time guarantees that it can be used in a standardised and scientific way. In addition, after explaining the construction purpose of the land use planning system, this paper also explains the tasks, design and corresponding steps to be completed to construct the system. Finally, this paper verifies the performance of this system through experimental research. Through the experimental results, we can see that the method proposed in this paper has a certain effect. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","big data; land planning; rural land; Traditional farming culture",,Article,Scopus,2-s2.0-85121130780
"Sabbagh R., Živković S., Gawlik B., Sreenivasan S.V., Stothert A., Majstorovic V., Djurdjanovic D.","57193714420;56368189800;57156203900;57293522200;6602435575;6603288002;6603566263;","Organization of big metrology data within the Cyber-Physical Manufacturing Metrology Model (CPM3)",2022,"CIRP Journal of Manufacturing Science and Technology","36",,,"90","99",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120904981&doi=10.1016%2fj.cirpj.2021.10.009&partnerID=40&md5=96eef6b6d9c30ec3d08157c7ec011a58","In this paper, we propose a novel data curation concept that enables data mining and analytics within the recently described Cyber-Physical Manufacturing Metrology Model (CPM3). The newly proposed methodology is based on organizing the metrology data into tree-based database structures using distance-based unsupervised clustering of the raw metrology data. Compared to traditionally utilized temporally organized lists, the new tree-based database organization of metrology data enables logarithmic acceleration of searches within the data and thus provides dramatic advantages for data mining. The newly proposed data curation methodology was evaluated in case studies involving hyper-spectral metrology of nanopatterned surfaces, coordinate measurement machine (CMM) inspection of aircraft engine turbines and imaging-based metrology of nano-volume droplets in the jet and fill stage of imprint lithography processes. Significant improvements in search speeds with minimal or no losses in search precision and recall were observed in all case-studies, with benefits of tree-based data organization growing with the size of the data. © 2021","Big data curation; Big data management; Cyber-Physical Manufacturing Systems; Industrial internet of things; Industry 4.0; Metrology","Big data; Coordinate measuring machines; Data mining; Industry 4.0; Big data curation; Big data management; Case-studies; Cybe-physical manufacturing system; Cyber physicals; Data curation; Database structures; Manufacturing metrologies; Metrology data; Tree-based; Information management",Article,Scopus,2-s2.0-85120904981
"Patil M.M., Rekha P.M., Solanki A., Nayyar A., Qureshi B.","54953604000;56490341200;36562669800;55201442200;35293201200;","Big data analytics using swarm-based long short-term memory for temperature forecasting",2022,"Computers, Materials and Continua","71","2",,"2347","2361",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120904242&doi=10.32604%2fcmc.2022.021447&partnerID=40&md5=d54cc3ba8aa80e103ea74b319fb7ad9f","In the past few decades, climatic changes led by environmental pollution, the emittance of greenhouse gases, and the emergence of brown energy utilization have led to global warming. Global warming increases the Earth’s temperature, thereby causing severe effects on human and environmental conditions and threatening the livelihoods of millions of people. Global warming issues are the increase in global temperatures that lead to heat strokes and high-temperature-related diseases during the summer, causing the untimely death of thousands of people. To forecast weather conditions, researchers have utilized machine learning algorithms, such as autoregressive integrated moving average, ensemble learning, and long short-term memory network. These techniques have been widely used for the prediction of temperature. In this paper, we present a swarm-based approach called Cauchy particle swarm optimization (CPSO) to find the hyperparameters of the long short-term memory (LSTM) network. The hyperparameters were determined by minimizing the LSTM validation mean square error rate. The optimized hyperparameters of the LSTM were used to forecast the temperature of Chennai City. The proposed CPSO-LSTM model was tested on the openly available 25-year Chennai temperature dataset. The experimental evaluation on MATLABR2020a analyzed the root mean square error rate and mean absolute error to evaluate the forecasted output. The proposed CPSO-LSTM outperforms the traditional LSTM algorithm by reducing its computational time to 25 min under 200 epochs and 150 hidden neurons during training. The proposed hyperparameter-based LSTM can predict the temperature accurately by having a root mean square error (RMSE) value of 0.250 compared with the traditional LSTM of 0.35 RMSE. © 2022 Tech Science Press. All rights reserved.","Big data; Climatic change; Deep learning; Forecasting swarm intelligence; Temperature","Big data; Brain; Data Analytics; Energy utilization; Errors; Global warming; Greenhouse gases; Learning algorithms; Mean square error; Particle swarm optimization (PSO); Swarm intelligence; Weather forecasting; Chennai; Climatic changes; Deep learning; Environmental pollutions; Error rate; Forecasting swarm intelligence; Hyper-parameter; Memory network; Root mean square errors; Temperature forecasting; Long short-term memory",Article,Scopus,2-s2.0-85120904242
"Sun Q., Wu Q.","57366595000;55378351200;","Target similarity matching algorithm of big data in remote sensing image based on Henon mapping",2022,"International Journal of Information and Communication Technology","20","1",,"51","64",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120833584&doi=10.1504%2fIJICT.2022.119316&partnerID=40&md5=33d09b242f9162177c4ea2b358f845cb","In order to overcome the problem of low matching accuracy in traditional big data target similarity matching algorithm of remote sensing image, this paper proposes a new target similarity matching algorithm based on Henon mapping. The randomness of big data target in remote sensing image is analysed by using the variation of Henon mapping invariant distribution. According to the randomness, the track of big data target in remote sensing image is selected to build a two-layer similarity matching model. The first layer of the model uses coarse granularity to reduce the dimension of big data, and the second layer uses the fine-grained representation of similar track set to output several tracks similar to the big data target of remote sensing image, so as to achieve the target similarity matching. The experimental results show that the proposed method has high matching accuracy, and the highest matching accuracy can reach 99.7%. Copyright © 2022 Inderscience Enterprises Ltd.","Big data target; Henon mapping; Remote sensing image; Similarity matching","Mapping; Random processes; Remote sensing; Big data target; Datum targets; Henon mappings; Image-based; Invariant distribution; Matchings; Remote sensing images; Similarity matching algorithms; Similarity-matching; Two-layer; Big data",Article,Scopus,2-s2.0-85120833584
"Eisa A., EL-Rashidy N., Alshehri M.D., El-Bakry H.M., Abdelrazek S.","57366526300;57211502814;57022260900;6603728802;57216186536;","Incremental learning framework for mining big data stream",2022,"Computers, Materials and Continua","71","2",,"2901","2921",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120790739&doi=10.32604%2fcmc.2022.021342&partnerID=40&md5=4d32a8f8316017a5730ddbef94cb4eca","At this current time, data stream classification plays a key role in big data analytics due to its enormous growth. Most of the existing classification methods used ensemble learning, which is trustworthy but these methods are not effective to face the issues of learning from imbalanced big data, it also supposes that all data are pre-classified. Another weakness of current methods is that it takes a long evaluation time when the target data stream contains a high number of features. The main objective of this research is to develop a new method for incremental learning based on the proposed ant lion fuzzy-generative adversarial network model. The proposed model is implemented in spark architecture. For each data stream, the class output is computed at slave nodes by training a generative adversarial network with the back propagation error based on fuzzy bound computation. This method overcomes the limitations of existing methods as it can classify data streams that are slightly or completely unlabeled data and providing high scalability and efficiency. The results show that the proposed model outperforms stateof- the-art performance in terms of accuracy (0.861) precision (0.9328) and minimal MSE (0.0416). © 2022 Tech Science Press. All rights reserved.","Ant lion optimization (ALO); Big data stream; Generative adversarial network (GAN); Incremental learning; Renyi entropy","Backpropagation; Big data; Classification (of information); Data Analytics; 'current; Ant lion optimization; Big data stream; Data stream; Data stream classifications; Generative adversarial network; Incremental learning; Learning frameworks; Optimisations; Renyi's entropy; Generative adversarial networks",Article,Scopus,2-s2.0-85120790739
"He D., Chen T., Huang H., Qiu W., Tang Y., Jiang J.","57366156500;57366448300;57366448400;57365869900;57365870000;57366301200;","Dynamic fault diagnosis means of the power message system based on big data",2022,"International Journal of Information and Communication Technology","20","1",,"83","96",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120774277&doi=10.1504%2fIJICT.2022.119320&partnerID=40&md5=1de391725e93220ece086c6c456f468c","Aiming at the poor fault diagnosis ability of traditional power information system, a dynamic fault diagnosis method based on big data for power information system is proposed. Firstly, the original fault information of power information system is sampled, and the collected fault characteristic data are reconstructed by multi feature and information fitting. Then, the attribute distribution detection and big data mining are carried out for the fault dynamic characteristics of power information system. According to the high-order spectrum feature distribution of the extracted power information system fault signals, the dynamic fault diagnosis and fuzzy clustering analysis are carried out for the power information system, and the fault diagnosis is optimised according to the classification results. The simulation results show that the dynamic fault diagnosis accuracy of power information system is high, the fault sample detection results are accurate and reliable, and the dynamic fault detection ability is improved. Copyright © 2022 Inderscience Enterprises Ltd.","Big data; Detection; Dynamic diagnosis; Electricity message system; Fault","Big data; Classification (of information); Computer aided diagnosis; Data mining; Fault detection; Information systems; Information use; Spectrum analysis; Detection; Dynamic diagnosis; Dynamic faults; Electricity message system; Fault; Fault diagnosis method; Faults diagnosis; Message systems; Power; Power information systems; Failure analysis",Article,Scopus,2-s2.0-85120774277
"Jara L., Ariza-Valderrama R., Fernández-Olivares J., González A., Pérez R.","57218862545;57362614900;6507954457;7404585342;7402543567;","Efficient inference models for classification problems with a high number of fuzzy rules[Formula presented]",2022,"Applied Soft Computing","115",,"108164","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120611517&doi=10.1016%2fj.asoc.2021.108164&partnerID=40&md5=7da260e26d5c9ec2d09724a6403c61ff","In data science there are problems that are not visible until you work with a sufficiently large number of data. This is the case, for example, with the design of the inference engine in fuzzy rule-based classification systems. The most common way to implement the winning rule inference method is to use sequential processing that reviews each of the rules in the rule set, to determine the best one and return the associated class. This implementation produces fast response times when the set of rules is small and is applied to a small set of examples. In this paper we explore new versions to implement this inference method, avoiding analyzing all the rules and focusing the analysis on the neighborhood of rules around the example. We study experimentally the conditions where each of them should be applied. Finally, we propose an implementation that combines all the studied versions offering good accuracy results and a significant reduction in the response time. © 2021 Elsevier B.V.","Big data; Explainable AI; Fuzzy reasoning; Fuzzy rule-based classification systems; Inference engine; Soft computing","Engines; Fuzzy inference; Fuzzy rules; Soft computing; Explainable AI; Fuzzy reasoning; Fuzzy rule based classification systems; Inference methods; Inference models; IS problems; Number of datum; Rule inference; Sequential processing; Soft-Computing; Big data",Article,Scopus,2-s2.0-85120611517
"Zhang J., Liu X., Tan X., Jia T., Senousi A.M., Huang J., Yin L., Zhang F.","57211967475;56288846300;57193565228;23394880800;57216933986;57211966516;36660069800;57210158687;","Nighttime Vitality and Its Relationship to Urban Diversity: An Exploratory Analysis in Shenzhen, China",2022,"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15",,,"309","322",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120562272&doi=10.1109%2fJSTARS.2021.3130763&partnerID=40&md5=8d76b080d2f2ff1f52a399b16a9cd9ab","Relationship between urban diversity and urban vitality is imperative for guiding better design in urban development, although existing frameworks are not able to efficiently examine the relationship at multiple scales. In this article, we propose a new framework to integrate nighttime light (NTL) imagery and multisource urban data into multiscale geographically weighted regression (MGWR) models to examine the varying relationship between diversity and vitality across space and time. NTL is used as a proxy for urban nighttime vitality. Public transport, taxi transit, and points of interest data are used to derive three aspects of urban diversity indices: ridership diversity, spatial interaction diversity, and built environment diversity. By comparing the models in holiday and nonholiday weeks in Shenzhen, China, the NTL-based vitality proxy was found to be strongly correlated with the urban diversity indices, given by the satisfactory goodness of fit (r-squared = 0.9) of the MGWR models. The spatially varying relationships between diversity indices and nighttime vitality were observed and patterns discussed. The analysis of the coefficients revealed the importance of stable public transport and fluctuating taxi trips for nighttime vitality. The new index proposed for the diversity of spatial interaction (DSI) is a strong indicator for nighttime vitality, adding to existing vitality indicators. Furthermore, this study found that DSI and density of catering have less temporal variation, indicating their robustness in measuring nighttime vitality. This study provided empirical insights into how nighttime vitality is related to urban diversity, demonstrating new applications of NTL for intracity studies. © 2008-2012 IEEE.","Multiscale geographically weighted regression (mgwr); Nighttime light (ntl); Spatiotemporal variation; Urban diversity; Urban vitality","Big data; Taxicabs; Urban growth; Urban transportation; Cultural difference; Geographically weighted regression; Multiscale geographically weighted regression; Night time lights; Public transportation; Remote-sensing; Spatial resolution; Spatio-temporal variation; Urban areas; Urban diversity; Urban vitality; Remote sensing; empirical analysis; public transport; spatiotemporal analysis; temporal analysis; temporal variation; urban development; China; Guangdong; Shenzhen",Article,Scopus,2-s2.0-85120562272
"Wan Z.","57358163000;","The application of big data in the legal improvement of agricultural product quality and safety governance",2022,"Acta Agriculturae Scandinavica Section B: Soil and Plant Science","72","1",,"200","213",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120341826&doi=10.1080%2f09064710.2021.2007994&partnerID=40&md5=244760350bc5353dbfe7da1f04adb616","The quality and safety of agricultural products is not only related to the health of consumers, but also to the sustainable and stable development of the economy, and even to the harmony and stability of the society. The application of the theory model of multiple co-governance of agricultural product quality and safety has certain theoretical support. This paper applies big data technology to agricultural product quality and safety governance, and uses big data methods to study the key control points in the process of agricultural product traceability. Moreover, based on the selected key control points, this paper studies the key traceability indicators corresponding to the key control points of each link. In addition, this paper combines multi-disciplinary knowledge to carry out a systematic study on the legal issues of multivariate co-governance of agricultural product quality and safety in my country from the perspective of law. From the experimental research and the final decision-making suggestions, we can see that the method proposed in this paper is feasible. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","agricultural product quality and safety; Big data; data mining; legal improvement",,Article,Scopus,2-s2.0-85120341826
"Luo C., Wu X.","57358160600;57357390700;","Research on the integrated development of leisure agriculture and red cultural tourism under the background of big data",2022,"Acta Agriculturae Scandinavica Section B: Soil and Plant Science","72","1",,"189","199",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120337359&doi=10.1080%2f09064710.2021.2007993&partnerID=40&md5=029906efa07828c53c18ebd2923ee3fe","In order to promote the integration of leisure agriculture and red cultural tourism, this paper improves the big data technology and combines the actual needs of tourism data fusion to construct an integrated analysis system of leisure agriculture and red cultural tourism-based big data. Through the data collection layer and the data information provided by the third party, this paper uses various technologies such as cloud computing and fuzzy recognition to classify and store massive amounts of data and information and establish a data warehouse to integrate various information resources to serve various smart applications. In addition, this paper combines the characteristics of leisure agriculture and red cultural tourism to integrate the two and builds an intelligent system with the support of big data technology. Finally, this paper carries out the performance verification of this system through experimental research. From the research results, it can be seen that the system constructed in this paper meets the needs of the integration of leisure agriculture and red cultural tourism. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","Big data; leisure agriculture; red culture; tourism integration",,Article,Scopus,2-s2.0-85120337359
"Cao J.","57357783400;","Coordinated development mechanism and path of agricultural logistics ecosystem based on big data analysis and IoT assistance",2022,"Acta Agriculturae Scandinavica Section B: Soil and Plant Science","72","1",,"214","224",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120330598&doi=10.1080%2f09064710.2021.2008476&partnerID=40&md5=31858772167299a405e941d255aa3c6c","In order to promote the development of agro-ecological economy and improve the reliability of the development model of agricultural circular economy, this paper explores the general framework of the coordinated development of the agricultural logistics ecosystem from the perspective of the construction of the agricultural logistics ecosystem, the composition of the ecosystem, the synergy stage, and the synergy goal. Moreover, this paper combines big data and Internet of Things technology to analyze its own path of the cooperative development mechanism of the agricultural logistics ecosystem, and uses typical cases to show the internal relationships between the main bodies of the agricultural logistics ecosystem. In addition, this paper attempts to explain the core essence of the agricultural logistics ecosystem and the internal relations of the subjects through the practice of enterprises. After constructing the system framework, this paper combines the intelligent algorithm to construct the intelligent model structure, and analyzes the system function realisation process based on the actual situation. Finally, this paper evaluates the performance of the algorithm model constructed in this paper through experimental research. From the experimental statistical results, it can be seen that the algorithm model constructed in this paper has a certain effect. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","agricultural logistics; Big data; development mechanism; ecosystem; Internet of Things",,Article,Scopus,2-s2.0-85120330598
"Gao F., Li H.","57278862700;57212676388;","Research on variable rate fertilisation machine based on big data analysis",2022,"Acta Agriculturae Scandinavica Section B: Soil and Plant Science","72","1",,"225","236",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120327155&doi=10.1080%2f09064710.2021.2008478&partnerID=40&md5=9723d58fd0906faf7e83c6175d5fd1b0","In order to improve the effect of intelligent fertilisation, according to actual fertilisation needs, this paper combines big data technology to perform data analysis, and designs a variable fertilisation mechanical structure that can be used for mixed fertilisation to realise mixed fertilisation of multiple crops. Moreover, from the perspective of precise fertilisation, this paper combines the actual needs of variable fertilisation to construct a control system, and adopts an adaptive federated filter structure to solve the influence of equipment errors on system position estimation. In addition, this paper solves the problems of excessive partial application and low fertiliser utilisation in traditional fertilisation, as well as the resulting waste, environmental pollution, and poor quality of agricultural products. Furthermore, this paper designs and manufactures a dual-variable application test device, and conducted a fertiliser discharge test on the test device. Finally, this paper uses big data technology to analyze the experimental data of variable fertilisation operation machinery. From the experimental research, it can be seen that the effect of the fertilisation operation machinery designed in this paper meets actual needs. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","Big data; machinery; system; variable fertilisation effect",,Article,Scopus,2-s2.0-85120327155
"Boomgard-Zagrodnik J.P., Brown D.J.","57357569900;57357695400;","Machine learning imputation of missing Mesonet temperature observations",2022,"Computers and Electronics in Agriculture","192",,"106580","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120305794&doi=10.1016%2fj.compag.2021.106580&partnerID=40&md5=47806af8dab4ea5f63141875b5e34129","Uninterrupted and reliable weather data is a necessary foundation for agricultural decision making, required for models based on accumulated growing degree days (GDD), chill units, and evapotranspiration. When a weather station experiences a mechanical or communications failure, a replacement (imputed) value should be substituted for any missing data. This study introduces a machine learning, network-based approach to imputing missing 15-minute and daily maximum/minimum air temperature observations from 8.5 years of air temperature, relative humidity, wind, and solar radiation observations at 134 AgWeatherNet (AWN) stations in Washington State. A random forest imputation model trained on temperature and humidity observations from the full network predicted 15-minute, daily maximum, and daily minimum temperature values with mean absolute errors of 0.43 °C, 0.53 °C, and 0.70 °C, respectively. Sensitivity experiments determined that imputation skill was related a number of external factors including volume and type of training data, proximity of surrounding stations, and regional topography. In particular, nocturnal cold air flows in the upper Yakima Valley of south-central Washington caused temperature to be less correlated with surrounding stations in the overnight hours. In a separate experiment, the imputation model was used to predict base- 10 °C GDD on 2020 July 1 trained entirely on 15-minute station data from previous years. Even with the entire season of observations missing, the model predicted the GDD value within an average error 1.4% with 125 of 134 stations within 5% of observations. Since missing data can typically be resolved within a timeframe of a few days, the network-based imputation model is a sufficient substitute for short periods of missing observational weather data. Other potential applications of an imputation model are briefly discussed. © 2021","Big data; Degree day models; Machine learning; Missing data imputation; Surface weather observations","Big data; Decision trees; Machine learning; Meteorology; Random errors; Topography; Weather forecasting; Air temperature; Decisions makings; Degree-day model; Growing degree-days; Missing data; Missing data imputations; Model-based OPC; Surface weather observations; Temperature observations; Weather data; Atmospheric temperature; air temperature; evapotranspiration; humidity; machine learning; solar radiation; topography; weather station; Washington",Article,Scopus,2-s2.0-85120305794
"Liu P., Wang J., Wang Z., Zhang Z., Wang S., Dorrell D.","55574230648;57212470915;57226151100;57209655165;56416689600;6701664122;","Cloud Platform-Oriented Electrical Vehicle Abnormal Battery Cell Detection and Pack Consistency Evaluation with Big Data: Devising an Early-Warning System for Latent Risks",2022,"IEEE Industry Applications Magazine","28","2",,"44","55",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120067836&doi=10.1109%2fMIAS.2021.3114654&partnerID=40&md5=82ba049f760e3ac8fbda6b3c6df41ebf","A battery is grouped into many cells, and inconsistency is unavoidable in the battery life cycle. If the battery is frequently charged or discharged without a balancer, the battery cells with the lowest capacity may be overcharged or overdischarged, which is one of the major reasons for battery thermal runaway, which can cause a fire. This article proposes a cloud data-based electric vehicle (EV) battery-voltage consistency evaluation technique for vehicles in service. The density-based spatial clustering of applications with noise (DBSCAN) method is employed to improve the computational efficiency of the variance of angle (VOA), which is a widely used outlier-detection scheme. The DBSCAN-VOA results are compared with VOA results, showing that the distinction capability of VOA is kept while the computational complexity is significantly reduced. To assess the pack's consistency under real operation, a benchmark open-circuit voltage (OCV)-based consistency approach is developed in a simulated lab environment. A big data-based online battery pack consistency-state evaluation technique is established using the deviation value statistical method, and the efficiency of the process is discussed. © 1975-2012 IEEE.",,"Big data; Charging (batteries); Efficiency; Life cycle; Open circuit voltage; Vehicles; Battery cells; Battery life; Cell detection; Cloud data; Cloud platforms; Density-based spatial clustering of applications with noise; Early Warning System; Electric vehicle batteries; Electrical vehicles; Thermal runaways; Computational efficiency",Article,Scopus,2-s2.0-85120067836
"Xia D., Bai Y., Zheng Y., Hu Y., Li Y., Li H.","55960844800;57217632552;57217634009;57224096264;35119124600;36930647900;","A parallel SP-DBSCAN algorithm on spark for waiting spot recommendation",2022,"Multimedia Tools and Applications","81","3",,"4015","4038",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119958017&doi=10.1007%2fs11042-021-11639-9&partnerID=40&md5=1fc55c53cb4212fc7d69f1b379476d57","It is challenging for complex urban transportation networks to recommend taxi waiting spots for mobile passengers because the traditional centralized mining platform cannot address the storage and calculation problems of GPS trajectory big data, and especially the boundary identification of DBSCAN is difficult on the Spark parallel processing framework. To this end, we propose a parallel DBSCAN optimization algorithm with the silhouette coefficient and the pickup rate on Spark in this paper, named SP-DBSCAN, where users merely input one parameter to complete the distributed recommendation of the best waiting spot. Specifically, under the Hadoop distributed computing platform, a general framework of distributed modeling for waiting spot recommendation on Spark is developed to solve the distributed storage and parallel computing issues of the serial algorithm in handling data partition and clustering of large-scale traffic data on a single machine. Moreover, we put forward a parallel SP-DBSCAN algorithm on Spark to recommend the best waiting spot for passengers, where the traditional DBSCAN algorithm is optimized via the silhouette coefficient and the boarding ratio to address the parameter sensitive problem and the issue of the center of the non-convex clustering graph is solved by giving one cluster with two centroids in the clustering hotspot areas. Finally, experimental results on four groups of real-world taxi GPS trajectory data sets demonstrate that compared with C-DBSCAN and P-DBSCAN, the recognition rate of SP-DBSCAN is increased by 1.6%, 6.2%, 3.47%, and 5.8%, respectively. The empirical study indicates that the clustering region generated by our SP-DBSCAN algorithm can satisfy the requirements that passengers can ride in the hotspot area when they have not successfully hitchhiked at a specific location and turned to the next spot randomly. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Big data analytics; DBSCAN; Image processing; Pickup rate; Silhouette coefficient; Spark; Taxi GPS trajectory data; Waiting spot recommendation","Big data; Clustering algorithms; Data Analytics; Digital storage; Global positioning system; Parameter estimation; Pickups; Scheduling algorithms; Taxicabs; Trajectories; Urban transportation; Clusterings; DBSCAN; DBSCAN algorithm; Hotspots; Images processing; Pickup rate; Silhouette coefficient; Taxi GPS trajectory data; Trajectories datum; Waiting spot recommendation; Image processing",Article,Scopus,2-s2.0-85119958017
"Zhao Y., Pawlak J., Sivakumar A.","57196713318;56841268600;55109209800;","Theory for socio-demographic enrichment performance using the inverse discrete choice modelling approach",2022,"Transportation Research Part B: Methodological","155",,,"101","134",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119957672&doi=10.1016%2fj.trb.2021.11.004&partnerID=40&md5=95365a927e3a68ec9c4b3b3464c5f00a","In light of the growing availability of big data sources and the essential role of socio-demographic information in travel behaviour and transport demand modelling more broadly, the enrichment of socio-demographic attributes for anonymous big datasets is a key issue that continues to be explored. The common shortcoming of existing socio-demographic enrichment approaches concerns their lack of consistent theory that can link their enrichment performance (i.e. the ability to correctly enrich the required attribute) to the underlying covariance structure in the anonymous big datasets. In other words, existing approaches are unable to indicate, prior to the enrichment, to what extent it will be successful. Instead, they require undertaking the enrichment itself to assess and validate it post factum, incurring the effort and cost of the activity. An alternative and arguably preferable way would be to have a prior indicator as to whether an enrichment is likely to be sufficiently effective for the desired application. Towards this end, this paper draws upon the Inverse Discrete Choice Modelling (IDCM) approach to demonstrate what is termed as the IDCM performance theory, which systematically and in a tractable manner links the socio-demographic enrichment performance of the IDCM approach to the structure of the underlying datasets. This is achieved by recalibration of the constant, a technique adopted from conventional discrete choice modelling practice, while also drawing upon information theory employed in the context of communication systems. The established IDCM performance theory is validated in two empirical applications where performance of the IDCM approach in enriching several socio-demographic attributes, given travel behaviour patterns, is successfully estimated. Additionally, the IDCM approach is found to perform comparably to commonly used methods in previous socio-demographic enrichment efforts. It is thus argued that the capability of the IDCM performance theory to predict and explain its enrichment performance under different data conditions can facilitate informed and transparent transferability of the IDCM framework in the socio-demographic enrichment for anonymous big datasets. © 2021 Elsevier Ltd","Big data; Discrete choice modelling; Information theory; Privacy; Socio-demographic enrichment; transferability","Inverse problems; Large dataset; Population statistics; Choice modeling approaches; Data-source; Discrete choice models; Modeling performance; Performance; Privacy; Socio-demographic enrichment; Sociodemographics; Transferability; Travel behaviour; Information theory; data set; demography; discrete choice analysis; modeling; transportation; travel behavior; Indicator indicator",Article,Scopus,2-s2.0-85119957672
"Liu K., Li T., Yang X., Yang X., Liu D., Zhang P., Wang J.","57203760858;7406372548;23020119200;57195236736;9279563000;57207033994;57353057800;","Granular cabin: An efficient solution to neighborhood learning in big data",2022,"Information Sciences","583",,,"189","201",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119897514&doi=10.1016%2fj.ins.2021.11.034&partnerID=40&md5=416cf8bcde82d1dc2e095da5fab524f1","Neighborhood Learning (NL) is a paradigm covering theories and techniques of neighborhood, which facilitates data organization, representation and generalization. While delivering impressive performances across various fields such as granular computing, cluster analysis, NL is argued to be computationally demanding, thereby limiting its utility and applicability. In this study, a simple and generic scheme named granular cabin is proposed for drastically speeding up the algorithmic implementation of NL. Specifically, this scheme is deployed to Neighborhood Rough Set (NRS) which is a typical NL methodology. And three major applications of NRS are concerned including approximation computation, neighborhood classification and feature selection. Extensive experiments demonstrate that NRS methodology enhanced by granular cabin consumes much less time. This study offers a promising solution that ensures the great potential of NL in big data. © 2021 Elsevier Inc.","Computational efficiency; Granular computing; Neighborhood learning; Neighborhood rough set","Big data; Cluster computing; Computation theory; Granular computing; Rough set theory; Computing clusters; Covering theories; Data generalization; Data organization; Data representations; Neighborhood learning; Neighborhood rough sets; Neighbourhood; Performance; Simple++; Computational efficiency",Article,Scopus,2-s2.0-85119897514
"Jaung W., Carrasco L.R.","55313069900;57352898200;","A big-data analysis of human-nature relations in newspaper coverage",2022,"Geoforum","128",,,"11","20",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119896055&doi=10.1016%2fj.geoforum.2021.11.017&partnerID=40&md5=540f6d269feb7dbe665af5dd72aeb252","Studying human-nature relations in large socio-ecological systems is vital for sustainability; however, it is challenging. Big-data analyses provide a new opportunity to carry out large-scale studies. To explore this opportunity, we examined 25,019 newspaper articles related to nature in 13 countries in Asia using machine learning techniques. The results revealed opportunities and challenges in shaping sustainable human-nature relations, which included the following: many human-nature relations might not yet be captured in literature; cultural ecosystem services might help increase the public's interests in sustainability; regional context matters; and the public might have lower interest in people's positive contributions to nature compared to other human-nature relations. The diversity of topics identified suggests that multiple conceptual frameworks, including ecosystem services, nature's contributions to people, ecological footprint, and social-ecological systems, are needed. This result highlights the need for pluralistic frameworks to comprehensively examine human-nature relations. Overall, the study demonstrates that big-data applications support the analysis of diverse and complex human-nature systems that could help advance the field forward. © 2021","Big data; Ecosystem services; Human-nature relationships; Nature's contribution to people; Socio-ecological systems","conceptual framework; data set; ecosystem service; machine learning; mass media; nature-society relations; sustainability; Asia",Article,Scopus,2-s2.0-85119896055
"Maathavan K.S.K., Venkatraman S.","57351784300;57321348700;","A secure encrypted classified electronic healthcare data for public cloud environment",2022,"Intelligent Automation and Soft Computing","32","2",,"765","779",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119827985&doi=10.32604%2fiasc.2022.022276&partnerID=40&md5=8ef3da94a335caf012664087dca540b9","The major operation of the blood bank supply chain is to estimate the demand, perform inventory management and distribute adequate blood for the needs. The proliferation of big data in the blood bank supply chain and data management needs an intelligent, automated system to classify the essential data so that the requests can be handled easily with less human intervention. Big data in the blood bank domain refers to the collection, organization, and analysis of large volumes of data to obtain useful information. For this purpose, in this research work we have employed machine learning techniques to find a better classification model for blood bank data. At the same time, it is vital to manage data storage requirements. The Cloud offers wide benefits for data storage and the simple, efficient technology is adapted in various domains. However, the data to be stored in the cloud should be secured in order to avoid data breaches. For this, a data encryption module has been incorporated into this research work. The com-bined model provides secure encrypted classified data to be stored in the cloud, which reduces human intervention and analysis time. Machine learning models such as Support Vector Machine (SVM), Multinomial Naive Bayes (MNB), Decision Tree (DT), Random Forest (RF), Gradient Boosting (GB), K-Nearest Neigh-bor (KNN) are used for classification. For data security, the Advanced Encryption Standard with Galois/Counter Mode (AES–GCM) encryption model is employed, which provides maximum security with minimum encryption time. Experimental results demonstrate the performance of machine learning and encryption techniques by processing blood bank data. © 2022, Tech Science Press. All rights reserved.","Big data; Classification; Cloud; Data security; Electronic health records (EHR); Encryption; Machine learning",,Article,Scopus,2-s2.0-85119827985
"Attaallah A., Alsuhabi H., Shukla S., Kumar R., Gupta B.K., Khan R.A.","57215220031;57252711500;57352223600;55492126400;57201359231;25724398200;","Analyzing the big data security through a unified decision-making approach",2022,"Intelligent Automation and Soft Computing","32","2",,"1071","1088",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119822232&doi=10.32604%2fiasc.2022.022569&partnerID=40&md5=bd98c3c7963fa2582cca7575f47dc92a","The use of cloud services, web-based software systems, the Internet of Things (IoT), Machine Learning (ML), Artificial Intelligence (AI), and other wireless sensor devices in the health sector has resulted in significant advancements and benefits. Early disease detection, increased accessibility, and high diagnostic reach have all been made possible by digital healthcare. Despite this remarkable achievement, healthcare data protection has become a serious issue for all parties involved. According to data breach statistics, the healthcare data industry is one of the major threats to cyber criminals. In reality, healthcare data breaches have increased at an alarming rate in recent years. Practitioners are developing a variety of tools, strategies, and approaches to solve healthcare data security concerns. The author has highlighted the crucial measurements and parameters in relation to enormous organizational circumstances for securing a vast amount of data in this paper. Security measures are those that prevent developers and organizations from achieving their objectives. The goal of this work is to identify and prioritize the security approaches that are used to locate and solve problems using different versions of two approaches that have been used to analyze big data security in the past. The Fuzzy Analytic Hierarchy Process (Fuzzy AHP) approach is being used by authors to examine the priorities and overall data security. In addition, the most important features in terms of weight have been quantitatively analyzed. Experts will discover the findings and conclusions useful in improving big data security. © 2022, Tech Science Press. All rights reserved.","Big data; Fuzzy AHP; IoT; Security assessment",,Article,Scopus,2-s2.0-85119822232
"Babour A., Khan J.I.","56426219500;57346225500;","Automatic Generation of a Metric Report: A Case Study of Scientometric Analytics",2022,"IEEE Access","10",,,"3923","3934",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119422891&doi=10.1109%2fACCESS.2021.3127207&partnerID=40&md5=7273d97232b0740d60a29ec2ef1a4c03","Automatic report generation is an emerging technology that mechanically generates documents in the form of a report consisting of text, tables, and figures about a specific topic. This paper proposes a model for automated generation of a scientometric research analysis report for any selected country. The scholarly database utilized in this study is Microsoft Academic Graph. Given the two-letter code of the selected country, starting study year, and ending study year data concepts to the employed database, the model extracts the datasets, including the scientific research information for the selected country in the specified period, and generates an in-depth analysis report about the country's research publications. The model consists of two main stages. The first stage extracts the datasets for the selected country from the utilized database using Azure Databricks and Azure Blob Storage services. The second stage utilizes a predefined scientometric research analysis report for generating a new report for the selected country. A case study on big data analysis for Saudi Arabian research publications was conducted. An evaluation was performed on the report within 10 evaluators to understand the practicability of the proposed model. They evaluated the report through four-point criteria on the user perceptions. The results indicated that the model was able to successfully create quite a pleasing scientific report in terms of factuality, coherency, sufficiency, and its ability to impart new knowledge for the readers. © 2013 IEEE.","Automated report generation; Big data; Coefficient collaboration; Collaborative research; Growth rate; scientometrics","Big data; Data mining; Digital storage; Growth rate; Automated report generation; Bibliometric; Case-studies; Coefficient collaboration; Collaboration; Collaborative research; Report generation; Research analysis; Scientometrics; Writing; Database systems",Article,Scopus,2-s2.0-85119422891
"Yildizbasi A., Arioz Y.","57193810899;57264489500;","Green supplier selection in new era for sustainability: A novel method for integrating big data analytics and a hybrid fuzzy multi-criteria decision making",2022,"Soft Computing","26","1",,"253","270",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119278427&doi=10.1007%2fs00500-021-06477-8&partnerID=40&md5=a5791da52846ec85a93de9a50ccbb686","Environmentally conscious supplier selection has become increasingly important in recent years. Green supplier selection is one of the vital decisions of supply chain management, as it is preferred for businesses in the market that adopt an environmental approach and green philosophy in line with material and moral benefits. In this context, the problem of choosing the most efficient green supplier is addressed with a three-step methodology using big data analytics that includes an integrated approach and hybrid fuzzy AHP-TOPSIS techniques. First of all, big data plays an important role in delivering meaningful results by reducing complexity to a more fundamental level. It is possible to obtain more consistent results by examining a series of criteria in green supplier selection at a more reasonable and operational level. Thus, the role of big data analytics provides an input for decision-making, which enables a systematic reduction to more concise data. Then, these inputs are evaluated in a fuzzy environment with hybrid MCDM techniques and the most efficient green supplier is determined among the suppliers. This authentic study sheds light on providing a significant competitive advantage to businesses in line with their strategic targets as well as having environmental contributions to sustainability. Graphical abstract: [Figure not available: see fulltext.] © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Big data analytics; Fuzzy decision making; Green supplier selection; Hybrid multi-criteria decision making; Sustainability","Advanced Analytics; Big data; Competition; Data Analytics; Supply chain management; Sustainable development; Fuzzy - Multi criteria decision making; Fuzzy Decision making; Green supplier; Green supplier selections; Hybrid multi-criteria decision making; Multi criteria decision-making; Multicriteria decision-making; Multicriterion decision makings; Novel methods; Supplier selection; Decision making",Article,Scopus,2-s2.0-85119278427
"Sepehri A., Vandchali H.R., Siddiqui A.W., Montewka J.","57222325108;55293744700;56818482100;57205016800;","The impact of shipping 4.0 on controlling shipping accidents: A systematic literature review",2022,"Ocean Engineering","243",,"110162","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119249322&doi=10.1016%2fj.oceaneng.2021.110162&partnerID=40&md5=49f1f28e1bf221f91a0f7899bddbbadd","Maritime shipping, with a significant role in global trade, confronts various accidents leading to loss of lives, properties, and the environment. Shipping 4.0 technologies are scaling up to address this problem by employing real-time data-driven technologies, including cyber-physical systems, advanced tracking and tracing, intelligent systems, and big data analytics. Despite growing attention, there is a general lack of clarity on the level and direction of progress in this field. Accordingly, this study aims to identify critical shipping accident risks, analyze the role of relevant shipping 4.0 technologies in controlling these risks, and consolidate the findings into a conceptual guiding framework directing future developments. Accordingly, a systematic review is performed that reveals how shipping 4.0 approaches address critical accident risks and the gaps that still exist. Overall, we found that the collision is the most frequent accident referred to, while the most frequent technology to control the accidents is the Automatic Identification System. In contrast, we see an evident lack of cloud computing, internet-of-things, and big data analytics, which play crucial roles in current industry 4.0 developments. © 2021 The Authors","Accident risks; Industry 4.0; Maritime; Shipping 4.0; Shipping accidents","Accidents; Automation; Big data; Data Analytics; Embedded systems; Industry 4.0; Intelligent systems; Real time systems; Accident risks; Global trade; Loss of life; Maritime; Maritime shipping; Property; Scaling-up; Shipping 4.0; Shipping accident; Systematic literature review; Ships",Article,Scopus,2-s2.0-85119249322
"Santilli V.","57191539232;","Application of machine learning techniques to physical and rehabilitative medicine",2022,"Annali di Igiene Medicina Preventiva e di Comunita","34","1",,"79","83",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119059676&doi=10.7416%2fai.2021.2444&partnerID=40&md5=09487a1bb60009311ae1ea2c17230686","Nowadays, digital information has increased exponentially in every field to such an extent that it generates huge amounts of electronic data, namely Big Data. In the field of Artificial Intelligence, Machine Learning can be exploited in order to transform the large amount of information to improve decision-making. We retrospectively evaluated the data collected from 2016 to 2018, using the database of approximately 4000 rehabilitation hospital discharges (SDO) of the Latium Region (Italy). Three models of machine learning algorithms were considered: Support of vector machine; Neural networks; Random forests. Applying this model, the estimate of the average error is 9.077, and specifically, considering the distinction between orthopedic and neurological patients, the average error obtained is 7.65 for orthopedic and 10.73 for neurological patients. SDO information flow can be used to represent and quantify the potential inadequacy and inefficiency of rehabilitation hospitalizations, although there are limitations such as the absence of description of pre-pathological conditions, changes in health status from the beginning to the end of hospitalization, specific short- and long-term outcomes of rehabilitation, services provided during hospitalization, as well as psycho-social variables. Furthermore, information from wearable devices capable of providing clinical parameters and movement data could be integrated into the dataset. © Società Editrice Universo (SEU), Roma, Italy","big data; data mining; disability; Machine learning; rehabilitation","algorithm; artificial intelligence; factual database; human; machine learning; retrospective study; Algorithms; Artificial Intelligence; Databases, Factual; Humans; Machine Learning; Retrospective Studies",Article,Scopus,2-s2.0-85119059676
"Jenni R.S., Shankar S.","57336869100;57220145130;","Semantic based greedy levy gradient boosting algorithm for phishing detection",2022,"Computer Systems Science and Engineering","41","2",,"525","538",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118974292&doi=10.32604%2fcsse.2022.019300&partnerID=40&md5=26bab7510fe3c42d75d816fc9b72fe92","The detection of phishing and legitimate websites is considered a great challenge for web service providers because the users of such websites are indistinguishable. Phishing websites also create traffic in the entire network. Another phishing issue is the broadening malware of the entire network, thus highlighting the demand for their detection while massive datasets (i.e., big data) are processed. Despite the application of boosting mechanisms in phishing detection, these methods are prone to significant errors in their output, specifically due to the combination of all website features in the training state. The upcoming big data system requires MapReduce, a popular parallel programming, to process massive datasets. To address these issues, a probabilistic latent semantic and greedy levy gradient boosting (PLS-GLGB) algorithm for website phishing detection using MapReduce is proposed. A feature selection-based model is provided using a probabilistic intersective latent semantic preprocessing model to minimize errors in website phishing detection. Here, the missing data in each URL are identified and discarded for further processing to ensure data quality. Subsequently, with the preprocessed features (URLs), feature vectors are updated by the greedy levy divergence gradient (model) that selects the optimal features in the URL and accurately detects the websites. Thus, greedy levy efficiently differentiates between phishing websites and legitimate websites. Experiments are conducted using one of the largest public corpora of a website phish tank dataset. Results show that the PLS-GLGB algorithm for website phishing detection outperforms state-of-the-art phishing detection methods. Significant amounts of phishing detection time and errors are also saved during the detection of website phishing. © 2022 CRL Publishing. All rights reserved.","Big data; Divergence; Gradient; Greedy levy; Latent semantic; Phishing detection; Probabilistic intersective; Web service providers","Big data; Computer crime; Data handling; Errors; Feature extraction; Parallel programming; Semantic Web; Semantics; Web services; Boosting algorithm; Divergence; Gradient; Gradient boosting; Greedy levy; Latent semantics; Phishing detections; Probabilistic intersective; Probabilistics; Web service providers; Websites",Article,Scopus,2-s2.0-85118974292
"Ragavan N., Rubavathi C.Y.","57203150872;57223000187;","A novel big data storage reduction model for drill down search",2022,"Computer Systems Science and Engineering","41","1",,"373","387",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118968774&doi=10.32604%2fcsse.2022.020452&partnerID=40&md5=0439708619bda732bd83bc2a0e10ea0c","Multi-level searching is called Drill down search. Right now, no drill down search feature is available in the existing search engines like Google, Yahoo, Bing and Baidu. Drill down search is very much useful for the end user to find the exact search results among the huge paginated search results. Higher level of drill down search with category based search feature leads to get the most accurate search results but it increases the number and size of the file system. The purpose of this manuscript is to implement a big data storage reduction binary file system model for category based drill down search engine that offers fast multilevel filtering capability. The basic methodology of the proposed model stores the search engine data in the binary file system model. To verify the effectiveness of the proposed file system model, 5 million unique keyword data are stored into a binary file, thereby analysing the proposed file system with efficiency. Some experimental results are also provided based on real data that show our storage model speed and superiority. Experiments demonstrated that our file system expansion ratio is constant and it reduces the disk storage space up to 30% with conventional database/file system and it also increases the search performance for any levels of search. To discuss deeply, the paper starts with the short introduction of drill down search followed by the discussion of important technologies used to implement big data storage reduction system in detail. © 2022 CRL Publishing. All rights reserved.","Big data; Binary file system; Drill down search; Storage reduction model","Data reduction; Digital storage; Drills; File organization; Infill drilling; Search engines; Binary file system; Binary files; Data storage; Drill down search; Drill-down; Filesystem; Multilevels; Reduction models; Storage reduction model; System models; Big data",Article,Scopus,2-s2.0-85118968774
"Liu C.","57331226700;","Research on evaluation and promotion strategy of guilin tourism innovation ability based on big data analysis",2022,"Journal of Applied Science and Engineering (Taiwan)","25","2",,"321","327",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118939088&doi=10.6180%2fjase.202204_25%282%29.0008&partnerID=40&md5=fc31384a6ff0562d146806c5fe0bf832","In order to build an intelligent tourism innovation ability evaluation system, this paper uses data mining technology to mine tour-ism information, combines modern people's tourism needs to construct a regional innovation ability evaluation system, and detects the effect of data mining with the support of logistic regression analysis. Moreover, this paper uses the life cycle theory framework to analyze the life cycle stages of regional tourism destinations, analyzes the internal factors that form the characteristics and laws of specific stages, and takes timely measures to artificially control and adjust the problems that need attention. In addition, this paper constructs an evaluation system of tourism innovation capability with intelligent effects. On this basis, this paper combines experimental analysis to evaluate the system constructed in this paper, and takes Guilin regional tourism as an example to perform the analysis. The research results show that the system constructed in this paper has a certain effect, and the corresponding suggestions are given. © 2022 Tamkang University. All right reserved.","Ability evaluation; Big data analysis; Improvement strategy; Tourism innovation","3D modeling; Big data; Data mining; Life cycle; Regional planning; Regression analysis; Tourism; Ability evaluation; Big data analyse; Data mining technology; Evaluation strategies; Improvement strategies; Innovation abilities; Logistic regression analysis; Promotion strategies; Regional innovation; Tourism innovation; Data handling",Article,Scopus,2-s2.0-85118939088
"Zhou X., Sun C., Niu X., Shi C.","57191062592;57328907800;57188624006;57204552247;","The modifiable areal unit problem in the relationship between jobs–housing balance and commuting distance through big and traditional data",2022,"Travel Behaviour and Society","26",,,"270","278",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118858132&doi=10.1016%2fj.tbs.2021.11.001&partnerID=40&md5=64caf9077f4c1092b4cacc395ebb3380","Jobs–housing balance aims to reduce commuting distance and alleviate traffic congestion as a transport policy. However, the measurement of jobs–housing balance is affected by the spatial areal unit employed in analysis and planning. The mechanism by which commuting distance is affected by jobs–housing balance remains controversial for several reasons, including inconsistency of spatial analysis units and whether socio-demographic attributes are controlled. Cellphone data exhibit superiority in identifying the journey-to-work trips for large sample sizes and entail high spatial precision that can be integrated into different analysis units. By combining cellphone data and traditional survey data with detailed socio-demographic attributes aggregated at different analysis units, this study examines the scale and zoning issues of jobs–housing relationships in Shanghai, China. When using large analysis units, the effect of jobs–housing balance was expanded and that of different socio-demographic attributes changed variously and unpredictably. Furthermore, the uniformity of the spatial analysis unit influenced the effect of jobs–housing balance on commuting distance. This study substantiates the effect of the modifiable areal unit problem (MAUP) on the relationship between jobs–housing balance and commuting distance considering socio-demographic attributes and demonstrates that the changing rules of the effects of that balance on the commuting distance are different from those of socio-demographic attributes as the scale changes. Consequently, this work enriches the literature on the MAUP in relation to the jobs–housing balance and obtains a thorough understanding of the MAUP effects through combining big and traditional data. © 2021 Hong Kong Society for Transportation Studies","Big data; Commuting distance; Jobs–housing balance; Modifiable areal unit problem; Multi scale",,Article,Scopus,2-s2.0-85118858132
"Chen X., Zhang J., Fu C.-W., Fekete J.-D., Wang Y.","57203691053;56191418000;7402802915;7005772385;22636172600;","Pyramid-based Scatterplots Sampling for Progressive and Streaming Data Visualization",2022,"IEEE Transactions on Visualization and Computer Graphics","28","1",,"593","603",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118684653&doi=10.1109%2fTVCG.2021.3114880&partnerID=40&md5=fefefb2dfe153ebdd7bbb1b41e2dee9a","We present a pyramid-based scatterplot sampling technique to avoid overplotting and enable progressive and streaming visualization of large data. Our technique is based on a multiresolution pyramid-based decomposition of the underlying density map and makes use of the density values in the pyramid to guide the sampling at each scale for preserving the relative data densities and outliers. We show that our technique is competitive in quality with state-of-the-art methods and runs faster by about an order of magnitude. Also, we have adapted it to deliver progressive and streaming data visualization by processing the data in chunks and updating the scatterplot areas with visible changes in the density map. A quantitative evaluation shows that our approach generates stable and faithful progressive samples that are comparable to the state-of-the-art method in preserving relative densities and superior to it in keeping outliers and stability when switching frames. We present two case studies that demonstrate the effectiveness of our approach for exploring large data. © 1995-2012 IEEE.","big data; progressive visualization; pyramid; sampling; scalability; Scatterplots; streaming visualization","Big data; Data handling; Data visualization; Statistics; Visualization; Density maps; Large data; Progressive visualization; Pyramid; Sampling method; Scatter plots; State-of-the-art methods; Streaming data; Streaming visualization; Visual analytics; Scalability; article; case report; clinical article; data visualization; human; quantitative analysis; relative density",Article,Scopus,2-s2.0-85118684653
"Dozier J.","7005128992;","Revisiting Topographic Horizons in the Era of Big Data and Parallel Computing",2022,"IEEE Geoscience and Remote Sensing Letters","19",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118665187&doi=10.1109%2fLGRS.2021.3125278&partnerID=40&md5=422ca1c6a803ddf5aca9e7fa41348ec7","Widely used to calculate illumination geometry for estimates of solar and emitted longwave radiation, and for correcting remotely sensed data for topographic effects, digital elevation models (DEMs) are now extensive globally at 10-30-m spatial resolution and locally at spatial resolutions down to a few centimeters. Globally, regionally, or locally, elevation datasets have many grid points. Many software packages calculate gradients over every grid cell or point, but in the mountains, shading by nearby terrain must also be assessed. Terrain may obscure a slope that would otherwise face the Sun. Four decades ago, a fast method to calculate topographic horizons at every point in an elevation grid required computations related only linearly to the size of the grid, but grids now have so many points that parallel computing still provides an advantage. Exploiting parallelism over terrain grids can use alternative strategies: among columns of a rotated grid, or simultaneously at multiple rotation angles, or on different tiles of a grid. On a multi-processor machine, the improvement in computing time approaches 2/3 the number of processors deployed. © 2004-2012 IEEE.","Big data applications; digital elevation models (DEMs); parallel processing; surface topography","Big data; Digital instruments; Forestry; Geomorphology; MATLAB; Parallel processing systems; Radiation effects; Surface topography; Surveying; Azimuth; Big data applications; Code; Computational modelling; Digital elevation model; Matlab; Parallel com- puting; Parallel processing; Spatial resolution; Landforms; fault geometry; geometry; parallel computing; spatial resolution; topographic mapping; topography",Article,Scopus,2-s2.0-85118665187
"Du X.","57326558900;","Research on roof waterproof construction of agricultural construction engineering based on big data technology",2022,"Acta Agriculturae Scandinavica Section B: Soil and Plant Science","72","1",,"43","53",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118652140&doi=10.1080%2f09064710.2021.1990387&partnerID=40&md5=09b52a6ccddc048296d3b70f2ca56ba6","To improve the roof waterproof construction effect of agricultural construction projects, this paper applies big data technology to construction management and construction decision-making of construction projects. Moreover, this paper improves the traditional big data technology, and from the perspective of the roof waterproof construction requirements of agricultural construction projects, this paper improves data mining to make it a core intelligent algorithm that can be used for data recognition and analysis of construction projects. In addition, this paper simulates the construction process according to the roof waterproof construction process of agricultural construction projects, builds the corresponding intelligent construction simulation model and builds the agricultural construction engineering roof waterproof construction simulation through simulation research. Finally, after obtaining the data, this paper conducts experimental research and applies the data mining system of this paper to the analysis of agricultural construction engineering. From the experimental research, it can be known that the method proposed in this paper has a certain effect. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","agricultural construction; Big data; binary tree segmentation; construction; roof waterproofing; space position index; visualised data system",,Article,Scopus,2-s2.0-85118652140
"Rudniy A.","25929527800;","Datawarehouse design for big data in academia",2022,"Computers, Materials and Continua","71","1",,"979","992",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118579404&doi=10.32604%2fcmc.2022.016676&partnerID=40&md5=063590b02bd6d4e56c8b96553cb02c08","This paper describes the process of design and construction of a datawarehouse (""DW"") for an online learning platformusing three prominent technologies, Microsoft SQL Server, MongoDB and Apache Hive. The three systems are evaluated for corpus construction and descriptive analytics. The case also demonstrates the value of evidence-centered design principles for data warehouse design that is sustainable enough to adapt to the demands of handling big data in a variety of contexts. Additionally, the paper addresses maintainability-performance tradeoff, storage considerations and accessibility of big data corpora. In this NSF-sponsored work, the data were processed, transformed, and stored in the three versions of a data warehouse in search for a better performing and more suitable platform. The data warehouse engines-a relational database, a No-SQL database, and a big data technology for parallel computations-were subjected to principled analysis. Design, construction and evaluation of a data warehouse were scrutinized to find improved ways of storing, organizing and extracting information. The work also examines building corpora, performing ad-hoc extractions, and ensuring confidentiality. It was found that Apache Hive demonstrated the best processing time followed by SQL Server and MongoDB. In the aspect of analytical queries, the SQL Server was a top performer followed by MongoDB and Hive. This paper also discusses a novel process for render students anonymity complying with Family Educational Rights and Privacy Act regulations. Five phases for DW design are recommended: 1) Establishing goals at the outset based on Evidence-Centered Design principles; 2) Recognizing the unique demands of student data and use; 3) Adopting a model that integrates cost with technical considerations; 4) Designing a comparative database and 5) Planning for a DW design that is sustainable. Recommendations for future research include attempting DW design in contexts involving larger data sets, more refined operations, and ensuring attention is paid to sustainability of operations. © 2022 Tech Science Press. All rights reserved.","Apache hive; Big data; Data warehouse; MongoDB; SQL server","Data handling; Data warehouses; Design; Digital storage; Students; Windows operating system; Apache hive; Corpus construction; Design and construction; Design Principles; Microsoft SQL Server; MongoDB; Online learning; Performance tradeoff; SQL servers; Warehouse design; Big data",Article,Scopus,2-s2.0-85118579404
"Ding X., Lv R., Pang X., Hu J., Wang Z., Yang X., Li X.","23097128600;57203162513;57203225064;57200112413;24537844500;57325232000;36509118600;","Privacy-preserving task allocation for edge computing-based mobile crowdsensing",2022,"Computers and Electrical Engineering","97",,"107528","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118578514&doi=10.1016%2fj.compeleceng.2021.107528&partnerID=40&md5=6cd4b338b22e4f8d1e69de3d641f0c76","In the era of big data, edge computing has coped greatly with the increase in data. Recently, edge computing has been incorporated into mobile crowdsensing (MCS) to collect large-scale data, but existing edge computing-based MCS (EC-MCS) ideally assumes that edge servers are trusted. In this paper, a novel mechanism is proposed that we use semi-honest entities to securely and efficiently complete task assignment in large-scale crowdsensing. Firstly, homomorphic encryption is used to encrypt users’ location information, and the collaboration between edge servers is used to complete task allocation under cipher-text. Then, the optimal users are selected to complete tasks and upload the encrypted sensing data. Moreover, a secure payment mechanism is proposed to avoid fraud problems in semi-honest edge servers. Finally, we analyze the security of our scheme theoretically and conduct a multi-dimensional simulation experiment to prove the effectiveness and availability of the proposed scheme. © 2021 Elsevier Ltd","Big data; Crowdsensing; Edge computing; Homomorphic encryption; Privacy preservation; Task allocation","Big data; Cryptography; Data privacy; Crowdsensing; Data edges; Edge computing; Edge server; Ho-momorphic encryptions; Homomorphic-encryptions; Large scale data; Privacy preservation; Privacy preserving; Task allocation; Edge computing",Article,Scopus,2-s2.0-85118578514
"Park W., Qureshi N.M.F., Shin D.R.","57194058144;57191380223;57325190700;","Pseudo nlp joint spam classification technique for big data cluster",2022,"Computers, Materials and Continua","71","1",,"517","535",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118576468&doi=10.32604%2fcmc.2022.021421&partnerID=40&md5=4be485e5632a7f02ae9dbf1e51e590ac","Spammail classification considered complex and error-prone task in the distributed computing environment. There are various available spammail classification approaches such as the naive Bayesian classifier, logistic regression and support vector machine and decision tree, recursive neural network, and long short-term memory algorithms. However, they do not consider the document when analyzing spam mail content. These approaches use the bagof- words method, which analyzes a large amount of text data and classifies features with the help of term frequency-inverse document frequency. Because there are many words in a document, these approaches consume a massive amount of resources and become infeasible when performing classification on multiple associated mail documents together. Thus, spam mail is not classified fully, and these approaches remain with loopholes. Thus, we propose a term frequency topic inverse document frequency model that considers the meaning of text data in a larger semantic unit by applying weights based on the document's topic. Moreover, the proposed approach reduces the scarcity problem through a frequency topic-inverse document frequency in singular value decomposition model. Our proposed approach also reduces the dimensionality, which ultimately increases the strength of document classification. Experimental evaluations show that the proposed approach classifies spam mail documents with higher accuracy using individual document-independent processing computation. Comparative evaluations show that the proposed approach performs better than the logistic regression model in the distributed computing environment, with higher document word frequencies of 97.05%, 99.17% and 96.59%. © 2022 Tech Science Press. All rights reserved.","Big data; Machine learning; NLP; Spam mail; TFT-IDF","Big data; Classification (of information); Decision trees; Information retrieval systems; Inverse problems; K-means clustering; Natural language processing systems; Neural networks; Semantics; Singular value decomposition; Text processing; Classification technique; Data clusters; Distributed computing environment; Error prone tasks; Inverse Document Frequency; Mail documents; Spam classification; Spam mails; Text data; TFT-IDF; Support vector machines",Article,Scopus,2-s2.0-85118576468
"AL-Marghilani A.A.","23468709200;","Target detection algorithm in crime recognition using artificial intelligence",2022,"Computers, Materials and Continua","71","1",,"809","824",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118570272&doi=10.32604%2fcmc.2022.021185&partnerID=40&md5=1be76cf0f4759951332b0e1435aa16bf","Presently, suspect prediction of crime scenes can be considered as a classification task, which predicts the suspects based on the time, space, and type of crime. Performing digital forensic investigation in a big data environment poses several challenges to the investigational officer. Besides, the facial sketches are widely employed by the law enforcement agencies for assisting the suspect identification of suspects involved in crime scenes. The sketches utilized in the forensic investigations are either drawn by forensic artists or generated through the computer program(composite sketches) based on the verbal explanation given by the eyewitness or victim. Since this suspect identification process is slow and difficult, it is required to design a technique for a quick and automated facial sketch generation. Machine Learning (ML) and deep learning (DL) models find it useful to automatically support the decision of forensics experts. The challenge is the incorporation of the domain expert knowledge with DL models for developing efficient techniques to make better decisions. In this view, this study develops a new artificial intelligence (AI) based DL model with face sketch synthesis (FSS) for suspect identification (DLFSS-SI) in a big data environment. The proposed method performs preprocessing at the primary stage to improvise the image quality. In addition, the proposed model uses a DL based MobileNet (MN) model for feature extractor, and the hyper parameters of the MobileNet are tuned by quasi oppositional firefly optimization (QOFFO) algorithm. The proposed model automatically draws the sketches of the input facial images.Moreover, a qualitative similarity assessment takes place with the sketch drawn by a professional artist by the eyewitness. If there is a higher resemblance between the two sketches, the suspect will be determined. To validate the effective performance of the DLFSS-SI method, a detailed qualitative and quantitative examination takes place. The experimental outcome stated that the DLFSSSI model has outperformed the compared methods in terms of mean square error (MSE), peak signal to noise ratio (PSNR), average actuary, and average computation time. © 2022 Tech Science Press. All rights reserved.","Artificial intelligence; Big data; Deep learning; Face sketch synthesis; Suspect identification","Crime; Deep learning; Digital forensics; Drawing (graphics); Face recognition; Learning algorithms; Mean square error; Optimization; Palmprint recognition; Signal detection; Signal to noise ratio; Classification tasks; Crime scenes; Data environment; Deep learning; Face sketch synthesis; Forensic investigation; Learning models; Suspect identification; Target detection algorithm; Time-space; Big data",Article,Scopus,2-s2.0-85118570272
"Kang J., Kong H., Lin Z., Dang A.","57223930216;57324032600;57223921846;7005839067;","Mapping the dynamics of electric vehicle charging demand within Beijing's spatial structure",2022,"Sustainable Cities and Society","76",,"103507","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118563864&doi=10.1016%2fj.scs.2021.103507&partnerID=40&md5=363b472150ef58a1226b0130f0554d15","Electric vehicles have been proliferating in large cities across the world, and we increasingly face challenges in estimating charging demand and in planning EV infrastructure. Focusing on Beijing as a case study, this research uses a novel data-driven method to measure the Charging Demand Indicators (CDI) derived from location-based service big data. Analyses through kernel density function reveal dynamic relations between the spatial patterns of CDI and the distribution of Public Charging Stations (PCS). Spatial match examination is conducted to discover areas of mismatch between charging demand and infrastructure supply. The results expose a CDI pattern which, although largely complies with the city's centripetal structure, demonstrates variations between weekdays and weekends and by EV travel distances. A spatial regression model confirms the influence of urban structure and distribution of amenities on EV charging behavior and suggests that particular land uses and location features have a significant association with EV charging demand. These findings shed light on the understanding of the spatial disparity between the CDI pattern and the current PCS distribution, which could inform future urban policies and planning of EV infrastructure with an emphasis on its coordination with land use, physical layout, and transit. © 2021","Charging demand indicator; Electric vehicle; Geospatial big data; Public charging station; Spatial regression model","Charging (batteries); Electric vehicles; Land use; Location based services; Regression analysis; Telecommunication services; Charging demand indicator; Charging demands; Charging station; Electric vehicle charging; EV Charging; Geo-spatial; Geospatial big data; Indicator patterns; Public charging station; Spatial regression model; Big data",Article,Scopus,2-s2.0-85118563864
"Nayak H.S., Silva J.V., Parihar C.M., Kakraliya S.K., Krupnik T.J., Bijarniya D., Jat M.L., Sharma P.C., Jat H.S., Sidhu H.S., Sapkota T.B.","57201700335;56342538700;35180142500;57194783073;55183790300;57193225365;6602564835;35466806200;7801446851;36747674500;39061985100;","Rice yield gaps and nitrogen-use efficiency in the Northwestern Indo-Gangetic Plains of India: Evidence based insights from heterogeneous farmers’ practices",2022,"Field Crops Research","275",,"108328","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118542630&doi=10.1016%2fj.fcr.2021.108328&partnerID=40&md5=b28aceee38f63d7797d739fc7cbac4ee","A large database of individual farmer field data (n = 4,107) for rice production in the Northwestern Indo-Gangetic Plains of India was used to decompose rice yield gaps and to investigate the scope to reduce nitrogen (N) inputs without compromising yields. Stochastic frontier analysis was used to disentangle efficiency and resource yield gaps, whereas data on rice yield potential in the region were retrieved from the Global Yield Gap Atlas to estimate the technology yield gap. Rice yield gaps were small (ca. 2.7 t ha−1, or 20% of potential yield, Yp) and mostly attributed to the technology yield gap (ca. 1.8 t ha−1, or ca. 15% of Yp). Efficiency and resource yield gaps were negligible (less than 5% of Yp in most districts). Small yield gaps were associated with high input use, particularly irrigation water and N, for which small yield responses were observed. N partial factor productivity (PFP-N) was 45–50 kg grain kg−1 N for fields with efficient N management and approximately 20% lower for the fields with inefficient N management. Improving PFP-N appears to be best achieved through better matching of N rates to the variety types cultivated and by adjusting the amount of urea applied in the 3rd split in correspondance with the amount of diammonium-phosphate applied earlier in the season. Future studies should assess the potential to reduce irrigation water without compromising rice yield and to broaden the assessment presented here to other indicators and at the cropping systems level. © 2021 The Authors","Big data; Fertilizer management; Global yield gap atlas; Stochastic frontier analysis; Sustainability assessment","crop yield; factor productivity; heterogeneity; irrigation; nitrogen; rice; yield response; Gangetic Plain; India",Article,Scopus,2-s2.0-85118542630
"Anthopoulos L., Kazantzi V.","35809721600;9277078100;","Urban energy efficiency assessment models from an AI and big data perspective: Tools for policy makers",2022,"Sustainable Cities and Society","76",,"103492","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118507585&doi=10.1016%2fj.scs.2021.103492&partnerID=40&md5=9e97cbefc893a904380ca2873346f535","Although energy efficiency is quite a cliché term, it is a topic that attracts an increasing attention the last decade, especially in the context of cities and as a means to address emerging challenges like sustainability and climate change. Several models have been introduced to conceptualize and calculate the urban energy system, and to demonstrate the variants that calibrate the local energy efficiency. Nevertheless, cutting-edge technologies like blockchain, electrical -and even autonomous- vehicles, smart building systems, Artificial Intelligence (AI) and big data etc. are growing within cities and question the identified urban energy efficiency, since they demand enormous amounts of power. In this regard, policy makers are concerned of the emerging technologies’ energy efficiency and their impact on the urban energy system and they attempt to introduce corresponding standards for their development. This article focuses on the impact of AI and big data in city's energy efficiency. More specifically, a literature analysis is performed and returned a taxonomy of existing energy efficiency assessment models under the lens of AI and big data. Moreover, the definition of a unified assessment model for AI and big data energy efficiency is approached. © 2021 Elsevier Ltd","AI; Assessment; Big data; City; Energy efficiency; Models; Smart City","Big data; Climate change; Decision making; Smart city; Assessment; Assessment models; Block-chain; City; Cutting edge technology; Energy efficiency assessment; Local energy; Policy makers; Urban energy; Urban energy systems; Energy efficiency",Article,Scopus,2-s2.0-85118507585
"Komatsu Y., Hironaka S., Tanizawa Y., Cai Z., Piao Y., Boku N.","7202336052;35495679100;57222021246;57155614600;56035147000;55359026700;","Treatment Pattern for Advanced Gastric Cancer in Japan and Factors Associated with Sequential Treatment: A Retrospective Administrative Claims Database Study",2022,"Advances in Therapy","39","1",,"296","313",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118325239&doi=10.1007%2fs12325-021-01931-3&partnerID=40&md5=ec4795afcce9b298ced496c6dc08e2d6","Introduction: Clinical trials have proven the efficacy and safety of new therapies for advanced gastric cancer (AGC), but how those therapies are used in the real world is poorly described. Real-world treatment patterns of antitumor therapies and factors associated with overall therapy duration in patients with AGC in Japan were investigated. Methods: This retrospective cohort study used a Japanese administrative claims database (June 2014 to September 2019). Patients with AGC who started the guideline-recommended first-line combination regimens with platinum and fluoropyrimidine agents between June 2015 and July 2019 were included. Cox regression analysis was performed to identify factors associated with overall therapy duration (first line to last administration of guideline-listed agent). Results: Of the 10,581 patients included, the most common first-line combination regimen without trastuzumab was S-1 plus oxaliplatin (4327/9069 patients; 47.7%) and with trastuzumab was capecitabine plus cisplatin (608/1512 patients; 40.2%). Most common second- and third-line regimens were ramucirumab plus taxane (3650/5358 patients; 68.1%) and nivolumab (1229/2390 patients; 51.4%), respectively. Factors positively associated with longer overall therapy duration were: oral fluoropyrimidine in first line (hazard ratio [95% confidence interval]: 0.63 [0.57–0.69]); trastuzumab in any line (0.73 [0.68–0.78]); treatment at a designated cancer hospital (0.89 [0.84–0.94]); dietary consultation within 1 month before/after start of first line (0.92 [0.86–0.98]); and treatment at a surgical department (0.94 [0.89–0.99]). Negatively associated factors were: edema (1.21 [1.07–1.37]); physical therapy (1.21 [1.12–1.31]); nutritional intervention (1.21 [1.14–1.28]) within 1 month before/after start of first line; thrombosis (1.13 [1.04–1.23]); renal disease (1.11 [1.02–1.21]); age (1.07 [1.02–1.13]); and peritoneal metastasis/ascites (1.06 [1.01–1.13]). Conclusions: In real-world treatment practice for AGC in Japan, therapy choice after the recommended first-line chemotherapy was consistent with guidelines. Factors associated with overall therapy duration were identified, which may assist in optimizing treatment sequence. © 2021, The Author(s).","Advanced gastric cancer; Big data; Real-world evidence; Treatment sequence",,Article,Scopus,2-s2.0-85118325239
"Hylving L., Lindberg S.","56102283900;55800478000;","Ethical dilemmas and big data: The case of the swedish transport administration",2022,"International Journal of Knowledge Management","18","1",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118307834&doi=10.4018%2fIJKM.290021&partnerID=40&md5=78c920d113b4cdbe24c893327c6ceffc","Using big data in organizations has the potential to improve innovation, accuracy, and efficiency. Big data is also connected with risks for both the organization and society at large. It is therefore vital to improve our understanding of the potential consequences of implementing and using big data. The researchers studied the Swedish Transport Administration to understand their attitude towards implementing big data to predict, for example the need for road maintenance. The analysis identified four moral dilemmas that the organization deals with in connection to big data. The researchers discuss these dilemmas from the perspective of practical wisdom. Practical wisdom is manifested in context-dependent actions connected to open-mindedness, reflection, and judgment. It can be summed up as “the reasonable thing to do” in a unique situation where “not-knowing” is a helpful resource when making wise decisions. This paper seeks to shed light on the importance of practical wisdom when implementing big data. Copyright © 2022, IGI Global.","Big Data; Case Study; Dilemmas; Ethics; Practical Wisdom","Philosophical aspects; Case-studies; Context dependent; Dilemma; Ethical dilemma; In contexts; Practical wisdom; Road maintenance; Swedishs; Big data",Article,Scopus,2-s2.0-85118307834
"Wu E.H.C., Hu J., Chen R.","7202128063;57317558700;57317277800;","Monitoring and forecasting COVID-19 impacts on hotel occupancy rates with daily visitor arrivals and search queries",2022,"Current Issues in Tourism","25","3",,"490","507",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118280087&doi=10.1080%2f13683500.2021.1989385&partnerID=40&md5=d5b53c7248f5e7a0bf407e3469d847af","The COVID-19 pandemic is greatly affecting the hospitality industry worldwide. Lodging demand is severely reduced as people's fear of coronavirus spreading risk in hotels. This research makes a timely contribution to the hospitality literature by proposing the mixed data sampling models (MIDAS) to monitor and forecast latest hotel occupancy rates with high-frequency big data sources, such as daily visitor arrivals and search query data. Quantitative evidence from Macau from January to July 2020 confirms that MIDAS models can measure the dynamic impacts of the COVID-19 pandemic on hotel occupancy and have a better prediction accuracy and explanation ability than competitive models. Industry practitioners can adopt this big data analytical framework to make daily or monthly updates of lodging demand, conduct scenario analysis, plan and trace the recovery schedule during and post COVID-19 phases. Finally, managerial implications and future work are highlighted. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","Big data; COVID-19; forecast; hotel occupancy rate; search query data; visitor arrivals","arrival date; COVID-19; epidemic; forecasting method; health risk; hotel industry; monitoring system; prediction; research work; sampling; China; Macau",Article,Scopus,2-s2.0-85118280087
"Zhou X., Cheng H., Chen F.","57316358600;57315917100;55497713000;","A connection access mechanism of distributed network based on block chain",2022,"International Journal of Circuits, Systems and Signal Processing","16",,,"224","231",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118196101&partnerID=40&md5=ae92488dc9eda696e2693daf90fd6ce1","Cross-border payment optimization technology based on block chain has become a hot spot in the industry. The traditional method mainly includes the block feature detection method, the fuzzy access method, the adaptive scheduling method, which perform related feature extraction and quantitative regression analysis on the collected distributed network connection access data, and combine the fuzzy clustering method to optimize the data access design, and realize the group detection and identification of data in the block chain. However, the traditional method has a large computational overhead for distributed network connection access, and the packet detection capability is not good. This paper constructs a statistical sequence model of adaptive connection access data to extract the descriptive statistical features of the distributed network block chain adaptive connection access data similarity. The performance of the strategy retrieval efficiency in the experiment is tested based on the strategy management method. The experiment performs matching query tests on the test sets of different query sizes. The different parameters for error rate and search delay test are set to evaluate the impact of different parameters on retrieval performance. The calculation method of single delay is the total delay or the total number of matches. The optimization effect is mainly measured by the retrieval delay of the strategy in the strategy management contract; the smaller the delay, the higher the execution efficiency, and the better the retrieval optimization effect. © 2022, North Atlantic University Union NAUN. All rights reserved.","Big data; Block chain; Connection access; Distributed network; Internet finance","Data mining; Efficiency; Feature extraction; Fuzzy clustering; Regression analysis; Access mechanism; Block-chain; Connection access; Cross-border; Distributed networks; Internet finance; Network connection; Network-based; Optimization effects; Strategy management; Big data",Article,Scopus,2-s2.0-85118196101
"Xu Z., Zhu G., Metawa N., Zhou Q.","57314963100;57217315490;57193643982;57109587900;","Machine learning based customer meta-combination brand equity analysis for marketing behavior evaluation",2022,"Information Processing and Management","59","1","102800","","",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118151715&doi=10.1016%2fj.ipm.2021.102800&partnerID=40&md5=af8a9186024a3186c30d4aaba7802ba1","At present, the focus of marketing research is mostly on the influencing factors, composition, and measurement of brand equity. The meta-combined brand equity analysis is based on two main research perspectives: financial perspective and customer perspective. While the financial perspective is based on the incremental discounted future cash flows that would result from a branded product's revenue over the revenue of an unbranded product, the brand equity from the customer's perspective is the consumer's reaction to brand marketing behavior, the impact on brand knowledge. The decision-making of marketing behaviors often faces choices related to ethics. Therefore, once the moral value of a company through marketing behavior is recognized by consumers, the ethical behavior presented in this article through marketing behavior will make consumers feel more about the brand. How does the brand equity of your customer's products affect you? In this experiment, shopping groups with the same shopping experience were selected. During the survey process, all customers in different periods and the same time were selected as far as possible based on the practicability of the survey. The study survey covered 4 main aspects; customer satisfaction, overall overview of customer satisfaction; the advantages and disadvantages of marketing strategies through quantitative analysis and to put forward reasonable marketing strategy improvement opinions and suggestions to improve customer satisfaction. Using the technique of parameter prediction of the financial industry, the experiment proved that the non-standard promotion behavior, the integrity of the enterprise and the social responsibility are three aspects (P<0.05) that have an impact on the customer's brand equity among the corporate marketing components. It was a detailed study of the current state of brand marketing strategies and customer satisfaction, found key indicators of brands that could improve customer satisfaction, and presented corresponding suggestions for optimizing marketing strategies. It shows that. It has the importance of good guidance and references to improve customer satisfaction in the industry. © 2021","Big Data; Brand equity; Brand marketing; Factor Analysis; Machine learning; Meta-combined approach","Big data; Consumer behavior; Customer satisfaction; Decision making; Finance; Philosophical aspects; Sales; Strategic planning; Surveys; Behavior evaluations; Brand equity; Brand marketing; Customer perspectives; Customers' satisfaction; Equity analysis; Factors analysis; Marketing research; Marketing strategy; Meta-combined approach; Machine learning",Article,Scopus,2-s2.0-85118151715
"Zarif A.","57221225835;","The ethical challenges facing the widespread adoption of digital healthcare technology",2022,"Health and Technology","12","1",,"175","179",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118136299&doi=10.1007%2fs12553-021-00596-w&partnerID=40&md5=0309ac827f85a3b4882c4869e5d9ed4e","With the rise of telemedicine, wearable healthcare, and the greater leverage of ‘big data’ for precision medicine, various challenges present themselves to organisations, physicians, and patients. Beyond the practical, financial, and clinical considerations, we must not ignore the ethical imperative for fair and just applications to improve the field of healthcare for all. Given the increasing personalisation of medicine and the role technology will play at the interface of healthcare delivery, a thorough understanding of the challenges presented is critical for future physicians who will navigate a novel environment. This article aims to explore the ethical challenges that the adoption of digital healthcare technology presents, contextualised at multiple levels. Potential solutions are suggested to initiate a discussion about the future of medicine and digital healthcare. © 2021, The Author(s).","Digital healthcare; Ethics; Patients; Prioritarianism; Rawls; Technology","adoption; adult; article; big data; ethics; health care delivery; human; personalized medicine; physician; telemedicine",Article,Scopus,2-s2.0-85118136299
"Niu C., Wang L.","57311387100;57310115400;","Big data-driven scheduling optimization algorithm for Cyber–Physical Systems based on a cloud platform",2022,"Computer Communications","181",,,"173","181",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117859412&doi=10.1016%2fj.comcom.2021.10.020&partnerID=40&md5=926bb5f3c0d3d0a41fcc4192065c2423","In this paper, we study big data-driven Cyber–Physical Systems (CPS) through cloud platforms and design scheduling optimization algorithms to improve the efficiency of the system. A task scheduling scheme for large-scale factory access under cloud–edge collaborative computing architecture is proposed. The method firstly merges the directed acyclic graphs on cloud-side servers and edge-side servers; secondly, divide the tasks using a critical path-based partitioning strategy to effectively improve the allocation accuracy; then achieves load balancing through reasonable processor allocation, and finally compares and analyses the proposed task scheduling algorithm through simulation experiments. The experimental system is thoroughly analysed, hierarchically designed, and modelled, simulated, and the experimental data analysed and compared with related methods. The experimental results prove the effectiveness and correctness of the worst-case execution time analysis method and the idea of big data-driven CPS proposed in this paper and show that big data knowledge can help improve the accuracy of worst-case execution time analysis. This paper implements a big data-driven scheduling optimization algorithm for Cyber–Physical Systems based on a cloud platform, which improves the accuracy and efficiency of the algorithm by about 15% compared to other related studies. © 2021 Elsevier B.V.","Big data-driven; Cloud platform; Cyber–Physical Systems; Scheduling optimization algorithm","Big data; Cloud computing; Computer architecture; Directed graphs; Efficiency; Multitasking; Optimization; Scheduling; Big data-driven; Cloud platforms; Computing architecture; Critical Paths; Cybe-physical systems; Cyber-physical systems; Data driven; Driven scheduling; Large-scales; Worst-case execution-time analysis; Scheduling algorithms",Article,Scopus,2-s2.0-85117859412
"Fleming J.K., Katayev A., Moorer C.M., Ward-Jeffries D.A., Terrell C.L.","55419569300;35744718800;6505910704;57310372600;57310907800;","Development of nation-wide reference intervals using an indirect method and harmonized assays",2022,"Clinical Biochemistry","99",,,"20","59",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117854116&doi=10.1016%2fj.clinbiochem.2021.10.001&partnerID=40&md5=4eadceae03273a285062100c46fb70ef","Objectives: For many years, clinical laboratories have either verified or estimated reference intervals (RI) for laboratory tests. Those calculations have largely been performed by direct sampling analysis of ostensibly healthy individuals or by post-analysis biochemical screening. Recently however, indirect calculations have come to the forefront as an IFCC endorsed method by using normal and abnormal patient data. Design and methods: Using a large database of patient test results from Laboratory Corporation of America, age and gender based RIs, inclusive of neonatal, pediatric, and geriatric populations, were determined using a modified indirect method of Hoffmann, and represent a diverse population distributed across the United States from a nation-wide system of laboratories and is unbiased with respect to age, gender, race or geography. Results: The tabulation of RIs using big data by an indirect method represent 72 M patient test results. The table includes 266 individual analytes consisting of approximately 2,700 age categories, including tests across multiple medical disciplines. Conclusions: To our knowledge, this is the largest collection of RIs that were calculated by an indirect method representing clinical chemistry, endocrinology, coagulation, and hematology analytes that have been derived with very powerful “Ns” for each age bracket. This process provides more robust RIs and allows for the determination of pediatric and geriatric RIs that would otherwise be difficult to obtain using traditional direct RI determinations. © 2021 The Canadian Society of Clinical Chemists","Big Data; Computerization; Data Mining; Harmonization; Indirect Method; Reference Intervals","article; automation; big data; child; clinical chemistry; controlled study; data mining; endocrinology; female; gender; geography; hematology; human; human tissue; major clinical study; male; newborn; race; reference value; United States; adolescent; adult; aged; bioassay; factual database; infant; middle aged; preschool child; reference value; theoretical model; very elderly; Adolescent; Adult; Aged; Aged, 80 and over; Big Data; Biological Assay; Child; Child, Preschool; Databases, Factual; Female; Humans; Infant; Infant, Newborn; Laboratories, Clinical; Male; Middle Aged; Models, Theoretical; Reference Values",Article,Scopus,2-s2.0-85117854116
"Chen P.-C., Lin Y.-T.","15749789600;57310597700;","Exposure assessment of PM2.5 using smart spatial interpolation on regulatory air quality stations with clustering of densely-deployed microsensors",2022,"Environmental Pollution","292",,"118401","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117829026&doi=10.1016%2fj.envpol.2021.118401&partnerID=40&md5=9fe32422b01cb619559ddf63a08f3a6a","Accurate mapping of air pollutants is essential for epidemiological studies and environmental risk assessments. Concentrations measured by air quality monitoring stations (AQMS) have primarily been used to assess the exposure of PM2.5. However, the low coverage and amount of monitoring stations affect the errors of spatial interpolation or geostatistical estimates. In contrast to other integrated approaches developed for improved air pollution estimates, this study utilizes data from low-cost microsensors densely deployed in Taiwan to improve the popular spatial interpolation approach called inverse distance weighting (IDW). A large dataset from thousands of low-cost sensors could improve spatial interpolation by describing the distribution of PM2.5 in detail. Therefore, this study presents a clustering-based method to assess the distribution of PM2.5. Then, a smarter IDW is performed based on correlated observations from the selected air quality stations. The publicly available data chosen for this investigation pertained to Taiwan, which has deployed 74 monitoring stations and more than 11,000 low-cost sensors since December 2020. The results of leave-one-out cross-validation indicate that there are fewer PM2.5 estimation errors in the developed approach than in estimations that use kriging across almost all of the months and sampled dates of 2019 and 2020, particularly those with higher PM2.5 spatial heterogeneities. Spatial heterogeneities could result in more significant estimation errors in mainstream approaches. The root mean square error of the monthly average estimate for PM2.5 ranged from 1.17 to 3.86 μg/m3. We also found that the clustering of one month characterizing the pattern of PM2.5 distribution could perform well in spatial interpolations based on historical data from monitoring stations. According to the information on the openaq platform, low-cost sensors are in demand in cities and areas. This trend might pave the way for the application of the proposed approach in other areas for superior exposure assessments. © 2021 The Authors","Big data; Exposure estimation model; Inverse distance weighting; Spatial clustering","Air quality; Cost benefit analysis; Inverse problems; Large dataset; Mean square error; Microsensors; Monitoring; Risk assessment; Clusterings; Estimation models; Exposure estimation; Exposure estimation model; Inverse distance weighting; Low-cost sensors; Monitoring stations; PM 2.5; Spatial clustering; Spatial interpolation; Costs; accuracy assessment; air quality; assessment method; atmospheric pollution; concentration (composition); data set; estimation method; interpolation; mapping method; particulate matter; pollution monitoring; sensor; spatial analysis; air pollution; air quality control; Article; concentration (parameter); controlled study; density; estimation error; feasibility study; geographic and geological phenomena; information processing; inverse distance weighting; kriging; land use; leave one out cross validation; measurement accuracy; meteorological phenomena; methodology; PM2.5 exposure; prediction; regulatory air quality station; seasonal variation; spatial interpolation; systematic error; Taiwan; terrain; air pollutant; air pollution; cluster analysis; environmental monitoring; particulate matter; spatial analysis; Taiwan; Air Pollutants; Air Pollution; Cluster Analysis; Environmental Monitoring; Particulate Matter; Spatial Analysis",Article,Scopus,2-s2.0-85117829026
"ElZahed M., Marzouk M.","57310114300;7005098184;","Smart archiving of energy and petroleum projects utilizing big data analytics",2022,"Automation in Construction","133",,"104005","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117756570&doi=10.1016%2fj.autcon.2021.104005&partnerID=40&md5=a2bf87466c74a2c400497770ba514c93","Complexity of the construction projects vary by the domain and type of the project. Due to the interaction between different disciplines and parties, Energy and Petroleum Projects (EPP) are considered among the most complex. This complexity produces a dense network of interrelated documents which are produced to cover the various aspects and details of the project. Analyzing this network which is composed of unstructured data is required to gain insights from old data, this task traditionally requires experience, knowledge, and awareness about the existence of the required data. Accordingly, a key asset of any company is the knowledge accumulated over the time from various projects. This research proposes a framework that enables salvaging traditional archives and include its data in searchable databases to increase the efficiency of archiving such accumulated data without affecting the normal workflow of companies, and hence decrease the manhours expenditure and reduce the time of archiving while not affecting the accuracy of the outcome. Due to the large diversity of the EPP projects, the research focuses on five main commodities which are selected based on the frequency of their existence in projects in addition to their monetary value as the main data to be stored which are Tanks, Air Coolers, Pumps, Generators and Distributed Control Systems (DCS). The key attributes of each commodity are identified based on technical questionnaires with technical specialists to act as the basis for building the proposed framework. The proposed framework integrates four modules to provide a complete solution to the problem, by implementing data mining techniques while harnessing the power of big data analytics tools to transform the existent unstructured data into structured data in the form of smart archives which can then be used to support ongoing business processes. © 2021 Elsevier B.V.","Archiving mechanisms; Big data; Data mining; Energy and petroleum projects; Optical character recognition","Advanced Analytics; Big data; Complex networks; Data Analytics; Distributed parameter control systems; Gasoline; Optical character recognition; Surveys; Archiving mechanism; Construction projects; Dense network; Energy; Energy and petroleum project; Gain insight; ITS data; Searchable database; Unstructured data; Work-flows; Data mining",Article,Scopus,2-s2.0-85117756570
"Al Khafaf N., Rezaei A.A., Moradi Amani A., Jalili M., McGrath B., Meegahapola L., Vahidnia A.","57203245357;57216611934;36843304800;22035084800;57296548800;24463641900;26649661200;","Impact of battery storage on residential energy consumption: An Australian case study based on smart meter data",2022,"Renewable Energy","182",,,"390","400",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117107703&doi=10.1016%2fj.renene.2021.10.005&partnerID=40&md5=c0573389f5f032088d9c0541d2bbea17","Advanced metering infrastructure has been widely recognized as a key enabling factor for delivering a range of benefits to the electricity industry as well as to energy consumers. The data is commonly used for billing and to educate consumers so they can make economically optimal decisions on choosing between electricity retail plans. More importantly, an in-depth analysis of this data can drive the development and implementation of new energy technologies and to enable evidence-based policy decisions to be made. In this paper, an Australian case study is presented on how residential battery installations lead to behavioral changes in the way energy is consumed. Furthermore, data-informed recommendations to both distribution network operators and policy-makers are also provided. The data comprises more than 5000 energy consumers with either distributed generation systems such as Photovoltaics (PV) and Energy Storage Systems (ESS), or without. The methodology focuses on the analysis of energy consumption of consumers with PV and ESS energy consumers and compares them against consumers without ESS. In addition, an economic analysis on the benefit of installing ESS is presented using payback period and internal rate of return. The main finding is that residential energy storage systems provide a range of benefits to the distribution network. It is recommended that Governments should continue supporting the rollout of such systems by subsidy programs and implementing necessary policy directives. © 2021 Elsevier Ltd","Big data; Case study; Energy consumption; Energy storage systems; Knowledge discovery; Smart meter","Battery storage; Big data; Decision making; Digital storage; Earnings; Economic analysis; Electric batteries; Electric power measurement; Housing; Investments; Smart meters; Battery storage; Case-studies; Electricity industry; Energy consumer; Energy storage system; Energy-consumption; Optimal decisions; Photovoltaics; Residential energy consumption; Storage systems; Energy utilization; decision analysis; economic analysis; electricity industry; energy policy; energy storage; energy use; fuel cell; government; residential energy; Australia",Article,Scopus,2-s2.0-85117107703
"Ullah I., Zahid H., Algarni F., Khan M.A.","57204077807;57205103910;55339318200;57203167713;","An access control scheme using heterogeneous signcryption for IoT environments",2022,"Computers, Materials and Continua","70","3",,"4307","4321",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117026570&doi=10.32604%2fcmc.2022.017380&partnerID=40&md5=7bca0a37ba13e1d4d9c3b8f71b3f70d8","When the Wireless Sensor Network (WSN) is combined with the Internet of Things (IoT), it can be employed in a wide range of applications, such as agriculture, industry 4.0, health care, smart homes, among others. Accessing the big data generated by these applications in Cloud Servers (CSs), requires higher levels of authenticity and confidentiality during communication conducted through the Internet. Signcryption is one of the most promising approaches nowadays for overcoming such obstacles, due to its combined nature, i.e., signature and encryption. A number of researchers have developed schemes to address issues related to access control in the IoT literature, however, the majority of these schemes are based on homogeneous nature. This will be neither adequate nor practical for heterogeneous IoT environments. In addition, these schemes are based on bilinear pairing and elliptic curve cryptography, which further requires additional processing time and more communication overheads that is inappropriate for real-time communication. Consequently, this paper aims to solve the above-discussed issues, we proposed an access control scheme for IoT environments using heterogeneous signcryption scheme with the efficiency and security hardiness of hyperelliptic curve. Besides the security services such as replay attack prevention, confidentiality, integrity, unforgeability, non-repudiations, and forward secrecy, the proposed scheme has very low computational and communication costs, when it is compared to existing schemes. This is primarily because of hyperelliptic curve lighter nature of key and other parameters. The AVISPA tool is used to simulate the security requirements of our proposed scheme and the results were under two backbends (Constraint Logic-based Attack Searcher (CL-b-AtSER) and On-the-Fly Model Checker (ON-t-FL-MCR)) proved to be SAFE when the presented scheme is coded in HLPSL language. This scheme was proven to be capable of preventing a variety of attacks, including confidentiality, integrity, unforgeability, non-repudiation, forward secrecy, and replay attacks. © 2022 Tech Science Press. All rights reserved.","Access control; Big data; Heterogeneous signcryption; Internet of Things (IoT)","Automation; Big data; Intelligent buildings; Model checking; Public key cryptography; Wireless sensor networks; Access control schemes; Agriculture industries; Forward secrecy; Heterogeneous signcryption; Hyper-elliptic curves; Internet of thing; Non repudiation; Replay attack; Signcryption; Smart homes; Internet of things",Article,Scopus,2-s2.0-85117026570
"Ajmi A.A., Mahmood N.S., Jamaludin K.R., Talib H.H.A., Sarip S., Kaidi H.M.","57224513614;57224509671;26434395500;35119607000;55660937100;57215690928;","Intelligent integrated model for improving performance in power plants",2022,"Computers, Materials and Continua","70","3",,"5783","5801",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116970024&doi=10.32604%2fcmc.2022.021885&partnerID=40&md5=17a4cee9458ec54583f19f9feecee508","Industry 4.0 is expected to play a crucial role in improving energy management and personnel performance in power plants. Poor performance problem in maintaining power plants is the result of both human errors, human factors and the poor implementation of automation in energy management. This problem can potentially be solved using artificial intelligence (AI) and an integrated management system (IMS). This article investigates the current challenges to improving personnel and energy management performance in power plants, identifies the critical success factors (CSFs) for an integrated intelligent framework, and develops an intelligent framework that enables power plants to improve performance. The theoretical basis is founded on a systematic literature review to locate 110 out of 3108 papers studied carefully to examine the performance architecture that best enables effective maintenance. The findings from this literature review are combined with expert judgment and the big data advantages of AI applications to develop an intelligent model. Data are collected from a power plant in Iraq. To ensure the reliability of the proposed model, various hypotheses are tested using structural equation modeling. The results confirm that the measurement model is acceptable, and that the hypotheses are supported and significant. A case study demonstrates the strong relationship and significance between big data of performance and the CSFs. It is hoped that this model will be adopted to enable performance improvement in power plants. © 2022 Tech Science Press. All rights reserved.","Artificial intelligence; Critical success factors; Decision making; Industry 4.0; Integrated management system; Maintenance","Artificial intelligence; Big data; Human resource management; Industry 4.0; Integrated control; Critical success factor; Decisions makings; Human errors; Improving performance; Integrated management systems; Integrated modeling; Performance; Performance problems; Poor performance; Success factors; Decision making",Article,Scopus,2-s2.0-85116970024
"Kayabay K., Gökalp M.O., Gökalp E., Erhan Eren P., Koçyiğit A.","57193878685;57119769400;56403164300;6603471003;15755652300;","Data science roadmapping: An architectural framework for facilitating transformation towards a data-driven organization",2022,"Technological Forecasting and Social Change","174",,"121264","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116906475&doi=10.1016%2fj.techfore.2021.121264&partnerID=40&md5=141893bcc136762b0bdb79fe1b0467a5","Leveraging data science can enable businesses to exploit data for competitive advantage by generating valuable insights. However, many industries cannot effectively incorporate data science into their business processes, as there is no comprehensive approach that allows strategic planning for organization-wide data science efforts and data assets. Accordingly, this study explores the Data Science Roadmapping (DSR) to guide organizations in aligning their business strategies with data-related, technological, and organizational resources. The proposed approach is built on the widely adopted technology roadmapping framework and customizes its context, architecture, and process by synthesizing data science, big data, and data-driven organization literature. Based on industry collaborations, the framework provides a hybrid and agile methodology comprising the recommended steps. We applied DSR with a research group with sector experience to create a comprehensive data science roadmap to validate and refine the framework. The results indicate that the framework facilitates DSR initiatives by creating a comprehensive roadmap capturing strategy, data, technology, and organizational perspectives. The contemporary literature illustrates prebuilt roadmaps to help businesses become data-driven. However, becoming data-driven also necessitates significant social change toward openness and trust. The DSR initiative can facilitate this social change by opening communication channels, aligning perspectives, and generating consensus among stakeholders. © 2021","Big data; Data science; Data-driven organization; Digital transformation; Technology management; Technology roadmapping","Big data; Competition; Metadata; Architectural frameworks; Competitive advantage; Data driven; Data-driven organization; Digital transformation; Roadmap; Roadmapping; Social changes; Technology managements; Technology roadmapping; Data Science; architecture; conceptual framework; data set; stakeholder; technological development",Article,Scopus,2-s2.0-85116906475
"Ghermandi A., Depietri Y., Sinclair M.","12141109100;53663544400;57203029074;","In the AI of the beholder: A comparative analysis of computer vision-assisted characterizations of human-nature interactions in urban green spaces",2022,"Landscape and Urban Planning","217",,"104261","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116596533&doi=10.1016%2fj.landurbplan.2021.104261&partnerID=40&md5=e1f59a90d9e9a2f3e2e6619886c46cf7","Big data from photo-sharing platforms offer unique opportunities for the study of human-nature interactions and landscape planning. Research increasingly relies on computer vision in artificial intelligence to identify elements of interest in photographs and user preferences and sentiment towards them. Studies largely rely on pre-trained models from one of several available cloud-based, commercial image recognition services, but the extent to which findings depend on the implemented technology has not yet been explored. Here, we analyze ∼ 10,000 outdoor photographs retrieved from three social media platforms and geolocated within green and blue spaces in Haifa (Israel) by means of machine tags from three popular cloud-based services. We find that clustering of the 45 investigated sites based on common characteristics of the photographs is considerably affected by the image recognition service chosen, especially for sites with limited data points (<80 photographs). Moreover, after associating the individual tags to specific aspects of the outdoor experience, we find substantial differences in the identification and ranking of outdoor recreational activities, characterization of the local biophysical environment (e.g., wildlife and vegetation), and feelings associated with the photographs. With no image recognition service clearly outperforming the others in all evaluation criteria, we argue that the optimal choice of image recognition service to rely on likely depends on the intended final application. Time and resource permitting, future studies should consider combining information from multiple sources for a characterization that is more nuanced and less prone to be affected by the idiosyncrasies of the individual technologies. © 2021 Elsevier B.V.","Big data; Cultural ecosystem services; Landscape planning; Passive crowdsourcing; Social media","comparative advantage; comparative study; detection method; greenspace; nature-society relations; urban economy; urban ecosystem",Article,Scopus,2-s2.0-85116596533
"Ignacz G., Yang C., Szekely G.","57202774374;57287959400;50162445100;","Diversity matters: Widening the chemical space in organic solvent nanofiltration",2022,"Journal of Membrane Science","641",,"119929","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116572033&doi=10.1016%2fj.memsci.2021.119929&partnerID=40&md5=3186472c0a460c2dc12c624a1c65a39f","Niche membrane technologies, such as organic solvent nanofiltration (OSN), offer considerable energy and operation cost reduction compared with conventional separation methods. However, despite their many advantages, their industrial implementation is hindered by small and specialized datasets, which hinders the development of more advanced prediction methods. In this study, we developed a medium-throughput system (MTS) for OSN with high robustness and low error. The MTS was used to generate a dataset containing 336 different molecules, and their rejection values were measured at two different pressures using three commercial DuraMem polyimide membranes with different molecular weight cut-off values in methanol. The diversity of the generated dataset was compared with the diversity values of other relevant datasets using 26 different chemometric molecular descriptors, including the heteroatom count, topological surface area, different shape descriptors, Van der Waals volume, logP, and logS. The rejection was found to be weakly dependent on the functional group and molecular weight at the lower end of the nanofiltration range. We proposed the use of a novel structural similarity-based indexing method for comparing solutes. Also, we established the first open-access and searchable dataset for OSN rejection values. The newly established www.osndatabase.com pilot website acts as the foundation of the dataset. © 2021 The Authors","Big data; Medium-throughput system; Molecular descriptors; Solute markers; Tanimoto similarity","Cost reduction; Membrane technology; Molecular weight; Nanofiltration; Organic solvents; Van der Waals forces; Chemical space; Costs reduction; Energy cost; Industrial implementation; Medium-throughput system; Molecular descriptors; Operations cost; Separation methods; Solute marker; Tanimoto similarity; Big data; organic solvent; polyimide; Article; comparative study; controlled study; high performance liquid chromatography; molecular weight; nanofiltration; nonhuman; pressure; surface area; volume; Cost Control; Index Terms; Indexing; Methods; Molecular Weight; Rejection; Solvents",Article,Scopus,2-s2.0-85116572033
"Chipkin I., Vidojevic J.","6508049318;56786404400;","Time and temporality in organisations: The case of Eskom",2022,"Development Southern Africa","39","2",,"237","250",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116474407&doi=10.1080%2f0376835X.2021.1981829&partnerID=40&md5=9057b2d244d9b8b1dfa396bad32a5e99","This paper proposes analysing organisations as temporal phenomena composed of multiple temporalities. We argue that the likelihood that an organisation is well placed to function instrumentally with regard to its formal mandate is when what we will call ‘operational time’ is dominant in the temporal regime. We propose that organisations perform poorly when other temporalities come to dominate the temporal regime and/or when the temporal regime is chronically disrupted. We apply this framework to a study of Eskom, the state power company in South Africa. In Eskom, for example, we show how operational time was displaced by a political temporality that ultimately destabilised the temporal regime of the organisation as a whole. © 2021 Government Technical Advisory Centre (GTAC).","Big Data; governance; Organisational performance; predicting performance; temporality",,Article,Scopus,2-s2.0-85116474407
"Ebada A.I., Elhenawy I., Jeong C.-W., Nam Y., Elbakry H., Abdelrazek S.","57195680188;54915283100;57251541500;8971732400;6603728802;57216186536;","Applying apache spark on streaming big data for health status prediction",2022,"Computers, Materials and Continua","70","2",,"3511","3527",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115992206&doi=10.32604%2fcmc.2022.019458&partnerID=40&md5=be7e4b526110cb71296ff83b82c89ac6","Big data applications in healthcare have provided a variety of solutions to reduce costs, errors, and waste. This work aims to develop a real-time system based on big medical data processing in the cloud for the prediction of health issues. In the proposed scalable system, medical parameters are sent to Apache Spark to extract attributes from data and apply the proposed machine learning algorithm. In this way, healthcare risks can be predicted and sent as alerts and recommendations to users and healthcare providers. The proposed work also aims to provide an effective recommendation system by using streaming medical data, historical data on a user's profile, and a knowledge database to make the most appropriate real-time recommendations and alerts based on the sensor's measurements. This proposed scalable system works by tweeting the health status attributes of users. Their cloud profile receives the streaming healthcare data in real time by extracting the health attributes via a machine learning prediction algorithm to predict the users' health status. Subsequently, their status can be sent on demand to healthcare providers. Therefore, machine learning algorithms can be applied to stream health care data from wearables and provide users with insights into their health status. These algorithms can help healthcare providers and individuals focus on health risks and health status changes and consequently improve the quality of life. © 2022 Tech Science Press. All rights reserved.","Apache Spark; Big data; Healthcare data; IoT data processing; Machine learning; Streaming processing","Big data; Data handling; Data mining; Forecasting; Health risks; Interactive computer systems; Internet of things; Learning algorithms; Machine learning; Medical computing; Real time systems; Apache spark; Health care providers; Health status; Healthcare data; IoT data processing; Machine learning algorithms; Medical data; Real- time; Scalable systems; Streaming processing; Health care",Article,Scopus,2-s2.0-85115992206
"Han H., Trimi S.","57061383900;8686568700;","Towards a data science platform for improving SME collaboration through Industry 4.0 technologies",2022,"Technological Forecasting and Social Change","174",,"121242","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115968241&doi=10.1016%2fj.techfore.2021.121242&partnerID=40&md5=d06529ad391a62215b76b5bc187a217d","Industry 4.0 (I4.0) is about realizing digital transformation by linking machines to plants, fleets, and humans through sensors and control elements in order to create smart networks, smart factories, smart manufacturing, and smart value chains. By leveraging I4.0 technologies, a small and medium enterprise (SME) can increase its organizational agility, adaptability, and resilience to cope with today's competitive environment by becoming a valuable and innovative partner in the power dynamics with its large buyer counterparts. However, SMEs face technology, trust, and big data challenges when they adopt I4.0 technologies. This study provides new solutions for SMEs to overcome these three challenges in implementing I4.0. Specifically, the paper proposes the following: (1) a roadmap for the application of I4.0 technologies to enhance the collaboration capabilities of SMEs; (2) a structure for I4.0 standardization to develop and sustain trust among partners; and (3) an improved data science platform for systematizing big data to extract critical information for collaboration solutions for SMEs. Additionally, the solutions are evaluated based on an application case of a Greek SME, demonstrating their potentials for practical implementation. © 2021","Cloud computing; Collaboration; Data science; Industry 4.0; SME; Trust","Big data; Data Science; Trusted computing; Cloud-computing; Collaboration; Control elements; Digital transformation; Enterprise collaboration; Sensor elements; Small-and-medium enterprise; Smart manufacturing; SMART network; Trust; Industry 4.0; competition (economics); computer system; small and medium-sized enterprise",Article,Scopus,2-s2.0-85115968241
"Wu J., Lu Y., Gao H., Wang M.","57197759352;57277762200;57225011266;56342596800;","Cultivating historical heritage area vitality using urban morphology approach based on big data and machine learning",2022,"Computers, Environment and Urban Systems","91",,"101716","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115954534&doi=10.1016%2fj.compenvurbsys.2021.101716&partnerID=40&md5=da3687270a9f00c20c4ded27a14f7cb5","The conservation of historical heritage can bring social benefits to cities by promoting community economic development and societal creativity. In the early stages of historical heritage conservation, the focus was on the museum-style concept for individual structures. At present, heritage area vitality is often adopted as a general conservation method to increase the vibrancy of such areas. However, it remains unclear whether urban morphological elements suitable for urban areas can be applied to heritage areas. This study uses ridge regression and LightGBM with multi-source big geospatial data to explore whether urban morphological elements that affect the vitality of heritage and urban areas are consistent or have different spatial distributions and daily variations. From a sample of 12 Chinese cities, our analysis shows the following results. First, factors affecting urban vitality differ from those influencing heritage areas. Second, factors influencing urban and heritage areas' vitality have diurnal variations and differ across cities. The overarching contribution of this study is to propose a quantitative and replicable framework for heritage adaptation, combining urban morphology and vitality measures derived from big geospatial data. This study also extends the understanding of forms of heritage areas and provides theoretical support for heritage conservation, urban construction, and economic development. © 2021 The Authors","Heritage conservation; Historical heritage; Urban morphology; Urban planning; Vitality","Economic and social effects; Machine learning; Morphology; Urban planning; Economic development; Geo-spatial data; Heritage conservation; Historical heritages; Individual structures; Morphological elements; Social benefits; Urban areas; Urban morphology; Vitality; Big data; data set; diurnal variation; heritage conservation; machine learning; spatial distribution; urban morphology; China",Article,Scopus,2-s2.0-85115954534
"Zhang H., Zang Z., Zhu H., Uddin M.I., Amin M.A.","57275555700;57275580900;57275487200;55946660600;57201246575;","Big data-assisted social media analytics for business model for business decision making system competitive analysis",2022,"Information Processing and Management","59","1","102762","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115889719&doi=10.1016%2fj.ipm.2021.102762&partnerID=40&md5=c36ce51f01b6a24e85f061f8ebce5c56","Business is based on manufacturing, purchasing, selling a product, and earning or making profits. Social media analytics collect and analyze data from various social networks such as Facebook, Instagram, and Twitter. Social media data analysis can help companies identify consumer desires and preferences, improve customer service and market analytics on social networks, and smarter product development and marketing investments. The business decision-making process is a step-by-step process that enables employees to resolve challenges by weighing evidence, evaluating possible solutions, and selecting a route. In this paper, Big Data-assisted Social Media Analytics for Business (BD-SMAB) Model increases awareness and affects decision-makers in marketing strategies. Companies can use big data analytics in many ways to enhance management. It can evaluate its competitors in real-time and change prices, make deals better than its competitors' sales, analyze competitors' unfavorable feedback and see if they can outperform that competitor. The proposed method examines social media analysis impacts on different areas such as real estate, organizations, and beauty trade fairs. This diversity of these companies shows the effects of social media and how positive decisions can be developed. Take better marketing decisions and develop a strategic approach. As a result, the BD-SMAB method enhance customer satisfaction and experience and develop brand awareness. © 2021","Big data; Companies; Customer; Data analysis; Decision-making; Social media","Advanced Analytics; Big data; Commerce; Customer satisfaction; Data Analytics; Data handling; Decision making; Economic and social effects; Investments; Social networking (online); Business decisions; Business models; Competitive analysis; Customer market; Customer-service; Decision-making systems; Decisions makings; Facebook; Social media; Social media analytics; Sales",Article,Scopus,2-s2.0-85115889719
"Lin C.-H., Lin Y.-C., Tang P.-W.","55967027800;57219925494;57226838850;","ADMM-ADAM: A New Inverse Imaging Framework Blending the Advantages of Convex Optimization and Deep Learning",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115665473&doi=10.1109%2fTGRS.2021.3111007&partnerID=40&md5=11b35c3ed07023c1bc6c72d7dc51423d","Alternating direction method of multipliers (ADMM) and adaptive moment estimation (ADAM) are two optimizers of paramount importance in convex optimization (CO) and deep learning (DL), respectively. Numerous state-of-the-art algorithms for solving inverse problems are achieved by carefully designing a convex criterion, typically composed of a data-fitting term and a regularizer. Even when the regularizer is convex, its mathematical form is often sophisticated, hence inducing a math-heavy optimization procedure and making the algorithm design a daunting task for software engineers. Probably for this reason, people turn to solve the inverse problems via DL, but this requires big data collection, quite time-consuming if not impossible. Motivated by these facts, we propose a new framework, termed as ADMM-ADAM, for solving inverse problems. As the key contribution, even just with small/single data, the proposed ADMM-ADAM is able to exploit DL to obtain a convex regularizer of very simple math form, followed by solving the regularized criterion using simple CO algorithm. As a side contribution, a state-of-the-art hyperspectral inpainting algorithm is designed under ADMM-ADAM, demonstrating its superiority even without the aid of big data or sophisticated mathematical regularization. © 2021 IEEE.","Big Data; Convex functions; Deep learning; Hyperspectral imaging; Imaging; Inverse problems; Optimization","Big data; Blending; Convex optimization; Deep learning; Differential equations; Estimation; Functions; Hyperspectral imaging; Problem solving; Spectroscopy; Adaptive moment estimation; Alternating direction method of multiplier; Alternating directions method of multipliers; Convex functions; Convex optimisation; Convex optimization; Deep learning; Imaging inverse problem.; Moment estimation; Optimisations; Inverse problems; algorithm; data; machine learning; optimization; software",Article,Scopus,2-s2.0-85115665473
"Jain D.K., Boyapati P., Venkatesh J., Prakash M.","56206778300;57203320225;55945555500;57395192600;","An Intelligent Cognitive-Inspired Computing with Big Data Analytics Framework for Sentiment Analysis and Classification",2022,"Information Processing and Management","59","1","102758","","",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115439346&doi=10.1016%2fj.ipm.2021.102758&partnerID=40&md5=9e938cc17c46ce691060ec27a0bbadb3","Advancements in recent networking and information technology have always been a natural phenomenon. The exponential amount of data generated by the people in their day-to-day lives results in the rise of Big Data Analytics (BDA). Cognitive computing is an Artificial Intelligence (AI) based system that can reduce the issues faced during BDA. On the other hand, Sentiment Analysis (SA) is employed to understand such linguistic based tweets, feature extraction, compute subjectivity and sentimental texts placed in these tweets. The application of SA on big data finds it useful for businesses to take commercial benefits insight from text-oriented content. In this view, this paper presents new cognitive computing with the big data analysis tool for SA. The proposed model involves various process such as pre-processing, feature extraction, feature selection and classification. For handling big data, Hadoop Map Reduce tool is used. The proposed model initially undergoes pre-processing to remove the unwanted words. Then, Term Frequency-Inverse Document Frequency (TF-IDF) is utilized as a feature extraction technique to extract the set of feature vectors. Besides, a Binary Brain Storm Optimization (BBSO) algorithm is being used for the Feature Selection (FS) process and thereby achieving improved classification performance. Moreover, Fuzzy Cognitive Maps (FCMs) are used as a classifier to classify the incidence of positive or negative sentiments. A comprehensive experimental results analysis ensures the better performance of the presented BBSO-FCM model on the benchmark dataset. The obtained experimental values highlights the improved classification performance of the proposed BBSO-FCM model in terms of different measures. © 2021","Big data analytics; Classification; Cognitive Computing; Feature selection; Hadoop Map Reduce; Intelligent models; Sentiment Analysis","Benchmarking; Big data; Classification (of information); Data Analytics; Data handling; Extraction; Feature extraction; Fuzzy Cognitive Maps; Classification performance; Cognitive Computing; Features extraction; Features selection; Hadoop map reduce; Intelligent models; Map-reduce; Optimisations; Pre-processing; Sentiment analysis; Sentiment analysis",Article,Scopus,2-s2.0-85115439346
"Mahdi M.A., Hosny K.M., Elhenawy I.","57189577760;57205214086;54915283100;","FR-Tree: A novel rare association rule for big data problem",2022,"Expert Systems with Applications","187",,"115898","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115364647&doi=10.1016%2fj.eswa.2021.115898&partnerID=40&md5=11ba9c26bb9d87c30a3ab3200771fb1e","In some situations, finding the rare association rule is of higher importance than the frequent itemset. Unique rules represent rare cases, activities, or events in real-world applications. It is essential to extract exceptional critical activity from vast routine data. This paper proposes a new algorithm called FR-Tree to mine the association rules and produce essential rules. This work aims to demonstrate that this algorithm is suitable for extracting rare association rules with high confidence. The proposed algorithm generates, filters, and classifies the all-important rules, either frequent or rare. The rare rules were produced without needing to set an additional threshold. Therefore, the proposed algorithm has an advantage incomparable with the other rare association rule techniques. The generated rules were tested using well-known datasets, and the performance was compared with the other rare association rule techniques. The results proved that our method outperformed the existing rare association rule techniques. © 2021 Elsevier Ltd","Association rules; Categorical data; Clustering; Data mining; Rare association rules","Association rules; Big data; Clustering algorithms; Filtration; Trees (mathematics); Categorical data; Clusterings; Critical activities; Data problems; Frequent itemset; High confidence; Performance; Rare association rules; Real-world; Data mining",Article,Scopus,2-s2.0-85115364647
"Weerasinghe K., Scahill S.L., Pauleen D.J., Taskin N.","56533869000;26538218500;6603058691;36522004100;","Big data analytics for clinical decision-making: Understanding health sector perceptions of policy and practice",2022,"Technological Forecasting and Social Change","174",,"121222","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115354725&doi=10.1016%2fj.techfore.2021.121222&partnerID=40&md5=ea18fd8c232df7803c604cc8e4ee58a6","The introduction and use of ‘big data and analytics’ is an on-going issue of discussion in health sectors globally. Healthcare systems of developed countries are trying to create more value and better healthcare through data and use of big data technologies. With an increasing number of articles identifying the value creation of big data and analytics for clinical decision-making, this paper examines how big data is applied, or not applied, in clinical practice. Using social representation theory as a theoretical foundation the paper explores people's perceptions of big data across all levels (policy making, planning, funding, and clinical care) of the New Zealand healthcare sector. The findings show that although adoption of big data technologies is planned for population health and health management, the potential of big data for clinical care has yet to be explored in the New Zealand context. The findings also highlight concern over data quality. The paper provides recommendations for policy and practice particularly around the need for engagement and participation of all levels to discuss data quality as well as big-data-based changes such as precision medicine and technology-assisted clinical decision-making tools. Future avenues of research are suggested. © 2021 Elsevier Inc.","Analytics; Big data; Clinical decision-making; Healthcare; Social representation theory","Behavioral research; Data Analytics; Decision making; Decision theory; Health care; Population statistics; Analytic; Clinical care; Clinical decision making; Data quality; Data technologies; Healthcare systems; New zealand; Representation theory; Social representation theory; Social representations; Big data; data set; decision making; future prospect; health care; health policy; perception; New Zealand",Article,Scopus,2-s2.0-85115354725
"Long J.A., Ren C.","35272465500;56841141100;","Associations between mobility and socio-economic indicators vary across the timeline of the Covid-19 pandemic",2022,"Computers, Environment and Urban Systems","91",,"101710","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115185810&doi=10.1016%2fj.compenvurbsys.2021.101710&partnerID=40&md5=032d715d7d9e30f48cccbc1cd232e910","Covid-19 interventions are greatly affecting patterns of human mobility. Changes in mobility during Covid-19 have differed across socio-economic gradients during the first wave. We use fine-scale network mobility data in Ontario, Canada to study the association between three different mobility measures and four socio-economic indicators throughout the first and second wave of Covid-19 (January to December 2020). We find strong associations between mobility and the socio-economic indicators and that relationships between mobility and other socio-economic indicators vary over time. We further demonstrate that understanding how mobility has changed in response to Covid-19 varies considerably depending on how mobility is measured. Our findings have important implications for understanding how mobility data should be used to study interventions across space and time. Our results support that Covid-19 non-pharmaceutical interventions have resulted in geographically disparate responses to mobility and quantifying mobility changes at fine geographical scales is crucial to understanding the impacts of Covid-19. © 2021","Big data; COVID-19; Mobility; Socio-economic; Spatial model","COVID-19; Fine-scale; Human mobility; Mobility; Mobility datum; Network mobility; Ontario; Socio-economics; Socioeconomic indicators; Spatial modelling; Big data; COVID-19; disease incidence; disease prevalence; disease severity; epidemic; mobility; pandemic; quantitative analysis; socioeconomic indicator; temporal variation; travel behavior; travel demand; Canada; Ontario [Canada]",Article,Scopus,2-s2.0-85115185810
"Zhang D., Pee L.G., Pan S.L., Cui L.","57212376830;23668683800;57203894889;52363660000;","Big data analytics, resource orchestration, and digital sustainability: A case study of smart city development",2022,"Government Information Quarterly","39","1","101626","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115131500&doi=10.1016%2fj.giq.2021.101626&partnerID=40&md5=7f51df6bf2c5dd7430d9c74c41dec13a","Smart cities are expected to improve the efficiency and effectiveness of urban management, including public services, public security, and environmental protection, and to ultimately achieve Sustainable Development Goal (SDG) 11 for making cities inclusive, safe, resilient, and sustainable. Big data have been identified as a key enabler in the development of smart cities. However, our understanding of how different data sources should be managed and integrated remains limited. By analyzing data applications in the development of a sustainable smart city, this case study identified three phases of development, each requiring a different approach to orchestrating diverse data sources. A framework identifying the phases, data-related issues, data orchestration and its interaction with other resources, focal capabilities, and development approaches is developed. This study benefits both researchers and practitioners by making theoretical contributions and by offering practical insights in the fields of smart cities and big data. © 2021","Big data; Digital sustainability; Resource orchestration; Smart city; Socio-technical issues",,Article,Scopus,2-s2.0-85115131500
"Liao S.-H., Widowati R., Cheng C.-J.","7401923068;57223238647;57260180200;","Investigating Taiwan Instagram users’ behaviors for social media and social commerce development",2022,"Entertainment Computing","40",,"100461","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115001785&doi=10.1016%2fj.entcom.2021.100461&partnerID=40&md5=9e302965050156a6d1684b933a840cdd","Instagram is a social media which refers to an online media platform that supports users' social behaviors, including parallel communication, interaction, leisure and entertainment. However, the evolution of today's online market and continuous changes in business models are critical factors driving the growth of the social commerce market. This study aims to investigate social media behaviors and social commerce analysis on Taiwan Instagram users on social media/networks and purchase behavioral preferences of different cluster groups in terms of suggesting social media and social commerce development. This study first stage is to develop a big data structure using a relational database approach. Based on an empirical survey in Taiwan, a total of 1,954 valid questionnaires response was incorporated into a database. The second stage is to implements big data analytics methods, including Apriori algorithms and Cluster analysis, to investigate Instagram users’ profiles and their social media and social commerce preferences. © 2021 Elsevier B.V.","Big data analytics; Electronic commerce; Instagram; Social commerce; Social media; Social media for interaction and entertainment","Advanced Analytics; Big data; Cluster analysis; Commerce; Data Analytics; Social networking (online); Surveys; Apriori algorithms; Business models; Critical factors; Empirical surveys; Parallel communication; Relational Database; Social behavior; Social commerces; Economic and social effects",Article,Scopus,2-s2.0-85115001785
"Di Vaio A., Hassan R., Alavoine C.","55922628500;56575985100;54880538100;","Data intelligence and analytics: A bibliometric analysis of human–Artificial intelligence in public sector decision-making effectiveness",2022,"Technological Forecasting and Social Change","174",,"121201","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114997041&doi=10.1016%2fj.techfore.2021.121201&partnerID=40&md5=dc2e576a8b4d023fa258b86dc14dad10","This study investigates the literary corpus of the role and potential of data intelligence and analytics through the lenses of artificial intelligence (AI), big data, and the human–AI interface to improve overall decision-making processes. It investigates how data intelligence and analytics improve decision-making processes in the public sector. A bibliometric analysis of a database containing 161 English-language articles published between 2017 and 2021 is performed, providing a map of the knowledge produced and disseminated in previous studies. It provides insights into key topics, citation patterns, publication activities, the status of collaborations between contributors over past studies, aggregated data intelligence, and analytics research contributions. The study provides a retrospective review of published content in the field of data intelligence and analytics. The findings indicate that field research has been concentrated mainly on emerging technologies' intelligence capabilities rather than on human–artificial intelligence in decision-making performance in the public sector. This study extends an ambidexterity theory in decision support, which enlightens how this ambidexterity can be encouraged and how it affects decision outcomes. The study emphasises the importance of the public sector adoption of data intelligence and analytics, as well as its efficiency. Furthermore, this study expands how researchers and practitioners interpret and understand data intelligence and analytics, AI, and big data for effective public sector decision-making. © 2021","Accountability and performance; Ambidexterity; Big data; Business intelligence; Human intellect; Industry 4.0; Intellectual capital","Artificial intelligence; Big data; Decision making; Decision support systems; Decision theory; Industry 4.0; Information analysis; Accountability and performance; Ambidexterity; Bibliometrics analysis; Data analytics; Data intelligence; Decisions makings; Human intellect; Intellectual capital; Performance; Public sector; Knowledge management; artificial intelligence; business; data set; database; decision making; human capital; public sector",Article,Scopus,2-s2.0-85114997041
"Pang S., Wang J., Xia L.","57226820368;57260458800;57260568000;","Information matching model and multi-angle tracking algorithm for loan loss-linking customers based on the family mobile social-contact big data network",2022,"Information Processing and Management","59","1","102742","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114986515&doi=10.1016%2fj.ipm.2021.102742&partnerID=40&md5=47849adaf23e1e30dbbe18dd96fd6f36","This article focuses on the tracking problem of loan loss-linking customers based on the family mobile social-contact big data network. By defining suspected loan loss-linking customer and family member intimacy, a mobile social-contact big data network of the loan loss-linking customers’ family members is constructed, and similarity of mobile phone usage habits, contact similarity, and contact point similarity in the mobile social-contact big data network are defined and studied accordingly. Then, the similarity matching degrees of mobile phone usage habits, contact locations, and contacts between suspected loan loss-linking customers and loan loss-linking customers are analyzed from the perspective of similarity. This establishes an information matching model of loan loss-linking customers, and proposes the multi-angle tracking algorithm of loan loss-linking customers, allowing information matching and multi-angle tracking for loan loss-linking customers, and applies the model and the algorithm to the loan data of a bank in China. The empirical results show that the proposed model and algorithm can track loan loss-linking customers, and the algorithm exhibits rapid convergence. This study has important significance and practical value for financial institutions to track loan loss-linking customers and recover funds. © 2021","Family member intimacy; Family mobile social-contact big data network; Information matching model; Loan loss-linking customers; Multi-angle tracking algorithm; Suspected loan loss-linking customer","Big data; Cellular telephones; Tracking (position); Angle tracking; Data network; Family member intimacy; Family mobile social-contact big data network; Information matching model; Loan loss-linking customer; Matching models; Multi angle; Multi-angle tracking algorithm; Social contacts; Suspected loan loss-linking customer; Tracking algorithm; Sales",Article,Scopus,2-s2.0-85114986515
"Ha S., Geum Y.","57259232000;25654780100;","Identifying new innovative services using M&A data: An integrated approach of data-driven morphological analysis",2022,"Technological Forecasting and Social Change","174",,"121197","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114952249&doi=10.1016%2fj.techfore.2021.121197&partnerID=40&md5=23d34931e512ec4e9e21f3cb4450d488","This study suggests a concrete framework for generating new service ideas using an M&A dataset. Addressing the limitations of previous works that neglected service-specific characteristics, we suggest methods to extract service-specific keywords and phrases from the text and restructure them to provide clear evidence for new service development. Therefore, we propose a process for building data-driven quality function deployment (QFD) and data-driven morphological analysis (MA). First, M&A transactions were collected from CrunchBase, which is an open platform that provides start-up information. Service actions and service contents are then extracted from the text using natural language processing. For each extracted keyword, a clustering analysis was performed to identify the new service patterns. For clustered service actions and contents, MA is employed to generate new service ideas. This study contributes to the technology management field by first employing M&A records for the data-driven morphological matrix and suggests how to extract service actions and service contents from the text. We also suggested a new systematic way of identifying new services using an integrated approach of QFD and MA. This work is expected to help managers in new service development by providing practical guidance and tools for utilizing textual data. © 2021 Elsevier Inc.","Big data; Data analytics; M&A; MA; New service development; QFD","Big data; Integrated control; Linguistics; Natural language processing systems; Quality control; Quality function deployment; Action contents; Data analytics; Data driven; Integrated approach; M&A; Morphological analysis; New service development; New services; Service actions; Service content; Data Analytics; data set; innovation; integrated approach",Article,Scopus,2-s2.0-85114952249
"López-Cubillos S., Muñoz-Ávila L., Roberson L.A., Suárez-Castro A.F., Ochoa-Quintero J.M., Crouzeilles R., Gallo-Cajiao E., Rhodes J., Dressler W., Martinez-Harms M.J., Runting R.K.","56052888600;57258058700;57008014400;55905548200;24587407600;25229839300;56769999000;57211633993;56072132200;25628111100;50162328700;","The landmark Escazú Agreement: An opportunity to integrate democracy, human rights, and transboundary conservation",2022,"Conservation Letters","15","1","e12838","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114891119&doi=10.1111%2fconl.12838&partnerID=40&md5=b7f3ed146f4117c6789094fef80c59c0","Latin America and the Caribbean (LAC) is one of the world's most biodiverse regions, but this diversity is threatened by the overexploitation of natural resources and internal social conflicts. In 2018, 33 LAC countries were invited to sign and ratify the landmark Escazú Agreement, which is the first legally binding environmental agreement to explicitly integrate human rights with environmental matters. The agreement outlines an approach to enhance the protection of environmental defenders, increase public participation in environmental decision-making, and foster cooperation among countries for biodiversity conservation. However, clear mechanisms to implement the ideals of the Agreement are currently lacking. We identify the key provisions of the Agreement and link these to tangible mechanisms which aim to integrate human rights and nature conservation. These mechanisms include technological (e.g., free online data), human-based (e.g., legal advice from multidisciplinary teams), and nature-based solutions (e.g., transboundary species management). As environmental assets––and threats to them––span national boundaries, the collaborative and participatory provisions of the agreement could catalyze coordinated transboundary environmental management. Because of the importance of this Agreement for the LAC region, we added a Spanish version of this manuscript in the Supplementary Material (versión del artículo en español en el material suplementario). © 2021 The Authors. Conservation Letters published by Wiley Periodicals LLC","access to information; big data; capacity building; cooperation; environmental defenders; environmental legislation; NGOs; stakeholders",,Article,Scopus,2-s2.0-85114891119
"Dellnitz A.","57190662054;","Big data efficiency analysis: Improved algorithms for data envelopment analysis involving large datasets",2022,"Computers and Operations Research","137",,"105553","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114835343&doi=10.1016%2fj.cor.2021.105553&partnerID=40&md5=642dab837e113e6ec79180f983d2923c","In general, data sets are growing larger and larger, and handling related issues is topic of big data. Similar trends and tendencies are evident in data envelopment analysis (DEA). DEA is a well-known instrument for determining the efficiencies of decision-making units (DMUs), applying linear programming. Still, as we will show, DEA suffers notably from the curse of dimensionality. Therefore, we propose improved decomposition-based algorithms involving different termination criteria and multithreading to address this issue. For some of these criteria, we prove the convergence of the algorithm; to the best of our knowledge, we are the first to prove this. Ultimately, from a computational point of view, we study the performance of the new big data strategy by an extensive numerical analysis, thus demonstrating the algorithm's scalability. © 2021 Elsevier Ltd","Big data; Data envelopment analysis; Parallel processing; Strong efficient activities","Data envelopment analysis; Decision making; Efficiency; Linear programming; Curse of dimensionality; Decision making unit; Efficiency analysis; Large datasets; Multi-threading; Termination criteria; Large dataset",Article,Scopus,2-s2.0-85114835343
"Xia D., Yang N., Jiang S., Hu Y., Li Y., Li H., Wang L.","55960844800;57224084687;57224071290;57224096264;35119124600;36930647900;8909440300;","A parallel NAW-DBLSTM algorithm on Spark for traffic flow forecasting",2022,"Neural Computing and Applications","34","2",,"1557","1575",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114722496&doi=10.1007%2fs00521-021-06409-5&partnerID=40&md5=1fa5469d2c9bd97a66db98977eecc302","Traffic flow forecasting (TFF) is critical for constructing intelligent transportation systems and offering real-time traffic applications, and especially accurate flow forecasting based on traffic big data can drive reliable strategies for traffic management and control. To consider the weight of the influence of the spatial correlation among the road segments and capture the nonlinear characteristics of traffic flow, this paper presents a parallel Normal Distribution and Attention Mechanism Weighted Deep Bidirectional Long Short-Term Memory (NAW-DBLSTM) algorithm on Spark. Specifically, we employ the resilient distributed data set (RDD) to preprocess large-scale mobile trajectory data (e.g., taxi GPS trajectory data), and then Kalman Filter (KF) is utilized to smooth the taxi trajectory big data. Next, the parallel NAW-DBLSTM algorithm is put forward on a Spark distributed computing platform to enhance the accuracy and scalability of TFF, combined with the attention mechanism and the normal distribution, and then the time window is used for TFF. Finally, the traffic flow is forecasted successfully on Spark by our NAW-DBLSTM algorithm with the real-world GPS trajectories of taxicabs. The experimental results demonstrate that, compared with LSTM, BiLSTM, DBLSTM, DNN, SVR, KNN, SAEs, BP, CNN, GRU, and ANNs, NAW-DBLSTM can produce better performance with the MAPE value that is 85.1%, 80.1%, 85.8%, 73.1%, 78.2%, 77.9%, 78.8%, 84.6%, 96.4%, 86.2%, and 73.2% lower than that of comparable algorithms, respectively. Particularly, the MAPE value of NAW-DBLSTM is 28.3%, 20.1%, 71.1%, and 79.1% lower than that of LSTM weighted with the normal distribution and the attention mechanism (NAW-LSTM), BiLSTM weighted with the normal distribution and the attention mechanism (NAW-BiLSTM), DBLSTM weighted with the normal distribution (NW-DBLSTM), and NAW-DBLSTM weighted without the time window (NT-NAW-DBLSTM), respectively. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Attention mechanism; Big data analytics; DBLSTM neural network; Normal distribution; Spark; Traffic flow forecasting","Advanced traffic management systems; Big data; Digital storage; Forecasting; Highway traffic control; Information management; Intelligent systems; Long short-term memory; Normal distribution; Real time systems; Taxicabs; Trajectories; Attention mechanisms; Distributed computing platform; Intelligent transportation systems; Nonlinear characteristics; Real time traffics; Spatial correlations; Traffic flow forecasting; Traffic management and controls; Street traffic control",Article,Scopus,2-s2.0-85114722496
"Alameen A.","57193394145;","Repeated attribute optimization for big data encryption",2022,"Computer Systems Science and Engineering","40","1",,"53","64",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114714457&doi=10.32604%2fCSSE.2022.017597&partnerID=40&md5=83961ff22ecfc94af9c819bb70133664","Big data denotes the variety, velocity, and massive volume of data. Existing databases are unsuitable to store big data owing to its high volume. Cloud computing is an optimal solution to process and store big data. However, the significant issue lies in handling access control and privacy, wherein the data should be encrypted and unauthorized user access must be restricted through efficient access control. Attribute-based encryption (ABE) permits users to encrypt and decrypt data. However, for the policy to work in practical scenarios, the attributes must be repeated. In the case of specific policies, it is not possible to avoid attribute repetition even after the application of Boolean optimization approaches to obtain a Boolean formula. For these policies, there exists a variety of evaluated secret shares for the repeated attributes. Therefore, the calculation of cipher text for these irreducible policies seems to be lengthy and computationally intensive. To address this problem, an improved meta-heuristic-based repeated attributes optimization on cipher-text policy-ABE (CP-ABE) is developed in this study. Here, the improved meta-heuristic concept is developed in the encryption phase, which returns the optimized single share value of each repeated attribute after considering all the attribute shares. The optimization process not only minimizes the encryption cost but also the communication cost. Herein, the improved sun flower optimization (SFO), called the newly updated SFO (NU-SFO) is used to perform the repeated attribute optimization in CP-ABE. Finally, the performance evaluation confirms the reliability and robustness of the developed scheme through comparisons with traditional constructions. © 2022 CRL Publishing. All rights reserved.","Big data; Cipher text policy; Encryption; Repeated attribute optimization","Access control; Big data; Boolean algebra; Heuristic algorithms; Optimization; Attribute-based encryptions; Boolean formulae; Boolean optimizations; Communication cost; Optimal solutions; Reliability and robustness; Traditional constructions; Unauthorized users; Cryptography",Article,Scopus,2-s2.0-85114714457
"Yuan K., Chi G., Zhou Y., Yin H.","57215155882;12803678000;57210572839;57254360300;","A novel two-stage hybrid default prediction model with k-means clustering and support vector domain description",2022,"Research in International Business and Finance","59",,"101536","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114690894&doi=10.1016%2fj.ribaf.2021.101536&partnerID=40&md5=4d558e097155cb2cde65d30ffbb307b9","Default prediction identifies the probability of a firm to default by establishing a prediction model. It reveals the functional relation between the features’ data at time t-m and default status at t. If the prediction of a defaulting company is wrong, it will mislead banks into making loans to a “defaulter,” causing huge losses; if the prediction of a non-defaulting company is wrong, it will result in a potential churn in high-quality customers. To support the lending decisions of banks and non-banking financial institutions, this study proposes a two-stage default prediction model that integrates k-means clustering for partitioning the sample and support vector domain description (SVDD) for predicting default (credit scoring). It also uses attributes’ data at time t-m (m = 1, 2, 3, 4, 5) and the default status at t to train the proposed model so that it can warn of default m years ahead. The results show that the predictive accuracy of the proposed two-stage default prediction model is better than that of single-stage models using only k-means clustering or support vector domain description, and the proposed model could achieve a five-year default prediction ability (AUC > 0.85). Further, the study implies that “retained earnings/total assets”, “financial expenses/gross revenue”, and “type of audit opinion” are three key features in default forecasting for Chinese listed enterprises. This study contributes to the field of multi-stage credit scoring research by demonstrating that a combination of different methods is worth considering to improve the performance of default prediction models. © 2021 Elsevier B.V.","Big data; Default prediction; K-means clustering; Optimal cluster number; Optimal kernel function; Support vector domain description",,Article,Scopus,2-s2.0-85114690894
"Qian Y., Li C.-X., Zou X.-G., Feng X.-B., Xiao M.-H., Ding Y.-Q.","57253227700;57252618200;55551790900;54787483500;18234453600;57253349200;","Research on predicting learning achievement in a flipped classroom based on MOOCs by big data analysis",2022,"Computer Applications in Engineering Education","30","1",,"222","234",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114615518&doi=10.1002%2fcae.22452&partnerID=40&md5=cdb722fa2a7a788a1d145aeda7d481a1","In the era of big data mining, educational data mining has become a principal research focus, with online education mining, such as massive open online courses' (MOOC) data analysis, representing an important source of it. Recent studies have found that learners have low passing rates on MOOCs. A number of studies have proposed prediction models for the dropout rate of learners on MOOCs. The improvement of MOOCs and the promotion of personalized education are the key points of online education. However, the selection and intervention of students with a tendency to drop out slows down the efficiency of teaching and increases the burden on teachers. This study's aim is to utilize back propagation neural networks and radar graphs in a flipped classroom based on MOOCs to predict students' future grades and to analyze the influence of teaching from various perspectives to support the promotion and reform of teaching and curriculum. Compared with the previous year, after the forecast and the adjustment, this year's student scores increase significantly. © 2021 Wiley Periodicals LLC",,"Backpropagation; Big data; Curricula; Data mining; E-learning; Education computing; Forecasting; Predictive analytics; Back propagation neural networks; Educational data mining; Learning achievement; Massive open online course; On-line education; Prediction model; Previous year; Research focus; Students",Article,Scopus,2-s2.0-85114615518
"Eom S., Jang M., Ji N.-S.","57210336845;57249675200;57249253500;","Human Mobility Change Pattern and Influencing Factors during COVID-19, from the Outbreak to the Deceleration Stage: A Study of Seoul Metropolitan City",2022,"Professional Geographer","74","1",,"1","15",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114396862&doi=10.1080%2f00330124.2021.1949729&partnerID=40&md5=3f85c2424cdff4f02551a59b07b9155e","Many countries have started to reopen economic activities after the COVID-19 pandemic; however, due to the disease’s long incubation period and high infectivity, social distancing remains an essential measure despite the start of vaccinations. It is therefore necessary to understand the changes in human mobility from the outbreak’s initiation to deceleration to control the disease’s transmission and revitalize the economy. This study suggests a methodology for investigating the changes in human mobility and their influencing factors using a case study of Seoul Metropolitan City, Korea. First, the changing patterns of human mobility were investigated based on mobile big data, including the acceleration and deceleration stages. The results showed that it varied by area and travel distance. Second, we clarified the influence of sociodemographic factors such as employee density and land use proportion on the change pattern of human mobility by applying a machine learning method. This finding implies that the effectiveness of policies such as social distancing can vary with sociodemographic factors. For example, areas with more real estate, public administration, and health care employees showed rapid recovery and faced a transmission risk by reopening economic activities. The suggested methodology can help understand human mobility and explore exit strategies. © 2021 by American Association of Geographers.","big data; COVID-19; human mobility; support vector machines; time series clustering","COVID-19; disease control; disease transmission; metropolitan area; migration; mobility; social policy; socioeconomic impact; support vector machine; time series; Seoul [South Korea]; South Korea",Article,Scopus,2-s2.0-85114396862
"Ma X., Zhang Y.","57248436200;57221367159;","Digital innovation risk management model of discrete manufacturing enterprise based on big data analysis",2022,"Journal of Global Information Management","30","7","286761","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114305136&doi=10.4018%2fJGIM.286761&partnerID=40&md5=bed6df90a735598eedf350ce3a3b8268","At present, most risk management work mainly relies on manpower, and manpower relies on the professional knowledge of relevant skilled workers to discover hidden safety risks in production activities. This article combines relevant big data theories and 4V characteristics to analyze and investigate safety production and big data, study data structure, data source, and data type. Using 5W1H scientific big data and applications, this analysis method analyzes the theoretical basis, applications, and beneficiaries of big data related to safety production, some of which are application links and important theoretical issues. Secondly, it studies the security risk management model based on big data, proposes a risk management model based on big data, the technical basis of big data, and the idea of a three-dimensional model, and applies the systematic space method, which is reflected in three aspects of risk management. In the end, a risk identification model based on big data, a risk assessment classification model, and a risk early warning and pre-control model are defined. © 2022 IGI Global. All rights reserved.","Big data analysis; Digital innovation; Discrete manufacturing; Risk management","Big data; Human resource management; Information management; Risk assessment; Safety engineering; Classification models; Discrete manufacturing; Production activity; Professional knowledge; Risk identification model; Risk management models; Security risk managements; Three-dimensional model; Risk management",Article,Scopus,2-s2.0-85114305136
"Cuzzocrea A., Fadda E., Mumolo E.","23388216900;57190034984;7004167355;","Cyber-attack detection via non-linear prediction of IP addresses: an innovative big data analytics approach",2022,"Multimedia Tools and Applications","81","1",,"171","189",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114169079&doi=10.1007%2fs11042-021-11390-1&partnerID=40&md5=af5bd2ca9cc9024063a710c3feca7d46","Computer network systems are often subject to several types of attacks. For example, an excessive traffic load sent to a web server for making it unusable is the main technique introduced by the Distributed Denial of Service (DDoS) attack. A well-known method for detecting attacks consists in analyzing the sequence of source IP addresses for detecting possible anomalies. With the aim of predicting the next IP address, the Probability Density Function of the IP address sequence is estimated. Anomalous requests are detected via predicting source’s IP addresses in future accesses to the server. Thus, when an access to the server occurs, the server accepts only the requests from the predicted IP addresses and it blocks all the others. The approaches used to estimate the Probability Density Function of IP addresses range from the sequence of IP addresses seen previously and stored in a database to address clustering, for instance via the K-Means algorithm. Instead, the sequence of IP addresses is considered as a numerical sequence in this paper, and non-linear analysis of this numerical sequence is applied. In particular, we exploited non-linear analysis based on Volterra Kernels and Hammerstein models. The experiments carried out with datasets of source IP address sequences show that the prediction errors obtained with Hammerstein models are smaller than those obtained both with the Volterra Kernels and with the sequence clustering based on the K-Means algorithm. © 2021, The Author(s).","Cyber-attack; Distributed Denial of Service; Hammerstein models","Advanced Analytics; Big data; Data Analytics; Denial-of-service attack; Forecasting; K-means clustering; Network security; Probability density function; Detecting attacks; Distributed denial of service attack; Hammerstein model; Non-linear predictions; Numerical sequence; Prediction errors; Sequence clustering; Volterra kernels; Internet protocols",Article,Scopus,2-s2.0-85114169079
"Ke C., Xiao F., Huang Z., Meng Y., Cao Y.","55210071800;7201709602;14325169600;57201150771;57202434403;","Ontology-Based Privacy Data Chain Disclosure Discovery Method for Big Data",2022,"IEEE Transactions on Services Computing","15","1",,"59","68",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114162346&doi=10.1109%2fTSC.2019.2921583&partnerID=40&md5=7d3a058b736b8d0d48a2a565567e07e4","To meet user's functional requirements, cloud computing and big data have become the most commonly used computing and data resources. Based on analysis, conversion, extraction and refinement for the big data, a disease can be prevented and group behavior can be predicted. However, each user's private data is also an element in big data. Users must provide private data to the service providers to meet their functional requirements. To gain economic benefits, some SaaS service providers have not been authorized to collect and analyze the user's sensitive private data, as a result, the user's private data is disclosed. In this paper, we propose a private data chain disclosure discovery method, to prevent a user's sensitive privacy information from being illegally disclosed. First, we measure the similarity degree and cost of the disclosure of the private data. Second, according to the similarity degree and cost of disclosure, the disclosure chain and key private data are detected in the process of interaction between user and SaaS service. Third, we propose a discovery framework for the private data chain and demonstrate its feasibility and effectiveness by experiments. © 2008-2012 IEEE.","Ontology; privacy data chain; privacy disclosure detection; similarity metric","Big data; Ontology; Software as a service (SaaS); Data chain; Functional requirement; Ontology's; Privacy data chain; Privacy disclosure detection; Privacy disclosures; Private data; Service provider; Similarity degree; Similarity metrics; Data privacy",Article,Scopus,2-s2.0-85114162346
"Tu W.-J., Yan F., Chao B.-H., Ma L., Ji X.-M., Wang L.-D.","36461935400;57226425017;57189505470;57226419203;7402840150;25951682000;","Thrombolytic DNT and fatality and disability rates in acute ischemic stroke: a study from Bigdata Observatory Platform for Stroke of China",2022,"Neurological Sciences","43","1",,"677","682",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114143096&doi=10.1007%2fs10072-021-05580-w&partnerID=40&md5=38bac843d3c039999a9dd81ee5b0d0b3","Objective: To evaluate whether shorter door-to-needle times (DNT) with intravenous tissue plasminogen activator (tPA) for acute ischemic stroke are associated with improved 1-year outcomes in Chinese patients. Methods: From August to September 2019, all first-ever ischemic stroke patients who were treated with intravenous tPA within 4.5 h of the time they were last known to be well from 232 hospitals in China were included. Patients were divided into four groups according to DNT time (≤ 45 min; 45–60 min; 60–90 min; > 90 min). All discharged patients would receive a telephone follow-up at 12-month after admission. Death and disability events were recorded. Results: Finally, 2370 patients were analyzed. The median age was 65 years, 66.6% were male, and 2.4% were of ethnic minorities. In the 1-year follow-up, 211 patients died (8.9%; 95%CI: 7.8–10.0%). The patients (53.1%) had DNT times of longer than 45 min, compared with those treated within 45 min, did not have significantly higher 1-year mortality (8.9% vs 8.9% [absolute difference, 0.03% {95% CI, − 0.05% to − 0.10%}, odd ratio {OR}, 1.00 {95% CI, 0.75 to 1.33}]). In addition, 385 patients (16.2%; 14.8–17.3%) out of those survivors had disability events. The patients had DNT times of longer than 45 min, compared with those treated within 45 min, did not have significantly higher 1-year disability rate (18.9% vs 16.7% [absolute difference, 1.9% {95% CI, 1.1% to 3.0%}, odd ratio {OR}, 1.22 {95% CI, 0.89 to 1.43}]). Conclusions: The results did not show that shorter DNT for tPA administration was significantly associated with better 1-year outcomes. © 2021, Fondazione Società Italiana di Neurologia.","Chinese; Door-to-needle times; Ischemic stroke; Prognosis","tissue plasminogen activator; fibrinolytic agent; tissue plasminogen activator; acute ischemic stroke; aged; Article; atrial fibrillation; big data; case fatality rate; cohort analysis; controlled study; diabetes mellitus; disability; ethnic group; family history; female; fibrinolytic therapy; follow up; hospital discharge; human; hyperlipidemia; hypertension; in-hospital mortality; incidence; length of stay; major clinical study; male; mortality rate; National Institutes of Health Stroke Scale; outcome assessment; prospective study; Rankin scale; recurrence risk; stroke survivor; telephone interview; time to treatment; transient ischemic attack; brain ischemia; cerebrovascular accident; complication; fibrinolytic therapy; time to treatment; treatment outcome; Aged; Brain Ischemia; Ethnic and Racial Minorities; Fibrinolytic Agents; Humans; Ischemic Stroke; Male; Stroke; Thrombolytic Therapy; Time-to-Treatment; Tissue Plasminogen Activator; Treatment Outcome",Article,Scopus,2-s2.0-85114143096
"Dehghani M., Shooshtarian M.R., Moosavi P., Zare F., Derakhshan Z., Ferrante M., Conti G.O., Jafari S.","55339569500;57195057580;57242500400;57242677500;55770305700;57143880700;24332549000;57243021200;","A process mining approach in big data analysis and modeling decision making risks for measuring environmental health in institutions",2022,"Environmental Research","203",,"111804","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114110504&doi=10.1016%2fj.envres.2021.111804&partnerID=40&md5=3cb284539a28e71b6b952a23c3a33c5f","This paper aimed to introduce a process-mining framework for measuring the status of environmental health in institutions. The methodology developed a new software-based index namely Institutional Environmental Health Index (IEHI) that was integrated from ontology-based Multi-Criteria Group Decision-Making models based on the principles of fuzzy modeling and consensus evaluation. Fuzzy Ordered Weighting Average (OWA) with the capability of modeling the uncertainties and decision-making risks along with Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) were employed as the computation engine. The performance of the extended index was examined through an applied example on 20 mosques as public institutions. IEHI could analyze big data collected by environmental health investigators and convert them to a single and interpretable number. The index detected the mosques with very unsuitable health conditions that should be in priority of sanitation and suitable ones as well. Due to the capability of defining the type and numbers of criteria and benefitting from specific and user-friendly software namely Group Fuzzy Decision-Making, this index is highly flexible and practical. The methodology could be used for numerating the environmental health conditions in any intended institution or occupation. The proposed index would provide e-health assessment by more efficient analysis of big data and risks that make more realistic decisions in environmental health system. © 2021 Elsevier Inc.","Data mining; Decision support models; Environmental health; Fuzzy logic; Risk","data mining; decision making; environmental effect; fuzzy mathematics; methodology; numerical model; software; article; big data; consensus; data mining; decision support system; environmental health; fuzzy logic; human; occupation; ontology; risk assessment; sanitation; sensitivity analysis; software; technique for order preference by similarity to ideal solution; telehealth; uncertainty; data analysis; decision making; environmental health; Big Data; Data Analysis; Decision Making; Environmental Health; Fuzzy Logic",Article,Scopus,2-s2.0-85114110504
"Yoffe H., Plaut P., Grobman Y.J.","57240779200;6602081212;55102783800;","Towards sustainability evaluation of urban landscapes using big data: a case study of Israel’s architecture, engineering and construction industry",2022,"Landscape Research","47","1",,"49","67",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114032101&doi=10.1080%2f01426397.2021.1970123&partnerID=40&md5=b0b188fd09df03801c1f72bf0816e324","Sustainability Rating Systems are standard methods for achieving sustainable development of buildings and urban landscapes. However, they suffer from low adoption and implementation rates, mainly due to labour-intensive evaluation processes. This study explores how recent advancements in big data, combined with the availability of new urban environment datasets, could advance sustainability rating systems in landscape development. We compared between existing computational technology (supply) and industry performance evaluation needs (demand) using a systematic review and survey of Israel’s professional communities as a case study. Of the existing indicators, Israeli professionals prioritised measuring socio-ecological indicators of landscapes in development projects, mainly at the urban level. Our review revealed that this level also holds available big data sustainability evaluation methods and technologies. Specifically, directed data for measuring ecology and volunteered and automated data for measuring social indicators. Such supply-demand links could significantly advance evaluation methods towards achieving a broader application of sustainable urban development. © 2021 Landscape Research Group Ltd.","big data; Israel; landscape architecture; Landscape sustainability; performance evaluation; sustainable development; urban landscape",,Article,Scopus,2-s2.0-85114032101
"Li P., Lü L.","57199005221;55474677400;","Research on a China 6b heavy-duty diesel vehicle real-world engine out NOx emission deterioration and ambient correction using big data approach",2022,"Environmental Science and Pollution Research","29","5",,"6949","6976",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113955072&doi=10.1007%2fs11356-021-15778-2&partnerID=40&md5=9ff80fc52016ffd8c0b1b7c2f40cc44e","China VI standard proposed higher requirements for durability of heavy-duty diesel vehicles emissions. Previous research which took advantages of both on-board sensors and big data approach to get the NOx deterioration factor was rather scarce. This paper used big data approach to study the deterioration of engine out NOx emission based on 254,622 km operation data getting from the on-board sensors or ECUs (Electronic Control Unit). Meanwhile, a formula for on-board NOx correction for ambient humidity and temperature had been fitted. The analyses revealed that the engine out NOx deterioration factor (DF) of the maximum weight steady-state condition was about 1.005 after 254,622 km durability test; as for transient conditions, the DF was not more than 1.092 during 254,622 km durability test. For a same steady working condition, the engine out NOx mass flow (g/h) was negatively linearly correlated with absolute humidity (Ha) (R2 = 0.997). If Ha was lower than 12 g/kg, Ha almost had no effect on engine out NOx concentration (ppm). Otherwise, there was also a negatively linear relationship between them (R2 = 0.978). It is hoped that the methods and conclusions of this paper could provide some enlightenment for future NOx emission deterioration research. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Big data; Deterioration factor; Engine out NOx emission; Heavy-duty diesel vehicle; NOx correction for humidity and temperature","concentration (composition); correction; data set; durability; nitric oxide; relative humidity; temperature effect; transport vehicle; China; gasoline; air pollutant; China; exhaust gas; motor vehicle; Air Pollutants; Big Data; China; Gasoline; Motor Vehicles; Vehicle Emissions",Article,Scopus,2-s2.0-85113955072
"Pallamala R.K., Rodrigues P.","57232165400;55966058700;","An Investigative Testing of Structured and Unstructured Data Formats in Big Data Application Using Apache Spark",2022,"Wireless Personal Communications","122","1",,"603","620",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113521894&doi=10.1007%2fs11277-021-08915-0&partnerID=40&md5=86e514305081b322a104748e6c6e5ee2","Apache Spark programming framework provides an effective and vigorous open-source solution for Big data testing and management. It offers a rich set of big data applications through a programming interface. Testing procedures deliver optimum solutions to complex business problems like analyzing large data volumes. A typical test engineer demands manual to automatic testing which assists the testing strategy with minimal effort. Big data testing means, verification, and validation of data while storing and processing it. This research paper demonstrates the challenges with new high-performance analytics for simpler and faster testing processing of relevant data. It enables, timely and accurate insights using Big data testing predict analytics and can manage large quantities of Structured, Semi-structured, and Unstructured data forms with spark analysis. These methods are evaluated using Extract, Transformation, Loading, and Apache spark procedures. The proposed Big data testing method has statistically outperformed conventional procedures. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Apache spark; Big data testing; Testing Whiz","Advanced Analytics; Automatic testing; Big data; Data handling; Open source software; Big data applications; Large data volumes; Open-source solutions; Programming framework; Programming interface; Testing procedure; Testing strategies; Unstructured data; Information management",Article,Scopus,2-s2.0-85113521894
"Monteiro A.M., Santos A.A.F.","56180311700;36770969200;","Option prices for risk-neutral density estimation using nonparametric methods through big data and large-scale problems",2022,"Journal of Futures Markets","42","1",,"152","171",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113190440&doi=10.1002%2ffut.22258&partnerID=40&md5=f9ca5f5ee0bc14e9de4904d012ffb8d3","Option pricing theory determines the structure of call and put option pricing functions. In nonparametric risk-neutral density estimation based on kernel functions, local constraints cannot induce a second derivative function that must integrate one. Convexity and monotonicity of pricing functions also cannot be enforced. A large-scale (optimization) approach is proposed for the risk-neutral density estimation, imposing an enlarged set of no-arbitrage constraints. We considered simulations using Heston's model and hypergeometric functions. The method is applied to samples of intraday data from VIX and S&P500 indexes. © 2021 Wiley Periodicals LLC","big data; large-scale optimization; nonparametric estimation; option pricing; risk-neutral density",,Article,Scopus,2-s2.0-85113190440
"Xu C., Du X., Fan X., Yan Z., Kang X., Zhu J., Hu Z.","57221160265;56506043800;7403394160;55461573900;57226736042;55575860800;57226817493;","A Modular Remote Sensing Big Data Framework",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112575489&doi=10.1109%2fTGRS.2021.3100601&partnerID=40&md5=e4754f82d35cfaa20e4dc8f90a8c8af2","Today, remote sensing (RS) data are already regarded as 'big data.' Developments in computer science have made it possible to explore the potential treasure within remote sensing big data, but only limited remote sensing research has made use of big data technology due to gaps in techniques between big data and remote sensing. In this research, we analyzed the full processing flow of remote sensing big data from the perspective of both computer science and remote sensing science and proposed a modular framework. Computation ready data (CRD), a dynamic data type for computation based on analysis ready data (ARD), is proposed to connect the two main modules of the framework, the data module and computation module. Compared with existing research, the proposed framework classifies and abstracts the key technical and research points of the processing of remote sensing big data as replaceable modules and bridges them through an open organization. Subsequently, we built a prototype platform with open-source technologies and carried out three experiments to validate the feasibility and advantages of the framework, namely normalized difference vegetation index (NDVI) production, water body change detection, and land use classification. Results indicate that this framework can greatly reduce experimental costs for remote sensing researchers. While the proposed framework has proven flexible and practical, further research is needed for the technical implementation of certain modules to achieve the original intention of the framework. © 1980-2012 IEEE.","Cloud computing; distributed system; open-source; remote sensing (RS) big data","Big data; Land use; Computation modules; Data technologies; Dynamic data types; Landuse classifications; Normalized difference vegetation index; Open-source technology; Remote sensing data; Technical implementation; Remote sensing; advanced technology; data; land use; NDVI; remote sensing; research; software",Article,Scopus,2-s2.0-85112575489
"Beerkens M.","36179370300;","An evolution of performance data in higher education governance: a path towards a ‘big data’ era?",2022,"Quality in Higher Education","28","1",,"29","49",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111878485&doi=10.1080%2f13538322.2021.1951451&partnerID=40&md5=1cfda3584bb69d906c80fe8a013eb0d8","Performance data in higher education has gone through a major development in the last few decades. Simple input measures have given way to increasingly nuanced and dynamic output measures and performance indicators have become an integral part of management at the organisational and system level. The evolution of higher education performance measurement shows a reiterative relationship between data availability, its purpose in a governance system and its target audience. Digitalisation of learning, management and communication systems has revolutionised data availability, creating new possibilities for ‘big data’ use. Based on insights from the past evolution, current experiments with ‘big data’ and lessons from other sectors, the article explores what the new ‘big data’ era might mean for higher education governance. The high volume of data but also its speed of accumulation and related analytical techniques, are likely to substantially transform the current relationship between data and performance but also create some technical, ethical and policy challenges. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","accountability; artificial intelligence; big data; governance; higher education; Performance",,Article,Scopus,2-s2.0-85111878485
"Sorensen V., Lansing J.S., Thummanapalli N., Cambria E.","57195954644;7003383124;57195958146;56140547500;","Mood of the Planet: Challenging Visions of Big Data in the Arts",2022,"Cognitive Computation","14","1",,"310","321",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111635698&doi=10.1007%2fs12559-020-09766-w&partnerID=40&md5=d6ccc30b0f10567789f43785017dc2d2","Mood of the Planet is an interactive physical-digital sculpture that has as its center-piece a large “arch” or “doorway” that emits colored light and sound as a form of visualization and sonification of the changing, live emotions expressed by people all around the Earth. It is the product of several disciplines, including the arts, computer science, linguistics and psychology. In particular, we use artificial intelligence to collect and analyze social media data and extract emotions from these using a brain-inspired and psychologically motivated emotion categorization model. Such emotions are then translated into colors and sounds that the audience can experience while passing through the arch. Feedback from the audience proved the Mood of the Planet to provide a more accurate, personal and tangible experience about the data-emotions dichotomy. © 2021, Springer Science+Business Media, LLC, part of Springer Nature.","Affective computing; Art and emotion; Emotion AI; Sentiment Analysis","Arches; Artificial intelligence; Big data; Earth (planet); Brain-inspired; Colored light; Social media datum; Sonifications; Arts computing",Article,Scopus,2-s2.0-85111635698
"Jeong J., Lee N., Shin Y., Shin D.","57214677708;57224176334;57224163201;57226450962;","Intelligent generation of optimal synthetic pathways based on knowledge graph inference and retrosynthetic predictions using reaction big data",2022,"Journal of the Taiwan Institute of Chemical Engineers","130",,"103982","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111558513&doi=10.1016%2fj.jtice.2021.07.015&partnerID=40&md5=78bcf132f256cc30511c75c2aafca9bc","The selection and design of suitable synthetic paths are important issues that affect the economics and productivity of chemical processes including reactions and the discovery of new chemicals. However, exploration of reaction information is difficult even with reaction databases, causing path explosion that occurs due to the huge search space and conflicting constrains such as economics, safety, efficiency, etc. In this study, we propose an intelligent system ASICS (Advanced System for Intelligent Chemical Synthesis), which supports synthetic path design at the basic stages of research and process design, based on the hybrid generative exploration and exploitation of reaction knowledge base, encoding big data of patented reactions, and machine learning-based retrosynthetic prediction. Based on the pseudo A* search, ASICS generates optimal synthetic paths minimizing scores of the synthetic reaction value function, composed of the synthetic accessibility score, likelihood score and similarity score. The preference in searching between confirmed reaction spaces and unexplored reaction spaces through prediction can be selected by the user. The suggested hybrid approach, combining the reaction knowledge base with the retrosynthetic prediction model, generates feasible and low-cost synthetic paths beyond the accumulated information in patented reactions. © 2021 Taiwan Institute of Chemical Engineers","Chemical reaction big data; Decision support system; Knowledge base inference; Knowledge graph, ML-based retrosynthetic prediction; Synthetic path generation","Big data; Economics; Forecasting; Indicators (chemical); Intelligent systems; Knowledge based systems; Knowledge representation; Predictive analytics; Advanced systems; Chemical process; Exploration and exploitation; Prediction model; Reaction database; Similarity scores; Synthetic pathways; Synthetic reactions; Search engines",Article,Scopus,2-s2.0-85111558513
"Sagan V., Maimaitijiang M., Paheding S., Bhadra S., Gosselin N., Burnette M., Demieville J., Hartling S., Lebauer D., Newcomb M., Pauli D., Peterson K.T., Shakoor N., Stylianou A., Zender C.S., Mockler T.C.","57200370110;56524539600;56585990900;57188991461;56153830000;57203372609;57226181937;55961174300;57211261740;10540943000;56400724500;57204395597;56010939600;57225449152;6603400519;6602442579;","Data-Driven Artificial Intelligence for Calibration of Hyperspectral Big Data",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110872210&doi=10.1109%2fTGRS.2021.3091409&partnerID=40&md5=f4f5ac275a1602c02f2c54c239f24dcb","Near-earth hyperspectral big data present both huge opportunities and challenges for spurring developments in agriculture and high-throughput plant phenotyping and breeding. In this article, we present data-driven approaches to address the calibration challenges for utilizing near-earth hyperspectral data for agriculture. A data-driven, fully automated calibration workflow that includes a suite of robust algorithms for radiometric calibration, bidirectional reflectance distribution function (BRDF) correction and reflectance normalization, soil and shadow masking, and image quality assessments was developed. An empirical method that utilizes predetermined models between camera photon counts (digital numbers) and downwelling irradiance measurements for each spectral band was established to perform radiometric calibration. A kernel-driven semiempirical BRDF correction method based on the Ross Thick-Li Sparse (RTLS) model was used to normalize the data for both changes in solar elevation and sensor view angle differences attributed to pixel location within the field of view. Following rigorous radiometric and BRDF corrections, novel rule-based methods were developed to conduct automatic soil removal; and a newly proposed approach was used for image quality assessment; additionally, shadow masking and plot-level feature extraction were carried out. Our results show that the automated calibration, processing, storage, and analysis pipeline developed in this work can effectively handle massive amounts of hyperspectral data and address the urgent challenges related to the production of sustainable bioenergy and food crops, targeting methods to accelerate plant breeding for improving yield and biomass traits. © 1980-2012 IEEE.","Bidirectional reflectance distribution function (BRDF) correction; high-throughput phenotyping; image quality assessment; shadow compensation; soil removal","Agricultural robots; Agriculture; Artificial intelligence; Big data; Biomass; Calibration; Distribution functions; Food storage; Image quality; Radiometry; Reflection; Automated calibration; Bidirectional reflectance distribution functions; Data-driven approach; Downwelling irradiance; Fully automated calibrations; Hyperspectral Data; Image quality assessment; Radiometric calibrations; Digital storage; alternative energy; artificial intelligence; bidirectional reflectance; biomass; plant breeding",Article,Scopus,2-s2.0-85110872210
"Lv H., Shi S., Gursoy D.","57225968226;56498325900;6603436465;","A look back and a leap forward: a review and synthesis of big data and artificial intelligence literature in hospitality and tourism",2022,"Journal of Hospitality Marketing and Management","31","2",,"145","175",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109976456&doi=10.1080%2f19368623.2021.1937434&partnerID=40&md5=ad7521cc1602a866a6a8417f2ad7f97d","In reaction to the growing attention paid to big data and artificial intelligence in hospitality and tourism research, we systematically reviewed 270 relevant studies to identify topical themes and trends. We first briefly reviewed the emergence definition of big data. Next, we introduced the methodology of literature collection and presented results of bibliometric analysis. Then, we identified types of big data used and the application of artificial intelligence in big data usage in hospitality and tourism research, followed by unveiling major themes of big data and artificial intelligence research in extant literature such as forecasting, industry development, marketing, performance analysis, consumer behaviors, attitudes, and so on. In addition, implications and challenges of applying big data and artificial intelligence in hospitality and tourism research and new directions for future research are identified. Finally, we discuss limitations of our review, proposing future research directions for scholars. © 2021 Taylor & Francis Group, LLC.","artificial intelligence; bibliometric analysis; Big data; hospitality; literature review; tourism",,Article,Scopus,2-s2.0-85109976456
"Miah S.J., Vu H.Q., Alahakoon D.","24178573200;36081495000;6602546111;","A social media analytics perspective for human-oriented smart city planning and management",2022,"Journal of the Association for Information Science and Technology","73","1",,"119","135",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109754559&doi=10.1002%2fasi.24550&partnerID=40&md5=722efa78213f2014f4ec2b748a22f393","Understanding how people engage in daily activities within a region can provide valuable information for smart city planners and strategic partners to use to assist in their decision-making processes. Such insights may relate to economic activities, sustainable city design, environmental impacts, and responses to climate change, contributing to the improvement in the quality of human life. Considerable attention is recently directed towards smart city initiatives that benefit majority of people, rather than projects that cater to the political, architectural, or vanity needs of a minority. However, understanding citizen requirements, behaviors, and opinions is difficult, and requires the use of technology and appropriate information sources. While social-media big data have provided opportunities to develop evidence-based insights into human daily activities, effective analytical methods to harness these opportunities remain in development. We propose a new analytical method to provide a deeper understanding of citizen activities by constructing building blocks in their activity storylines, with analysis of these storylines providing evidence-based insights into their activities. Results demonstrate the usefulness of our method to smart city planners and strategic partners, providing invaluable insights to assist them in making decisions regarding sustainable smart city development. © 2021 Association for Information Science and Technology.",,"Advanced Analytics; Behavioral research; Climate change; Decision making; Economics; Environmental impact; Planning; Social networking (online); Urban growth; Analytical method; Building blockes; Decision making process; Economic activities; Information sources; Social media analytics; Strategic partners; Sustainable cities; Smart city; analytic method; article; big data; city planning; daily life activity; human; human experiment; social media analysis",Article,Scopus,2-s2.0-85109754559
"Özsoy K., Aksoy B.","57201669397;57209338440;","Real-Time Data Analysis with Artificial Intelligence in Parts Manufactured by FDM Printer Using Image Processing Method",2022,"Journal of Testing and Evaluation","50","1",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109208002&doi=10.1520%2fJTE20210125&partnerID=40&md5=28620df6564f7331c1dd62a8ad3e75c5","In this study, samples manufactured with polylactic acid (PLA) plastic material using the fused deposition modeling (FDM) type printer were analyzed during the manufacturing process using image processing and real-time big data analysis. The purpose of real-time big data analysis is to provide an effective and efficient guide to the user in the manufacturing process regarding the manufactured part's mechanical properties. In this study, compression samples were prepared according to ASTM D695-15, Standard Test Method for Compressive Properties of Rigid Plastics, test standards and subjected to mechanical tests. In the first stage of the research, using artificial neural networks (ANNs), processing parameters were estimated with 92.5 % accuracy according to the R2 performance evaluation criterion. In the second stage, each layer's infill percentage and layer thickness of the compression sample were analyzed using image processing techniques. In the final stage of the study, using the Python programming language, a user-specific visual interface is designed for showing the results and graphics related to the material processing step in FDM 3D printing. © 2021 ASTM International, 100 Barr Harbor Drive, PO Box C700, West Conshohocken, PA 19428-2959.","additive manufacturing; artificial intelligence; artificial neural networks; data analysis; fused deposition modeling; image processing","Big data; Data handling; Fused Deposition Modeling; Image analysis; Information analysis; Neural networks; Plastics industry; Printing presses; Processing; User interfaces; Visual languages; Compressive properties; Fused deposition modeling (FDM); Image processing - methods; Image processing technique; Performance evaluation criteria; Polylactic acid plastics; Python programming language; Real time data analysis; Manufacturing data processing",Article,Scopus,2-s2.0-85109208002
"Götz F.M., Gosling S.D., Rentfrow P.J.","56681836600;16733981500;6602274526;","Small Effects: The Indispensable Foundation for a Cumulative Psychological Science",2022,"Perspectives on Psychological Science","17","1",,"205","215",,17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109019626&doi=10.1177%2f1745691620984483&partnerID=40&md5=759eff24ed17c491ead14e649bcde7e0","We draw on genetics research to argue that complex psychological phenomena are most likely determined by a multitude of causes and that any individual cause is likely to have only a small effect. Building on this, we highlight the dangers of a publication culture that continues to demand large effects. First, it rewards inflated effects that are unlikely to be real and encourages practices likely to yield such effects. Second, it overlooks the small effects that are most likely to be real, hindering attempts to identify and understand the actual determinants of complex psychological phenomena. We then explain the theoretical and practical relevance of small effects, which can have substantial consequences, especially when considered at scale and over time. Finally, we suggest ways in which scholars can harness these insights to advance research and practices in psychology (i.e., leveraging the power of big data, machine learning, and crowdsourcing science; promoting rigorous preregistration, including prespecifying the smallest effect size of interest; contextualizing effects; changing cultural norms to reward accurate and meaningful effects rather than exaggerated and unreliable effects). Only once small effects are accepted as the norm, rather than the exception, can a reliable and reproducible cumulative psychological science be built. © The Author(s) 2021.","questionable research practices; research culture; scientific community; small effects","article; big data; crowdsourcing; effect size; human; machine learning; psychological and psychiatric procedures; psychology; reward; theoretical study",Article,Scopus,2-s2.0-85109019626
"Hong J., Kim I., Song J., Ahn B.K.","57222333141;55263688600;56121669400;36700954100;","Socio-demographic factors and lifestyle associated with symptomatic hemorrhoids: Big data analysis using the National Health insurance Service-National Health screening cohort (NHIS-HEALS) database in Korea",2022,"Asian Journal of Surgery","45","1",,"353","359",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108804350&doi=10.1016%2fj.asjsur.2021.06.020&partnerID=40&md5=f6280f1da5b128b22f88cef621d42fcd","Objective: The prevalence of hemorrhoids has been reported to be 7–14%. However, there have been no large-scale studies. This study aims to investigate the incidence of hemorrhoids in Korea by analyzing big data and to find the associated risk factors. Methods: This was a retrospective analysis using the Health Insurance Cohort database of the National Health Insurance Corporation of Korea in 2002–2015. The study was divided into two models: the diagnostic (DM) and surgical model (SM). Socio-demographic and lifestyle behavioral characteristics were analyzed as risk factors. Results: Overall, 467,567 participants were included. The incidence density of hemorrhoids was 13.9 and 5.7 per 1000 person-years in the DM and SM, respectively. Hemorrhoids occurred more frequently in men and metropolitan areas in both models. The incidence was highest in the 40s. The incidence rates were highest in the high income, smoking, alcohol and the exercise group of 1–4 times a week in both models. The adjusted hazard ratio (HR) was higher in men and decreased with increasing age. It was higher in the metropolitan area. The high-income level and alcohol consumption were risk factors in the DM and SM, respectively. The HR of the exercise group was higher than that of the non-exercise group in both models. Conclusions: The diagnostic and surgical incidence density was 13.9 and 5.7 per 1000 person-years, respectively. Hemorrhoids occurred most frequently in men in their 40s. The metropolitan area, high income level and alcohol consumption were associated with an increased frequency of hemorrhoids. © 2021","Hemorrhoids; Incidence density; Lifestyle behaviors; Risk factors; Socio-demographic characteristics","data analysis; hemorrhoid; human; incidence; lifestyle; male; public health; retrospective study; risk factor; South Korea; Big Data; Data Analysis; Hemorrhoids; Humans; Incidence; Life Style; Male; National Health Programs; Republic of Korea; Retrospective Studies; Risk Factors",Article,Scopus,2-s2.0-85108804350
"Wang N.","57224899707;","Application of DASH client optimization and artificial intelligence in the management and operation of big data tourism hotels",2022,"Alexandria Engineering Journal","61","1",,"81","90",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108553532&doi=10.1016%2fj.aej.2021.04.080&partnerID=40&md5=b43e9c76fa935b9e4ece527eedd459a7","The hotel uses modern information technology to establish a customer data group, which contains information about the customer's information on space configuration, color design, etc., and uses scientific operation methods to accelerate the rapid, transformation and upgrading of the hotel. The rapid development of information technology represented by the big intelligent mobile cloud has a lot of functions, can meet the needs of different customers and diversified characteristics at the same time, and provides a complete, stable and accurate information integration platform for hotel system management. The main research directions of DASH include client adaptive algorithm design, multi-client fairness and resource utilization issues, client improvement under multiple servers, and server load balancing. The development and application of scientific and technological information has broken through the development path of the hotel management system, providing new development space and opportunities for the improvement of the hotel management system, but at the same time it will also face more new challenges. Therefore, this article focuses on the characteristics of the tourism accommodation management system under the big data environment, and analyzes the development of the hotel management system under the network environment. The purpose is to provide some reference and reference significance. © 2021 Faculty of Engineering, Alexandria University","Artificial intelligence; Big data; DASH client optimization; Hotel operations","Adaptive algorithms; Artificial intelligence; Balancing; Engineering education; Environmental management; Hotels; Information management; Sales; Customer data; Customer information; DASH client optimization; Data groups; Hotel managements; Hotel operation; Management systems; Modern information technologies; Optimisations; Space configuration; Big data",Article,Scopus,2-s2.0-85108553532
"Ren S., Zhang Y., Sakao T., Liu Y., Cai R.","55157931500;8305738300;15127561700;55812026500;57223435717;","An Advanced Operation Mode with Product-Service System Using Lifecycle Big Data and Deep Learning",2022,"International Journal of Precision Engineering and Manufacturing - Green Technology","9","1",,"287","303",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105847721&doi=10.1007%2fs40684-021-00354-3&partnerID=40&md5=28392a525eaf08fef3ae483bb067936a","As a successful business strategy for enhancing environmental sustainability and decreasing the natural resource consumption of societies, the product-service system (PSS) has raised significant interests in the academic and industrial community. However, with the digitisation of the industry and the advancement of multisensory technologies, the PSS providers face many challenges. One major challenge is how the PSS providers can fully capture and efficiently analyse the operation and maintenance big data of different products and different customers in different conditions to obtain insights to improve their production processes, products and services. To address this challenge, a new operation mode and procedural approach are proposed for operation and maintenance of bigger cluster products, when these products are provided as a part of PSS and under exclusive control by the providers. The proposed mode and approach are driven by lifecycle big data of large cluster products and employs deep learning to train the neural networks to identify the fault features, thereby monitoring the products’ health status. This new mode is applied to a real case of a leading CNC machine provider to illustrate its feasibility. Higher accuracy and shortened time for fault prediction are realised, resulting in the provider’s saving of the maintenance and operation cost. © 2021, The Author(s).","Big data; Fault diagnosis; Lifecycle; Product-service system; Production machine; Sharing","Big data; Computer control systems; Deep learning; Maintenance; Sustainable development; Environmental sustainability; Industrial communities; Maintenance and operation; Operation and maintenance; Product-service systems; Products and services; Productservice system (PSS); Resource consumption; Life cycle",Article,Scopus,2-s2.0-85105847721
"Li W., He Q., Luo X., Wang Z.","57192673478;55217854300;36460171200;57219674242;","Assimilating Second-Order Information for Building Non-Negative Latent Factor Analysis-Based Recommenders",2022,"IEEE Transactions on Systems, Man, and Cybernetics: Systems","52","1",,"485","497",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105337506&doi=10.1109%2fTSMC.2020.3002762&partnerID=40&md5=a5ce6e25fe85a4e5ce70269780ae1acc","A non-negative latent factor analysis (NLFA)-based recommender can make precise recommendations by correctly representing the non-negative characteristic of industrial data. It commonly relies on a nonconvex and bilinear optimization process, where the effects of first-order solvers maybe significantly reduced. Higher order solvers like a Newton-type method are expected to make a breakthrough; however, its computation efficiency and scalability are greatly limited due to the numerous parameters involved in a Hessian matrix. To address this issue, this article proposes an approach for assimilating second-order information for building NLFA-based recommenders. The key idea is an inner second-order solver that employs a Hessian-free method for avoiding the highly expensive manipulations of a Hessian matrix. Empirical studies on eight data cases emerging from real industrial applications indicate that the proposed approach outperforms state-of-the-art models in prediction accuracy with affordable computational burden. © 2013 IEEE.","Big data; Hessian-free method; industrial application; non-negative latent factor analysis (NLFA); non-negative model; recommender system; second-order optimization","Big data; Factorization; Matrix algebra; Multivariant analysis; Recommender systems; Hessian matrices; Hessian-free method; Industrial datum; Latent factor analysis; Non negatives; Non-negative latent factor analyse; Non-negative models; Nonconvex optimization; Second order optimization; Second orders; Factor analysis",Article,Scopus,2-s2.0-85105337506
"Yang C., Weng Y., Huang B., Ikbal M.A.","57223184873;57223202961;57223198610;57221518919;","Development and optimization of CAD system based on big data technology",2022,"Computer-Aided Design and Applications","19","S2",,"112","123",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105124894&doi=10.14733%2fcadaps.2022.S2.112-123&partnerID=40&md5=b88c67b01da2a6c8d9fea51044cc540f","The intelligent CAD system, facilities demand analysis and design requirements of system design and realize intelligent design and the key technologies requirement for the process of CAD system. In order to solve a series of problems in the design process of large assemblies and improve the efficiency and quality of the design, this paper has made an in-depth analysis of the characteristics and design techniques of large assemblies, and studied the parametric and modular design technology. The functional units of the large assembly are divided into modules to realize the parametric design and establish the standard model library of the large assembly. On this basis, based on the theoretical basis of recursive algorithm, the automatic update of assembly model, intelligent assembly based on model perception, and automatic generation of engineering drawings and BOM are realized, and a large assembly intelligent CAD system is developed. © 2022 CAD Solutions, LLC.","Intelligent CAD; Modular design; Parametric design","Big data; Quality control; Automatic Generation; Automatic updates; Engineering drawing; In-depth analysis; Intelligent designs; Parametric design; Recursive algorithms; The standard model; Computer aided design",Article,Scopus,2-s2.0-85105124894
"Wu H., Zhao Y.-P., Hui-Jun T.","57218139779;57196238736;57222963688;","A hybrid of fast K-nearest neighbor and improved directed acyclic graph support vector machine for large-scale supersonic inlet flow pattern recognition",2022,"Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering","236","1",,"109","122",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104245369&doi=10.1177%2f09544100211008601&partnerID=40&md5=4aa1eb539e8fbd4b33ef2bfaba93d8db","Inlet flow pattern recognition is one of the most crucial issues and also the foundation of protection control for supersonic air-breathing propulsion systems. This article proposes a hybrid algorithm of fast K-nearest neighbors (F-KNN) and improved directed acyclic graph support vector machine (I-DAGSVM) to solve this issue based on a large amount of experimental data. The basic idea behind the proposed algorithm is combining F-KNN and I-DAGSVM together to reduce the classification error and computational cost when dealing with big data. The proposed algorithm first finds a small set of nearest samples from the training set quickly by F-KNN and then trains a local I-DAGSVM classifier based on these nearest samples. Compared with standard KNN which needs to compare each test sample with the entire training set, F-KNN uses an efficient index-based strategy to quickly find nearest samples, but there also exists misclassification when the number of nearest samples belonging to different classes is the same. To cope with this, I-DAGSVM is adopted, and its tree structure is improved by a measure of class separability to overcome the sequential randomization in classifier generation and to reduce the classification error. In addition, the proposed algorithm compensates for the expensive computational cost of I-DAGSVM because it only needs to train a local classifier based on a small number of samples found by F-KNN instead of all training samples. With all these strategies, the proposed algorithm combines the advantages of both F-KNN and I-DAGSVM and can be applied to the issue of large-scale supersonic inlet flow pattern recognition. The experimental results demonstrate the effectiveness of the proposed algorithm in terms of classification accuracy and test time. © IMechE 2021.","big data; directed acyclic graph support vector machine; flow pattern recognition; k-nearest neighbors; machine learning; Supersonic inlet","Classification (of information); Flow graphs; Flow patterns; Graph algorithms; Inlet flow; Intake systems; Motion compensation; Nearest neighbor search; Pattern recognition systems; Propulsion; Support vector machines; Text processing; Trees (mathematics); Air Breathing Propulsions; Class separability; Classification accuracy; Classification errors; Computational costs; Directed acyclic graph support vector machines; K-nearest neighbors; Protection control; Learning algorithms",Article,Scopus,2-s2.0-85104245369
"Cetindamar D., Shdifat B., Erfani E.","6602901147;57215346624;57217221571;","Understanding Big Data Analytics Capability and Sustainable Supply Chains",2022,"Information Systems Management","39","1",,"19","33",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103537668&doi=10.1080%2f10580530.2021.1900464&partnerID=40&md5=382c97faec29ade1e4b41fcc3ed7e284","This paper presents the knowledge available in the literature regarding big data analytics capability (BDAC) and sustainable supply chain performance (SSCP). A detailed analysis of systematic literature reviews points out the lack of studies bridging these two separate streams of work. The paper puts forward a research agenda for researchers interested in understanding the impact of big data on sustainability. © 2021 Taylor & Francis.","big data; big data analytics capability; dynamic capabilities; supply chain management; Sustainability","Big data; Data Analytics; Supply chains; Research agenda; Sustainable supply chains; Systematic literature review; Advanced Analytics",Article,Scopus,2-s2.0-85103537668
"Daviet R., Nave G., Wind J.","57217238984;36761348200;8454538100;","Genetic Data: Potential Uses and Misuses in Marketing",2022,"Journal of Marketing","86","1",,"7","26",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101046993&doi=10.1177%2f0022242920980767&partnerID=40&md5=ab7a9fbdf59a4601bb271b4a77b77e14","Advances in molecular genetics have led to the exponential growth of the direct-to-consumer genetic testing industry, resulting in the assembly of massive privately owned genetic databases. This article explores the potential impact of this new data type on the field of marketing. Drawing on findings from behavioral genetic research, the authors propose a framework that incorporates genetic influences into existing consumer behavior theory and use it to survey potential marketing uses of genetic data. Applications include business strategies that rely on genetic variants as bases for segmentation and targeting, creative uses that develop consumers’ sense of community and personalization, use of genetically informed study designs to test causal relations, and refinement of consumer theory by uncovering biological mechanisms underlying behavior. The authors further evaluate ethical challenges related to autonomy, privacy, misinformation, and discrimination that are unique to the use of genetic data and are not sufficiently addressed by current regulations. They conclude by proposing an agenda for future research. © American Marketing Association 2021.","behavioral genetics; big data; consumer genomics; DNA data; ethics; segmentation; technology",,Article,Scopus,2-s2.0-85101046993
"Lasaponara R., Abate N., Masini N.","6603298083;57209746103;8931289200;","On the Use of Google Earth Engine and Sentinel Data to Detect 'Lost' Sections of Ancient Roads. The Case of Via Appia",2022,"IEEE Geoscience and Remote Sensing Letters","19",,,"","",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100833048&doi=10.1109%2fLGRS.2021.3054168&partnerID=40&md5=c02bd06f6e32246ec485f524bcb686f6","The currently available tools and services as open and free cloud resources to process big satellite data opened up a new frontier of possibilities and applications including archeological research. These new research opportunities also pose several challenges to be faced, as, for example, the data processing and interpretation. This letter is about the assessment of different methods and data sources to support a visual interpretation of EO imagery. Multitemporal Sentinel 1 and Sentinel 2 data sets have been processed to assess their capability in the detection of buried archeological remains related to some lost sections of the ancient Via Appia road (herein selected as case study). The very subtle and nonpermanent features linked to buried archeological remains can be captured using multitemporal (intra- and inter-year) satellite acquisitions, but this requires strong hardware infrastructures or cloud facilities, today also available as open and free tools as Google Earth Engine (GEE). In this study, a total of 2948 Sentinel 1 and 743 Sentinel 2 images were selected (from February 2017 to August 2020) and processed using GEE to enhance and unveil archeological features. Outputs obtained from both Sentinel 1 and Sentinel 2 have been successfully compared with in situ analysis and high-resolution Google Earth images. © 2004-2012 IEEE.","Big data; Copernicus; Google earth engine (GEE); Remote sensing for archeology; Sentinel 1 (S-1); Sentinel 2 (S-2)","Engines; Image enhancement; Google earths; High resolution; In-situ analysis; Multi-temporal; Research opportunities; Satellite acquisition; Satellite data; Visual interpretation; Data handling; algorithm; road; satellite data; Sentinel; software",Article,Scopus,2-s2.0-85100833048
"Sestino A., De Mauro A.","57218456709;16244425400;","Leveraging Artificial Intelligence in Business: Implications, Applications and Methods",2022,"Technology Analysis and Strategic Management","34","1",,"16","29",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100795345&doi=10.1080%2f09537325.2021.1883583&partnerID=40&md5=fcdb28c39b14a0003bb3733c48fa72d2","The concept of Artificial Intelligence (AI) as a business-disruptive technology has developed in academic and professional literature in a chaotic and unstructured manner. This study aims to provide clarity over the phenomenon of business activation of AI by means of a comprehensive and systematic literature review, aimed at suggesting a clear description of what Artificial Intelligence is today. The study analyses a corpus of 3780 contributions through an original combination of two established machine learning algorithms (LDA and hierarchical clustering). The review produced a structured classification of the various streams of current research and a list of promising emerging trends. Results have shed light on six topics attributable to three different themes, namely Implications, Applications and Methods (IAM model). Our analysis could provide researchers and practitioners with a meaningful overview of the body of knowledge and research agenda, to exploit AI as an effective enabler to drive business value. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","Artificial intelligence; big data; business innovation; business management; marketing; technology management",,Article,Scopus,2-s2.0-85100795345
"Asadianfam S., Shamsi M., Rasouli Kenari A.","57217213973;35230987100;35203890800;","Hadoop Deep Neural Network for offending drivers",2022,"Journal of Ambient Intelligence and Humanized Computing","13","1",,"659","671",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100502259&doi=10.1007%2fs12652-021-02924-4&partnerID=40&md5=9d04ef669430cb5d14623928eabdb33a","Deep learning is recently regarded as the closest artificial intelligence model to human brain. It is about learning multiple levels of representation and abstraction that help to make sense of data such as images, sound, and text. Based on MapReduce framework and Hadoop distributed file system, this paper proposes a distributed approach for detect offending drivers and training the Deep Neural Network models such as Convolutional Neural Network (CNN) and Long Short Term Memory network (LSTM). Its implementation and performance are evaluated on Big Data platform Hadoop. The intelligence growing process of human brain requires learning from Big Data. The main contribution of this paper is that it is implemented to analyze traffic big data and to detect offending drivers in Hadoop by CNN with Support Vector Machine (SVM) and LSTM. The efficiency of the proposed method is computed by using experimental and theoretical analysis. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.","Convolutional Neural Network; Deep learning; Deep Neural Network; Hadoop; Long Short Term Memory network; Object detection","Big data; Brain; Convolutional neural networks; Deep learning; Deep neural networks; File organization; Support vector machines; Data platform; Distributed approaches; Growing process; Hadoop distributed file systems; Mapreduce frameworks; Multiple levels; Neural network model; Short term memory; Long short-term memory",Article,Scopus,2-s2.0-85100502259
"Singh N.","24784773300;","Developing Business Risk Resilience through Risk Management Infrastructure: The Moderating Role of Big Data Analytics",2022,"Information Systems Management","39","1",,"34","52",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096128942&doi=10.1080%2f10580530.2020.1833386&partnerID=40&md5=46200cbcc762f618f2d4111dc4dd1fbd","This article evaluates how firms are able to enhance the effectiveness of their Risk Management Infrastructure (RMI) by using Big Data Analytics (BDA) to improve their risk resilience capability. A theoretical model is developed and the corresponding hypothesis is tested through a survey instrument and quantitative analysis. The results show that if firms adopt BDA capabilities then it positively moderates the impact of RMI on risk resilience, improving the effectiveness of a risk management infrastructure. © 2020 Taylor & Francis.","Big Data Analytics; Business Risk Resilience; IS strategy; IT Infrastructure capability; Risk Management Infrastructure","Advanced Analytics; Big data; Data Analytics; Business risks; Management infrastructure; Survey instruments; Theoretical modeling; Risk management",Article,Scopus,2-s2.0-85096128942
"Loi M., Hauser C., Christen M.","57188780053;55016260800;7005443809;","Highway to (Digital) Surveillance: When Are Clients Coerced to Share Their Data with Insurers?",2022,"Journal of Business Ethics","175","1",,"7","19",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095613828&doi=10.1007%2fs10551-020-04668-1&partnerID=40&md5=0abadf566560494066d57197754fc7ee","Clients may feel trapped into sharing their private digital data with insurance companies to get a desired insurance product or premium. However, private insurance must collect some data to offer products and premiums appropriate to the client’s level of risk. This situation creates tension between the value of privacy and common insurance business practice. We argue for three main claims: first, coercion to share private data with insurers is pro tanto wrong because it violates the autonomous choice of a privacy-valuing client. Second, we maintain that irrespective of being coerced, the choice of accepting digital surveillance by insurers makes it harder for the client to protect his or her autonomy (and to act spontaneously and authentically). The violation of autonomy also makes coercing customers into digital surveillance pro tanto morally wrong. Third, having identified an economically plausible process involving no direct coercion by insurers, leading to the adoption of digital surveillance, we argue that such an outcome generates further threats against autonomy. This threat provides individuals with a pro tanto reason to prevent this process. We highlight the freedom dilemma faced by regulators who aim to prevent this outcome by constraining market freedoms and argue for the need for further moral and empirical research on this question. © 2020, The Author(s).","Big data; Coercion; Data sharing; Insurance; Threats",,Article,Scopus,2-s2.0-85095613828
"Ogbuke N.J., Yusuf Y.Y., Dharma K., Mercangoz B.A.","57209234585;6701539345;57218850413;57192228368;","Big data supply chain analytics: ethical, privacy and security challenges posed to business, industries and society",2022,"Production Planning and Control","33","2-3",,"123","137",,10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090430565&doi=10.1080%2f09537287.2020.1810764&partnerID=40&md5=1e924c8c15309b7b85016907f30628d3","This study conducted a comprehensive review of big data supply chain analytics (BDSCA). The paper explored the application of big data in supply chain management and its benefits for organisations and society. The paper also examined the ethical, security, privacy and operational challenges of big data techniques, as well as the potential reputational damages to businesses. The review outlined four principal facets, namely: Big data analytics, applications, ethics and privacy issues, and how organizations employed this emerging tool to anticipate and even predict the future and direct their operations. These principle facets are built across the multiple levels and unique conceptual standpoints indicated by 7 themes and 14 sub-themes. These themes were generated based on 120 articles (2005−2020) drawn mainly from leading academic journals. Overall, there is a considerable consensus across current literature that big data analytics extend far beyond just reinventing the supply chain. It has the potential to support more responsive next-generation of global companies who are operating in an increasingly challenging and uncertain environment. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","Big data; business analytics; ethical issues; Industry 4.0; supply chain management","Big data; Data Analytics; Data privacy; Philosophical aspects; Supply chain management; Academic journal; Emerging tools; Multiple levels; Operational challenges; Privacy and security; Privacy issue; Reputational damage; Uncertain environments; Advanced Analytics",Article,Scopus,2-s2.0-85090430565
"Hughes L., Dwivedi Y.K., Rana N.P., Williams M.D., Raghavan V.","57205405210;35239818900;50262828700;7410004434;57190980898;","Perspectives on the future of manufacturing within the Industry 4.0 era",2022,"Production Planning and Control","33","2-3",,"138","158",,12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089981221&doi=10.1080%2f09537287.2020.1810762&partnerID=40&md5=94178f5c8b6a5a6258742e3faa1dc2a7","The technological choices facing the manufacturing industry are vast and complex as the industry contemplates the increasing levels of digitization and automation in readiness for the modern competitive age. These changes broadly categorized as Industry 4.0, offer significant transformation challenges and opportunities, impacting a multitude of operational aspects of manufacturing organizations. As manufacturers seek to deliver increased levels of productivity and adaptation by innovating many aspects of their business and operational processes, significant challenges and barriers remain. The roadmap towards Industry 4.0 is complex and multifaceted, as manufacturers seek to transition towards new and emerging technologies, whilst retaining operational effectiveness and a sustainability focus. This study approaches many of these significant themes by presenting a critical evaluation of the core topics impacting the next generation of manufacturers, challenges and key barriers to implementation. These factors are further evaluated via the presentation of a new Industry 4.0 framework and alignment of I4.0 themes with the UN Sustainability Goals. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","artificial intelligence (AI); big data; Industry 4.0; internet of things (IoT); sustainability","Sustainable development; Critical evaluation; Emerging technologies; Manufacturing industries; Manufacturing organizations; New industry; Operational aspects; Operational effectiveness; Operational process; Industry 4.0",Article,Scopus,2-s2.0-85089981221
"Wiech M., Boffelli A., Elbe C., Carminati P., Friedli T., Kalchschmidt M.","57201408770;57006817800;36245091900;57218672912;6508328636;6507679023;","Implementation of big data analytics and Manufacturing Execution Systems: an empirical analysis in German-speaking countries",2022,"Production Planning and Control","33","2-3",,"261","276",,4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089980336&doi=10.1080%2f09537287.2020.1810766&partnerID=40&md5=eb6a848ff043819c90b4d32daf75fc55","Many firms have started Industry 4.0 (I4.0) initiatives in recent years, without having a sound understanding of the effects generated by the technologies introduction. This research provides indications of what to expect from the implementation of two key technologies for I4.0: big data analytics and manufacturing execution systems. The study explores the relationships between these technologies’ implementation and a set of performance effects. Additionally, it analyses the influence of the organisational structure. A set of hypotheses derived from literature builds the basis for the quantitative analysis of an industry survey with 116 participants from German-speaking countries. The results show that these technologies have distinct, partially unexpected, performance effects. Furthermore, this research provides evidence that the organisational structure of technology implementation plays no significant role in the attainment of higher technology implementation levels. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","firm performance; Industry 4.0; internal enablers; organisational structure; technology implementation","Big data; Data Analytics; Industrial research; Manufacture; Surveys; Empirical analysis; Industry surveys; Key technologies; Manufacturing Execution System; Organisational structure; Performance effect; Technology implementation; Advanced Analytics",Article,Scopus,2-s2.0-85089980336
"Barlette Y., Baillette P.","24777990000;55508913000;","Big data analytics in turbulent contexts: towards organizational change for enhanced agility",2022,"Production Planning and Control","33","2-3",,"105","122",,11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089785597&doi=10.1080%2f09537287.2020.1810755&partnerID=40&md5=733093e0adedc09e7c234b9de249031a","This conceptual paper addresses the capabilities offered by big data analytics (BDA), critical in turbulent environments, in the context of the Industry 4.0 revolution. BDA enhances organizational agility, as it is highly valuable for the identification of opportunities and threats (i.e. the sensing process of agility); however, with the exception of aiding decision-making tools, seizing its benefits for reacting (i.e. responding) is more difficult without organizational adaptation. Hence, the changes required to exploit the full capabilities offered by BDA, particularly at the organizational level, are considerable. From a theoretical perspective, through a narrative literature review, this paper highlights several possibilities of organizational changes for enhancing a company’s agility and leveraging BDA to achieve enhanced performance in turbulent contexts. The findings are then related to three organizational theories. This work ends in a conceptual model and several research paths for future studies. From a managerial perspective, this paper highlights, through several examples, the importance of top management’s role in achieving the necessary organizational changes to enhance the companies’ responding capabilities. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","agility; Big data analytics; Industry 4.0; organization; turbulence","Big data; Data Analytics; Decision making; Decision making tool; Literature reviews; Organizational adaptation; Organizational agility; Organizational change; Organizational levels; Organizational theory; Turbulent environments; Advanced Analytics",Article,Scopus,2-s2.0-85089785597
"Yu J., Couldry N.","57202196473;15019158400;","Education as a domain of natural data extraction: analysing corporate discourse about educational tracking",2022,"Information Communication and Society","25","1",,"127","144",,5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085765414&doi=10.1080%2f1369118X.2020.1764604&partnerID=40&md5=80e35efbe25024c627d31ff42582cd16","Digital platforms and learning analytics are becoming increasingly widespread in the education sector: commercial corporations argue their benefits for teaching and learning, thereby endorsing the continuous automated collection and processing of student data for measurement, assessment, management, and identity formation. Largely missing in these discourses, however, are the potential costs of datafication for pupils’ and teachers’ agency and the meaning of education itself. This article explores the general discursive framing by which these surveillant practices in education have come to seem natural. Through a study of commercial suppliers of educational platforms, we show how the prevailing vision of datafication in their discourses categorises software systems, not teachers, as central to education, reimagining space, time, and agency within educational processes around the organisation of data systems and the demands of commercial data production. Not only does this legitimate the new connective environment of dataveillance (that is, surveillance through data processing), but it also naturalises a wider normative environment in which teachers and students are assigned new roles and responsibilities. In the process, the panoptic possibilities of ubiquitous commercial access to personal educational data are presented as part of a virtuous circle of knowledge production and even training for good citizenship. This broader rethinking of education through surveillance must itself be critiqued. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","Big data; data collection; datafication; dataveillance; surveillance",,Article,Scopus,2-s2.0-85085765414
"Liu Z., Pu J.","54928123600;57212315564;","Analysis and research on intelligent manufacturing medical product design and intelligent hospital system dynamics based on machine learning under big data",2022,"Enterprise Information Systems","16","2",,"193","207",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076442550&doi=10.1080%2f17517575.2019.1701713&partnerID=40&md5=cce6df1890ca34cbff48d7c1b0d228c6","In order to promote the application of machine learning in intelligent manufacturing, especially in medical products and intelligent hospitals, based on the theoretical research of machine learning under big data, intelligent medical disease diagnosis and classification products are designed. Two algorithms, MLP and SVM, are used to compare the accuracy, recall and F1. It is concluded that the effect of medical diagnosis classification of both methods is maintained above and below 90%. At the same time, the research proves that the corresponding accuracy can be improved only when the fuzzy matching is suitable for more positive examples. © 2019 Informa UK Limited, trading as Taylor & Francis Group.","intelligent hospital; intelligent manufacturing; intelligent medical product design; Machine learning; system dynamics","Big data; Computer aided diagnosis; Hospitals; Industrial research; Manufacture; Product design; Support vector machines; System theory; Disease diagnosis; Fuzzy matching; Intelligent Manufacturing; Medical products; On-machines; Positive examples; System Dynamics; Theoretical research; Learning systems",Article,Scopus,2-s2.0-85076442550
"Chen M.","57211322885;","The influence of big data analysis of intelligent manufacturing under machine learning on start-ups enterprise",2022,"Enterprise Information Systems","16","2",,"347","362",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075426617&doi=10.1080%2f17517575.2019.1694180&partnerID=40&md5=53ff36462d4889dc916a62eee988e2f7","In order to solve the problem that enterprises waste resources and make wrong decision caused by lack of comprehensive grasp of production, so as to realise intelligent manufacturing under the background of big data and improve the level of intelligent manufacturing of the newly created enterprises, machine learning method is used to make reasonable predictions on the customer’s favourite products and combinations. The research results show that the diagnosis and introspection are predicted through the manufacturing data collection assessment to realise intelligent manufacturing system, optimisation of intelligent manufacturing decision space, provide visual guidance for intelligent manufacturing. © 2019 Informa UK Limited, trading as Taylor & Francis Group.","big data; data prediction; Machine learning; new venture","Big data; Engineering education; Industrial research; Learning systems; Machine learning; Data collection; Data prediction; Intelligent Manufacturing; Intelligent manufacturing system; Machine learning methods; New ventures; Research results; Visual guidance; Manufacture",Article,Scopus,2-s2.0-85075426617
"Wu Y., Liu L., Pu C., Cao W., Sahin S., Wei W., Zhang Q.","57203225094;55628583368;7007061293;57191493998;57191893369;57209882960;56102954300;","A Comparative Measurement Study of Deep Learning as a Service Framework",2022,"IEEE Transactions on Services Computing","15","1",,"551","566",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074821487&doi=10.1109%2fTSC.2019.2928551&partnerID=40&md5=df3f826b7a3bd5ed85f876746e36c90c","Big data powered Deep Learning (DL) and its applications have blossomed in recent years, fueled by three technological trends: A large amount of digitized data openly accessible, a growing number of DL software frameworks in open source and commercial markets, and a selection of affordable parallel computing hardware devices. However, no single DL framework, to date, dominates in terms of performance and accuracy even for baseline classification tasks on standard datasets, making the selection of a DL framework an overwhelming task. This paper takes a holistic approach to conduct empirical comparison and analysis of four representative DL frameworks with three unique contributions. First, given a selection of CPU-GPU configurations, we show that for a specific DL framework, different configurations of its hyper-parameters may have a significant impact on both performance and accuracy of DL applications. Second, to the best of our knowledge, this study is the first to identify the opportunities for improving the training time performance and the accuracy of DL frameworks by configuring parallel computing libraries and tuning individual and multiple hyper-parameters. Third, we also conduct a comparative measurement study on the resource consumption patterns of four DL frameworks and their performance and accuracy implications, including CPU and memory usage, and their correlations to varying settings of hyper-parameters under different configuration combinations of hardware, parallel computing libraries. We argue that this measurement study provides in-depth empirical comparison and analysis of four representative DL frameworks, and offers practical guidance for service providers to deploying and delivering DL as a Service (DLaaS) and for application developers and DLaaS consumers to select the right DL frameworks for the right DL workloads. © 2008-2012 IEEE.","accuracy; big data; Deep learning as a service; deep neural networks","Big data; Computer hardware; Deep neural networks; Job analysis; Libraries; Parameter estimation; Personnel training; Accuracy; Application developers; Comparative measurements; Empirical - comparisons; Parallel processing; Resource consumption; Runtimes; Task analysis; Deep learning",Article,Scopus,2-s2.0-85074821487
"Oo M.C.M., Thein T.","57210957465;24725450800;","An efficient predictive analytics system for high dimensional big data",2022,"Journal of King Saud University - Computer and Information Sciences","34","1",,"1521","1532",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072009637&doi=10.1016%2fj.jksuci.2019.09.001&partnerID=40&md5=4bb8e85af56ca052c60335e0cc088397","The excessive growth of high dimensional big data has resulted in a greater challenge for data scientists to efficiently obtain valuable knowledge from these data. Traditional data mining techniques are not fit to process big data. Predictive analytics has grown in prominence alongside the emergence of big data. In this paper, an efficient predictive analytics system for high dimensional big data is proposed by enhancing scalable random forest (SRF) algorithm on the Apache Spark platform. SRF is enhanced by optimizing the hyperparameters and prediction performance is improved by reducing the dimensions. The effectiveness of the proposed system is examined on five real-world datasets. Experimental results demonstrated that the proposed system achieves the highly competitive performance compared with RF algorithm implemented by Spark MLlib. © 2019 King Saud University","Big data; Dimension reduction; High dimensionality; Parameter optimization; Predictive analytics; Scalable random forest",,Article,Scopus,2-s2.0-85072009637
"Salmaso L., Pegoraro L., Giancristofaro R.A., Ceccato R., Bianchi A., Restello S., Scarabottolo D.","55911856100;57221993402;6506284855;57220113595;57220615738;6507950232;57210947450;","Design of experiments and machine learning to improve robustness of predictive maintenance with application to a real case study",2022,"Communications in Statistics: Simulation and Computation","51","2",,"570","582",,5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071982567&doi=10.1080%2f03610918.2019.1656740&partnerID=40&md5=eb2fd4c8c015b4b08d01d3949f8b8555","When deploying predictive analytics in a Big Data context, some concerns may arise regarding the validity of the results obtained. The reason for this is linked to flaws which are intrinsic to the nature of the Big Data Analytics methods themselves. In this article a new approach is proposed with the aim of mitigating new problems which arise. This novel method consists of a two-step workflow in which a Design of Experiments (DOE) study is conducted prior to the usual Big Data Analytics and machine learning modeling phase. The advantages of the new approach are presented and an industrial application of the method in predictive maintenance is described in detail. © 2019 Taylor & Francis Group, LLC.","Big Data; Design of Experiments (DOE); Machine learning; Predictive maintenance","Big data; Data Analytics; Design of experiments; Learning systems; Machine learning; Predictive maintenance; Data contexts; Machine learning models; New approaches; Real case; Predictive analytics",Article,Scopus,2-s2.0-85071982567
"Luna J.M., Fardoun H.M., Padillo F., Romero C., Ventura S.","36170791300;26435262800;57188873275;55865135900;8846948400;","Subgroup discovery in MOOCs: a big data application for describing different types of learners",2022,"Interactive Learning Environments","30","1",,"127","145",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070218665&doi=10.1080%2f10494820.2019.1643742&partnerID=40&md5=21ffeee15a0d7b22d8896c535d4a64c5","The aim of this paper is to categorize and describe different types of learners in massive open online courses (MOOCs) by means of a subgroup discovery (SD) approach based on MapReduce. The proposed SD approach, which is an extension of the well-known FP-Growth algorithm, considers emerging parallel methodologies like MapReduce to be able to cope with extremely large datasets. As an additional feature, the proposal includes a threshold value to denote the number of courses that each discovered rule should satisfy. A post-processing step is also included so redundant subgroups can be removed. The experimental stage is carried out by considering de-identified data from the first year of 16 MITx and HarvardX courses on the edX platform. Experimental results demonstrate that the proposed MapReduce approach outperforms traditional sequential SD approaches, achieving a runtime that is almost constant for different courses. Additionally, thanks to the final post-processing step, only interesting and not-redundant rules are discovered, hence reducing the number of subgroups in one or two orders of magnitude. Finally, the discovered subgroups are easily used by courses' instructors not only for descriptive purposes but also for additional tasks such as recommendation or personalization. © 2019 Informa UK Limited, trading as Taylor & Francis Group.","big data; categorizing students; MOOCs; Subgroup discovery; types of learners",,Article,Scopus,2-s2.0-85070218665
"JayaLakshmi A.N.M., Krishna Kishore K.V.","57204351515;36086160000;","Performance evaluation of DNN with other machine learning techniques in a cluster using Apache Spark and MLlib",2022,"Journal of King Saud University - Computer and Information Sciences","34","1",,"1311","1319",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055258292&doi=10.1016%2fj.jksuci.2018.09.022&partnerID=40&md5=fa545c16072f8b4e994d4587ca33fa15","Sentiment analysis on large data has become challenging due to the diversity, and nature of data. Advancements in the internet, along with large data availability have obviated the traditional limitations to distributed computing. The objective of this work is to carry out sentiment analysis on Apache Spark distributed Framework to speed up computations and enhance machine performance in diverse environments. The analysis, such as polarity identification, subjective analysis and email spam etc., are carried on various text datasets. After pre-processing, Term Frequency-Inverse Document Frequency (TF-IDF) and unsupervised Spark-Latent Dirichlet Allocation (LDA) clustering algorithms are used for feature extraction and selection to improve the accuracy. Deep Neural Networks (DNN), Support Vector Machines (SVM), Tree ensemble classifiers are used to evaluate the performance of the framework on single node and cluster environments. Finally, the proposed work aims at building an approach for enhancing machine performance, more in terms of runtime over accuracy. © 2018","Apache Spark framework; Big data; Classification; Distributed computing; Sentiment analysis; Text mining",,Article,Scopus,2-s2.0-85055258292
"Francis D.P., Raimond K.","57195680806;24776550700;","A practical streaming approximate matrix multiplication algorithm",2022,"Journal of King Saud University - Computer and Information Sciences","34","1",,"1455","1465",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053697670&doi=10.1016%2fj.jksuci.2018.09.010&partnerID=40&md5=6c9efcc058324b7cc14e2516f0f5f7b4","Approximate Matrix Multiplication (AMM) has emerged as a useful and computationally inexpensive substitute for actual multiplication of large matrices. Randomized as well as deterministic solutions to AMM were provided in the past. The latest work provides a deterministic algorithm that solves AMM more accurately than the other works. It is a streaming algorithm that is both fast and accurate. But, it is less robust to noise and is also liable to have less than optimal performance in the presence of concept drift in the input matrices. We propose an algorithm that is more accurate, robust to noise, invariant to concept drift in the data, while having almost the same running time as the state-of-the-art algorithm. We also prove that theoretical guarantees exist for the proposed algorithm. An empirical performance improvement of up to 90% is obtained over the previous algorithm. We also propose a general framework for parallelizing the proposed algorithm. The two parallelized versions of the algorithm achieve up to 1.9x and 3.6x speedups over the original version of the proposed algorithm. © 2018","Approximate matrix multiply; Big data; Frequent directions; Matrix sketching; Parallelism; Streaming",,Article,Scopus,2-s2.0-85053697670
