Authors,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,Link,Abstract,Author Keywords,Index Keywords,Document Type,Source,EID
"De Almeida M.O., Montezuma T., De Oliveira Júnior H.A., Ferri C.P.","57204797742;36999331700;56642805600;7103090584;","Opportunities to improve reporting of rapid response in health technology assessment",2022,"International Journal of Technology Assessment in Health Care","38","1","e4","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122409775&doi=10.1017%2fS0266462321000635&partnerID=40&md5=b06d8b04f077b800fd01377fa53bf968","Introduction Mini health technology assessment (HTA) reports have been used to support policy makers and health systems by providing a timely summary of scientific evidence. The objective of this meta-epidemiologic study was to evaluate the quality of reporting of mini-HTA reports published in Brazil. Methods An electronic search for all mini-HTA reports published between 2014 and March 2019 was conducted in the SISREBRATS and CONITEC databases. The study selection and data extraction were performed by two independent assessors. The following data were extracted: bibliographic data; research question; characteristics of the population, health technologies and outcomes assessed; eligibility criteria; information about searches and study selection; risk of bias assessment; quality of evidence assessment; synthesis of results; and recommendation about the technology evaluated. A descriptive analysis was used to summarize the information retrieved from all the included mini-HTA reports. Results We included 103 mini-HTA reports, the great majority of which (92.3 percent) focused on the coverage of the technologies in the healthcare system, with more than 60 percent being about drugs. Only five mini-HTA reports (4.8 percent) gave reasons for the choice of outcomes, and fifteen (14.5 percent) discriminated between primary and secondary outcomes. All mini-HTAs reported the databases searched and 99 percent of them reported using Medline. Sixty percent of the mini-HTA reported assessing the risk of bias, and 52 percent reported assessing the quality of evidence. Conclusion The quality of reporting of the mini-HTA reports performed in Brazil is insufficient and needs to be improved to guarantee transparency and replicability. © The Author(s), 2021. Published by Cambridge University Press.","Health technology assessment; Methodological quality; Rapid response","Data mining; Health; Health risks; Population statistics; Risk assessment; Search engines; Bibliographic data; Data extraction; Epidemiologic-study; Health systems; Health technology assessments; Methodological quality; Policy makers; Rapid response; Scientific evidence; Support policy; Quality control; Article; biomedical technology assessment; Brazil; data base; health care system; Medline; outcome assessment; public reporting (health care); rapid response team; risk assessment",Article,Scopus,2-s2.0-85122409775
"Jiang Z., Huynh H.N.","57425648200;36876230500;","Unveiling music genre structure through common-interest communities",2022,"Social Network Analysis and Mining","12","1","35","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124987262&doi=10.1007%2fs13278-022-00863-2&partnerID=40&md5=d7da924ac9db4f4d3256f23bc2a89467","Using a dataset of more than 90,000 metal music reviews written by over 9000 users in a period of 15 years, we analyse the genre structure of metal music with the aid of review text information. We model the relationships between genres using a user-oriented network, based on the written reviews. We then perform community detection and employ a network “averaging” method to obtain stable genre clusters, in order to analyse the structures of clusters both locally within each cluster and globally over the entire network. In addition to identifying the clusters, we use Dependency Parsing and modified Term Frequency–Inverse Document Frequency to extract significant and unique features of each cluster. These structures and review text information can allow us to understand how music audience (fans) perceive similar and different genres, and also assist in classifying different genres which share common-interest user communities, offering a more objective way in grouping music genres. Furthermore, the classification can also help recommendation engines provide more targeted suggestions of music, and potentially help musicians to select genre labels for their music, and design music to better cater to preferences of their audiences based on previous reviews. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature.","Community detection; Complex networks; Metal music; Music genre; Social networks; Text mining","Classification (of information); Complex networks; Data mining; Music; Social networking (online); Text processing; Averaging method; Common interests; Community detection; Interest communities; Metal music; Music genre; Network-based; Social network; Text information; User oriented; Population dynamics",Article,Scopus,2-s2.0-85124987262
"Sánchez-de-Madariaga R., Martinez-Romo J., Escribano J.M.C., Araujo L.","24725585600;24476245900;57429958700;57197210585;","Semi-supervised incremental learning with few examples for discovering medical association rules",2022,"BMC Medical Informatics and Decision Making","22","1","20","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123612698&doi=10.1186%2fs12911-022-01755-3&partnerID=40&md5=8342574681206edee1be283db96839b6","Background: Association Rules are one of the main ways to represent structural patterns underlying raw data. They represent dependencies between sets of observations contained in the data. The associations established by these rules are very useful in the medical domain, for example in the predictive health field. Classic algorithms for association rule mining give rise to huge amounts of possible rules that should be filtered in order to select those most likely to be true. Most of the proposed techniques for these tasks are unsupervised. However, the accuracy provided by unsupervised systems is limited. Conversely, resorting to annotated data for training supervised systems is expensive and time-consuming. The purpose of this research is to design a new semi-supervised algorithm that performs like supervised algorithms but uses an affordable amount of training data. Methods: In this work we propose a new semi-supervised data mining model that combines unsupervised techniques (Fisher’s exact test) with limited supervision. Starting with a small seed of annotated data, the model improves results (F-measure) obtained, using a fully supervised system (standard supervised ML algorithms). The idea is based on utilising the agreement between the predictions of the supervised system and those of the unsupervised techniques in a series of iterative steps. Results: The new semi-supervised ML algorithm improves the results of supervised algorithms computed using the F-measure in the task of mining medical association rules, but training with an affordable amount of manually annotated data. Conclusions: Using a small amount of annotated data (which is easily achievable) leads to results similar to those of a supervised system. The proposal may be an important step for the practical development of techniques for mining association rules and generating new valuable scientific medical knowledge. © 2022, The Author(s).","Association rules discovery; Machine learning; Medical records; Semi-supervised approach","algorithm; article; data mining; learning; machine learning; medical record; medical society; mining; plant seed; prediction",Article,Scopus,2-s2.0-85123612698
"Yang T., Chen Y., Xu J., Li J., Liu H., Liu N.","57428760500;57429995400;57429291000;57429643900;56070562800;57266232300;","Bioinformatics screening the novel and promising targets of curcumin in hepatocellular carcinoma chemotherapy and prognosis",2022,"BMC Complementary Medicine and Therapies","22","1","21","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123598106&doi=10.1186%2fs12906-021-03487-9&partnerID=40&md5=e4d9b570e14d12667197148fb9d76e64","Background: The aim of present study was to screen the novel and promising targets of curcumin in hepatocellular carcinoma diagnosis and chemotherapy. Methods: Potential targets of curcumin were screened from SwissTargetPrediction, ParmMapper and drugbank databases. Potential aberrant genes of hepatocellular carcinoma were screened from Genecards databases. Fifty paired hepatocellular carcinoma patients’ gene expression profiles from the GEO database were used to test potential targets of curcumin. Besides, GO analysis, KEGG pathway enrichment analysis and PPI network construction were used to explore the underlying mechanism of candidate hub genes. ROC analysis and Kaplan-Meier analysis were used to evaluate the diagnostic and prognostic value of candidate hub genes, respectively. Real-time PCR was used to verify the results of bioinformatics analysis. Results: Bioinformatics analysis results suggested that AURKA, CDK1, CCNB1, TOP2A, CYP2B6, CYP2C9, and CYP3A4 genes served as candidate hub genes. AURKA, CDK1, CCNB1 and TOP2A were significantly upregulated and correlated with poor prognosis in hepatocellular carcinoma, AUC values of which were 95.7, 96.9, 98.1 and 96.1% respectively. There was not significant correlation between the expression of CYP2B6 and prognosis of hepatocellular carcinoma, while CYP2C9 and CYP3A4 genes were significantly downregulated and correlated with poor prognosis in hepatocellular carcinoma. AUC values of CYP2B6, CYP2C9, and CYP3A4 were 96.0, 97.0 and 88.0% respectively. In vitro, we further confirmed that curcumin significantly downregulated the expression of AURKA, CDK1, and TOP2A genes, while significantly upregulated the expression of CYP2B6, CYP2C9, and CYP3A4 genes. Conclusions: Our results provided a novel panel of AURKA, CDK1, TOP2A, CYP2C9, and CYP3A4 candidate genes for curcumin related chemotherapy of hepatocellular carcinoma. © 2022, The Author(s).","Bioinformatics analysis; Chemotherapy; Curcumin; Hepatocellular carcinoma; Prognosis","aurora A kinase; curcumin; cyclin B1; cyclin dependent kinase 1; cytochrome P450; cytochrome P450 2B6; cytochrome P450 2C9; cytochrome P450 3A4; dimethyl sulfoxide; glyceraldehyde 3 phosphate dehydrogenase; penicillin derivative; streptomycin; transcription factor FOXO; antineoplastic agent; curcumin; area under the curve; Article; bioinformatics; cancer diagnosis; cancer prognosis; cell culture; cell viability; cell viability assay; chemotherapy; controlled study; differential gene expression; fetal bovine serum; gene; gene expression; gene expression profiling; gene ontology; genetic profile; genetic susceptibility; Hep-G2 cell line; human; human cell; in vitro study; Kaplan Meier method; KEGG; leukemia; liver cell carcinoma; mRNA expression level; nonhuman; p53 signaling; pancreas cancer; pathway enrichment analysis; principal component analysis; protein expression; protein phosphorylation; protein protein interaction; real time polymerase chain reaction; receiver operating characteristic; RNA isolation; signal transduction; upregulation; biology; data mining; down regulation; gene expression regulation; liver cell carcinoma; liver tumor; mortality; phytotherapy; predictive value; protein analysis; Antineoplastic Agents; Carcinoma, Hepatocellular; Computational Biology; Curcumin; Data Mining; Down-Regulation; Gene Expression Regulation, Neoplastic; Humans; Liver Neoplasms; Phytotherapy; Predictive Value of Tests; Protein Interaction Maps",Article,Scopus,2-s2.0-85123598106
"Müller B., Castro L.J., Rebholz-Schuhmann D.","57214663350;57224314885;6507852707;","Ontology-based identification and prioritization of candidate drugs for epilepsy from literature",2022,"Journal of Biomedical Semantics","13","1","3","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123575315&doi=10.1186%2fs13326-021-00258-w&partnerID=40&md5=953b77a8699b3d0fafe5cd49cf4d52e0","Background: Drug repurposing can improve the return of investment as it finds new uses for existing drugs. Literature-based analyses exploit factual knowledge on drugs and diseases, e.g. from databases, and combine it with information from scholarly publications. Here we report the use of the Open Discovery Process on scientific literature to identify non-explicit ties between a disease, namely epilepsy, and known drugs, making full use of available epilepsy-specific ontologies. Results: We identified characteristics of epilepsy-specific ontologies to create subsets of documents from the literature; from these subsets we generated ranked lists of co-occurring neurological drug names with varying specificity. From these ranked lists, we observed a high intersection regarding reference lists of pharmaceutical compounds recommended for the treatment of epilepsy. Furthermore, we performed a drug set enrichment analysis, i.e. a novel scoring function using an adaptive tuning parameter and comparing top-k ranked lists taking into account the varying length and the current position in the list. We also provide an overview of the pharmaceutical space in the context of epilepsy, including a final combined ranked list of more than 70 drug names. Conclusions: Biomedical ontologies are a rich resource that can be combined with text mining for the identification of drug names for drug repurposing in the domain of epilepsy. The ranking of the drug names related to epilepsy provides benefits to patients and to researchers as it enables a quick evaluation of statistical evidence hidden in the scientific literature, useful to validate approaches in the drug discovery process. © 2022, The Author(s).","Drug discovery; Drug repurposing; Enrichment analysis; Epilepsy; Information extraction; Knowledge discovery; Ontology; Open discovery process; Text mining; Top-k","drug; biological ontology; data mining; drug repositioning; epilepsy; human; Biological Ontologies; Data Mining; Drug Repositioning; Epilepsy; Humans; Pharmaceutical Preparations",Article,Scopus,2-s2.0-85123575315
"Wang W., Jiang X., Tian S., Liu P., Dang D., Su Y., Lookman T., Xie J.","57419420500;57189508121;57420467600;57216910377;57420467700;13408937200;7003587333;7402994953;","Automated pipeline for superalloy data by text mining",2022,"npj Computational Materials","8","1","9","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123212834&doi=10.1038%2fs41524-021-00687-2&partnerID=40&md5=dded4be4ba764e6011738c84e5daab7f","Data provides a foundation for machine learning, which has accelerated data-driven materials design. The scientific literature contains a large amount of high-quality, reliable data, and automatically extracting data from the literature continues to be a challenge. We propose a natural language processing pipeline to capture both chemical composition and property data that allows analysis and prediction of superalloys. Within 3 h, 2531 records with both composition and property are extracted from 14,425 articles, covering γ′ solvus temperature, density, solidus, and liquidus temperatures. A data-driven model for γ′ solvus temperature is built to predict unexplored Co-based superalloys with high γ′ solvus temperatures within a relative error of 0.81%. We test the predictions via synthesis and characterization of three alloys. A web-based toolkit as an online open-source platform is provided and expected to serve as the basis for a general method to search for targeted materials using data extracted from the literature. © 2022, The Author(s).",,"Chemical analysis; Cobalt alloys; Data mining; Learning algorithms; Natural language processing systems; Pipelines; Superalloys; Chemical composition data; Data driven; High quality; Large amounts; Materials design; Property; Property data; Scientific literature; Text-mining; γ'solvus temperature; Forecasting",Article,Scopus,2-s2.0-85123212834
"Bayrak T.","13805364800;","A comparative analysis of the world’s constitutions: a text mining approach",2022,"Social Network Analysis and Mining","12","1","26","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122925008&doi=10.1007%2fs13278-022-00857-0&partnerID=40&md5=b626084e461e6ee1a63aeec3397acca9","A constitution is the foundation for a government in almost every society around the world. Analyzing and studying constitutions of different countries may allow one to understand better how various countries form the basic structure of their governments, the legal and cultural aspects of their people, the responsibilities of key institutions, and the most basic rights of the people. In this study, we performed text mining to make a comparison between the US constitution and the constitutions of various countries located in four different geographic regions of Europe, Asia, the Middle East, and Africa with respect to how they define and outline the liberties and rights of their citizens and the relationship between the key institutions of their governments. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature.","Constitution analysis; Text mining; Textual data","Basic structure; Comparative analyzes; Constitution analyse; Cultural aspects; Geographics; Legal aspects; Middle East; Text-mining; Textual data; Data mining",Article,Scopus,2-s2.0-85122925008
"Li L., Hoefsloot H., de Graaf A.A., Acar E., Smilde A.K.","57410071600;35548539200;7004317044;8940517500;7005843679;","Exploring dynamic metabolomics data with multiway data analysis: a simulation study",2022,"BMC Bioinformatics","23","1","31","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122763284&doi=10.1186%2fs12859-021-04550-5&partnerID=40&md5=268aea3c9559e07b617a242b5bbea854","Background: Analysis of dynamic metabolomics data holds the promise to improve our understanding of underlying mechanisms in metabolism. For example, it may detect changes in metabolism due to the onset of a disease. Dynamic or time-resolved metabolomics data can be arranged as a three-way array with entries organized according to a subjects mode, a metabolites mode and a time mode. While such time-evolving multiway data sets are increasingly collected, revealing the underlying mechanisms and their dynamics from such data remains challenging. For such data, one of the complexities is the presence of a superposition of several sources of variation: induced variation (due to experimental conditions or inborn errors), individual variation, and measurement error. Multiway data analysis (also known as tensor factorizations) has been successfully used in data mining to find the underlying patterns in multiway data. To explore the performance of multiway data analysis methods in terms of revealing the underlying mechanisms in dynamic metabolomics data, simulated data with known ground truth can be studied. Results: We focus on simulated data arising from different dynamic models of increasing complexity, i.e., a simple linear system, a yeast glycolysis model, and a human cholesterol model. We generate data with induced variation as well as individual variation. Systematic experiments are performed to demonstrate the advantages and limitations of multiway data analysis in analyzing such dynamic metabolomics data and their capacity to disentangle the different sources of variations. We choose to use simulations since we want to understand the capability of multiway data analysis methods which is facilitated by knowing the ground truth. Conclusion: Our numerical experiments demonstrate that despite the increasing complexity of the studied dynamic metabolic models, tensor factorization methods CANDECOMP/PARAFAC(CP) and Parallel Profiles with Linear Dependences (Paralind) can disentangle the sources of variations and thereby reveal the underlying mechanisms and their dynamics. © 2022, The Author(s).","CANDECOMP/PARAFAC; Dynamic metabolomics data; Paralind; Tensor factorization","Data handling; Data mining; Factorization; Linear systems; Metabolism; Metabolites; Numerical methods; Tensors; CANDECOMP/PARAFAC; Data analysis-methods; Dynamic metabolomic data; Ground truth; Linear dependence; Metabolomics data; Multi-way data analysis; Parallel profile with linear dependence; Sources of variation; Tensor factorization; Dynamics; computer simulation; human; metabolomics; Computer Simulation; Humans; Metabolomics",Article,Scopus,2-s2.0-85122763284
"Zvyagintseva D., Sigurdsson H., Kozin V.K., Iorsh I., Shelykh I.A., Ulyantsev V., Kyriienko O.","57223969985;56017529200;57200283879;22835225400;7003951067;55062303000;53064029400;","Machine learning of phase transitions in nonlinear polariton lattices",2022,"Communications Physics","5","1","8","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122703475&doi=10.1038%2fs42005-021-00755-5&partnerID=40&md5=df1492a22d8e615d37bab42209928035","Polaritonic lattices offer a unique testbed for studying nonlinear driven-dissipative physics. They show qualitative changes of their steady state as a function of system parameters, which resemble non-equilibrium phase transitions. Unlike their equilibrium counterparts, these transitions cannot be characterised by conventional statistical physics methods. Here, we study a lattice of square-arranged polariton condensates with nearest-neighbour coupling, and simulate the polarisation (pseudospin) dynamics of the polariton lattice, observing regions with distinct steady-state polarisation patterns. We classify these patterns using machine learning methods and determine the boundaries separating different regions. First, we use unsupervised data mining techniques to sketch the boundaries of phase transitions. We then apply learning by confusion, a neural network-based method for learning labels in a dataset, and extract the polaritonic phase diagram. Our work takes a step towards AI-enabled studies of polaritonic systems. © 2022, The Author(s).",,"Data mining; Machine learning; Phase diagrams; Phonons; Polarization; Statistical Physics; Nearest-neighbor coupling; Nonequilibrium phase transitions; Polariton condensates; Polaritonics; Polaritons; Pseudospin; Qualitative changes; Steady state; Steady state polarization; Systems parameters; Photons",Article,Scopus,2-s2.0-85122703475
"Elangovan A., Li Y., Pires D.E.V., Davis M.J., Verspoor K.","56950365000;57192526255;26028060700;35419216300;12772581800;","Large-scale protein-protein post-translational modification extraction with distant supervision and confidence calibrated BioBERT",2022,"BMC Bioinformatics","23","1","4","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122283263&doi=10.1186%2fs12859-021-04504-x&partnerID=40&md5=c6110b010fc432b081ea0b8f506e5557","Motivation: Protein-protein interactions (PPIs) are critical to normal cellular function and are related to many disease pathways. A range of protein functions are mediated and regulated by protein interactions through post-translational modifications (PTM). However, only 4% of PPIs are annotated with PTMs in biological knowledge databases such as IntAct, mainly performed through manual curation, which is neither time- nor cost-effective. Here we aim to facilitate annotation by extracting PPIs along with their pairwise PTM from the literature by using distantly supervised training data using deep learning to aid human curation. Method: We use the IntAct PPI database to create a distant supervised dataset annotated with interacting protein pairs, their corresponding PTM type, and associated abstracts from the PubMed database. We train an ensemble of BioBERT models—dubbed PPI-BioBERT-x10—to improve confidence calibration. We extend the use of ensemble average confidence approach with confidence variation to counteract the effects of class imbalance to extract high confidence predictions. Results and conclusion: The PPI-BioBERT-x10 model evaluated on the test set resulted in a modest F1-micro 41.3 (P =5 8.1, R = 32.1). However, by combining high confidence and low variation to identify high quality predictions, tuning the predictions for precision, we retained 19% of the test predictions with 100% precision. We evaluated PPI-BioBERT-x10 on 18 million PubMed abstracts and extracted 1.6 million (546507 unique PTM-PPI triplets) PTM-PPI predictions, and filter ≈ 5700 (4584 unique) high confidence predictions. Of the 5700, human evaluation on a small randomly sampled subset shows that the precision drops to 33.7% despite confidence calibration and highlights the challenges of generalisability beyond the test set even with confidence calibration. We circumvent the problem by only including predictions associated with multiple papers, improving the precision to 58.8%. In this work, we highlight the benefits and challenges of deep learning-based text mining in practice, and the need for increased emphasis on confidence calibration to facilitate human curation efforts. © 2021, The Author(s).","BioBERT; Deep learning; Distant supervision; Natural language processing; Post-translational modifications; Protein-protein interaction","Abstracting; Cost effectiveness; Deep learning; Filtration; Forecasting; Natural language processing systems; Proteins; BioBERT; Confidence predictions; Deep learning; Distant supervision; High confidence; Human curation; Large scale proteins; Post-translational modifications; Protein-protein interactions; Test sets; Calibration; protein; data mining; human; Medline; protein processing; Data Mining; Humans; Protein Processing, Post-Translational; Proteins; PubMed",Article,Scopus,2-s2.0-85122283263
"Rahman M.S., Biswas P.K., Saha S.K., Moni M.A.","57216556337;57201068071;55428843600;35119094400;","Identification of glycophorin C as a prognostic marker for human breast cancer using bioinformatic analysis",2022,"Network Modeling Analysis in Health Informatics and Bioinformatics","11","1","7","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122175335&doi=10.1007%2fs13721-021-00352-0&partnerID=40&md5=541c6d1f315347bc49700b15e4299d2c","Breast cancer is an expanding threat that leads to many women's death worldwide. Despite the improvement of the early detection methods and treatment, still, there is a high number of breast cancer mortality. To increase patient survival in breast cancer, identifying novel biomarkers is essential for therapeutics targets. The Glycophorin C (GYPC) gene is correlated with patient survival, which can be a possible biomarker for early detection in breast cancer progression. However, the expression of GYPC is not clearly defined in breast cancer. Here, we widely analyzed the expression pattern of GYPC in breast cancer and patient survival datasets through several bioinformatics tools. GYPC mRNA expression using ONCOMINE, GENT2, and GTX2 webs. Also, The co-expression profile of GYPC has been repossessed from Ma breast four datasets from Oncomine dataset. Our study revealed that mRNA expression of GYPC is strongly correlated with the survival of breast cancer patients, suggesting its role as a tumor suppressor. The downregulation of GYPC in breast cancer tissue is examined by promoter methylation and copy number alterations. The downregulation of GYPC expression was significantly correlated with high patient survival. Moreover, we performed pathway analysis via Enricher and gene ontology web using 20 positively correlated genes. Consequently, our analyzed data suggested that GYPC might be an essential therapeutics and prognostic biomarker in breast cancer. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature.","Biomarker discovery; Breast cancer; Data mining; Glycophorin C; In silico; Survival","Alkylation; Bioinformatics; Biomarkers; Diseases; Gene Ontology; Genes; Bio-marker discovery; Breast Cancer; Down-regulation; Glycophorin; Glycophorin C; In-silico; mRNA expression; Patient survivals; Prognostic markers; Survival; Data mining; epidermal growth factor receptor 2; estrogen receptor; glycophorin C; messenger RNA; progesterone receptor; protein p53; adrenal cortex carcinoma; adult; aged; Article; assessment of humans; bioinformatics; breast cancer; cancer classification; cancer grading; cancer growth; cancer mortality; cancer staging; cancer survival; controlled study; copy number variation; distant metastasis free survival; down regulation; endometrium carcinoma; female; gene amplification; gene expression; gene mutation; gene ontology; human; human tissue; luminal A breast cancer; luminal B breast cancer; major clinical study; menopause; mouth cancer; Nottingham Prognostic Index status; overall survival; PAM50 status; pancreas cancer; post progression survival; protein expression; protein methylation; recurrence free survival; Robust Instrinsic Molecular Subtype Predictors Classification; Robust SCM Classification; Robust SSP Classification; Scarff Bloom and Richardson grade status; triple negative breast cancer; tumor suppressor gene; tumor volume; upregulation",Article,Scopus,2-s2.0-85122175335
"Wunderlich F., Memmert D.","57190607889;16039986900;","A big data analysis of Twitter data during premier league matches: do tweets contain information valuable for in-play forecasting of goals in football?",2022,"Social Network Analysis and Mining","12","1","23","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122010136&doi=10.1007%2fs13278-021-00842-z&partnerID=40&md5=902afc5b4eac7828196eb978984040f3","Data-related analysis in football increasingly benefits from Big Data approaches and machine learning methods. One relevant application of data analysis in football is forecasting, which relies on understanding and accurately modelling the process of a match. The present paper tackles two neglected facets of forecasting in football: Forecasts on the total number of goals and in-play forecasting (forecasts based on within-match information). Sentiment analysis techniques were used to extract the information reflected in almost two million tweets from more than 400 Premier League matches. By means of wordclouds and timely analysis of several tweet-based features, the Twitter communication over the full course of matches and shortly before and after goals was visualized and systematically analysed. Moreover, several forecasting models including a random forest model have been used to obtain in-play forecasts. Results suggest that in-play forecasting of goals is highly challenging, and in-play information does not improve forecasting accuracy. An additional analysis of goals from more than 30,000 matches from the main European football leagues supports the notion that the predictive value of in-play information is highly limited compared to pre-game information. This is a relevant result for coaches, match analysts and broadcasters who should not overestimate the value of in-play information. The present study also sheds light on how the perception and behaviour of Twitter users change over the course of a football match. A main result is that the sentiment of Twitter users decreases when the match progresses, which might be caused by an unjustified high expectation of football fans before the match. © 2021, The Author(s).","Big data; Data mining; Football forecasting; In-play forecasting; Social networks; Twitter","Big data; Data handling; Data mining; Decision trees; Learning systems; Sentiment analysis; Social networking (online); Sports; Analysis techniques; Football forecasting; Forecasting accuracy; Forecasting models; In-play forecasting; Machine learning methods; Predictive values; Random forest modeling; Sentiment analysis; Social network; Forecasting",Article,Scopus,2-s2.0-85122010136
"Chou D., Jiang M.","57221147410;36179647500;","A Survey on Data-driven Network Intrusion Detection",2022,"ACM Computing Surveys","54","9","182","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121021540&doi=10.1145%2f3472753&partnerID=40&md5=3b1ec477a9810f3969b943d3c9b0dae2","Data-driven network intrusion detection (NID) has a tendency towards minority attack classes compared to normal traffic. Many datasets are collected in simulated environments rather than real-world networks. These challenges undermine the performance of intrusion detection machine learning models by fitting machine learning models to unrepresentative ""sandbox""datasets. This survey presents a taxonomy with eight main challenges and explores common datasets from 1999 to 2020. Trends are analyzed on the challenges in the past decade and future directions are proposed on expanding NID into cloud-based environments, devising scalable models for large network data, and creating labeled datasets collected in real-world networks. © 2021 Association for Computing Machinery.","data mining; machine learning; Network intrusion detection","Data mining; Intrusion detection; Large dataset; Machine learning; Cloud-based; Data driven; Intrusion-Detection; Larger networks; Machine learning models; Network intrusion detection; Performance; Real-world networks; Scalable Modelling; Simulated environment; Surveys",Article,Scopus,2-s2.0-85121021540
"Bodaghi A., Oliveira J.","57203779226;7202452493;","A longitudinal analysis on Instagram characteristics of Olympic champions",2022,"Social Network Analysis and Mining","12","1","3","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119355519&doi=10.1007%2fs13278-021-00838-9&partnerID=40&md5=9beb423219479a32f6439f80f249294f","This study examines Olympic champions’ characteristics on Instagram to first understand how age and gender affect the characteristics and their interrelations and second to see if the future changes in those characteristics are predictable. We crawled Instagram data of individual gold medalists in the Rio2016 Olympics for four months and utilized a content analytic method to analyze their photograph posts. The cross-sectional analysis shows that as the champions age, the follower engagement rate decreases in both genders. However, men increase their pure self-presentation posts while women extend their circle of followings. In its approval, the longitudinal analysis shows that when a higher engagement rate is achieved, men and women champions lose their tendency to increase self-presenting posts and the number of followings, respectively. In the light of relative theories and the previous literature, these findings contribute to a better understanding of athletes’ cyber behavior in social media. Moreover, the findings serve as a guide for sport researchers seeking to grasp the ways that aid athletes to better interact with their followers and build the personal brand, which involves sponsorship and other promotional opportunities. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature.","Data mining; Gold medalists; Instagram; Olympics; Social network analysis; User engagement","Gold; Social networking (online); Sports; Analytic method; Cross sectional analysis; Cyber behaviors; Gold medalist; Instagram; Longitudinal analysis; Olympics; Self presentations; Social Network Analysis; User engagement; Data mining",Article,Scopus,2-s2.0-85119355519
"Ranjan R., Vathsala H., Koolagudi S.G.","57337847500;56901391500;23397105900;","Profile generation from web sources: an information extraction system",2022,"Social Network Analysis and Mining","12","1","2","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119052065&doi=10.1007%2fs13278-021-00827-y&partnerID=40&md5=dfc24638cabbb595c4a1ddd05476aebb","The Internet space has a vast collection of information which is not always structured. These sources of information such as social media, news articles, blogs, speeches and videos often contain information that could be utilized to generate decision making tools such as reports about events and individuals. Using this information is a long and tedious process if done manually. Over the years, a lot of research has been done in data mining and natural language processing techniques to facilitate the consumption of this vast amount of data. The current work describes ProfileGen, an information extraction system that uses a variety of these data sources to form a profile of a given person. There are two parts to this application: The first part uses information publicly available on social media sites, news articles on news websites and blogs and compiles this information to form a corpus about the given person, and in the second part, the information is ranked using machine learning techniques, so as to provide information in the order of importance. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature.","Biography generation; Data mining; Information extraction; Natural language processing; ProfileGen; Recurrent neural network","Data mining; Decision making; Information retrieval; Information retrieval systems; Information use; Learning algorithms; Sentiment analysis; Social networking (online); Biography generation; Decision making tool; Information extraction; Information extraction systems; News articles; News video; Profilegen; Social media; Sources of informations; Web sources; Recurrent neural networks",Article,Scopus,2-s2.0-85119052065
"Larestani A., Mousavi S.P., Hadavimoghaddam F., Ostadhassan M., Hemmati-Sarapardeh A.","57226119296;57220545413;57211296040;54783285600;55556311300;","Predicting the surfactant-polymer flooding performance in chemical enhanced oil recovery: Cascade neural network and gradient boosting decision tree",2022,"Alexandria Engineering Journal","61","10",,"7715","7731",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123728426&doi=10.1016%2fj.aej.2022.01.023&partnerID=40&md5=50c57638e6f9b2e99de049b79fb6e699","Surfactant-polymer flooding is one of the most important enhanced oil recovery (EOR) techniques, which refers to the injection of surfactant slugs and polymer drives. Two crucial decision-making parameters in EOR operations are net present value (NPV) and oil recovery factor (RF). Herein, various intelligent models, based on multilayer perceptron (MLP), cascade neural network (CNN), radial basis function (RBF), neural networks as well as support vector regression (SVR), and decision tree (DT) algorithms are proposed toward estimating these two parameters with respect to polymer drive size, surfactant slug size, the salinity of polymer drive, Kv/Kh ratio, surfactant concentration, and polymer concentration in polymer drive and surfactant slug. The results exhibited the outperformance of the CNN model trained with the Levenberg Marquardt algorithm in forecasting the RF and NPV with average absolute errors of 0.66% and 1.95%, respectively. Moreover, the results of the sensitivity analysis reflected that the most effective inputs on the predicted value of RF were surfactant concentration and surfactant slug size, while surfactant concentration and polymer concentration in surfactant slug could considerably affect the NPV model's output. Lastly, the outlier detection analysis revealed that the employed data is valid and only two points were detected as outliers. © 2022 THE AUTHORS","Artificial neural networks; Enhanced oil recovery; Intelligent model; Net present value; Recovery factor; Surfactant-polymer flooding","Computer system recovery; Data mining; Decision trees; Digital storage; Floods; Multilayer neural networks; Oil well flooding; Radial basis function networks; Sensitivity analysis; Cascade neural networks; Enhanced-oil recoveries; Intelligent models; Performance; Polymer concentrations; Polymer flooding; Recovery factors; Surfactant concentrations; Surfactant-polymer flooding; The net present value (NPV); Enhanced recovery",Article,Scopus,2-s2.0-85123728426
"Chen Y., Liu D., Liu Y., Zheng Y., Wang B., Zhou Y.","57205355848;55730458100;57189095780;57414916500;57195293751;56580678200;","Research on user generated content in Q&A system and online comments based on text mining",2022,"Alexandria Engineering Journal","61","10",,"7659","7668",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122967275&doi=10.1016%2fj.aej.2022.01.020&partnerID=40&md5=4b04613f58a3c71dc4060fd721a4a81b","For information asymmetry in e-commerce platforms, question answering system (Q&A) and online comments are used to build trust relationship. Although online reviews play a significant role in reducing information asymmetry, information overload and distortion are also not negligible. In the study, text mining was used to extract various questions from Q&A system and online comments to obtain the content that consumers are most concerned about in online shopping, and the similarities and differences of the two mining results are compared. The results show that Q&A system has a great influence on users' decision making. The Q&A system is not only complementary to online comments, but also can provide validation for the information of online comments. © 2022 THE AUTHORS","Cluster analysis; Content classification; Q&A system; User generated content","Data mining; Decision making; Electronic commerce; Text processing; Commerce platforms; Content classification; E- commerces; Information asymmetry; Question answering system system; Question answering systems; Text-mining; Trust relationship; User generated content; User-generated; Cluster analysis",Article,Scopus,2-s2.0-85122967275
"Cheng S., Chen C., Pan S., Huang H., Zhang W., Feng Y.","57397166200;56215995200;36646395300;57397917500;57206986787;57397356500;","Citywide package deliveries via crowdshipping: minimizing the efforts from crowdsourcers",2022,"Frontiers of Computer Science","16","5","165327","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122291825&doi=10.1007%2fs11704-021-0568-5&partnerID=40&md5=5e274b5f9f39c9bf49cf80f79869caff","Most current crowdsourced logistics aim to minimize systems cost and maximize delivery capacity, but the efforts of crowdsourcers such as drivers are almost ignored. In the delivery process, drivers usually need to take long-distance detours in hitchhiking rides based package deliveries. In this paper, we propose an approach that integrates offline trajectory data mining and online route-and-schedule optimization in the hitchhiking ride scenario to find optimal delivery routes for packages and drivers. Specifically, we propose a two-phase framework for the delivery route planning and scheduling. In the first phase, the historical trajectory data are mined offline to build the package transport network. In the second phase, we model the delivery route planning and package-taxi matching as an integer linear programming problem and solve it with the Gurobi optimizer. After that, taxis are scheduled to deliver packages with optimal delivery paths via a newly designed scheduling strategy. We evaluate our approach with the real-world datasets; the results show that our proposed approach can complete citywide package deliveries with a high success rate and low extra efforts of taxi drivers. © 2022, Higher Education Press.","crowdshipping; dynamic elivery optimization; hitchhiking rides; package delivery; taxi scheduling","Crowdsourcing; Data mining; Integer programming; Scheduling; 'current; Crowdshipping; Delivery routes; Dynamic elivery optimization; Hitchhiking ride; Offline; Optimisations; Package delivery; Route planning; Taxi scheduling; Taxicabs",Article,Scopus,2-s2.0-85122291825
"Gumiel Y.B., Oliveira L.E.S.E., Claveau V., Grabar N., Paraiso E.C., Moro C., Carvalho D.R.","57016767100;57022994800;23088341000;6602326045;56028874700;36717883800;55887138200;","Temporal Relation Extraction in Clinical Texts",2022,"ACM Computing Surveys","54","7","144","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115442922&doi=10.1145%2f3462475&partnerID=40&md5=3ae85faa661ae0581054282264e1be62","Unstructured data in electronic health records, represented by clinical texts, are a vast source of healthcare information because they describe a patient's journey, including clinical findings, procedures, and information about the continuity of care. The publication of several studies on temporal relation extraction from clinical texts during the last decade and the realization of multiple shared tasks highlight the importance of this research theme. Therefore, we propose a review of temporal relation extraction in clinical texts. We analyzed 105 articles and verified that relations between events and document creation time, a coarse temporality type, were addressed with traditional machine learning-based models with few recent initiatives to push the state-of-the-art with deep learning-based models. For temporal relations between entities (event and temporal expressions) in the document, factors such as dataset imbalance because of candidate pair generation and task complexity directly affect the system's performance. The state-of-the-art resides on attention-based models, with contextualized word representations being fine-tuned for temporal relation extraction. However, further experiments and advances in the research topic are required until real-time clinical domain applications are released. Furthermore, most of the publications mainly reside on the same dataset, hindering the need for new annotation projects that provide datasets for different medical specialties, clinical text types, and even languages. © 2021 ACM.","clinical data; natural language processing; Temporal relation extraction","Clinical research; Data handling; Data mining; Deep learning; Extraction; Learning algorithms; Clinical data; Continuity of cares; Learning Based Models; Relation extraction; State of the art; Task complexity; Temporal expressions; Temporal relation; Temporal relation extraction; Unstructured data; Natural language processing systems",Article,Scopus,2-s2.0-85115442922
"Al-Wesabi F.N., Malibari A.A., Mustafa Hilal A., NEMRI N., Kumar A., Gupta D.","57211901842;6506143515;57373102200;54420791100;56096607800;56985108600;","Intelligent ensemble of voting based solid fuel classification model for energy harvesting from agricultural residues",2022,"Sustainable Energy Technologies and Assessments","52",,"102040","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124409751&doi=10.1016%2fj.seta.2022.102040&partnerID=40&md5=8495334038992980664b6c58494773f3","In recent times, utilization of renewable energy resources for transportation and electric power generation can be the sustainable way that reduces the risk of environmental, climatic, economic, political, and security concerns related to fossil fuel combustion. Among the several renewable energy sources, biomass is commonly used due to the fulfilment of ecological compatibility due to the fact that it is attained from plant and animal waste. At the same time, the classification of fuel materials becomes difficult when the material is previously processed or gathered from an environment that makes it challenging to discern. Therefore, with the constant development and diversification of energy harvesting from agricultural residues, there is a requirement to design a model for the classification of solid fuels. The recent developments of machine learning (ML) and deep learning (DL) techniques can be used for the solid fuel classification. The ML and DL models comprise many interdisciplinary areas, such as statistics, mathematics, artificial neural networks, data mining, optimization, and artificial intelligence. With this motivation, this paper designs a new intelligent ensemble of voting based solid fuel classification (IEVB-SFC) model for energy harvesting from agricultural residue. The proposed IEVB-SFC technique involves different stages of operations such as data acquisition, data preprocessing, classification, and ensemble process. At the primary stage, the data preprocessing is carried out in three different ways such as data transformation, class labeling, and data normalization. Besides, the IEVB-SFC technique comprises three different DL models as long short term memory (LSTM), gated recurrent unit (GRU), and convolutional neural network based LSTM (CNN-LSTM). Finally, an ensemble of three DL models takes place by the use of voting technique and thereby determines the appropriate solid fuel class labels, show the novelty of the work. The experimental results showcased the betterment of the IEVB-SFC technique over the recent state of art techniques with the maximum accuracy of 0.97. © 2022 Elsevier Ltd","Agricultural residue; Biomass; Deep learning; Energy harvesting; Machine learning; Renewable energy source; Solid fuel classification","Agriculture; Data acquisition; Data mining; Electric power generation; Energy harvesting; Fossil fuels; Long short-term memory; Metadata; Waste incineration; Classification models; Classification technique; Data preprocessing; Deep learning; Fossil fuel combustion; Learning models; Renewable energy source; Renewable energy sources (biomass); Solid fuel classification; Solid fuels; Agricultural wastes",Article,Scopus,2-s2.0-85124409751
"Yan W.-T., Yang Y.-D., Hu X.-M., Ning W.-Y., Liao L.-S., Lu S., Zhao W.-J., Zhang Q., Xiong K.","57209340886;57221596863;57216623441;57221603009;57425257300;57285793200;57221874842;57195397473;15833685900;","Do pyroptosis, apoptosis, and necroptosis (PANoptosis) exist in cerebral ischemia? Evidence from cell and rodent studies",2022,"Neural Regeneration Research","17","8",,"1761","1768",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122060515&doi=10.4103%2f1673-5374.331539&partnerID=40&md5=b912aa552421bab921ee99f131ba9f6d","Some scholars have recently developed the concept of PANoptosis in the study of infectious diseases where pyroptosis, apoptosis and necroptosis act in consort in a multimeric protein complex, PANoptosome. This allows all the components of PANoptosis to be regulated simultaneously. PANoptosis provides a new way to study the regulation of cell death, in that different types of cell death may be regulated at the same time. To test whether PANoptosis exists in diseases other than infectious diseases, we chose cerebral ischemia/reperfusion injury as the research model, collected articles researching cerebral ischemia/reperfusion from three major databases, obtained the original research data from these articles by bibliometrics, data mining and other methods, then integrated and analyzed these data. We selected papers that investigated at least two of the components of PANoptosis to check its occurrence in ischemia/reperfusion. In the cell model simulating ischemic brain injury, pyroptosis, apoptosis and necroptosis occur together and this phenomenon exists widely in different passage cell lines or primary neurons. Pyroptosis, apoptosis and necroptosis also occurred in rat and mouse models of ischemia/reperfusion injury. This confirms that PANoptosis is observed in ischemic brain injury and indicates that PANoptosis can be a target in the regulation of various central nervous system diseases. © 2022 Wolters Kluwer Medknow Publications. All rights reserved.","Apoptosis; Brain; Central nervous system; Ischemia/reperfusion; Middle cerebral artery occlusion; Necroptosis; Oxygen and glucose deprivation; PANoptosis; Pyroptosis; Regulated cell death","caspase; caspase 3; caspase 7; caspase 8; caspase 9; glucose; interleukin 18; interleukin 1beta; lactate dehydrogenase; oxygen; protein Bax; protein bcl 2; animal cell; animal model; apoptosis; Article; brain ischemia; cell death; cell viability assay; cerebral ischemia reperfusion injury; data mining; flow cytometry; in vivo study; lactate dehydrogenase blood level; middle cerebral artery occlusion; necroptosis; nerve cell culture; nonhuman; protein expression; pyroptosis; rat; rodent; TUNEL assay",Article,Scopus,2-s2.0-85122060515
"Jin X., Hu H.","57218844977;57461054700;","Research and implementation of smart energy investment and financing system design based on energy mega data mining",2022,"Energy Reports","8",,,"1226","1235",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125017504&doi=10.1016%2fj.egyr.2022.02.044&partnerID=40&md5=88252a32d8db35480016f1a238005562","With the accelerated pace of energy reform, State Grid Corporation of China has actively formulated and implemented a big data strategy, established a big data center, tapped the value of massive energy data resources, improved the data asset management system, and used data to drive management reform and transformation and upgrading. In order to optimize the current energy investment and financing system, this paper attempts to analyze the problems existing in the current rural energy investment and financing system and explore the digital transformation of energy enterprises based on data mining technology. Based on energy big data, this paper constructs an intelligent energy investment and financing system with energy investment and financing mechanism, and emphatically analyzes the factors that influence the strategic choice of each participant, in order to seek the strategy of evolution and stability. Higher emission reduction targets will prompt enterprises to increase their emission reduction efforts, increase their demand for carbon quotas, and at the same time increase the total amount of carbon quotas that can be allocated in the market, resulting in a decline in market clearing carbon prices. In the process of socialist development in the new period, we should seize the opportunity, proceed from the whole, make overall plans, clarify the future development direction, and make bold innovations in financing methods. © 2022","Energy enterprises; Energy mega data; Investment and financing system","Big data; Carbon; Commerce; Data mining; Digital storage; Information management; Investments; Metadata; Datacenter; Energy; Energy enterprise; Energy investment and financings; Energy mega data; Energy reforms; Investment and financing system; Investment and financings; Smart energies; Emission control",Article,Scopus,2-s2.0-85125017504
"Qiao J., Yang S., Chen Z., Zhuang Z., Chen L.","57459596300;57459990900;57459731300;57459468800;35226168000;","A quantitative study of policy-driven changes and forecasts in the development of the hydrogen transportation industry",2022,"Energy Reports","8",,,"1218","1225",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124949020&doi=10.1016%2fj.egyr.2022.02.057&partnerID=40&md5=de18cdbeedf565ea18255dbf604cac5a","Through data mining and analysis of the word frequency and occurrence position of industrial policy keywords, the main policy parameters affecting industrial development are determined, and the functional relationship between industrial policy and industrial development is obtained through multi-parameter non-linear regression: Yit−1y1,y2,y3,y4,y5=β1itX1+β2itlnX2+β3itlnX3+β4itX1it∗lnX3+ɛit. The time series function of the industrial development index: Y(t)=0.174∗e(0.256∗t) is established and the industrial development under the influence of next year's policy is predicted. It is concluded from the mathematical expression of the statistical model that there is a certain coupling effect between different policies, and that industrial development is influenced by the joint effect on the parent and sub-industries. This ultimately proves that there is a clear correlation between policy and industry development. © 2022 The Author(s)","Entropy weighting; Hydrogen energy; Multi-parameter non-linear regression; Principal component analysis","Data mining; Entropy weighting; Hydrogen Energy; Industrial development; Industrial policies; LnX 3; Multi-parameter non-linear regression; Multiparameters; Non-linear regression; Policy development; Principal-component analysis; Principal component analysis",Article,Scopus,2-s2.0-85124949020
"Burke H.M., Tingley R., Dorin A.","57457667800;36901885500;6603576183;","Tag Frequency Difference: Rapid estimation of image set relevance for species occurrence data using general-purpose image classifiers",2022,"Ecological Informatics","69",,"101598","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124835705&doi=10.1016%2fj.ecoinf.2022.101598&partnerID=40&md5=6d0e610f727e9bc41d934700bc7dacad","iEcology is used to supplement traditional ecological data by sourcing large quantities of media from the internet. Images and their metadata are widely available online and can provide information on species occurrence, behaviour and visible traits. However, this data is inherently noisy and data quality varies significantly between sources. Many iEcology studies utilise data from a single source for simplicity and efficiency. Hence, a tool to compare the suitability of different media sources in addressing a particular research question is needed. We provide a simple, novel way to estimate the fraction of images within multiple unverified datasets that potentially depict a specified target fauna. Our method, the Sum of Tag Frequency Differences (STFD), uses any pretrained, general-purpose image classifier. One of the method's innovations is that it does not require training the classifier to recognise the target fauna. Instead, STFD analyses the frequency of the generic text-tags returned by a classifier for multiple datasets and compares them to the corresponding frequencies of an authoritative image dataset that depicts only the target organism. From this comparison, STFD allows us to deduce the fraction of images of the target in unverified datasets. To validate the STFD approach, we processed images from five sources: Flickr, iNaturalist, Instagram, Reddit and Twitter. For each media source, we conducted an STFD analysis of three fauna invasive to Australia: Cane toads (Rhinella marina), German wasps (Vespula germanica), and the higher-level colloquial taxonomic classification, “wild rabbits”. We found the STFD provided an accurate assessment of image source relevance across all data sources and target organisms. This was demonstrated by the consistent, very strong correlation (toads r ≥0.97, wasps r ≥0.95, wild rabbits≥ 0.95) between STFD predictions, and the fraction of target images in a source dataset observed by a human expert. The STFD provides a low-cost, simple and accurate comparison of the relevance of online image sources to specific fauna for iEcology applications. It does not require expertise in machine learning or training neural-network species-specific classifiers. The method enables researchers to assess multiple image sources to select those warranting detailed investigation for the development of tools for web-scraping, citizen science campaigns, further monitoring or analysis. © 2022","Biodiversity monitoring; Computer vision; Data mining; iEcology; Social media","correlation; data quality; estimation method; machine learning; Australia",Article,Scopus,2-s2.0-85124835705
"Li Y., Wang X., He Y., Wang Y., Wang Y., Wang S.","55795424800;57282775300;57190841870;44462347000;56274770600;7410333793;","Deep Spatial-Temporal Feature Extraction and Lightweight Feature Fusion for Tool Condition Monitoring",2022,"IEEE Transactions on Industrial Electronics","69","7",,"7349","7359",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124796212&doi=10.1109%2fTIE.2021.3102443&partnerID=40&md5=6f8c1540a46436f3217c018c475ee47a","Tool condition monitoring (TCM) is vital to maintain the quality of workpieces during machining. Recently, data-driven methods based on multisensory data have been applied to TCM. The quality of extracted features is a key to realizing a successful data-driven TCM. However, the extracted features in the previous study are focused on the multicollinearity of multisensory data, which is incapable of identifying the informative and discriminative information in the long time period aspect. This article proposed a novel method for TCM using deep spatial-temporal feature extraction and lightweight feature fusion techniques. A key to the proposed method is the extraction of multicollinearity as spatial features (SPs), and the capture of long-range dependencies and nonlinear dynamics as temporal features (TFs), to fully characterize tool wear change using multisensory data. Then, a lightweight feature fusion method is used to fuse SPs, TFs, and statistical features for further removing redundant information employing the kernel-principal component analysis. Finally, support vector machines is used to predict the tool conditions using the fusion feature. Experiments on a milling machine and a gear hobbing machine are carried out to verify the effectiveness and generalization of the proposed method respectively. © 1982-2012 IEEE.","Kernel-principal component analysis (KPCA); lightweight feature fusion; spatial features (SPs); temporal features (TFs); tool condition monitoring (TCM)","Condition monitoring; Cutting tools; Data mining; Extraction; Principal component analysis; Regression analysis; Support vector machines; Features fusions; Kernel principal component analyses (KPCA); Kernel-principal component analyse; Lightweight feature fusion; Spatial feature; Spatial features; Temporal feature; Temporal features; Tool condition monitoring; Feature extraction",Article,Scopus,2-s2.0-85124796212
"Xie M.","57232488700;","Smart Grid Borderless Access Control Technology based on network security situational awareness",2022,"Energy Reports","8",,,"415","423",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124416372&doi=10.1016%2fj.egyr.2022.01.203&partnerID=40&md5=7d435d80295fc72bd777eae1c47ef51d","The rapid development of new energy generation, the expansion of power monitoring system control scope, more complex structure, existing protection measures are difficult to resist complex network attacks. We proposed a borderless access control technology based on smart grid network security situational awareness. Based on the zero-trust mechanism, the malicious code pre-detection of hardware multifactor dynamic scrambling is proposed, and the model of abnormal behavior and intelligent cleaning of multidimensional network metadata based on deep data mining and graph analysis is established, which improves the accuracy of abnormal traffic location, tracking, and traceability, and the accuracy is as high as 97.74 percent. © 2022 The Author(s)","Borderless access control; Network security situational awareness; Smart grid","Access control; Complex networks; Data mining; Electric power transmission networks; Smart power grids; Access control technologies; Borderless access control; Energy generations; Network security situational awareness; Networks security; New energies; Security situational awareness; Smart grid; System control; Technology-based; Network security",Article,Scopus,2-s2.0-85124416372
"Wu Q., Zhang M., Liao L.","57212567751;57204196943;57447818500;","Analysis of electricity stealing based on user electricity characteristics of electricity information collection system",2022,"Energy Reports","8",,,"488","494",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124411649&doi=10.1016%2fj.egyr.2022.01.143&partnerID=40&md5=a5b1fe0dd25f149616b580f263fc571d","In order to effectively combat electricity theft and improve the ability to recognize electricity theft, this paper proposes an anti-electricity theft method based on the analysis of users’ electricity consumption behavior. First, determine the appropriate outlier detection electrical parameters through reasonable research on users., Using the outlier detection algorithm to filter the user's electrical data, preprocessing the original data, performing multi-feature fusion analysis on the user's electrical parameters, using the discriminant rule to filter out the outliers, and then using the electricity based on the clustering algorithm Pattern analysis for detection, to achieve active identification of low-voltage power theft. In this paper, combined with the user profile data of the electricity consumption information collection system, it conducts in-depth analysis of the user's electricity consumption data. Through the discovery of abnormal electricity consumption data users, the analysis finds out the electricity theft users and improves the quality and efficiency of the anti-stealing work. © 2022 The Author(s)","Anti-electricity theft; Data mining; Electricity data analysis","Anomaly detection; Clustering algorithms; Crime; Electric network parameters; Filtration; Pattern recognition; Quality control; Statistics; Anti-electricity theft; Collection systems; Electrical parameter; Electricity characteristics; Electricity data analyse; Electricity theft; Electricity-consumption; Information collections; Outlier Detection; Outlier detection algorithm; Data mining",Article,Scopus,2-s2.0-85124411649
"El-Hajj W., Hajj H.","21833644200;35117824200;","An optimal approach for text feature selection",2022,"Computer Speech and Language","74",,"101364","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123950111&doi=10.1016%2fj.csl.2022.101364&partnerID=40&md5=28f638174f4efda690ee9a9ec78eb1be","Traditionally, feature selection is conducted by first deriving a candidate list of features, then ranking and selecting the top features based on predefined threshold. These methods are highly dependent on the choice of the threshold, and therefore lead to sub-optimal text categorization results. In this paper, we address the selection problem by suggesting a one-step method designed to optimally select the subset of features. The selection is formulated mathematically as an optimization problem with the objective of maximizing classification accuracy while simultaneously deriving and choosing the most discriminative features. Our method, MFX, is applicable to many of the conventional methods, with two distinguishing aspects. First, it is based on considering all documents from the same category as one extended document, instead of analyzing individual documents. Second, it considers choosing the most discriminative terms that are frequent and common across all documents of the same category, and minimally present in other categories. Moreover, MFX is language-independent. It was tested on the well-known benchmark Reuters RCV1 dataset. To showcase its language independence, MFX was also tested on Arabic datasets extracted from Arabic news sources. The results indicated that MFX always performed similar to or better than other well-known feature selection methods. MFX with a Support Vector Machine (SVM) classifier was also shown to outperform recent text classification algorithms based on neural networks and word embeddings. © 2022 Elsevier Ltd","Arabic text mining; Data mining; Feature selection; Text categorization; Text mining","Classification (of information); Data mining; Support vector machines; Text processing; Arabic text mining; Arabic texts; Candidate list; Feature-based; Features selection; Optimal approaches; Selection problems; Text categorization; Text feature selections; Text-mining; Feature extraction",Article,Scopus,2-s2.0-85123950111
"Tiwari A., Chaturvedi A.","57194021767;55980929100;","A hybrid feature selection approach based on information theory and dynamic butterfly optimization algorithm for data classification",2022,"Expert Systems with Applications","196",,"116621","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124994444&doi=10.1016%2fj.eswa.2022.116621&partnerID=40&md5=34b9598adb14f733d224687cc0582694","The ubiquitous usage of feature selection in search space optimization, information retrieval, data mining, signal processing, software fault prediction, and bioinformatics is paramount to expert and intelligent systems. Most of the conventional feature selection methods implemented are based on filter and wrapper approaches that suffer from poor classification accuracy, high computational cost, and selection of irrelevant and redundant features. This is due to the limitations of the employed objective functions leading to overestimation of the feature significance. On the contrary, hybrid feature selection methods formulated from information theory and nature-inspired metaheuristic algorithms are preferred because of their high computational efficiency, scalability in avoiding redundant and less informative features, and independence from the classifier. However, these methods have three common drawbacks: (1) poor trade-off between exploration and exploitation phase, (2) getting stuck into an optimal local solution, and (3) avoiding irrelevancy and redundancy of selected features. The first and the second drawback is related to metaheuristic algorithm implementation, while the third is concerned with applied information-theoretic paradigms. To address the aforementioned problems, we developed a new hybrid feature selection method, namely, the Iterative Feature Selection using Dynamic Butterfly Optimization Algorithm based Interaction Maximization (IFS-DBOIM) that combines Dynamic Butterfly Optimization Algorithm (DBOA) with a mutual information-based Feature Interaction Maximization (FIM) scheme for selecting the optimal feature subset. There is evidence that DBOA performs better in exploration, exploitation, and avoidance of local optima entrapment, and FIM comparatively scores the maximum relevancy with minimum redundancy of the new features with previously selected ones. The performance of the proposed method is compared using twenty publicly available datasets with ten baseline feature selection approaches. The results revealed that IFS-DBOIM outperforms other approaches on most datasets, maximizing the percent classification accuracy with the least number of features. The nonparametric Wilcoxon rank test confirms the statistical significance of these outcomes. Moreover, this method realizes the best trade-off between accuracy and stability. © 2022 Elsevier Ltd","Classification accuracy; Dynamic butterfly optimization algorithm; Feature interaction maximization; Feature selection; Mutual information","Classification (of information); Computation theory; Computational efficiency; Data mining; Degrees of freedom (mechanics); Economic and social effects; Feature extraction; Filtration; Information filtering; Intelligent systems; Iterative methods; Redundancy; Search engines; Signal processing; Classification accuracy; Dynamic butterfly optimization algorithm; Feature interaction maximization; Feature interactions; Feature selection methods; Features selection; Hybrid feature selections; Meta-heuristics algorithms; Mutual informations; Optimization algorithms; Optimization",Article,Scopus,2-s2.0-85124994444
"Sun W., Liu S., Zhang X., Zhu H.","57192690966;57257511200;57194623862;56041439900;","Performance of hyperspectral data in predicting and mapping zinc concentration in soil",2022,"Science of the Total Environment","824",,"153766","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124591826&doi=10.1016%2fj.scitotenv.2022.153766&partnerID=40&md5=980126b871e44f59b98b248369b2c50a","Reflectance spectroscopy in visible, near-infrared, and short-wave infrared (VNIR-SWIR) region has been recognized as a promising alternative for prediction of heavy metal concentration in soil. Compared with VNIR-SWIR reflectance spectroscopy, VNIR reflectance spectroscopy is less affected by atmospheric water vapor and has relatively high signal to noise ratio. The performances of VNIR and VNIR-SWIR hyperspectral data in predicting and mapping heavy metal concentration in soil were explored. In this study, laboratory spectra of soil samples collected from an agricultural area and Advanced Hyperspectral Imaging (AHSI) remote sensing imagery were used to predict and map zinc (Zn) concentration with genetic algorithm and partial least squares regression (GA-PLSR). The entire spectral regions of VNIR-SWIR and VNIR and spectral subsets extracted from the entire spectral regions were used in the prediction. For the laboratory spectra, the combination of the spectral bands extracted from the absorption features at 500 nm and in 600–800 nm obtained the highest prediction accuracy with the root mean square error (RMSE) and coefficient of determination (R2) values of 8.90 mg kg−1 and 0.72. For soil spectra from AHSI remote sensing imagery, the highest prediction accuracy was achieved by using the spectral bands extracted from the absorption feature in 600–800 nm with the RMSE and R2 values of 9.02 mg kg−1 and 0.75. Soil Zn concentration maps were generated with the established prediction models using AHSI remote sensing imagery. Analysis on the Zn concentration maps shows that the prediction model established using the spectral bands extracted from the absorption feature in 600–800 nm has a better performance in mapping Zn concentration. The results indicate that VNIR hyperspectral data outperforms VNIR-SWIR hyperspectral data in predicting and mapping Zn concentration in soil, which provides an alternative to the application of hyperspectral data in soil science. © 2022 Elsevier B.V.","Heavy metal concentration; Hyperspectral remote sensing imagery; Spectral subset selection; VNIR spectroscopy; VNIR-SWIR spectroscopy","Data mining; Forecasting; Genetic algorithms; Heavy metals; Hyperspectral imaging; Infrared devices; Infrared radiation; Least squares approximations; Mean square error; Near infrared spectroscopy; Reflection; Remote sensing; Signal to noise ratio; Zinc; Heavy metal concentration; Hyperspectral remote sensing; Hyperspectral remote sensing imagery; Infrared: spectroscopy; Remote sensing imagery; Short wave infrared; Spectral subset selection; Subset selection; Visible near-infrared; Visible, near-infrared, and short-wave infrared spectroscopy; VNIR spectroscopy; Soils; zinc; absorption; Advanced Hyperspectral Imaging; agricultural land; Article; concentration (parameter); hyperspectral imaging; measurement accuracy; near infrared spectroscopy; optical spectroscopy; prediction; remote sensing; short wave infrared spectroscopy; soil",Article,Scopus,2-s2.0-85124591826
"Parra J.S., de Souza Z.M., de Oliveira S.R.M., Farhate C.V.V., Marques J., Júnior, Siqueira D.","57457528500;6603234996;57457621800;56668772700;57201547948;35225259100;","Phosphorus adsorption prediction through Decision Tree Algorithm under different topographic conditions in sugarcane fields",2022,"Catena","213",,"106114","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124819522&doi=10.1016%2fj.catena.2022.106114&partnerID=40&md5=8e93f41fe7dd1e00c7186da3694bb005","Phosphorus availability in the soil is essential for plant growth. In Brazil, phosphorous is poorly available in the soil due to its high adsorption in the form of phosphates. This phenomenon requires much studying to assist in the nutritional management of crops. To that end, predicting the fraction of adsorbed phosphorus can be approximated by using attributes that influence soil formation and structure. This study aimed to predict soil phosphorus adsorption based on soil attributes in sugarcane crops with different relief types using data mining techniques. The experiment was carried out in sugarcane agricultural areas, experimental plots with differentiated relief (concave or convex), and identical agricultural practices. The soil was classified as an Alfisol with udic moisture (Udalf) regime and medium to clayey texture. The dataset constituted a matrix of 4580 observations. The analyzed variables corresponded to the chemical, physical, geophysical, and mineralogical attributes in the 0–0.2 m topsoil. Data analysis was carried out based on a decision tree induction model, with an 85% accuracy rate and a high level of agreement between variables. The decision tree recognized magnetic susceptibility as the attribute with the most significant influence on the prediction of soil phosphorus adsorption, validating the relation among adsorption processes and the magnetic properties of oxide minerals characteristic of Brazilian agricultural regions. © 2022 Elsevier B.V.","Classification techniques; Data mining; Macronutrients; Magnetic susceptibility; Soil attributes","adsorption; agricultural land; Alfisol; algorithm; data mining; phosphorus; prediction; sugar cane; topography; Brazil",Article,Scopus,2-s2.0-85124819522
"Fonseca-Galindo J.C., de Castro Surita G., Neto J.M., de Castro C.L., Lemos A.P.","57214693066;57221148565;57215378286;25929885000;25638788000;","A multi-agent system for solving the Dynamic Capacitated Vehicle Routing Problem with stochastic customers using trajectory data mining",2022,"Expert Systems with Applications","195",,"116602","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124583373&doi=10.1016%2fj.eswa.2022.116602&partnerID=40&md5=c0d3ef5d11f638f6c0f052d131e51198","The worldwide growth of e-commerce has created new challenges for logistics companies, such as delivering products quickly and cheaply. This paper presents a heuristic to solve the last-mile route creation problem dynamically. The heuristic is based on a multi-agent system integrated with trajectory data mining techniques to extract territorial patterns and use them to solve the Dynamic Capacitated Vehicle Routing Problem with Stochastic Customers. Our solution approach is focused on a linear-time heuristic that depends only on the Warehouse system configurations and not on the total number of packages processed, which is suitable for express delivery logistics companies that must process a large number of packages per day. We compare our proposal with benchmark algorithms from the literature; additionally, we evaluate its performance and robustness under different scenarios. Results show that our solution approach is effective for scenarios in which routes must be set dynamically from a continuous stream of packages. © 2022","Big Data; Data mining; Dynamic Capacitated Vehicle Routing Problem with stochastic customers; E-commerce logistics; Multi-agent systems","Benchmarking; Big data; Electronic commerce; Multi agent systems; Sales; Stochastic systems; Vehicle routing; Vehicles; Capacitated vehicle routing problem; Data-mining techniques; Dynamic capacitated vehicle routing problem with stochastic customer; E- commerces; E-commerce logistic; Last mile; Logistics company; Solution approach; Stochastics; Trajectory data minings; Data mining",Article,Scopus,2-s2.0-85124583373
"Johansson U., Sönströd C., Löfström T., Boström H.","7006799558;6506289300;14042012800;7006594020;","Rule extraction with guarantees from regression models",2022,"Pattern Recognition","126",,"108554","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124506084&doi=10.1016%2fj.patcog.2022.108554&partnerID=40&md5=2b7ebcf9a0b62bafe81afbbafa373caf","Tools for understanding and explaining complex predictive models are critical for user acceptance and trust. One such tool is rule extraction, i.e., approximating opaque models with less powerful but interpretable models. Pedagogical (or black-box) rule extraction, where the interpretable model is induced using the original training instances, but with the predictions from the opaque model as targets, has many advantages compared to the decompositional (white-box) approach. Most importantly, pedagogical methods are agnostic to the kind of opaque model used, and any learning algorithm producing interpretable models can be employed for the learning step. The pedagogical approach has, however, one main problem, clearly limiting its utility. Specifically, while the extracted models are trained to mimic the opaque, there are absolutely no guarantees that this will transfer to novel data. This potentially low test set fidelity must be considered a severe drawback, in particular when the extracted models are used for explanation and analysis. In this paper, a novel approach, solving the problem with test set fidelity by utilizing the conformal prediction framework, is suggested for extracting interpretable regression models from opaque models. The extracted models are standard regression trees, but augmented with valid prediction intervals in the leaves. Depending on the exact setup, the use of conformal prediction guarantees that either the test set fidelity or the test set accuracy will be equal to a preset confidence level, in the long run. In the extensive empirical investigation, using 20 publicly available data sets, the validity of the extracted models is demonstrated. In addition, it is shown how normalization can be used to provide individualized prediction intervals, thus providing highly informative extracted models. © 2022 The Author(s)","Conformal prediction; Explainable AI; Interpretability; Predictive regression; Rule extraction","Conformal mapping; Data mining; Extraction; Forecasting; Learning algorithms; Conformal predictions; Explainable AI; Interpretability; Prediction interval; Predictive models; Predictive regression; Regression modelling; Rules extraction; Test sets; Users' acceptance; Regression analysis",Article,Scopus,2-s2.0-85124506084
"González-Pardo J., Ceballos-Santos S., Manzanas R., Santibáñez M., Fernández-Olmo I.","57449642500;57384877400;55340091000;22942251600;57204446766;","Estimating changes in air pollutant levels due to COVID-19 lockdown measures based on a business-as-usual prediction scenario using data mining models: A case-study for urban traffic sites in Spain",2022,"Science of the Total Environment","823",,"153786","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124457557&doi=10.1016%2fj.scitotenv.2022.153786&partnerID=40&md5=30a0a6d3161c40206c433b131437a0ee","In response to the COVID-19 pandemic, governments declared severe restrictions throughout 2020, presenting an unprecedented scenario of reduced anthropogenic emissions of air pollutants derived mainly from traffic sources. To analyze the effect of these restrictions derived from COVID-19 pandemic on air quality levels, relative changes in NO, NO2, O3, PM10 and PM2.5 concentrations were calculated at urban traffic sites in the most populated Spanish cities over different periods with distinct restrictions in 2020. In addition to the changes calculated with respect to the observed air pollutant levels of previous years (2013–2019), relative changes were also calculated using predicted pollutant levels for the different periods over 2020 on a business-as-usual scenario using Multiple Linear Regression (MLR) models with meteorological and seasonal predictors. MLR models were selected among different data mining techniques (MLR, Random Forest (RF), K-Nearest Neighbors (KNN)), based on their higher performance and accuracy obtained from a leave-one-year-out cross-validation scheme using 2013–2019 data. A q-q mapping post-correction was also applied in all cases in order to improve the reliability of the predictions to reproduce the observed distributions and extreme events. This approach allows us to estimate the relative changes in the studied air pollutants only due to COVID-19 restrictions. The results obtained from this approach show a decreasing pattern for NOx, with the largest reduction in the lockdown period above −50%, whereas the increase observed for O3 contrasts with the NOx patterns with a maximum increase of 23.9%. The slight reduction in PM10 (−4.1%) and PM2.5 levels (−2.3%) during lockdown indicates a lower relationship with traffic sources. The developed methodology represents a simple but robust framework for exploratory analysis and intervention detection in air quality studies. © 2022 The Authors","Air quality; COVID-19; K-Nearest Neighbors; Multiple Linear Regression; q-q mapping; Random Forest","Data mining; Decision trees; Linear regression; Locks (fasteners); Mapping; Motion compensation; Nearest neighbor search; Nitrogen oxides; Urban growth; Air pollutants; Business-as-usual; COVID-19; K-near neighbor; Multiple linear regressions; Nearest-neighbour; Pollutant levels; Q-q mapping; Random forests; Urban traffic; Air quality; air pollutant; Article; coronavirus disease 2019; cross validation; human; lockdown; particulate matter 10; particulate matter 2.5; prediction; random forest; travel restriction",Article,Scopus,2-s2.0-85124457557
"Yang P., Hao S., Han M., Xu J., Yu S., Chen C., Zhang H., Ning K.","57191904781;57189322313;56197282800;57192428132;57194537975;57205705868;55586875600;8887034300;","Analysis of antibiotic resistance genes reveals their important roles in influencing the community structure of ocean microbiome",2022,"Science of the Total Environment","823",,"153731","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124421263&doi=10.1016%2fj.scitotenv.2022.153731&partnerID=40&md5=1a7f7c262706bf8308c8ebb9a11af319","Antibiotic resistance gene (ARG) content is a well-established driver of microbial abundance and diversity in an environment. By reanalyzing 132 metagenomic datasets from the Tara Oceans project, we aim to unveil the associations between environmental factors, the ocean microbial community structure and ARG contents. We first investigated the structural patterns of microbial communities including both prokaryotes such as bacteria and eukaryotes such as protists. Additionally, several ARG-dominant horizontal gene transfer events between Protist and Prokaryote have been identified, indicating the potential roles of ARG in shaping the ocean microbial communities. For a deeper insight into the role of ARGs in ocean microbial communities on a global scale, we identified 1926 unique types of ARGs and discovered that the ARGs are more abundant and diverse in the mesopelagic zone than other water layers, potentially caused by limited resources. Finally, we found that ARG-enriched genera were often more abundant compared to their ARG-less neighbors in the same environment (e.g. coastal oceans). A deeper understanding of the ARG-microbiome relationships could help in the conservation of the oceanic ecosystem. © 2022","Antibiotic resistance gene; Data-mining; Human impact; Marine microbiome","Antibiotics; Data mining; Gene transfer; Genes; Oceanography; Antibiotic resistance genes; Community structures; Gene content; Human impact; Marine microbiome; Metagenomics; Microbial abundances; Microbial communities; Microbial diversity; Microbiome; Microorganisms; anthropogenic effect; antibiotic resistance; bacterium; community structure; data mining; gene; marine ecosystem; microbial community; protist; antibiotic resistance; Article; community structure; data mining; environmental factor; environmental protection; metagenomics; microbial community; nonhuman",Article,Scopus,2-s2.0-85124421263
"Cabello Ruiz R., Jiménez Ramírez A., Escalona Cuaresma M.J., González Enríquez J.","57447008100;53979785300;57447630400;57447138400;","Hybridizing humans and robots: An RPA horizon envisaged from the trenches",2022,"Computers in Industry","138",,"103615","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124409289&doi=10.1016%2fj.compind.2022.103615&partnerID=40&md5=d991e897982bf72b4803111527848e8a","After the initial hype on RPA, companies have more realistic expectations of this technology. Its current mature vision relegates the end-to-end robotic automation to a less suitable place and considers the human-robot collaboration as the most natural way for automating robotic processes in real-world settings. This hybrid RPA implies a vertical segmentation of process activities, i.e., some activities are conducted by humans while robots do others. The literature lacks a general method that considers the technical aspect of the solution, the psychological impact of the automation, and the governance mechanisms that a running hybrid process requires. In this sense, this paper proposes an iterative method dealing with all these aspects and results from a series of industrial experiences. Additionally, the paper deeply discusses the role of process mining in this kind of method and how it can continuously boost its iterations. The initial validation of the method in real-world processes reports substantial benefits in terms of efficiency. © 2022 The Author(s)","Computer-human interaction; Process mining; Robotic process automation","Human computer interaction; Human robot interaction; Iterative methods; Robotics; 'current; Computer Human Interaction; End to end; Human-robot collaboration; Process automation; Process mining; Real world setting; Robotic automation; Robotic process automation; Vertical segmentations; Data mining",Article,Scopus,2-s2.0-85124409289
"Liu J., Zhang R., Liu W., Zhang Y., Gu D., Tong M., Wang X., Xue J., Wang H.","57446198800;57445774500;57222319367;57444688200;57446198900;57445341900;57444688300;57445556000;57189379030;","Context2Vector: Accelerating security event triage via context representation learning",2022,"Information and Software Technology","146",,"106856","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124295479&doi=10.1016%2fj.infsof.2022.106856&partnerID=40&md5=37a14cd29f97e0b58f97f423d184e6c2","Context: Security teams are overwhelmed by thousands of alerts and events everyday, which are comprehensively collected for threat analysis in security operations center. Although methods based on rules, intelligence and data mining are utilized, the alert fatigue situation is still a challenging problem, slowing down the overall threat investigation process. Objective: ‘Event polysemy’ phenomenon broadly exists in large-scale event dataset, which means that events of the same category can reveal different purposes in different contexts. This paper aims at exploring, revealing and evaluating the latent patterns embedding in the event contexts, to gain insight on context semantics and reduce manual intervention in event triage tasks. Method: A context representation learning based method, named Context2Vector, is proposed. Contexts are extracted from multiple behavioral views. Then, both dense event representations and sparse topic representations are learnt at the same time and in the same space. A human-in-the-loop topic annotation process is involved and finally, a context deviation detection based method is integrated to generate explainable and informative labels for automated context semantic decoding. Results: Various experiments are conducted on a enterprise-scale event dataset. The topic annotation, context related feature importance and top-N event ranking evaluation results show that Context2Vector outperforms traditional methods on the high-risk event identification problems, improving the attacker recall rate by up to 2.25 times within limited events to be investigated. Conclusion: It is concluded that event contexts imply practicable and abundant information in regard to behaviors and intents of real threat actors. More precise profiling of network entities can be extracted from contexts, compared to rules, intelligence, and anomaly detectors used in practice. © 2022 Elsevier B.V.","Context modeling; Event triage; Representation learning; Topic model","Large dataset; Learning systems; Semantics; Context models; Context representation; Event context; Event triage; Investigation process; Representation learning; Security events; Security operation center; Threats analysis; Topic Modeling; Data mining",Article,Scopus,2-s2.0-85124295479
"Benaissa M.","57212990187;","Theoretical investigation of structural, energetic and magnetic properties of B2 type MnM alloys, DFT and data mining approach",2022,"Computational Condensed Matter","31",,"e00656","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124284712&doi=10.1016%2fj.cocom.2022.e00656&partnerID=40&md5=4e2a33ffbb239d7dbf7223b75c76afbd","We present crystal structure, energetic stability, magnetic properties and charge transfer of MnM (M: Li, Mg, Al, Ti, V, Cr, Fe, Co, Ni, Cu, Zn, Sr, Y, Rh, Pd, Cd, In, Sn, La, Hf, Ta, W, Re, Os, Ir, Au, Hg, Tl and Bi) equiatomic alloys in the B2 type crystal structure, carried out using density functional theory (DFT) within the generalized gradient approximation framework. The calculated lattice parameters of the studied compounds are in correlation with the M metallic radius rM. The formation energy calculations revealed the energetic stability of MnTa and MnHf alloys for the first time. The studied compounds exhibit different magnetic behavior with magnetic moments ranging between 0 μB and 4 μB per formula unit. The energy difference between the ferri/ferromagnetic and anti-ferri/ferromagnetic coupling of the spins has also been calculated in order to obtain better insight into the ground state magnetic exchange coupling. Finally, the previously calculated properties were studied using the data mining tool principal component analyses PCA to gain more insight into the trends and the correlations between the properties of these different alloys. © 2022 Elsevier B.V.","Ab-initio calculations; Data mining; DFT; Magnetism; Mn alloys; Rare earth; Spintronic",,Article,Scopus,2-s2.0-85124284712
"Liu T., Israel M.","57234943200;55781226100;","Uncovering students’ problem-solving processes in game-based learning environments",2022,"Computers and Education","182",,"104462","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124268368&doi=10.1016%2fj.compedu.2022.104462&partnerID=40&md5=22d9ac7b3c68ed3a01a8eb577a8dc9a1","As one of the most desired skills for contemporary education and career, problem-solving is fundamental and critical in game-based learning research. However, students' implicit and self-controlled learning processes in games make it difficult to understand their problem-solving behaviors. Observational and qualitative methods, such as interviews and exams, fail to capture students' in-process difficulties. By integrating data mining techniques, this study explored students' problem-solving processes in a puzzle-based game. First, we applied the Continuous Hidden Markov Model to identify students' problem-solving phases and the transition probabilities between these phases. Second, we employed sequence mining techniques to investigate problem-solving patterns and strategies facilitating students' problem-solving processes. The results suggested that most students were stuck in certain phases, with only a few able to transfer to systematic phases by applying efficient strategies. At the beginning of the puzzle, the most popular strategy was testing one dimension of the solution at each attempt. In contrast, the other two strategies (remove or add untested dimensions one by one) played pivotal roles in promoting transitions to higher problem-solving phases. The findings of this study shed light on when, how, and why students advanced their effective problem-solving processes. Using the Continuous Hidden Markov Model and sequence mining techniques, we provide considerable promise for uncovering students' problem-solving processes, which helps trigger future scaffolds and interventions to support students’ personalized learning in game-based learning environments. © 2022 Elsevier Ltd","Data science applications in education; Games; Human-computer interface","Computer aided instruction; Computer games; Data mining; Education computing; Hidden Markov models; Learning systems; Scaffolds; Application in education; Continuous hidden Markov model; Data science application in education; Game; Game-based learning environments; Human computer interfaces; Problem solving process; Problem-solving; Science applications; Sequence mining; Students",Article,Scopus,2-s2.0-85124268368
"Pang Y., Zhou X., Zhang J., Sun Q., Zheng J.","57203515096;55743249000;36995158700;57443162000;57443732700;","Hierarchical electricity time series prediction with cluster analysis and sparse penalty",2022,"Pattern Recognition","126",,"108555","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124232218&doi=10.1016%2fj.patcog.2022.108555&partnerID=40&md5=45b8f23f4d6b245f6885e1ec14d38ce4","In big data applications, hierarchical time series prediction is an important element of decision-making and concerns the inherent aggregation consistency, which is maintained by reconciliation methods. The paper proposes a novel multiple alternative clustering time series analysis based hierarchical electricity time series prediction method. Instead of adhering the aggregation consistency passively, we first exploit time series mining to construct a hierarchy, and then apply an optimal reconciliation method to improve the prediction accuracy. In particular, k-means clustering method is employed to cluster time series for many times with different k so as to make a large number of time series clusters (patterns), and then the clusters (patterns) based hierarchies are constructed respectively. With the large number of clusters hierarchies and the original geographical hierarchy, an optimal aggregation consistency reconciliation based prediction approach is proposed. Furthermore, the sparse penalty is adapted in our method for “ideal” clusters selection to improve the prediction performance. Compared with the state-of-the-art methods on real-life datasets, our method achieves the improvement of 11.13% and 24.07% accurate one-step ahead forecasts on electricity load and solar power data respectively. © 2022","Data mining; Hierarchical time series forecasting; Machine learning","Cluster analysis; Data mining; Decision making; Forecasting; K-means clustering; Solar energy; Time series analysis; Alternative clustering; Big data applications; Cluster patterns; Clustering time series; Decisions makings; Hierarchical time series forecasting; Time series forecasting; Time series prediction; Time-series analysis; Times series; Machine learning",Article,Scopus,2-s2.0-85124232218
"Li K., Yan D., Liu Y., Zhu Q.","55522528800;57209217131;57224014799;57222351252;","A network-based feature extraction model for imbalanced text data",2022,"Expert Systems with Applications","195",,"116600","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124090688&doi=10.1016%2fj.eswa.2022.116600&partnerID=40&md5=8f3768ef6031acf5b858da0a075c2282","The explosive growth of text data has attracted many researchers to explore the efficient method to extract valuable hidden information. Many technologies, especially deep learning methods, have achieved great success in text analysis. However, the most powerful methods always require a considerable quantity of data for training, which may suffer from imbalanced data in some cases. In this paper, we propose a network-based Convolution Neural Network (NCNN) to mitigate the effect of imbalanced data. The proposed model first generates new synthetic samples for the imbalanced data based on the random walking of the network. Then an extra layer called Polar Layer is introduced to connect the output from the network model of the text to the classical CNN. Two electing strategies (n-NCNN and x-NCNN) are proposed to improve the performance of NCNN further. In the experimental section, the proposed model is applied to Reuters 21578 and WebKb. By comparing with six approaches, we prove the effectiveness of the proposed NCNN model on the imbalanced text data. © 2022 Elsevier Ltd","CNN; Complex Network; Imbalanced Data; Random Walk; Text Analysis","Data mining; Deep learning; CNN; Convolution neural network; Explosive growth; Extraction modeling; Features extraction; Imbalanced data; Network-based; Random Walk; Text analysis; Text data; Complex networks",Article,Scopus,2-s2.0-85124090688
"Li D., Liu J., Feng L., Cheng G., Zeng Y., Dong B., Chen Y.F.","57226876848;56735728200;36782157400;57192589291;57225071672;57225922494;24319858500;","Towards automated extraction for terrestrial laser scanning data of building components based on panorama and deep learning",2022,"Journal of Building Engineering","50",,"104106","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123997412&doi=10.1016%2fj.jobe.2022.104106&partnerID=40&md5=f89783514c39ae9595d88f0e618130ed","Terrestrial laser scanning has been widely used in the dimensional quality assessment (DQA) on building components, but the extraction of component data is mostly done by a specialized software which is not applicable to the building components assembled in a factory. The need for manual operations makes it difficult to fulfill the notion of efficient component manufacturing. Notably, due to the successful applications in image processing, deep learning has been extended to the field of quality inspection and damage diagnosis of building components, which can quickly locate targets in images. As deep learning has excellent performance for target detection and current scanner can produce a panorama with the scan resolution, it is possible to automatically extract the targets using deep learning by correlating the panorama with the scanned data. To the best of our knowledge, little research has been on the combined use of panorama and deep learning for automated data extraction of building components. Therefore, this paper proposes an approach using the panorama and deep learning to automatically extract building component data for DQA. To determine suitable scan parameters, parametric tests are first carried out on scan distance, scan resolution, scan color and scan angle. Experimental test is then conducted on a deformed concrete-filled steel tubular column to validate the proposed approach. © 2022 Elsevier Ltd","Building components; Deep learning; Panorama; Terrestrial laser scanning","Data mining; Extraction; Laser applications; Scanning; Steel beams and girders; Automated extraction; Component based; Deep learning; Dimensional quality; Laser scanning data; Manual operations; Panorama; Quality assessment; Specialized software; Terrestrial laser scanning; Deep learning",Article,Scopus,2-s2.0-85123997412
"Kim S.-H., Lee D.-H., Kim K.-J.","57215072284;55698968500;26662511100;","EWMA-PRIM: Process optimization based on time-series process operational data using the exponentially weighted moving average and patient rule induction method",2022,"Expert Systems with Applications","195",,"116606","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123986979&doi=10.1016%2fj.eswa.2022.116606&partnerID=40&md5=7303d7b31269c14f850f423be1448d8e","Currently, many manufacturing companies are obtaining a large amount of operational data from manufacturing lines due to advances in information technology. Thus, various data mining methods have been applied to analyze the data to optimize the manufacturing process. Most of the existing data mining-based optimization methods assume that the relationships between input and response variables do not change over time. However, because it often takes a long time to collect a large amount of operational data, the relationships may change during the data collection. In such a case, the operational data is regarded as time-series data and recent data should be regarded to be more important than old data. In this study, we employed a patient rule induction method (PRIM), which is one of the data mining methods applied for process optimization. In addition, we employed an exponentially weighted moving average (EWMA) statistic to assign a larger weight to the recent data. Based on the PRIM and EWMA, the proposed method attempts to obtain optimal intervals for input variables where current performance of the response is better. The proposed method is illustrated with a hypothetical example and validated through a real case study of a steel manufacturing process. © 2022","Big data; Data mining; Exponentially weighted moving average; Manufacturing process optimization; Patient rule induction method; Time-series","Big data; Optimization; Process control; Time series; Data mining methods; Exponentially weighted moving average; Large amounts; Manufacturing process; Manufacturing process optimization; Operational data; Patient rule induction method; Process optimisation; Rule Induction Methods; Times series; Data mining",Article,Scopus,2-s2.0-85123986979
"Alipour-Vaezi M., Aghsami A., Jolai F.","57218718976;56928367000;56218703500;","Prioritizing and queueing the emergency departments’ patients using a novel data-driven decision-making methodology, a real case study",2022,"Expert Systems with Applications","195",,"116568","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123891797&doi=10.1016%2fj.eswa.2022.116568&partnerID=40&md5=31f1740c2f1fe08c4d8807a510252a5f","One of the principal problems in epidemic disruptions like the COVID-19 pandemic is that the number of patients needing hospitals’ emergency departments’ services significantly grows. Since COVID-19 is an infectious disease, any aggregation has to be prevented accordingly. However, few aggregations cannot be prevented, including hospitals. To the best of our knowledge, COVID-19 is a life-threatening disease, especially for people in poor health conditions. Therefore, it sounds reasonable to optimize the health care queuing systems to minimize the infection rate by prioritizing patients based on their health condition so patients with a higher risk of infection will leave the queue sooner. In this paper, relying on data mining models and expert's opinions, we propose a method for patient classification and prioritizing. The optimal number of servers (treatment systems) will be determined by benefiting from a mixed-integer model and the grasshopper optimization algorithm. © 2022 Elsevier Ltd","Classification; COVID-19; Data Mining; Grasshopper Optimization Algorithm; Queueing Systems","Behavioral research; Decision making; Emergency rooms; Health risks; Integer programming; Queueing networks; Queueing theory; Case-studies; COVID-19; Data driven decision; Decision making methodology; Emergency departments; Grasshopper optimization algorithm; Health condition; Optimization algorithms; Queueing system; Real case; Data mining",Article,Scopus,2-s2.0-85123891797
"Shaheen A.M., El-Seheimy R.A., Xiong G., Elattar E., Ginidi A.R.","56719308400;6504618921;55630591700;36131132200;56305614500;","Parameter identification of solar photovoltaic cell and module models via supply demand optimizer",2022,"Ain Shams Engineering Journal","13","4","101705","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123616730&doi=10.1016%2fj.asej.2022.101705&partnerID=40&md5=47aaa709d5e68026bbb8278f2f84691e","The extraction of photovoltaic (PV) module parameters is regarded as a critical topic for assessing the performance of PV energy systems. The Supply-Demand-Based Optimization Algorithm (SDOA) is employed in this work to extract the unknown parameters of PV models. The SDOA mimics the stability and instability modes between the supply and the demand one in order that the quantity and price of commodities converge to the equilibrium point after a specified number of repetitions. It is frequently used to handle complicated nonlinear problems because of its ease of implementation and powerful optimization capabilities. The Triple-Diode Model (TDM) is extensively adopted in PV module mathematical models. The optimal nine TDM parameters are determined for the PVM 752GaAs PV thin film cell, whereas other solar irradiation and temperature values are used for the SQ 150 and MSX 60 modules. When used on the TDM model, the SDOA was used to verify the fitness values and standard deviation errors. Furthermore, the obtained result achieved by SDOA are contrasted with new techniques established in 2020 which are Backtracking Search Algorithm (BSA), Grey Wolf Optimizer (GWO), Bernstein-Levy Search Differential Evolution Algorithm (BSDE), Crow search Optimizer (CSO), and Manta Ray Foraging Optimizer (MRFO). For accurate and consistent results, thirty runs of the algorithm are executed for the modules and the standard deviations of fitness values are less than 1 × 10–18 for the triple diode model. In addition to this, a practical PV power plant system that lie in the Guizhou Power Grid of China is used to validate the efficiency of SDOA compared to other recent algorithms. Thus, SDOA is considered as a competitive optimizer among other reported techniques in the literature or recent techniques for PV parameter extraction. © 2022","Benchmarking solar modules; Practical solar modules; PV parameters extraction; PV triple-diode; Supply-demand-based optimization","Data mining; Diodes; Electric power transmission networks; Evolutionary algorithms; Extraction; Parameter estimation; Parameter extraction; Photoelectrochemical cells; Solar cells; Solar panels; Solar power generation; Statistics; Thin films; Benchmarking solar module; Optimisations; Parameters extraction; Photovoltaic parameter extraction; Photovoltaic parameters; Photovoltaic triple-diode; Photovoltaics; Practical solar module; Solar module; Supply-demand; Supply-demand-based optimization; Optimization",Article,Scopus,2-s2.0-85123616730
"Liu D., Wang D.","57407196600;55899917800;","Evaluation of the synergy degree of industrial de-capacity policies based on text mining: A case study of China's coal industry",2022,"Resources Policy","76",,"102547","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122636079&doi=10.1016%2fj.resourpol.2021.102547&partnerID=40&md5=c54f846ad3882baaf6269ea73df5e874","The problem of overcapacity has become a serious concern, restricting the development of China's economy. Furthermore, the synergy of the de-capacity policy, which is overlooked by the existing research, is a key factor affecting overcapacity governance. Therefore, based on the policy synergy theory, this study innovatively constructs a collaborative evaluation framework and a quantitative model for coal de-capacity policies from horizontal, vertical, and temporal dimensions. In addition, text mining and a quantitative analysis of the collected coal de-capacity policies from 2009 to 2020 are conducted. The results show that the overall degree of synergy in China's coal de-capacity policy is relatively low; in particular, local governments tend to focus on the “self” and “now” when formulating coal de-capacity policies. In regional terms, the synergy degree of the central and local governments significantly differ in each dimension, and the synergy degree in all dimensions of the local government is significantly lower than that of the central government. In dimension terms, compared with horizontal and vertical synergy, temporal synergy has the most significant impact on the comprehensive synergy of de-capacity policies. The above results provide a new decision-making reference for the formulation, design, and optimization of de-capacity policies in China's economy—especially the coal industry. © 2022 Elsevier Ltd","China's industries; De-capacity policies; Synergy degree","Coal industry; Data mining; Decision making; Public policy; Case-studies; Central government; China industry; De-capacity policy; Key factors; Local government; Over capacity; Policy-based; Synergy degree; Text-mining; Coal; coal industry; decision making; local government; policy approach; quantitative analysis; synergism; China",Article,Scopus,2-s2.0-85122636079
"Tabar V.S., Ghassemzadeh S., Tohidi S., Siano P.","57192364441;57195264256;36651646400;6603170751;","Enhancing information security of renewable smart grids by utilizing an integrated online-offline framework",2022,"International Journal of Electrical Power and Energy Systems","138",,"107954","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122633690&doi=10.1016%2fj.ijepes.2022.107954&partnerID=40&md5=02acae51eafe9e289a0259c25c762c3d","Renewable energies are extensively utilized in smart grids. Due to the widespread use of information and communication technologies in such networks, their security has become a critical issue. This paper aims to enhance the information security of renewable smart grids under cyber-physical attacks. In this regard, it is assumed that the non-legitimate agents manipulate the data of solar and wind sensors to deteriorate the safe operation. Here, a stochastic real-time procedure based on the observation-action method is utilized to simulate the behavior of attackers. Then, to improve the security and mitigate the impact of such a vulnerability, an integrated framework composed of offline and online units is designed. To construct the offline framework, a data mining process including k-nearest neighbour and support vector machine algorithms is implemented based on real historical data. Furthermore, the online framework tracks the real-time data according to a sensor pre-secured by a firewall. The results show that the proposed framework is capable to relieve the influence of cyber-physical attacks where at least 79% of success rate will be achievable under simultaneous false data injection attacks. © 2022 Elsevier Ltd","Data mining; False data; Information security; Machine learning; Renewable smart grid","Computer crime; Cyber attacks; Cyber Physical System; Data mining; E-learning; Information use; Nearest neighbor search; Network security; Smart power grids; Stochastic systems; Support vector machines; Critical issues; Cyber physicals; False data; Information and Communication Technologies; Offline; Physical attacks; Renewable energies; Renewable smart grid; Smart grid; Solar and winds; Electric power transmission networks",Article,Scopus,2-s2.0-85122633690
"Lee S., Ryu Y., Park H.-J., Lee I.-S., Chae Y.","57393118200;57226285882;7601566951;55575603100;57374739400;","Characteristics of five-phase acupoints from data mining of randomized controlled clinical trials followed by multidimensional scaling",2022,"Integrative Medicine Research","11","2","100829","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122096325&doi=10.1016%2fj.imr.2021.100829&partnerID=40&md5=9b5d929e222b8199e6755dd2ef92f58a","Background: An unbiased assessment of clinical outcomes may provide greater insight into the characteristics of individual acupoints. In this study, we used machine-learning methods to examine clinical trial data for diseases treated using prescribed five-phase acupoint patterns. Methods: We performed a search of acupuncture treatment regimens used in randomized controlled trials included in the Cochrane Database of Systematic Reviews. The frequencies of 60 five-phase acupoints were calculated based on 421 clinical trials on 30 diseases. The characteristics of prescribed five-phase acupoints were further analyzed using multidimensional scaling and K-means clustering. Results: Among the five-phase acupoints, stream and sea acupoints were the most widely used, with well, spring, and river acupoints less common. Multidimensional scaling and cluster analysis revealed that the LR3, ST36, GB34, BL60, KI3, LI11, and HT7 acupoints exhibited distinct characteristics based on distances representing the similarity between acupoint indications. Conclusions: The results suggest that stream and sea acupoints exhibit distinct characteristics compared to the other acupoints. Such data-driven approaches will improve our understanding of five-phase acupoints and facilitate the establishment of new models of analysis and educational resources for major acupoint characteristics. © 2021","Acupoint indication; Clinical trials; Clustering; Data mining; Multidimensional scaling",,Article,Scopus,2-s2.0-85122096325
"Wu H., Peng Z., Guo S., Yang Y., Xiao B.","57201983074;57169325400;7403650425;35308464100;35200159800;","VQL: Efficient and Verifiable Cloud Query Services for Blockchain Systems",2022,"IEEE Transactions on Parallel and Distributed Systems","33","6",,"1393","1406",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115713757&doi=10.1109%2fTPDS.2021.3113873&partnerID=40&md5=059d703c83611ba2fafb3fd34435a4de","Despite increasingly emerging applications, a primary concern for blockchain to be fully practical is the inefficiency of data query. Direct queries on the blockchain take much time by searching every block, while indirect queries on a blockchain database greatly degrade the authenticity of query results. To conquer the authenticity problem, we propose a Verifiable Query Layer (VQL) that can be deployed in the cloud to provide both efficient and verifiable data query services for blockchain systems. The middleware layer extracts data from the underlying blockchain system and efficiently reorganizes them in databases. To prevent falsified data from being stored in the middleware, a cryptographic fingerprint is calculated based on each constructed database. The database fingerprint will be first verified by miners and then written into the blockchain. Moreover, public users can verify the entire databases or several databases that interest them in the middleware layer. We implement VQL together with the verification schemes and conduct extensive experiments based on a practical blockchain system. The evaluation results demonstrate that VQL can efficiently support various data query services and guarantee the authenticity of query results for blockchain systems. © 1990-2012 IEEE.","blockchain systems; Cloud query service; data authenticity; verifiable query","Authentication; Bitcoin; Blockchain; Distributed computer systems; Middleware; Peer to peer networks; Query languages; Query processing; Search engines; Block-chain; Blockchain system; Cloud query service; Data authenticity; Data query; Peer-to-peer computing; Query efficiency; Query results; Query service; Data mining",Article,Scopus,2-s2.0-85115713757
"Xie S., Ding F., Chen S., Wang X., Li Y., Ma K.","57224162766;57451809200;57221618761;57218289611;57223665308;57452572400;","Prediction of soil organic matter content based on characteristic band selection method",2022,"Spectrochimica Acta - Part A: Molecular and Biomolecular Spectroscopy","273",,"120949","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124630326&doi=10.1016%2fj.saa.2022.120949&partnerID=40&md5=b149c95b208103713fcadb34b4d11ce4","Soil organic matter (SOM) is a key index for evaluating soil fertility and plays a vital role in the terrestrial carbon cycle. Visible and near-infrared (Vis-NIR) spectroscopy is an effective method for determining soil properties and is often used to predict SOM content. However, the key prerequisite for effective prediction of SOM content by Vis-NIR spectroscopy lies in the selection of appropriate preprocessing methods and effective data mining techniques. Therefore, in this study, six commonly used spectral preprocessing methods and effective characteristic band selection methods were selected to process the spectrum to predict SOM content. This study aims to determine a stable spectral preprocessing method and explore the predictive performance of different characteristic band selection methods. The results showed that: (i) The first derivative (FD) is the most stable spectral preprocessing method that can effectively improve the spectral characteristic information and the prediction effect of the model. (ii) The prediction effect of SOM content based on characteristic band selection methods is generally better than the full-spectra data. (iii) The precision of FD preprocessing spectrum combined with successive projections algorithm (SPA) in the partial least square regression prediction model of SOM content is the best. (iv) Although the prediction effect of the model based on the optimal band combination algorithm is slightly lower than that of SPA, it shows stable prediction performance, which provides a feasible method for SOM content prediction. In summary, the characteristic band selection method combined with FD can significantly improve the prediction accuracy of SOM content. © 2022","Characteristic band selection; Soil organic matter; Spectral preprocessing method; Vis-NIR spectroscopy","Biogeochemistry; Data mining; Finite difference method; Infrared devices; Near infrared spectroscopy; Organic compounds; Soils; Bands selections; Characteristic band selection; Characteristic bands; Pre-processing method; Selection methods; Soil organic matter contents; Soil organic matters; Spectral preprocessing; Spectral preprocessing method; Visible and near-infrared spectroscopy; Forecasting",Article,Scopus,2-s2.0-85124630326
"Gou J., Sun L., Du L., Ma H., Xiong T., Ou W., Zhan Y.","57357456900;57432260700;56481023300;57192720841;37361709600;55613280300;7102620105;","A representation coefficient-based k-nearest centroid neighbor classifier",2022,"Expert Systems with Applications","194",,"116529","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123731083&doi=10.1016%2fj.eswa.2022.116529&partnerID=40&md5=f99821388c32a10c0a20b99d7cde4467","K-nearest neighbor rule (KNN) has been regarded as one of the top 10 methods in the field of data mining. Due to its simplicity and effectiveness, it has been widely studied and applied to various classification tasks. In this article, we develop a novel representation coefficient-based k-nearest centroid neighbor method (RCKNCN), which aims to further improve the classification performance and reduce the method's sensitivity to the neighborhood size k, especially in the cases of small sample size. Different from existing KNN-based methods, RCKNCN is able to capture both the proximity and the geometry of k-nearest neighbors, and learn to differentiate the contribution of each neighbor to the classification of a testing sample through a linear representation method. Moreover, under the RCKNCN framework, we also propose a novel weighted majority voting algorithm using the representation coefficients associated with individual nearest centroid neighbors, which are deemed to hold more discriminative information of the neighbors. To fully study the classification performance of RCKNCN, we compare it with the state-of-the-art KNN-based methods on many data sets that are widely used in the literature. The extensive experiments demonstrate the effectiveness and robustness of our method in various classification tasks. © 2022 Elsevier Ltd","K-nearest centroid neighbor rule; K-nearest neighbor rule; Nearest centroid neighborhood; Pattern recognition","Classification (of information); Data mining; Motion compensation; Nearest neighbor search; Pattern recognition; Classification performance; Classification tasks; K nearest centroid neighbors; K-nearest centroid neighbor rules; K-Nearest-neighbor rule; Learn+; Nearest centroid neighborhoods; Neighborhood size; Rule-based method; Small Sample Size; k-nearest neighbors",Article,Scopus,2-s2.0-85123731083
"Kong L., Yang Q., Zhou Q., Xing J., Sun X., Zou R.","57432170200;8641105000;57193616209;57210525443;57431970500;57431367800;","Embedding knowledge into BIM: A case study of extending BIM with firefighting plans",2022,"Journal of Building Engineering","49",,"103999","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123713425&doi=10.1016%2fj.jobe.2022.103999&partnerID=40&md5=d1d0df0f51d3b24d6cfc184085012620","Building information modelling (BIM) has revolutionized production and operations management in the construction industry. Currently, however, the BIM environment can integrate only basic static information. Expert knowledge information to guide the building activity process is lacking. Consequently, with the increasing complexity of building services, BIM has difficulty adapting to the needs of rapid collaboration in the building operation process. To solve this problem, first, we present the basic knowle-dge-embedded model. The model provides services for knowledge generation and knowledge embedding. Moreover, taking BIM-based fire emergency management as an example, the formal modelling and credibility verification of knowledge are adopted to standardize the knowledge generated from experts to reduce future input errors and interpretation. Second, to provide the capability of process knowledge representation in BIM, we embedded the knowledge of the fire emergency management process into BIM by extending the Industry Foundation Classes (IFC) standard and providing a BIM knowledge extraction algorithm using the IFC standard, thereby extracting the control strategy from BIM to achieve the reasoning of knowledge. Finally, we prototyped BIM-Fire in a real building scenario and validated the effectiveness of the method. The automatic control of knowledge-driven BIM-based activities is realized. © 2022","Automatic control; BIM; Embedded knowledge; Strategy extraction","Architectural design; Automation; Construction industry; Data mining; Embeddings; Knowledge representation; Process control; Risk management; Building Information Modelling; Case-studies; Embedded knowledge; Embeddings; Emergency management; Fire emergencies; Firefighting plans; Industry foundation classes standard; Model-based OPC; Strategy extraction; Extraction",Article,Scopus,2-s2.0-85123713425
"Karaca M., Alvarado M.M., Gahrooei M.R., Bihorac A., Pardalos P.M.","57223340824;57190568249;36924794000;6602479553;7005330875;","Frequent pattern mining from multivariate time series data",2022,"Expert Systems with Applications","194",,"116435","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123591887&doi=10.1016%2fj.eswa.2021.116435&partnerID=40&md5=ce1ba6619e2c2a80c4a99d869fe2961f","Discrete sequential data is the collection of an ordered series of discrete events or abstractions from a set of data points collected at different time periods. These processes are one of the most common and important process types encountered in various domains. It is customary to discover similarities and to detect the indicators of anomalies in multivariate form in a supervised setting. In this paper, we first use an effective data transformation technique that transforms multivariate time series into multivariate sequences and use a tree-based method to mine frequent patterns from multivariate time series. However, this problem is costly in terms of solution time and memory consumption. Specifically, this study aims to improve computational efficiency for memory with reasonable solution time. We demonstrate the efficiency of this algorithm on standard datasets and then apply the method to a real healthcare problem. © 2022 Elsevier Ltd","Discrete sequential data; Electronic healthcare data; Frequent pattern mining; Knowledge discovery; Multivariate time series","Computational efficiency; Efficiency; Health care; Metadata; Time series; Trees (mathematics); Discrete abstraction; Discrete events; Discrete sequential data; Electronic healthcare; Electronic healthcare data; Frequent patterns minings; Multivariate time series; Sequential data; Solution time; Time-series data; Data mining",Article,Scopus,2-s2.0-85123591887
"Li H., Jia R., Wan X.","36608494900;57351987700;56717137900;","Time series classification based on complex network",2022,"Expert Systems with Applications","194",,"116502","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123451841&doi=10.1016%2fj.eswa.2022.116502&partnerID=40&md5=d974028e80c0149e978635bf4aadbf95","Time series classification is an important topic in data mining. Time series classification methods have been studied by many researchers. A feature-weighted classification method is proposed based on complex network. There are four stages in the proposed classification algorithm. First, we obtain visible points and edge weights by applying an improved weighted visibility graph algorithm to transform the original time series into weighted complex networks. Next, weighted features are calculated based on visible points and weights, and unable-weighted features are obtained by utilizing NetworkX. After that, we combine two types of features to obtain a total features matrix. We then reconsider the weight of each independent feature by utilizing the importance function of random forest to measure the relative importance of features and construct a importance weighted features matrix. Finally, we utilize a traditional classifier called random forest to cluster the weighted features matrix and generate classification results. Simultaneously, a novel feature weight calculation method is applied during the classification process. We compare the proposed method to other classification methods and the results indicate that the proposed method can improve classification accuracy for time series datasets. © 2022 Elsevier Ltd","Complex network; Data mining; Random forest; Time series classification","Classification (of information); Data mining; Decision trees; Matrix algebra; Time series; Classification algorithm; Classification methods; Edge weights; Feature matrices; Graph algorithms; Random forests; Time series classifications; Times series; Visibility graphs; Weighted features; Complex networks",Article,Scopus,2-s2.0-85123451841
"Soleimani S., Eckels S., Campbel M.","55983046700;6602079285;57211960513;","Parametric study and application of a data-mining model in 2D and 3D micro-fin tubes",2022,"Applied Thermal Engineering","207",,"118165","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124507205&doi=10.1016%2fj.applthermaleng.2022.118165&partnerID=40&md5=2761cd219bf84da063a2e5aaaeacfe93","Micro-fins (&lt;0.5 mm tall) are an engineered roughness with the advantage of reducing thermal resistance and the disadvantage of increased pressure drop when applied inside a tube in heat-exchange applications. The competing effects highlight the need for careful optimization that identifies micro-fin surfaces with the potential to match heat exchanger design needs. Hence, the objectives of this study are chosen to enable efficient optimization in future studies. The main goals are: (1) study the effects of micro-fin design variables on heat transfer and friction factors; and (2) evaluate the potential of a data-mining model as a surrogate of computational fluid dynamic (CFD) models in 2 dimensional (D) and 3D micro-fin tubes. This study applied conductive and convective heat transfer and turbulent fluid flow simulation to evaluate the performance of different 2D and 3D micro-fin tubes. Different configurations were generated by varying micro-fin height (e), helix angle (α), number of starts (Nf), and discontinuity features. Coupled solid and periodic fluid domains were applied in ANSYS Fluent 19.1. Performance was mapped for 210 different simulations (including a smooth tube) using a realizable k-ε turbulence model at Reynolds number (Re) of 48,928. Two different least squares-support vector regression (LS-SVR) models were employed to estimate the Colburn j factor as a function of geometric variables and the Fanning friction factor (f) as a function of geometric variable and j factor. Results of the parametric study showed that the best 2D micro-fin tube can enhance efficiency index (η) up to 1.18. Results of the LS-SVR model showed that the percentage of average absolute error (AAE) between simulated and estimated j and f factors are 2.05% and 2.93% for 3D micro-fin tubes, respectively. © 2022","2D and 3D Micro-fin tubes; Efficiency index; Least squares support vector regression (LS-SVR); Turbulent numerical simulation","Computational fluid dynamics; Data mining; Efficiency; Fins (heat exchange); Flow of fluids; Friction; Function evaluation; Heat resistance; Pressure drop; Reynolds number; Three dimensional computer graphics; Tubes (components); Turbulence models; 2d and 3d micro-fin tube; Data mining models; Efficiency index; J factors; Least square support vector regression; Least squares support vector regression; Micro fins; Microfin tube; Parametric study; Turbulent numerical simulation; Heat convection",Article,Scopus,2-s2.0-85124507205
"Kong Q., Zhang X., Xu W., Long B.","57020844800;57239196500;55730741100;57208017996;","A novel granular computing model based on three-way decision",2022,"International Journal of Approximate Reasoning","144",,,"92","112",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124988963&doi=10.1016%2fj.ijar.2022.01.015&partnerID=40&md5=5f6d764f3de33ead13a755a8e0f1e3b0","Granular computing and three-way decision are two very important methods in the field of knowledge discovery and data mining. In this paper, based on the idea of three-way decision, all attributes in the information table first are divided into three disjoint parts named indispensable attributes, rejected attributes and neutral attributes, respectively. According to the three parts of attributes, many basic and important information granules and granular structures can be induced from the information table. Then a novel granular computing model is proposed by the description operator. On the one hand, many mathematical properties related to the model proposed in this paper are systematically discussed. On the other hand, we make a preliminary and meaningful attempt to deal with network security by using this model. In addition, in order to apply the model more conveniently, two algorithms for computing description set, description degree, attribute reduction and reduction degree are developed. Finally, through numerical experiments, the validity of the algorithms and the related factors that affect the effectiveness of the algorithms are discussed in detail. © 2022 Elsevier Inc.","Attribute reduction; Description operator; Granular computing; Information granules; Network security; Three-way decisions","Data mining; Decision tables; Granular computing; Granulation; Information granules; Attribute reduction; Granular computing models; Granular structuress; Granule structure; Information table; Knowledge discovery and data minings; Mathematical properties; Model-based OPC; Networks security; Three-way decision; Network security",Article,Scopus,2-s2.0-85124988963
"Tepe C., Erdim M.","35732398300;57218582292;","Classification of surface electromyography and gyroscopic signals of finger gestures acquired by Myo armband using machine learning methods",2022,"Biomedical Signal Processing and Control","75",,"103588","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124953853&doi=10.1016%2fj.bspc.2022.103588&partnerID=40&md5=236af5957c7ae76ffae7e20e86f727aa","Gestures of the human hand can be identified through processing of surface electromyography (sEMG) signals. The human hand can perform many gestures via manipulation of the fingers. With correct classification of finger gestures, the mobility of a prosthetic hand can be increased and provide greater functionally. In this study, reliable classification was obtained for sEMG finger data acquired from a Myo armband placed on the lower forearm. In order to improve classification, gyroscopic signals, not previously used in other studies, were investigated in the sEMG finger data. Data was acquired from ten normal subjects using the Myo armband to identify 6 finger gestures: thumb, index finger, middle finger, little finger, ring finger and rest. Participants repeated each gesture thirty times. sEMG signals were preprocessed to extract features. 17 features were used in the feature matrix. By using the sequential forward feature selection method, the highest performance feature set was determined. Support Vector Machine, K-Nearest Neighbor and multilayer artificial neural network were used as classification algorithm. The classification was made using the Classification Learner Application and Neural Network Pattern Recognition Tool in Matlab®. The best performance with the features extracted only from sEMG data was 94.40% using the Artificial Neural Networks (ANN) method. The best performance with the features extracted from both sEMG and gyroscopic data was 96.30% (p-value < 0.05)with the ANN method. It is seen that gyroscopic signals can increase classification performance. © 2022","Data processing and classification; Finger gestures; Gyroscopic; Myo armband; sEMG","Biomedical signal processing; Data handling; Data mining; Multilayer neural networks; Nearest neighbor search; Pattern recognition; Support vector machines; Artificial neural network methods; Data processing and classification; Finger gestures; Gyroscopic; Human hands; Machine learning methods; Myo armband; Performance; Surface electromyography; Surface electromyography signals; Classification (of information); adult; article; artificial neural network; classification algorithm; clinical article; controlled study; feature selection; female; finger; forearm; gesture; human; human experiment; index finger; k nearest neighbor; little finger; machine learning; male; middle finger; ring finger; support vector machine; surface electromyography; thumb",Article,Scopus,2-s2.0-85124953853
"Huang X., Li Q., Tai Y., Chen Z., Liu J., Shi J., Liu W.","18436902100;55911980700;36984437500;36633870500;57219538251;57449464300;57214602102;","Time series forecasting for hourly photovoltaic power using conditional generative adversarial network and Bi-LSTM",2022,"Energy","246",,"123403","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124476354&doi=10.1016%2fj.energy.2022.123403&partnerID=40&md5=179c7c3068910470bb97fc24484c0cfe","More and more photovoltaic (PV) power generation is incorporated into the grid. However, the intermittence and fluctuation of solar energy have brought huge challenges to the safe and stable operation of the power grid. PV power forecasting is one of the effective ways to solve the above problems, so it has become an important research topic. However, the existing research based on deep learning models mainly focuses on more complex network structures, optimization algorithms, and data decomposition. These hybrid models have encountered a development bottleneck in extracting the inherent features of PV power and related data, and a new idea and method are needed. This paper proposes a novel TSF-CGANs (time series forecasting based on CGANs, TSF-CGANs) algorithm considering conditional generative adversarial networks (CGANs) combined with convolutional neural networks (CNN) and Bi-directional long short-term memory (Bi-LSTM) for improving the accuracy of hourly PV power prediction. We design the generator in the TSF-CGANs network as a regression prediction model, which can extract the features based on historical data and random noise vector by the complex models, and finally use the Bi-LSTM model to output the predicted value. At the same time, the discriminator judges the authenticity of the generated predicted value and the actual value. In the continuous game between the generator and the discriminator, the parameters of the generator are optimized and more accurate prediction results are obtained. The performance of the proposed method is demonstrated with a real-world dataset. Compared with LSTM, recurrent neural network (RNN), back-propagation neural network (BP), support vector machine (SVM), and Persistence models, the values of five performance evaluation indicators, RMSE, MAE, nRMSE, R2, and R, show that the proposed model has better performance in prediction accuracy. Compared with the traditional BP, the TSF-CGANs model reduced the RMSE by 32%, Compared with the Persistence, the forecast skill (FS) of TSF-CGANs is 0.4863. The results indicate that it is feasible to use the generator to realize time series prediction in the proposed TSF-CGANs network. The core idea of TSF-CGANs method is to improve the prediction accuracy of the generator through the continuous game between the generator and the discriminator, which provides a new idea for the training process of the prediction method based on deep learning. © 2022 Elsevier Ltd","Bi-LSTM; Conditional generative adversarial network; Convolutional neural networks; PV power Forecasting","Backpropagation; Complex networks; Convolution; Convolutional neural networks; Data mining; Forecasting; Generative adversarial networks; Solar cells; Solar energy; Solar power generation; Structural optimization; Support vector machines; Time series; Bi-directional; Bi-directional long short-term memory; Conditional generative adversarial network; Convolutional neural network; Performance; Photovoltaic power; Photovoltaic power forecasting; Power forecasting; Prediction accuracy; Time series forecasting; Long short-term memory; forecasting method; photovoltaic system; power generation; time series",Article,Scopus,2-s2.0-85124476354
"Xu H., Liu Y., Shu C.-M., Bai M., Motalifu M., He Z., Wu S., Zhou P., Li B.","57447252700;54785011800;57154890400;57222172727;57391739200;57447868900;57447382800;57447622100;57447252800;","Cause analysis of hot work accidents based on text mining and deep learning",2022,"Journal of Loss Prevention in the Process Industries","76",,"104747","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124412832&doi=10.1016%2fj.jlp.2022.104747&partnerID=40&md5=467ee29fc9ae487e0322bd9a223576ba","Hot work accidents have significant consequences. Admittedly, preventing hot work accidents requires managers to analyze the accident profoundly and learn from the requisite documents that contain the detailed process and causes of the accident. However, the analysis process of unstructured records is manual, leading managers to failing to quickly analyze the cause of the accident and identify the key causes. Therefore, we used deep learning to automatically classify and predict the cause of accidents by the textual features and text mining methods that quickly extract the cause information to optimize accident record analysis. Initially, the latent Dirichlet allocation model was adopted to extract the topic words in the accidents to form cause topics, and convolutional neural networks were trained to predict the cause of the accident based on the previous cause topics. Then, the key causes of hot work accidents were extracted by qualifying the importance score of the cause, which included no gas detection and continuous monitoring, uncleaned combustibles, improper protection measures, and violations of regulations by workers. Managers can utilize these key causes to formulate optimal safety strategies to reduce the number of accidents. This study provides valuable suggestions for improving the process safety management of hot work in China. Moreover, the results of our method can be used as important documents for the safety education and training of the enterprise. © 2022 Elsevier Ltd","Deep learning; Hot work; Key cause; Process safety management; Text mining","Classification (of information); Convolutional neural networks; Data mining; Deep learning; Managers; Personnel training; Statistics; Analysis process; Causes analysis; Causes of accidents; Deep learning; Hot-work; Key cause; Learn+; Process safety management; Text-mining; Work accidents; Accidents",Article,Scopus,2-s2.0-85124412832
"Hu Z., Wang L., Tran V., Chen H.","57447900100;57018361000;56996914900;57157903100;","Efficiently mining spatial co-location patterns utilizing fuzzy grid cliques",2022,"Information Sciences","592",,,"361","388",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124378393&doi=10.1016%2fj.ins.2022.01.059&partnerID=40&md5=d1840b10aa6502593bdf3ac428031d68","Spatial co-location pattern (SCP) mining discovers subsets of spatial feature types whose objects frequently co-locate in a geographic space. Many existing methods treat the space as homogeneous, use absolute Euclidean distance to measure the neighbor relationship between objects and use a participation index to measure the prevalence of SCPs. Several issues arise: (1) it may be that the distance between objects cannot be accurately defined since it is a relative and fuzzy concept; (2) the degree of neighborliness and sharing relationships between objects are neglected; (3) current methods for collecting participating objects by generating candidate table instances utilizing combined search techniques are computationally expensive. In this paper, we propose a method based on fuzzy grid cliques to find all prevalent SCPs. Specifically, fuzzy theory is introduced to define the proximity between objects. The fuzzy participating contribution index (FPCI) is defined to measure the prevalence of SCPs, and it considers both the neighbor degree and sharing relationship between objects. Based on the defined proximity, a basic mining framework based on fuzzy grid cliques is proposed. We first design a naive algorithm based on the participating objects’ filtering and verification called POFV, which uses a fuzzy grid clique search technology instead of combination search to collect participating objects and avoids enumerating all table instances. To solve a dilemma within POFV, we develop a maximal fuzzy grid cliques search based algorithm called MFGC, which can effectively reuse information. Experiments on both real and synthetic data sets verify the superiority of our proposed approaches, by showing that MFGC greatly outperforms the baseline algorithm and more efficiently captures SCPs. © 2022 Elsevier Inc.","Fuzzy grid clique; Fuzzy neighbor relationship; Maximal clique; Spatial co-location pattern (SCP); Spatial data mining","Location; Feature types; Fuzzy grid clique; Fuzzy grids; Fuzzy neighbor relationship; Maximal clique; Pattern mining; Spatial co-location pattern; Spatial co-location patterns; Spatial data mining; Spatial features; Data mining",Article,Scopus,2-s2.0-85124378393
"Tartarotti Nepomuceno Duarte K., Andrade Nascimento Moura M., Sergio Martins P., Garcia de Carvalho M.A.","57446213500;57444702800;57445569200;57445139400;","Brain Extraction in Multiple T1-weighted Magnetic Resonance Imaging slices using Digital Image Processing techniques",2022,"IEEE Latin America Transactions","20","5",,"831","838",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124299403&doi=10.1109%2fTLA.2022.9693568&partnerID=40&md5=d5bba888315b2a99d938629cbd3332c0","Brain Imaging has been source of several studies in the literature, mostly due to its importanceboth to predict and to analyze certain diseases or conditions. Extracting the brain from patient images for medical analysis can provide useful diagnostic and prognostic information.To this end, digital image processing algorithms have been applied to medical tasks with a focus on the identification of the brain. This work proposes a brain extraction framework based on three major steps: 1) Dataset and Image Selection; 2) Preprocessing; and 3) Largest Connected Component extraction. Our data are obtained from the OASIS dataset.The preprocessing step is applied in order to enhance contrast and eliminate possible noise from the T1-weighted MRI. Largest Connected Component extraction is performed by initially detecting the largest element in the image (i.e. the brain gray matter) and then by extracting it through mathematical morphology operators. The unsupervised framework extracts the brain in different axial slices without adjustments. The main contribution of this work is a method using only digital image processing for automatically identifying the brain from several different slices, which differs from the literature since is performed without parameter resetting. Five metrics were applied to evaluate our results: Specificity, Recall, Accuracy, F-Measure, and Precision. In our first experiment, two metrics resulted in more than 90% in efficiency (Specificity and Precision), two of them surpassed 80% (F-Measure and Accuracy), and Sensitivity exceeded 70%. Our second experiment compares our results with those produced by related works, having been ranked in the top positions of Sensitivity and Specificity. ©","Brain; Data mining; Diseases; Histograms; Image edge detection; Magnetic resonance imaging; Software algorithms","Brain mapping; Data mining; Diagnosis; Edge detection; Extraction; Image analysis; Mathematical morphology; Mathematical operators; Morphology; Brain extraction; Brain imaging; Condition; Connected component extraction; Digital image processing technique; F measure; Image edge detection; Largest connected component; Software algorithms; T1-weighted magnetic resonance imaging; Magnetic resonance imaging",Article,Scopus,2-s2.0-85124299403
"Fujio C., Ogawa H.","57220084388;36620501100;","Physical insights into multi-point global optimum design of scramjet intakes for ascent flight",2022,"Acta Astronautica","194",,,"59","75",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124229310&doi=10.1016%2fj.actaastro.2022.01.036&partnerID=40&md5=401b52aa2915ca37fae44e571a23591d","Scramjet propulsion offers promise for flexible and sustainable space transportation. Hypersonic airflow compression through the intake plays a major role in successful scramjet-powered ascent. It is thus of crucial importance to design high-performance intakes that can work efficiently and robustly throughout the ascent trajectory. The present research is conducted to gain physical insight into multi-point global optimum scramjet intake design and associated requirements by means of multi-point design optimization and analytical investigation in inviscid and viscous regimes. It has been found and verified theoretically that at any operating conditions, compression efficiency, drag, compression ratio, and mean exit temperature can theoretically be determined uniquely in an interrelated manner under calorically perfect and adiabatic flow assumptions, provided one of them is given or known except for compression efficiency. The multi-point optimization study in the inviscid regime has verified that the multi-point global optimum design can exist with respect to compression efficiency and drag. On the other hand, while the theoretical analysis also prescribes that the global optimum design can exist for a single operating condition in viscous flow, trade-off relations have been found to exist between two operating conditions in the viscous regime, where boundary layer is responsible for counteracting behaviors between compression efficiency and drag at different operating conditions. The insights gained contribute to the guiding principle for intake design of scramjet engines for access-to-space. © 2022 IAA","Computational fluid dynamics; Data mining; Evolutionary algorithms; Hypersonic airbreathing propulsion; Multi-objective design optimization; Space transportation","Boundary layers; Computational fluid dynamics; Data mining; Economic and social effects; Efficiency; Evolutionary algorithms; Hypersonic vehicles; Lift drag ratio; Propulsion; Ramjet engines; Air Breathing Propulsions; Ascent flight; Compression efficiency; Global optimum designs; Hypersonic airbreathing propulsion; Multi-objective design optimization; Multi-points; Operating condition; Scramjet propulsion; Space transportations; Drag",Article,Scopus,2-s2.0-85124229310
"Li B., Wei Y., Sun X., Bo L., Chen D., Tao C.","57439738400;57204919030;24829988300;57204660270;57208146531;57439049500;","Towards the identification of bug entities and relations in bug reports",2022,"Automated Software Engineering","29","1","24","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124033388&doi=10.1007%2fs10515-022-00325-1&partnerID=40&md5=77b456ac54a5c5c13dbd1365ee2f9d0e","During the bug fixing process, developers usually analyze the historical relevant bug reports in bug repository to support various bug analysis and fixing activities. There are rich semantics and relationships in the bug reports, which can be helpful for bug retrieval, recommendation, and repair. In this paper, our purpose is to quickly extract effective knowledge of bug report from two perspectives: entity recognition and relation extraction to assist bug understanding and fixing. Meanwhile, we hope to strengthen the relevance of bug reports through the effective extraction of bug knowledge. In order to effectively extract the bug entities and relations in the bug report, we first define 8 types of relations between the bug entities and incorporate neural network Recurrent Neural Network (RNN) and RNN based on shortest dependency path (SDP-RNN) to automatically identify bug entities and their relations in bug reports. Results We evaluate the effectiveness of our method through four experimental questions. From the results, the bug knowledge extracted by our method can effectively represent the semantics and relations in the bug report, and obtain F1 scores of 79.32% and 63.8% in entity recognition and relation extraction, respectively. The proposed approach can efficiently extract the structured bug knowledge in the bug report, and further enhance the correlation between the bug reports and the effectiveness of the bug knowledge through the representation of these structured bug knowledge units. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Bug analysis; Bug entity recognition; Neural networks; Relation extraction","Data mining; Extraction; Program debugging; Semantics; Bug analyse; Bug entity recognition; Bug fixing process; Bug reports; Entity recognition; F1 scores; Network-based; Neural-networks; Relation extraction; Types of relations; Recurrent neural networks",Article,Scopus,2-s2.0-85124033388
"Thuy N.N., Wongthanavasu S.","57209713176;14051173200;","Hybrid filter–wrapper attribute selection with alpha-level fuzzy rough sets",2022,"Expert Systems with Applications","193",,"116428","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123007773&doi=10.1016%2fj.eswa.2021.116428&partnerID=40&md5=4c6760cc5dae6e3f621fc17d0e723ab7","Selection of important attributes/features from decision information systems plays a vital role in data mining and machine learning tasks. It is regarded as a very interesting, but challenge problem, especially when faced with continuous numerical/real attributes. Neighborhood rough sets and fuzzy rough sets based attribute selection methods are well-known for dealing effectively with numerical/real attributes. However, characteristics of data may be described incompletely by neighborhood classes in the neighborhood rough set model, while the fuzzy rough sets based approach is still quite time-consuming because of the complex calculations on fuzzy equivalence classes. To address these limitations, we apply the concept of sets of level α (α-cut sets) in the fuzzy set theory to construct α-level fuzzy equivalence classes which provide a foundation for developing basic concepts of a new α-level fuzzy rough set model. We will see that under the properties of the α-cut sets, the α-level fuzzy equivalence classes not only help to significantly reduce the computational cost, but also preserve most of the information about the relationships between the objects, and even can decrease some noise in the data. Based on the α-level fuzzy rough set model, we define new reducts and then propose the FSFCF algorithm for attribute subset selection from the decision information systems containing continuous data. It is important to emphasize some advantages of the proposed method. First, in order to evaluate and select optimal attributes, we use an α-level fuzzy certainty factor with the comprehensive consideration to all objects in the universe. Second, the FSFCF algorithm is designed in the hybrid filter–wrapper approach to reduce the size of selected attribute subset as well as enhance the classification accuracy. Therefore, the proposed method can significantly improve the performance of the attribute selection for continuous data. To verify the effectiveness of FSFCF, we implement experiments on a variety of real-world data sets. The results demonstrated that the proposed method outperforms the compared state-of-the-art methods in terms of the computational time, the size of reduct and the classification accuracy for almost all of data sets. © 2022 Elsevier Ltd","Attribute selection; Decision information systems; Numerical/real attributes; Reducts; α-level fuzzy certainty factor; α-level fuzzy rough sets","Classification (of information); Computation theory; Data mining; Equivalence classes; Filtration; Fuzzy filters; Fuzzy set theory; Information systems; Information use; Numerical methods; Attribute selection; Certainty factors; Decision information system; Fuzzy equivalence; Fuzzy-rough sets; Numerical/real attribute; Reduct; Rough set models; Α-level fuzzy certainty factor; Α-level fuzzy rough set; Rough set theory",Article,Scopus,2-s2.0-85123007773
"Zhang S., Wang C., Liao P., Xiao L., Fu T.","57204940734;56308653200;57416259700;56193036200;57204470021;","Wind speed forecasting based on model selection, fuzzy cluster, and multi-objective algorithm and wind energy simulation by Betz's theory",2022,"Expert Systems with Applications","193",,"116509","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122989202&doi=10.1016%2fj.eswa.2022.116509&partnerID=40&md5=701bef5bf59e70985edcd614d4add79c","Wind energy is of increasing interest to wind farm administrators as a clean and renewable energy source. Accurate wind speed forecasting and effective wind energy simulation can increase the capability of wind power combined with a grid and decrease the operating cost of wind farms. However, many previous studies have been restricted to wind speed forecasting, ignoring wind energy simulations. Thus, grid management cannot effectively estimate the power production of wind farms and leads to an increase in the abandonment wind rate in wind farms. In this study, a wind farm auxiliary management system is developed, which includes two modules: wind speed forecasting and wind energy simulation. In the wind speed forecasting module, first, a data mining algorithm is used to analyze different features of wind speed time series data in a wind farm. Subsequently, a feature selection algorithm is used to determine the representative wind speed time series of the wind farm, and it is combined with a data preprocessing method to effectively eliminate the noise of the original wind speed time series. Second, six hybrid neural network forecasting models based on a modified multi-objective algorithm are established to forecast wind speed. Finally, they are combined with a model selection strategy to yield the best forecasting value for each time point. In the wind energy simulation module, using Betz's theory, the physical transformation process of a wind turbine is estimated to determine the range of wind power generation. © 2022 Elsevier Ltd","Betz's theory; Fuzzy c-means cluster; Model selection strategy; Modified multi-objective algorithm; Wind speed forecasting","Clustering algorithms; Data mining; Electric power generation; Electric power system interconnection; Electric power transmission networks; Electric utilities; Forecasting; Fuzzy clustering; Speed; Time series; Wind; Betz theory; Fuzzy c-means clusters; Model Selection; Model selection strategy; Modified multi-objective algorithm; Multi objective algorithm; Wind energy simulation; Wind farm; Wind speed forecasting; Wind speed time series; Wind power",Article,Scopus,2-s2.0-85122989202
"Liang B., Cai J., Yang H.","57407192500;24478242000;36683563300;","A new cell group clustering algorithm based on validation & correction mechanism",2022,"Expert Systems with Applications","193",,"116410","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122623908&doi=10.1016%2fj.eswa.2021.116410&partnerID=40&md5=4f3b4caa97b0e1ded15f098e4c385145","Clustering used to discover hidden patterns in unlabeled data sets is an important task in data mining. Therefore, clustering validation applied to evaluating the clustering results has been recognized as one of the vital issues for clustering applications. The existing validation index is often used to evaluate the results rather than guide the clustering process dynamically. However, using the index can automatically adjust the algorithm's operation according to the actual data distribution, thereby improving the algorithm's adaptability. Simulating the division and aggregation process of cells in biology, in this paper, we propose a new two-phase (grouping and merging) cell group clustering algorithm by using a continuous validation and correction mechanism. In the grouping phase, a new clustering internal validation index called Split Index (SI) is utilized to evaluate the cohesion of a cell group continuously, and then a validation and correction mechanism is adopted to validate and split the cell groups so that the SI of each cell group can meet the split threshold ε, finally the cell nucleus of each cell group is determined by finding the sum of the minimum distances from the nucleus to other samples. In the merging phase, the cell group merge method is adopted to merge all the reachable cell groups in a density-reachable manner. Ultimately the clustering problem of arbitrarily distributed samples is completed. Experiments on the synthetic and the UCI Machine Learning Repository data set show that the validation index can effectively guide the clustering process, and the algorithm can deal with various data sets, including imbalanced data sets and spherical non-spherical clusters. © 2021 Elsevier Ltd","Cell nucleus; Clustering internal validation index; Split index; Validation & correction mechanism","Clustering algorithms; Cytology; Data mining; Machine learning; Merging; Cell nucleus; Clustering internal validation index; Clustering process; Clusterings; Correction mechanism; Data set; Group clustering; Split index; Validation & correction mechanism; Validation index; Cells",Article,Scopus,2-s2.0-85122623908
"Xiong B., Lou L., Meng X., Wang X., Ma H., Wang Z.","57222351425;57397958600;57221193628;57222350418;57200159633;57225970784;","Short-term wind power forecasting based on Attention Mechanism and Deep Learning",2022,"Electric Power Systems Research","206",,"107776","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122309646&doi=10.1016%2fj.epsr.2022.107776&partnerID=40&md5=67d5ce9587f103a29f52d0de5731fb68","Wind power forecasting is an important means to alleviate the pressure of peak and frequency regulation in power systems and improve the acceptance capacity of wind power. However, physical attribute data related to wind power have different effects on its forecasting, and the long-term sequence of original features has redundant information, which makes wind power prediction a daunting challenge. To address these problems, this paper proposes a multi-dimensional extended features fusion model called AMC-LSTM to predict wind power. The Attention Mechanism is utilized to dynamically assign the weight of physical attribute data, which effectively deals with the model's failure to distinguish the difference in importance of input data. Convolutional neural network (CNN) is used for short-term abstract feature extraction to obtain local high-dimensional features, and then Long short-term memory (LSTM) is used to extract the long-term trend of local high-dimensional features, which can effectively reduce the problem of inaccurate prediction caused by the mixing of original data. The extracted temporal features and physical features are fused to predict wind power. Using actual operation data of wind turbine, we verified that the proposed AMC-LSTM hybrid model is capable of integrating multi-scale extended features and providing better performance for short-term wind forecasting. © 2022","Attention Mechanism; Feature weight; Features fusion; Wind power forecasting","Convolutional neural networks; Data mining; Weather forecasting; Wind power; Attention mechanisms; Attribute data; Extended features; Feature weight; Features fusions; Frequency regulations; Higher dimensional features; Power; Short-term wind power forecasting; Wind power forecasting; Long short-term memory",Article,Scopus,2-s2.0-85122309646
"Pathik B., Sharma M.","57352074000;55468794600;","Source code change analysis with deep learning based programming model",2022,"Automated Software Engineering","29","1","15","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122179574&doi=10.1007%2fs10515-021-00305-x&partnerID=40&md5=c8c129f43f9cdbc0f124f73b7ca9e0c1","Analyzing the change in source code is a very crucial activity for object-oriented parallel programming software. This paper suggested an Impact analysis method with Attention BiLSTM (IABLSTM) for detecting the changes and their affected part in the object-oriented software system. Classical approaches based on control flow graph, program dependence analysis, latent dirichlet allocation, and data mining have been used for change impact analysis. A Path2Vec approach is presented in the paper, combining a deep learning technique with word embedding to analyze and identify the change. The paper considers two versions of a python program for experiment and generates the abstract syntax tree (AST). Then extract the path to produce a token sequence. Next, convert the token sequence into unique vectors by applying a word embedding layer. The BiLSTM network encodes the sequence into a vector representation. After that, compare the embedded output with the use of cosine distance metrics. We trained the neural network model with the embedded outcome. Then decode the resultant token sequence into a path of AST. Finally, convert the AST path back to code using the un-parsing technique. To strengthen the parallel programming based proposed model, we combined the attention mechanism to emphasize and detect the differences in the code. The model is detecting the change of code efficiently. The experimental results show that our proposed model's change detection accuracy increases significantly compared with other conventional models for change impact analysis. The proposed method can also be applied for impact analysis on object-oriented based parallel programming. The empirical evaluation shows that the model outperforms change detection with approximately 85% validation accuracy. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Abstract syntax tree; Attention; Change impact analysis; Deep learning; Distance metrics; Path2Vec; Un-parsing; Word embedding","Codes (symbols); Data flow analysis; Data mining; Embeddings; Flow graphs; Object oriented programming; Parallel programming; Statistics; Syntactics; Trees (mathematics); Abstract Syntax Trees; Attention; Change impact analysis; Deep learning; Distance metrics; Embeddings; Path2vec; Token sequences; Un-parsing; Word embedding; Deep learning",Article,Scopus,2-s2.0-85122179574
"Amaral L.C.M., Roshan A., Bayat A.","57392286700;57391770700;14628468500;","Review of Machine Learning Algorithms for Automatic Detection of Underground Objects in GPR Images",2022,"Journal of Pipeline Systems Engineering and Practice","13","2","04021082","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121995657&doi=10.1061%2f%28ASCE%29PS.1949-1204.0000632&partnerID=40&md5=10b150c34fd598e4c2214e6e4be464de","Ground-penetrating radar (GPR) is a nondestructive tool that has gained popularity after giving promising results in different areas - such as utility engineering, transportation engineering, civil engineering, and geology - with relatively low cost. Even as the number of applications for GPR increases, the interpretation of GPR data is still challenging, in part due to varying ground conditions. Researchers are continuously working on the development of new analysis methods to address these challenges. Computer vision algorithms, including neural networks and convolution neural networks, have advanced significantly over the past decade, and researchers have utilized these algorithms to extract information from GPR images and thus improve the interpretation of GPR data. This paper presents a review of literature that employs computer vision and machine learning algorithms, such as YOLO V3, Viola-Jones, and AlexNet, for automatic extraction of information from GPR images. The uptake in the use of automatic detection algorithms for GPR is increased by the ability to rapidly quantify and locate buried targets that previously could only be identified by professionals with a high level of expertise and training. © 2021 American Society of Civil Engineers.",,"Computer vision; Cost engineering; Data mining; Geological surveys; Geophysical prospecting; Image enhancement; Learning algorithms; Machine learning; Object detection; Radar imaging; Analysis method; Automatic Detection; Ground conditions; Ground Penetrating Radar; Low-costs; Machine learning algorithms; Nondestructive tools; Radar data; Transportation engineering; Utility engineering; Ground penetrating radar systems",Article,Scopus,2-s2.0-85121995657
"Moreno Schneider J., Rehm G., Montiel-Ponsoda E., Rodríguez-Doncel V., Martín-Chozas P., Navas-Loro M., Kaltenböck M., Revenko A., Karampatakis S., Sageder C., Gracia J., Maganza F., Kernerman I., Lonke D., Lagzdins A., Bosque Gil J., Verhoeven P., Gomez Diaz E., Boil Ballesteros P.","36606547100;28767949900;25654093800;35204031900;57210581382;57195404822;44461327800;55225811200;57191664371;57219630964;55392626700;57219628002;57193091389;57211836386;57219639642;57031866000;57209197843;57388356400;57388835000;","Lynx: A knowledge-based AI service platform for content processing, enrichment and analysis for the legal domain",2022,"Information Systems","106",,"101966","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121918359&doi=10.1016%2fj.is.2021.101966&partnerID=40&md5=1f7a75a77d6ec29ad54e02053f56a4fd","The EU-funded project Lynx focuses on the creation of a knowledge graph for the legal domain (Legal Knowledge Graph, LKG) and its use for the semantic processing, analysis and enrichment of documents from the legal domain. This article describes the use cases covered in the project, the entire developed platform and the semantic analysis services that operate on the documents. © 2022 The Authors","Applications; Knowledge discovery/representation; Systems; Text analytics; Tools","Data mining; Semantics; Content analysis; Content processing; Knowledge based; Knowledge discovery/representation; Knowledge graphs; Legal domains; Semantic analysis; Service platforms; System; Text analytics; Knowledge graph",Article,Scopus,2-s2.0-85121918359
"Park S., Kim H.M.","57219967349;8445320100;","Phrase Embedding and Clustering for Sub-Feature Extraction from Online Data",2022,"Journal of Mechanical Design, Transactions of the ASME","144","5","054501 EN","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120993469&doi=10.1115%2f1.4052904&partnerID=40&md5=0f4994b8828dae2c565420efd7ea19b8","Recently, online user-generated data have been used as an efficient resource for customer analysis. In the product design area, various methods for analyzing customer preference for product features have been suggested. However, most of them focused on feature categories rather than product components which are crucial in practical applications. To address that limitation, this paper proposes a new methodology for extracting sub-features from online data. First, the method detects phrases in the data and filtered them using product manual documents. The filtered phrases are embedded into vectors, and then they are divided into several groups by two clustering methods. The resulting clusters are labeled by analyzing items in each cluster. Finally, cue phrases for sub-features are obtained by selecting clusters with labels representing product features. The proposed methodology was tested on smartphone review data. The result provides feature clusters containing sub-feature phrases with high accuracy. The obtained cue phrases will be used in analyzing customer preferences for sub-features and this can help product designers determine the optimal component configuration in embodiment design. © 2021 International Union of Crystallography. All rights reserved.","data mining; feature extraction; online data","Extraction; Feature extraction; Product design; Sales; Clusterings; Cue phrase; Customer analysis; Customer preferences; Embeddings; Features extraction; Online data; Online users; Product feature; User-generated; Data mining",Article,Scopus,2-s2.0-85120993469
"Araujo A.F., Gôlo M.P.S., Marcacini R.M.","57296646200;57287395500;37013380700;","Opinion mining for app reviews: an analysis of textual representation and predictive models",2022,"Automated Software Engineering","29","1","5","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119969463&doi=10.1007%2fs10515-021-00301-1&partnerID=40&md5=5d40eb0d9084b03467f2ad98ed088a8e","Popular mobile applications receive millions of user reviews. These reviews contain relevant information for software maintenance, such as bug reports and improvement suggestions. The review’s information is a valuable knowledge source for software requirements engineering since the apps review analysis helps make strategic decisions to improve the app quality. However, due to the large volume of texts, the manual extraction of the relevant information is an impracticable task. Opinion mining is the field of study for analyzing people’s sentiments and emotions through opinions expressed on the web, such as social networks, forums, and community platforms for products and services recommendation. In this paper, we investigate opinion mining for app reviews. In particular, we compare textual representation techniques for classification, sentiment analysis, and utility prediction from app reviews. We discuss and evaluate different techniques for the textual representation of reviews, from traditional Bag-of-Words (BoW) to the most recent state-of-the-art Neural Language models (NLM). Our findings show that the traditional Bag-of-Words model, combined with a careful analysis of text pre-processing techniques, is still competitive. It obtains results close to the NLM in the classification, sentiment analysis and utility prediction tasks. However, NLM proved to be more advantageous since they achieved very competitive performance in all the predictive tasks covered in this work, provide significant dimensionality reduction, and deals more adequately with semantic proximity between the reviews’ texts. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Machine learning; Mobile applications; Opinion mining; Requirements engineering","Application programs; Data mining; Information retrieval; Machine learning; Mobile computing; Quality control; Requirements engineering; Semantics; Bug reports; Knowledge sources; Language model; Mobile applications; Predictive models; Representation model; Requirement engineering; Sentiment analysis; Textual representation; User reviews; Sentiment analysis",Article,Scopus,2-s2.0-85119969463
"Sinnl M.","55781194100;","Exact and heuristic algorithms for the maximum weighted submatrix coverage problem",2022,"European Journal of Operational Research","298","3",,"821","833",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112153187&doi=10.1016%2fj.ejor.2021.07.035&partnerID=40&md5=81c2599053c807df7297e283fc63d5af","The maximum weighted submatrix coverage problem is a recently introduced problem with applications in data mining. It is concerned with selecting K submatrices of a given numerical matrix such that the sum of the matrix-entries, which occur in at least one of the selected submatrices, is maximized. In the paper introducing the problem, a problem-specific constraint programming approach was developed and embedded in a large neighborhood-search to obtain a heuristic. A compact integer linear programming formulation was also presented, but deemed inefficient due to its size. In this paper, we introduce new integer linear programming formulations for the problem, one of them is based on Benders decomposition. The obtained Benders decomposition-based formulation has a nice combinatorial structure, i.e., there is no need to solve linear programs to separate Benders cuts. We present preprocessing procedures and valid inequalities for all formulations. We also develop a greedy randomized adaptive search procedure for the problem, which is enhanced with a local search. A computational study using the instances from literature is done to evaluate the effectiveness of our new approaches. Our algorithms manage to find improved primal solutions for ten out of 17 real-world instances, and optimality is proven for two real-world instances. Moreover, for over 700 of 1617 large-scale synthetic instances, our algorithms find improved primal solutions compared to the heuristics from the literature. © 2021 The Author(s)","Benders decomposition; Branch-and-cut; Combinatorial optimization; Data mining; Local search","Combinatorial optimization; Constraint theory; Data mining; Heuristic algorithms; Local search (optimization); Matrix algebra; Stochastic programming; Benders' decompositions; Branch-and-cut; Coverage problem; Exact algorithms; Heuristics algorithm; Integer linear programming formulation; Local search; Optimisations; Real-world; Submatrix; Integer programming",Article,Scopus,2-s2.0-85112153187
"Castano S., Falduti M., Ferrara A., Montanelli S.","56267928600;57210122497;7201825221;6506356009;","A knowledge-centered framework for exploration and retrieval of legal documents",2022,"Information Systems","106",,"101842","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109613081&doi=10.1016%2fj.is.2021.101842&partnerID=40&md5=ca6821bc9f1648b26ecab7348928a6b5","Automated legal knowledge extraction systems are strongly demanded, to support annotation of legal documents as well as knowledge extraction from them, to provide useful and relevant suggestions to legal actors (e.g., judges, lawyers) for managing incoming new cases. In this paper, we propose CRIKE (CRIme Knowledge Extraction), a knowledge-based framework conceived to support legal knowledge extraction from a collection of legal documents, based on a reference legal ontology called LATO (Legal Abstract Term Ontology). We first introduce LATO-KM, the knowledge model of LATO where legal knowledge featuring documents in the collection is properly formalized as conceptual knowledge, in form of legal concepts and relationships, and terminological knowledge, in form of term-sets associated with legal concepts. Then, we present the bootstrapping cycle of CRIKE that aims to progressively enrich the terminological knowledge layer of LATO by extracting new terms from legal documents to be used for enriching the term-set associated with a corresponding legal concept. Finally, to evaluate the results obtained through CRIKE, we discuss experimental results on a real dataset of 180,000 court decisions of the State of Illinois taken from the Caselaw Access Project (CAP). © 2021 Elsevier Ltd","Legal document retrieval and exploration; Legal knowledge extraction; Legal knowledge model","Authentication; Extraction; Knowledge based systems; Laws and legislation; Ontology; Terminology; Conceptual knowledge; Court decisions; Knowledge based framework; Knowledge extraction; Knowledge layers; Knowledge model; Legal documents; Legal knowledge; Data mining",Article,Scopus,2-s2.0-85109613081
"Polyvyanyy A., Kalenkova A.","25654203000;36600541900;","Conformance checking of partially matching processes: An entropy-based approach",2022,"Information Systems","106",,"101720","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100094565&doi=10.1016%2fj.is.2021.101720&partnerID=40&md5=71755360c4732b1fd53b429e9a825750","Conformance checking is an area of process mining that studies methods for measuring and characterizing commonalities and discrepancies between processes recorded in event logs of IT-systems and designed processes, either captured in explicit process models or implicitly induced by information systems. Applications of conformance checking range from measuring the quality of models automatically discovered from event logs, via regulatory process compliance, to automated process enhancement. Recently, process mining researchers initiated a discussion on the desired properties the conformance measures should possess. This discussion acknowledges that existing measures often do not satisfy the desired properties. Besides, there is a lack of understanding by the process mining community of the desired properties for conformance measures that address partially matching processes, i.e., processes that are not identical but differ in some process steps. In this article, we extend the recently introduced precision and recall conformance measures between an event log and process model that are based on the concept of entropy from information theory to account for partially matching processes. We discuss the properties the presented extended measures inherit from the original measures as well as properties for partially matching processes the new measures satisfy. All the presented conformance measures have been implemented in a publicly available tool. We present qualitative and quantitative evaluations based on our implementation that show the feasibility of using the proposed measures in industrial settings. © 2021 Elsevier Ltd","Conformance checking; Entropy; Partial matching; Process mining; Properties","Automation; Compliance control; Entropy; Information theory; Regulatory compliance; Automated process; Conformance checking; Entropy based approach; Industrial settings; Precision and recall; Process Modeling; Quantitative evaluation; Regulatory process; Data mining",Article,Scopus,2-s2.0-85100094565
"Chen L., Jiang S., Liu J., Wang C., Zhang S., Xie C., Liang J., Xiao Y., Song R.","57198901639;57209503646;57208656857;57253651400;57219632653;57217270786;57188696905;24377046200;35185614100;","Rule mining over knowledge graphs via reinforcement learning",2022,"Knowledge-Based Systems","242",,"108371","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125124697&doi=10.1016%2fj.knosys.2022.108371&partnerID=40&md5=882688128a19adf4806a5af647d90d4c","Knowledge graphs (KGs) are an important source repository for a wide range of applications and rule mining from KGs recently attracts wide research interest in the KG-related research community. Many solutions have been proposed for the rule mining from large-scale KGs, which however are limited in the inefficiency of rule generation and ineffectiveness of rule evaluation. To solve these problems, in this paper we propose a generation-then-evaluation rule mining approach guided by reinforcement learning. Specifically, a two-phased framework is designed. The first phase aims to train a reinforcement learning agent for rule generation from KGs, and the second is to utilize the value function of the agent to guide the step-by-step rule generation. We conduct extensive experiments on several datasets and the results prove that our rule mining solution achieves state-of-the-art performance in terms of efficiency and effectiveness. © 2022","Reinforcement learning; Representation learning; Rule mining","Data mining; Knowledge graph; Evaluation rules; Knowledge graphs; Large-scales; Reinforcement learning agent; Representation learning; Research communities; Research interests; Rule evaluation; Rule generation; Rule mining; Reinforcement learning",Article,Scopus,2-s2.0-85125124697
"Lan Y., Xu X., Fang Q., Zeng Y., Liu X., Zhang X.","57267624100;57071888700;55853738600;56567100100;57423725100;57208868466;","Transfer reinforcement learning via meta-knowledge extraction using auto-pruned decision trees",2022,"Knowledge-Based Systems","242",,"108221","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124894426&doi=10.1016%2fj.knosys.2022.108221&partnerID=40&md5=f086dd30ec60d53d78a0a91f3e86e5e8","Transfer reinforcement learning (RL) has recently received increasing attention to make RL agents have better learning performance in target Markov decision problems (MDPs) by using the knowledge learned in source MDPs. However, it is still an open and challenging problem to improve the transfer capability and interpretability of RL algorithms. In this paper, we propose a novel transfer reinforcement learning approach via meta-knowledge extraction using auto-pruned decision trees. In source MDPs, pre-trained policies are firstly learned via RL algorithms using general function approximators. Then, a meta-knowledge extraction algorithm is designed with an auto-pruned decision tree model, where the meta-knowledge is learned by re-training the auto-pruned decision tree based on the data samples generated from the pre-trained policies. The state spaces of meta-knowledge are determined by estimating the uncertainty of state–action pairs in pre-trained policies based on the entropy value of leaf nodes. In target MDPs, according to whether the state is in the state set of meta-knowledge, a hybrid policy is generated by integrating the meta-knowledge and the policies learned on the target MDPs. Based on the proposed transfer RL approach, two meta-knowledge-based transfer reinforcement learning (MKRL) algorithms are developed for MDPs with discrete action spaces and continuous action spaces, respectively. Experimental results in several benchmark tasks show that the MKRL algorithm outperforms other baselines in terms of learning efficiency and interpretability in the target MDPs with generic cases of task similarity. © 2022 Elsevier B.V.","Explainable artificial intelligence; Meta-knowledge; Reinforcement learning; Transfer reinforcement learning","Data mining; Extraction; Knowledge based systems; Learning algorithms; Reinforcement learning; Uncertainty analysis; Action spaces; Explainable artificial intelligence; Interpretability; Knowledge based; Knowledge extraction; Markov decision problem; Meta-knowledge; Reinforcement learning algorithms; Reinforcement learning approach; Transfer reinforcement learning; Decision trees",Article,Scopus,2-s2.0-85124894426
"Hung L.-C., Hu Y.-H., Tsai C.-F., Huang M.-W.","55879300300;7407119412;7404966986;57226349175;","A dynamic time warping approach for handling class imbalanced medical datasets with missing values: A case study of protein localization site prediction",2022,"Expert Systems with Applications","192",,"116437","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121968022&doi=10.1016%2fj.eswa.2021.116437&partnerID=40&md5=da7458656160971f3430304e194eb940","Class imbalanced medical datasets, such as cancer prediction, contain imbalanced numbers of data in different classes leading to skewed class distribution, which makes it very difficult for a classifier to distinguish between minority (i.e. cancer) and majority (i.e. non-cancer) classes. Related studies in the literature have proposed different types of solutions for the class imbalance problem including data level, algorithmic level, and cost-sensitive learning approaches. However, none of these potential solutions have considered the issue of missing attribute values residing in the class imbalanced medical datasets, especially for the minority class. Missing value imputation is commonly used for the construction of some models where statistical or machine learning techniques are used to produce estimations to replace the missing values. However, the existing imputation methods require a certain number of observed data to produce their estimations, the major challenge for them being that the amount of observed data (with no missing values) in the minority class is very limited, or that some data are not complete. In this paper, we proposed a novel approach, namely Dynamic Time Warping-based Imputation (DTWI), to handle class imbalanced datasets with missing values. Based on the similarity measurement technique of DTW, all of the data (with or without missing values) in the minority class can be used for missing value imputation. The experimental results based on 10 different class imbalanced medical datasets show that when the missing rates in the minority classes are smaller than 30%, DTWI performs similarly to the baseline K-NN imputation method and better than the mean/mode imputation and case deletion methods. When the missing rates are larger than 30%, DTWI significantly outperform the other techniques. © 2021 Elsevier Ltd","Class imbalance; Data mining; Dynamic time warping; Machine learning; Missing value imputation","Classification (of information); Diseases; Machine learning; Nearest neighbor search; Case-studies; Class imbalance; Different class; Dynamic time warping; Imputation methods; Medical data sets; Missing rate; Missing value imputation; Missing values; Observed data; Data mining",Article,Scopus,2-s2.0-85121968022
"Du X., Yu F.","57390718400;35365764700;","A fast algorithm for mining temporal association rules in a multi-attributed graph sequence",2022,"Expert Systems with Applications","192",,"116390","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121964067&doi=10.1016%2fj.eswa.2021.116390&partnerID=40&md5=967de3585ff2037a98b5924c5c52e087","In real life, there exist a lot of attributed graphs each of which contains attribute information as well as structural information. As time goes on, a group of attributed graphs form an attributed graph sequence. Being the generalization of single-attributed graph sequences, multi-attributed graph sequences are arising vastly and quickly. Mining the temporal associations hidden in a multi-attributed graph sequence is in urgent need from data owners. To meet the need and fill the gap of research on mining such kind of temporal associations, we first give a definition of temporal association rules for describing temporal associations in a multi-attributed graph sequence, and then propose a fast algorithm for mining temporal association rules in a multi-attributed graph sequence which is based on the anti-monotonicity of support. The proposed algorithm is designed in two steps, namely finding frequent temporal association rules and verifying the credibility of these rules. Equipped with two novel joining and pruning strategies, the proposed algorithm exhibits much higher efficiency which is specially pursued in the process of rule mining. Experiments performed on synthetic datasets and real datasets show that the proposed algorithm is effective and more efficient than other existing algorithms. © 2021 Elsevier Ltd","Mining algorithm; Mining problem; Multi-attributed graph sequence; Temporal association rules","Association rules; Attribute information; Attributed graphs; Fast algorithms; Graph sequences; Information information; Mining algorithms; Mining problems; Multi-attributed graph sequence; Temporal association; Temporal association rule; Data mining",Article,Scopus,2-s2.0-85121964067
"Abbasimehr H., Bahrini A.","36518269600;53863208600;","An analytical framework based on the recency, frequency, and monetary model and time series clustering techniques for dynamic segmentation",2022,"Expert Systems with Applications","192",,"116373","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121962300&doi=10.1016%2fj.eswa.2021.116373&partnerID=40&md5=d53ba62d4b3ce07d3753627417d66b30","Nowadays, banks use data mining and business intelligence tools and techniques to analyze their customers’ behavior. Customer segmentation is a widely adopted analytical tool to identify distinct groups of customers. Most studies have adopted a static segmentation approach, leading to missing important trends and patterns. In this study, we propose a framework that represents each customer behavior as a time-series sequence of the Recency, Frequency, and Monetary variables, and then exploits time-series clustering algorithms to carry out customer segmentation. The framework consists of state-of-the-art clustering algorithms, including hierarchical, spectral, and k-shape algorithms to divide customers into homogeneous groups and implement point of sale devices transaction data of grocery and appliance retailers. We divide customers into four segments, analyze the behavioral trends, and provide marketing suggestions for each segment. Our results show that, in terms of the computed validity indices, the best clustering model for grocery retailers is reached using hierarchical clustering with the Complexity-Invariant Distance measure, and for appliance retailers, the best segmentation is achieved by applying the spectral clustering with the Complexity-Invariant Distance measure. © 2021 Elsevier Ltd","Analytical customer relationship management; Customer segmentation; Distance measure; Dynamic customer behavior; Time series clustering","Cluster analysis; Data mining; Public relations; Sales; Time series; Analytical customer relationship management; Clustering techniques; Customer behavior; Customer relationship management; Customers segmentations; Distance measure; Dynamic customer behavior; Dynamic segmentation; Time series clustering; Tools and techniques; Clustering algorithms",Article,Scopus,2-s2.0-85121962300
"Wang L., Gao T., Zhou B., Tang H., Xiang F.","57070558600;57388578800;57388894300;16551287000;54394392500;","Manufacturing service recommendation method toward industrial internet platform considering the cooperative relationship among enterprises",2022,"Expert Systems with Applications","192",,"116391","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121907105&doi=10.1016%2fj.eswa.2021.116391&partnerID=40&md5=817b8289b61301d5d44bbb08f11d279f","The popularization of the service-oriented manufacturing mode makes increasing customers configure the required manufacturing services from Industry Internet platforms. However, most recommendation methods mainly focus on the QoS indicators of the overall manufacturing service combination without considering the collaboration relationship between manufacturing services. A novel multi-attribute recommendation method of manufacturing service combination considering the historical collaboration relationship of the online platforms is studied in this paper. First, an evaluation indicator named manufacturing service collaboration frequency indicator (CFI) is introduced, which uses an improved machine learning algorithm combining FP-growth and Simrank to mine the frequent terms and similarities of the manufacturing service collaboration process. Then, a multi-objective evolutionary algorithm considering CFI and QoS is proposed to recommend a more reliable combination of manufacturing services for customers. Finally, comparative experiments are conducted to demonstrate the effectiveness and practicability of our method. © 2021 Elsevier Ltd","Collaboration relationship; Data mining; Manufacturing service collaboration; Multi-attribute recommendation","Evolutionary algorithms; Learning algorithms; Machine learning; Manufacture; Collaboration relationship; Cooperative relationships; Manufacturing service; Manufacturing service collaboration; Multi-attribute recommendation; Multi-attributes; Recommendation methods; Service oriented manufacturing; Service recommendations; Services combinations; Data mining",Article,Scopus,2-s2.0-85121907105
"Jackulin Mahariba A., R. A.U., Rajan G.B.","57193098138;57384209100;57204214006;","An efficient automatic accident detection system using inertial measurement through machine learning techniques for powered two wheelers",2022,"Expert Systems with Applications","192",,"116389","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121641739&doi=10.1016%2fj.eswa.2021.116389&partnerID=40&md5=770cd4d89c349a59a435db95c2c555a2","The two-wheeler accidents in most populated and developing countries have become vulnerable and six accidents happen every hour on average. This paper proposes an efficient automatic accident detection system that attempts to detect the occurrences of the accidents in powered two-wheelers (PTW) automatically using vehicle-dependent parameters and the physiological parameters of the rider in real-time. The proposed system builds an accident detection system in PTW using three steps namely, critical event detection system, accident detection system, and severity assessment system. The critical event detection system reads the accelerometer sensor values from the On-Board Diagnostic (OBD) unit mounted on the PTW and classifies the state of the vehicle as normal, fall-like, and fall through the enhanced decision tree algorithm. The enhanced decision tree algorithm uses a tanh function to calculate entropy values. The rules are extracted to fix the threshold by pruning the decision tree to identify the fall of the vehicle and the rider. Due to the unstable nature of PTW and the rider, a novel Adaptive Sequence Window algorithm (ASW) is proposed to substantiate and validate the occurrence of accidents based on the sequence of states identified. Once the accident is detected, the Decision Support System (DSS) running on the OBD mounted on the PTW decides the severity of the accident by combining the three parameters namely fall of the vehicle, fall of the rider, and pulse rate of the rider using the first-order predicate logic rules. The enhanced decision tree algorithm outperforms the other classifiers such as naïve Bayes, artificial neural network, and recurrent neural network with an accuracy of 99.8%. The OBD unit mounted on the PTW and the rider's helmet is used to detect the occurrence of accidents automatically along with its severity with less time. The ASW algorithm enables the system to detect the fall of the vehicle and rider within five minutes and prevents false positives. Further, the information can be communicated based on the severity of the accident to the emergency medical service for quick response. © 2021 Elsevier Ltd","Accident detection; Accident severity; Fall identification; Two-wheeler accident","Accidents; Data mining; Decision trees; Developing countries; Emergency services; Physiological models; Recurrent neural networks; Vehicles; Accident detection systems; Accident detections; Accident severity; Critical events; Decision-tree algorithm; Detection system; Events detection; Fall identification; Two wheelers; Two-wheeler accident; Diagnosis",Article,Scopus,2-s2.0-85121641739
"Torres-González M., Rubio-Bellido C., Bienvenido-Huertas D., Alducin-Ochoa J.M., Flores-Alés V.","57208928165;55347352500;57201717825;56097040700;6503998662;","Long-term environmental monitoring for preventive conservation of external historical plasterworks",2022,"Journal of Building Engineering","47",,"103896","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121446822&doi=10.1016%2fj.jobe.2021.103896&partnerID=40&md5=c397a97e50b8a843fc108fbb9bf077ab","In this study, an analysis of the environmental conditions in a Csa climatic zone for the conservation of plasterwork of the Real Alcázar of Seville is carried out. The measurements obtained from in-situ monitoring are compared with the measurements provided by AEMET (State Meteorological Agency in Spain) during the reference year and the study is completed by estimating future environmental conditions using two alternative approaches: a morphing process from the EPW of the climatic zone and the application of M5P data mining algorithms. An optimal temperature range is established for the conservation of the plasterwork that prevents their dehydration or the freezing of water particles contained. The transformation of gypsum into bassanite, the risks associated with exposures to high relative humidity and the consequences of the slight hygroscopicity of the material and the environmental conditions that must be developed to favor the growth of mold on the surface or the cracking of polychromies that embellish these plaster decorations on numerous occasions are analyzed. The results obtained allow us to establish preventive conservation measures not only on the plasterwork but also on the Real Alcázar of Seville and that architectural heritage located in the subtropical dry-summer climate. © 2021 Elsevier Ltd","Architectural heritage; Plasterwork; Preservation; Relative humidity; Temperature","Architectural heritage; Climatic zone; Environmental conditions; Environmental Monitoring; In-situ monitoring; Meteorological agency; Plasterwork; Preservation; Preventive conservation; Seville; Data mining",Article,Scopus,2-s2.0-85121446822
"de Sousa R.S., Boukerche A., Loureiro A.A.F.","57193435479;7005819374;7006541504;","On the prediction of large-scale road-network constrained trajectories",2022,"Computer Networks","206",,"108337","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123916031&doi=10.1016%2fj.comnet.2021.108337&partnerID=40&md5=e3d7aecb59f3c20e42104447451995e4","Trajectory data mining-based applications benefit from the increasing availability of vehicle trajectory and road network datasets. For instance, the application of trajectory prediction makes it possible to design more efficient routing protocols for vehicular networks. This paper proposes a novel cluster-based framework for the long-term prediction of road-network constrained trajectories. The framework employs a new hierarchical agglomerative clustering algorithm and trains prediction models from historical trajectory datasets. Experimental results show the framework's effectiveness and efficiency to predict trajectories with different characteristics in a new real-world, large-scale scenario. Furthermore, the framework outperformed the related work in terms of prediction accuracy and time complexity. © 2021 Elsevier B.V.","Big data; Clustering; Data mining; Datasets; Prediction; Trajectories","Clustering algorithms; Data mining; Large dataset; Roads and streets; Trajectories; Clusterings; Constrained trajectories; Dataset; Efficient routing; Large-scales; Road network; Routing-protocol; Trajectory data minings; Trajectory prediction; Vehicle trajectories; Forecasting",Article,Scopus,2-s2.0-85123916031
"Lee C., Baek Y., Lin J.C.-W., Truong T., Yun U.","57396765200;57211338088;56449520400;35763325000;57388239400;","Advanced uncertainty based approach for discovering erasable product patterns",2022,"Knowledge-Based Systems","241",,"108134","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124424023&doi=10.1016%2fj.knosys.2022.108134&partnerID=40&md5=add2aa35f8ceeae26e294830945dd467","It is uncertain whether products will be manufactured as defective products. Producing the defective products causes the waste of time and finance in the industrial fields. When the financial crisis occurs in the manufacturing plants, production lines of the less profitable products should be stopped. To overcome the financial crisis, it is required to find the production lines that manufacture the less profitable commodities and are likely to produce the faulty merchandises and remove the corresponding production lines in order to increase the profit. Erasable pattern mining has been suggested to analyze and solve the financial problems by discovering patterns with low profit. As databases become more diverse due to the environment and characteristics of data, demands for erasable pattern mining that processes the various databases increase. An uncertain database, which is one of diverse types of databases, includes the probability related to the item, such as the probability of existence and the probability of defects. Erasable pattern mining, which considers the probability of each item, can discover pattern results which are more suitable for purpose of users. Motivated by this, we propose an efficient tree-based erasable pattern mining algorithm from the uncertain database. Considering the probability of the item, the suggested algorithm extracts erasable patterns with low profits from the uncertain database. Since both profit of the product and the probability of the item are considered, the discovered patterns are more meaningful than the patterns found by the existing erasable pattern mining. Moreover, the proposed method improves the mining operations by utilizing a list structure as well as a tree structure. We performed a performance evaluation on diverse real and synthetic datasets in order to prove the proposed algorithm outperforms the existing erasable pattern mining algorithms. The differences in runtime, memory usage, and scalability are shown through the comparison of performance with other state-of-the-art algorithms and the suggested algorithm. © 2022 Elsevier B.V.","Data mining; Erasable pattern mining; Pattern pruning; Uncertain database","Database systems; Decision trees; Defects; Finance; Manufacture; Probability; Profitability; Defective products; Erasable pattern mining; Financial crisis; Pattern mining; Pattern mining algorithms; Pattern pruning; Product patterns; Production line; Uncertain database; Uncertainty; Data mining",Article,Scopus,2-s2.0-85124424023
"Yi T., Zhang S., Bu Z., Du J., Fang C.","57447459100;57447946500;54789425900;57447331400;57447331500;","Link prediction based on higher-order structure extraction and autoencoder learning in directed networks",2022,"Knowledge-Based Systems","241",,"108241","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124393837&doi=10.1016%2fj.knosys.2022.108241&partnerID=40&md5=bff282fabd17f400bace918489a29a2a","Link prediction, which aims to estimate the likelihood of links based on the observed links and/or node attributes, is a common task in complex network analysis and graph data mining. Modeling and exploring high-order structure, also called network motif, are essential for understanding the fundamental structure that control and mediate the behavior of various complex systems. Most existing link prediction methods do not fully consider the high-order structure in the network. In addition, they mainly focus on undirected networks and therefore ignore the directionality of links. To this end, we propose a novel Autoencoder model based on high-order structure to solve the problems of existing link prediction methods. Specifically, we first provide an efficient motif adjacency matrix learning algorithm, which can be used to extract high-order structure in directed networks, and construct multiple motif adjacency matrices. Then, we extend the Graph Autoencoder (GAE) and Variational Autoencoder (VAE) frameworks to solve the link prediction problem in directed networks, which consists of three core modules. (i) First (Node Embedding): a novel encoder scheme is presented to learn the node embeddings from each motif adjacency matrix; (ii) Second (Information Fusion): a robust attention scheme is further introduced to aggregate information of node embeddings learned from multiple motif adjacency matrices to form the final semantically rich node embedding matrix; (iii) Third (Link Prediction): an efficient decoder scheme is finally proposed to reconstruct the target directed network in a multiscale way, by jointly utilizing node embeddings and PageRank centrality, the directions of links can be effectively inferred based on the classic gravitational heuristic formula. Extensive experiments were applied on various types of directed networks to validate the performance of our proposed approach through comparisons with the state-of-the-art link prediction technologies. © 2022 Elsevier B.V.","Attention; Autoencoder; Directed networks; Higher-order structure; Link prediction; PageRank","Complex networks; Data mining; Directed graphs; Embeddings; Matrix algebra; Adjacency matrix; Attention; Auto encoders; Directed network; Embeddings; High-order structure; Higher-order structure; Link prediction; Page ranks; Prediction methods; Forecasting",Article,Scopus,2-s2.0-85124393837
"Zhou J., Ye M., Liu Z.","57203393264;55127160700;57270517300;","Towards a general correlation for minimum fluidization velocity in gas-fluidized beds: Based on a database mining from the literature",2022,"Chemical Engineering Science","251",,"117455","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124105465&doi=10.1016%2fj.ces.2022.117455&partnerID=40&md5=e6b00c3fbfa4287e567104440263d436","As one of the most significant parameters in fluidized beds design and operation, the minimum fluidization velocity (Umf) has received utmost attention since 1940s. As an effective reference for evaluating the fluidization characteristic, Umf is conventionally predicted using the empirical correlation developed based on experiments for the specified gas-solid system. Despite more than 100 correlations proposed in the literature, these correlations show great diversity in either the applicable regimes or the mathematical formulae. Thus a general correlation for accurately determining Umf is highly desired. In this work, with a recently established database of Umf mined from published papers, the empirical correlations with four different formulae were first assessed for Geldart Groups A, B, and D particles, respectively. In view of the application limitations, modifications to different formulae of empirical correlations have been made to achieve satisfactory performances in predicting Umf for either Geldart Groups A, B, or D particles. Note that Umf is essentially determined by the balance between the gravitational force and drag force for a gas-solid system, a new formula based on a fluidized bed drag correlation derived from direct numerical simulations (DNS) was proposed. By using this formula, we developed a general correlation for Umf in gas-fluidized beds, which exhibits the best performance for all Geldart Groups A, B, and D particles when compared with empirical correlations reported in the literature. © 2022 Elsevier Ltd","Data mining; Gas-fluidized bed; General correlation; Minimum fluidization velocity","Drag; Fluidization; Fluidized beds; Gases; Bed designs; Design and operations; Different formulae; Empirical correlations; Gas-fluidized beds; Gas-solid; General correlations; Minimum fluidization velocity; Performance; Solid systems; Data mining",Article,Scopus,2-s2.0-85124105465
"Mao X., Xia L., Yang L., You Y., Luo P., Li Y., Wu Y., Jiang G.","57365470500;57365542200;57365326000;57220949964;26032172400;35368703400;57215865452;7401706624;","Data mining of natural hazard biomarkers and metabolites with integrated metabolomic tools",2022,"Journal of Hazardous Materials","427",,"127912","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120739554&doi=10.1016%2fj.jhazmat.2021.127912&partnerID=40&md5=7f05c470b89beeb7590ced50fe93d283","Data mining was one of the most important challenges in natural product analysis and biomarker discovery. In this work, we proposed an integrated data analysis protocol for natural products annotation and identification in data–dependent acquisition. Firstly, natural products and structure–related compounds could be identified by comparing mass spectrum behavior with commercial standard. Secondly, diagnostic fragmentation filtering (DFF) function in MZmine (http://mzmine.github.io/) was investigated for screening specific conjugation compounds with the same neutral loss. Thirdly, we present feature–based molecular networking (FBMN) in GNPS (https://gnps.ucsd.edu/) as a chromatographic feature detection and alignment tool. In addition, FBMN could enable natural products analysis based on molecular networks. This proposed integrated protocol should facilitate metabolomic data mining and biomarker discovery. © 2021 Elsevier B.V.","Data mining; High–resolution mass spectrum; Metabolites; Metabolomic; Natural hazard biomarkers","Biomarkers; Filtration; Hazards; Mass spectrometers; Mass spectrometry; Metabolites; Bio-marker discovery; Data analysis protocol; Data dependent; Feature-based; High-resolution mass spectrum; Metabolomics; Natural hazard; Natural hazard biomarker; Natural product analysis; Natural products; Data mining; mycotoxin; biological marker; biological product; biomarker; data mining; metabolite; molecular analysis; natural hazard; Alternaria; Article; chemical structure; cherry; comparative study; conjugation; data mining; data processing; deconvolution; dehydrogenation; demethylation; diagnostic fragmentation filtering; feature detection; filtration; food safety; mass spectrometry; metabolomics; nonhuman; retention time; workflow; data mining; Biological Products; Biomarkers; Data Mining; Metabolomics",Article,Scopus,2-s2.0-85120739554
"Tavakoli M., Faraji A., Vrolijk J., Molavi M., Mol S.T., Kismihók G.","57219055741;57387119600;57219411609;57205770104;8848851400;24067067100;","An AI-based open recommender system for personalized labor market driven education",2022,"Advanced Engineering Informatics","52",,"101508","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125135025&doi=10.1016%2fj.aei.2021.101508&partnerID=40&md5=b4ee26f49533c01abcc904d1d8a99162","Attaining those skills that match labor market demand is getting increasingly complicated, not in the last place in engineering education, as prerequisite knowledge, skills, and abilities are evolving dynamically through an uncontrollable and seemingly unpredictable process. Anticipating and addressing such dynamism is a fundamental challenge to twenty-first century education. The burgeoning availability of data, not only on the demand side but also on the supply side (in the form of open educational resources) coupled with smart technologies, may provide a fertile ground for addressing this challenge. In this paper, we propose a novel, Artificial Intelligence (AI) driven approach to the development of an open, personalized, and labor market oriented learning recommender system, called eDoer. We discuss the complete system development cycle starting with a systematic user requirements gathering, and followed by system design, implementation, and validation. Our recommender prototype (1) derives the skill requirements for particular occupations through an analysis of online job vacancy announcements; (2) decomposes skills into learning topics; (3) collects a variety of open online educational resources that address those topics; (4) checks the quality of those resources and topic relevance with three intelligent prediction models; (5) helps learners to set their learning goals towards their desired job-related skills; (6) recommends personalized learning pathways and learning content based on individual learning goals; and (7) provides assessment services for learners to monitor their progress towards their desired learning objectives. Accordingly, we created a learning dashboard focusing on three Data Science related jobs and conducted an initial validation of eDoer through a randomized experiment. Controlling for the effects of prior knowledge as assessed by means of a pretest, the randomized experiment provided tentative support for the hypothesis that learners who engaged with personal recommendations provided by eDoer to acquire knowledge of basic statistics, attained higher scores on the posttest than those who did not. The hypothesis that learners who received personalized content in terms of format, length, level of detail, and content type, would achieve higher scores than those receiving non-personalized content was not supported. © 2022 The Author(s)","Educational data mining; Open educational resources; Recommender systems","Commerce; Data mining; Employment; Learning systems; Quality control; Demand-side; Educational data mining; Knowledge abilities; Labour market; Learning goals; Market demand; Market driven; Open educational resources; Personalized content; Randomized experiments; Recommender systems",Article,Scopus,2-s2.0-85125135025
"Liu Y., Wan X., Xu W., Shi L., Bai Z., Wang F.","57214947275;55747287600;57353221000;57199773302;26659793400;56734854400;","A novel approach to investigate effects of front-end structures on injury response of e-bike riders: Combining Monte Carlo sampling, automatic operation, and data mining",2022,"Accident Analysis and Prevention","168",,"106599","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125115181&doi=10.1016%2fj.aap.2022.106599&partnerID=40&md5=559dac8372c5e0f4fa82609043870404","Transportation safety related to e-bikes is becoming more problematic with the growing popularity in recent decade years, however, rare studies focused on the protection for e-bike riders in traffic accidents. This paper aimed to investigate the relationship between vehicle front-end structures and rider's injury based on a novel approach including modeling, sampling, and analyzing. Firstly, a parametrized model for front-end structures of the vehicle was developed with nine parameters to realize the standardization of multi-body models of car to e-bike collision considering three stature riders and different impacting velocities. Secondly, a framework, combining Monte Carlo sampling for twelve initial variables and automatic operation for 1000 impact simulations, was built to obtain valid results automatically and then to construct a big dataset. Finally, according to the sensitive variables to riders’ vulnerable regions, the decision tree algorithm was further adopted to develop the decision or prediction model on injuries. The novel approach achieved the stochastical generation of vehicle shapes and the automatic operation of multi-body models. The results showed that the rider's head, pelvis, and thighs were more vulnerable to being injured in the car to e-bike perpendicular accidents. The three decision tree models (HIC15, lateral force of pelvis, bending moment of upper leg) were validated to be accurate and reliable according to the confusion matrix with the precision of more than 80% and the receiver operating characteristic curves (ROC) with the under area more than 85%. Based on decision tree models, not only the effects of front-end structural parameters on the corresponding injury but also the interaction mechanism between various variables can be clearly interpreted. Each route from the same root node to hierarchical middle nodes then to various leaf nodes represented a decision-making process. And the different branches under the same decision node directly illustrated the correlation between variables, which is highly readable and comprehensible. During the safety performance design of front-end structures, the rational value of variables could be decided according to decision routes that resulted in lower injury levels; Even if the accident was inevitable, the collision parameters could be controlled within a certain range for the least injury according to the prediction rules. Based on the novel framework coupling Monte Carlo sampling and automatic operation, it's foreseeable to apply the parametric and standard car-to-e-bike collision models to develop the virtual test system and to optimize front-end shapes for rider's protection. © 2022 Elsevier Ltd","Automatic operation; Data mining; e-Bike accidents; Front-end structures; Injury response",,Article,Scopus,2-s2.0-85125115181
"Yang J., Jiang P., Zheng M., Zhou J., Liu X.","57221928861;57216527435;57198745112;57212684458;56972743900;","Investigating the influencing factors of incentive-based household waste recycling using structural equation modelling",2022,"Waste Management","142",,,"120","131",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125017261&doi=10.1016%2fj.wasman.2022.02.014&partnerID=40&md5=9d559a11e75aa067947d43ae89133210","Household waste recycling management is one of the primary challenges of urban development. Incentive-based recycling systems have been used worldwide to increase the willingness of residents to take part in waste recycling. However, the factors that influence the amount of recyclables collected under the current incentive-based recycling systems have not been investigated thoroughly. In this study, the relationships between influencing factors and recycling behaviour were analysed using partial least squares structural equation modelling under a proposed analysis framework. A real-world case study in Shanghai of China was employed to demonstrate the framework's effectiveness. Six major observations were uncovered based on the studied communities: (1) The amount of recyclables collected increased by 190.9% during the pilot period of the new incentive-based recycling policy. (2) The recycling promotion effect of the new policy reached a peak after approximately three months during the pilot period. (3) Recycling motivation and publicity efforts improved recycling behaviour significantly, but the sense of community belonging and exogenous factors like rainy days and holidays did not necessarily have direct impacts on recycling behaviour. (4) Recycling motivation significantly mediated the relationship between the sense of community belonging and waste recycling behaviour. (5) Although publicity efforts in the studied communities did not necessarily enhance recycling motivation, publicity efforts promoted recycling behaviour significantly in the incentive-based recycling system in Shanghai. (6) Although the studied recycling company has made substantial efforts to formulate attractive recycling prices, its current pricing mechanism still has much room for improvement. This analysis framework and our observations offer insights for government authorities to move towards an enhanced incentive-based recycling system. © 2022","Data mining; Household waste recycling; Incentive-based recycling; Influencing factors; Structural equation modelling","Costs; Data mining; Least squares approximations; Motivation; Urban growth; Waste management; 'current; Analysis frameworks; Household waste recycling; Incentive-based recycling; Influencing factor; Recyclables; Recycling behaviors; Recycling systems; Structural equation models; Wastes recycling; Recycling; article; China; data mining; domestic waste; government; human; incentive; mass medium; motivation; partial least squares regression; price; recycling; structural equation modeling",Article,Scopus,2-s2.0-85125017261
"Liu J., Huang M., Li Z., Zhao L., Zhu Y.","57220786893;55725715700;36606407700;57208125968;55589491400;","A deep learning method for predicting microvoid growth in heterogeneous polycrystals",2022,"Engineering Fracture Mechanics","264",,"108332","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124892782&doi=10.1016%2fj.engfracmech.2022.108332&partnerID=40&md5=878fc5c7be29a1ee751e62bd26cb778e","In heterogeneous polycrystals, microvoid growth presents inherent randomness and dispersion, which generally follows a statistical law. This statistical characteristic intrinsically arises from randomly distributed grain-orientations around microvoids. It remains a huge challenge to explicitly depict the inherent correlation between microvoid growth and grain-orientation distribution by conventional deterministic damage models. In recent years, deep learning has gained in popularity in materials science and has been demonstrated to exhibit excellent data-mining abilities. To our best knowledge, deep learning has not been applied to investigate the statistical damage-evolution issues hitherto. In this work, a novel microvoid growth model based on deep neural network is creatively designed, incorporating both convolutional and long short-term memory components. The former extracts the spatial grain-orientation information, and the latter captures the causal effect of strain history on the microvoid growth. Moreover, to train and test the deep learning-based model, a microvoid-growth database is generated through a large number of crystal plasticity-based finite element simulations, incorporating randomly-oriented grains and different void locations. All the sample data (i.e., the grain-orientation distributions, microvoid locations and microvoid-growth curves) are processed by specific methods (e.g., the pixel-based method) to be amenable for the training process. Our results show that this novel model well captures the statistical characteristic of the microvoid growth in heterogeneous polycrystals. It is expected that the deep learning-based method can provide a new way to predict the microvoid growth at the grain-level. © 2022","Crystal plasticity; Deep learning; Heterogeneous polycrystals; Microvoid growth; Statistical damage model","Data mining; Deep neural networks; Plasticity; Polycrystals; Crystal plasticity; Damage modelling; Deep learning; Grain orientation; Heterogeneous polycrystal; Micro voids; Microvoid growths; Orientation distributions; Statistical characteristics; Statistical damage model; Statistics",Article,Scopus,2-s2.0-85124892782
"Khaji Z., Fakoor M.","55992231600;55570217500;","A Semi-theoretical criterion based on the combination of strain energy release rate and strain energy density concepts (STSERSED): Establishment of a new approach to predict the fracture behavior of orthotropic materials",2022,"Theoretical and Applied Fracture Mechanics","118",,"103290","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124887762&doi=10.1016%2fj.tafmec.2022.103290&partnerID=40&md5=c6e3e66954ca3bd2089cb4be515089a5","Motivated by previous research performed by the authors assessing the fracture behavior in orthotropic materials under mixed-mode I/II loading, this article aims to propose a novel Semi-theoretical criterion for desired crack-fiber angles to assess the fracture of composite materials. Contrary to all previously proposed fracture criteria, in this criterion a mid-point experimental data related to the mixed-mode I/II loading is employed instead of mode I fracture toughness. Utilizing an arbitrary combination of mixed-mode I/II experimental data leads to a significantly simple and accurate fracture assessment criterion. More accurate estimation is achieved using more experimental data in mixed-mode I/II. For the first time, due to the importance of providing a fracture criterion, a Semi-theoretical criterion based on a combination of Strain Energy Release Rate (SER) and Strain Energy Density (SED) criteria, namely STSERSED, is adopted for more accurate anticipating of the fracture of orthotropic materials. Using SED criterion, mode I fracture toughness is theoretically derived and substitutes in the equations of maximum strain release rate. Furthermore, fitting mid-point experimental data into the presented criterion compensate the portion of wasted energy due to the creation of Fracture Process Zone (FPZ) and therefore more accurate fracture estimation is possible. With the passage of pure mode I and the emergence of mode II, the effects of FPZ increase, so changing the critical point from pure mode I to mid-point experimental data makes the extracted fracture behavior closer to the experimental data. Due to using experimental data and the relevant theoretical criterion simultaneously, this criterion can be used in engineering applications to extract the crack behavior of orthotropic materials. The verification of the STSERSED criterion is evaluated by comparing the fracture limit curves (FLC's) with the available experimental data. The extracted results based on the proposed criterion highlight the achievement of this criterion to assess the fracture of orthotropic materials. © 2022 Elsevier Ltd","Maximum strain energy release rate criterion; Mid-point experimental data; Minimum strain energy density; Orthotropic materials; Semi-theoretical criterion","Cracks; Data mining; Energy release rate; Fracture mechanics; Strain energy; Strain rate; Fracture behavior; Maximum strain energy release rate criteria; Maximum strains; Mid-point experimental data; Minimum strain energy density; Mixed mode; Orthotropic materials; Semi-theoretical criteria; Strain energy density; Strain energy release rate; Fracture toughness",Article,Scopus,2-s2.0-85124887762
"Li Q., Xiang P.-Z., Zhang C.","57454069300;57454282600;57195227482;","Feature Information Recognition of Waste Recycling Resource Set Based on Data Mining",2022,"International Journal of Information Systems in the Service Sector","14","2",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124671848&doi=10.4018%2fIJISSS.290545&partnerID=40&md5=8f536b08b3a607da3f99d5746a1c9330","Based on data mining, this paper analyzes the development characteristics of waste recycling resources, classifies the characteristics of waste recycling resources, constructs the evaluation level of characteristic information of waste recycling resources set, and quantitatively analyzes the factors influencing the development of waste recycling resources by using data mining method. This paper analyzes the cause-and-effect classification and importance, analyzing the main factors affecting the development of renewable resources recovery, putting forward the countermeasures and suggestions on how to develop the recycling of renewable resources and further improving the overall operation and supervision system of waste chain. Copyright © 2022, IGI Global.","Data Mining; Feature Information Recognition; Renewable Resources; Waste","Recycling; Cause and effects; Data mining methods; Development characteristics; Feature information; Feature information recognition; Information recognition; Paper analysis; Renewable resource; Resources set; Wastes recycling; Data mining",Article,Scopus,2-s2.0-85124671848
"Zarindast A., Poddar S., Sharma A.","57194202416;57197621655;35574710700;","A Data-Driven Method for Congestion Identification and Classification",2022,"Journal of Transportation Engineering Part A: Systems","148","4","04022012","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124658264&doi=10.1061%2fJTEPBS.0000654&partnerID=40&md5=ef828a4393259ed7ba454dd3e5c04f0e","Congestion detection is one of the key steps in reducing delays and associated costs in traffic management. With the increasing use of global positioning system (GPS)-based navigation, promising speed data are now available. This study used extensive historical probe data (year 2016) in Des Moines, Iowa. We used Bayesian change point detection to segment the speed signal and detect temporal congestion. The detected congestion events were then classified as recurrent congestion (RC) or nonrecurrent congestion (NRC). This paper thus presents a robust statistical, big-data-driven expert system and a big-data-mining methodology for identifying both recurrent and nonrecurrent congestion. © 2022 American Society of Civil Engineers.","Big data; Congestion classification; Recurrent and nonrecurrent congestion; Traffic congestion detection","Big data; Classification (of information); Data mining; Global positioning system; Traffic congestion; Associated costs; Bayesian; Change point detection; Congestion classification; Congestion detection; Data-driven methods; Delay costs; Recurrent and non-recurrent congestions; Traffic congestion detections; Traffic management; Expert systems; Bayesian analysis; classification; data mining; data set; detection method; GPS; identification method; navigation; traffic congestion; Des Moines; Iowa; United States",Article,Scopus,2-s2.0-85124658264
"Mathapo M.C., Mugwabana T.J., Tyasi T.L.","57216591741;57208165006;56819033500;","Prediction of body weight from morphological traits of South African non-descript indigenous goats of Lepelle-Nkumbi Local Municipality using different data mining algorithm",2022,"Tropical Animal Health and Production","54","2","102","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124636631&doi=10.1007%2fs11250-022-03096-9&partnerID=40&md5=27e66eae448b2c3f59b55b97668465f1","Body weight is a vital trait which can assist farmers on selecting animals to use during breeding season. Therefore, the study was conducted to develop the best model to predict body weight from morphological traits through classification and regression tree (CART) and chi-square automatic interaction detector (CHAID) and to determine the relationship between body weight and some morphological traits. A total of 700 South African non-descript indigenous goats (female = 417 and male = 283) between the age of 1 and 5 years old were used in the study. Body weight and some morphological traits viz. body length (BL), heart girth (HG), withers height (WH), rump height (RH), and rump length (RL) were measured in the study. CART, CHAID, and Pearson’s correlation were used for data analysis. CART and CHAID algorithms indicated that predictor factors such as BL, HG, age, and villages had statistical influence on body weight of goats. The study suggests that BL can be used to estimate body weight of South African non-descript indigenous goats. Goodness of fit test suggests that CHAID is a suitable algorithm for prediction of body weight of South African non-descript indigenous goats. Correlation findings indicated that BW had positive highly statistical correlation (P < 0.01) with BL in male and female goats with correlation values of r = 0.65 and r = 0.65, respectively. Findings suggest that improving BL of South African non-descript indigenous goats might improve body weight. © 2022, The Author(s), under exclusive licence to Springer Nature B.V.","Chi-square automatic interaction detector; Classification and regression tree; Goodness of fit; Morphological traits","algorithm; animal; body weight; data mining; female; goat; male; phenotype; Algorithms; Animals; Body Weight; Data Mining; Female; Goats; Male; Phenotype",Article,Scopus,2-s2.0-85124636631
"Jurczuk K., Czajkowski M., Kretowski M.","23008553900;24483199400;55994587000;","GPU-based acceleration of evolutionary induction of model trees",2022,"Applied Soft Computing","119",,"108503","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124627776&doi=10.1016%2fj.asoc.2022.108503&partnerID=40&md5=05a51cb2f0c0553705e3ded8062f33ba","Evolutionary algorithms (EAs) are naturally prone to parallel processing. However, when they are applied to data mining, the fitness calculations start to dominate and the typical population-based decomposition limits the parallel efficiency. When dealing with large-scale data, the scalable solution may become a real challenge. In this article, we propose a GPU-based parallelization of evolutionary induction of model trees. Such trees are a special case of decision tree (DT) that is designed to solve regression problems. The evolutionary approach allows not only a robust prediction but also to preserve the simplicity of DTs. However, the global approach is much more computationally demanding than state-of-the-art greedy inducers, and thus hard to apply to large-scale data mining directly. A parallelized induction of model trees (with univariate tests in the internal nodes and multiple linear regression models in the leaves) requires a carefully designed decomposition strategy. Six GPU-supported procedures are designed to successively: redistribute, sort and rearrange dataset samples, next, calculate models and fitness, and finally gather the results. Experimental validation is performed on real-life and artificial datasets, using various (low- and high-end) GPU accelerators. Results show that the GPU-supported solution enables time-efficient global induction of model trees on large-scale data, which until now was reserved for greedy methods. The obtained speedup is very satisfactory (even up to hundreds of times). The solution is scalable for datasets of different sizes and dimensions. © 2022 Elsevier B.V.","Decision trees; Evolutionary data mining; GPU parallel computing; Large-scale data; Regression","Data mining; Graphics processing unit; Linear regression; Population statistics; Evolutionary data minings; Fitness calculation; GPU parallel computing; Large scale data; Model trees; Parallel efficiency; Parallel processing; Parallelizations; Regression problem; Scalable solution; Decision trees",Article,Scopus,2-s2.0-85124627776
"Ma L., Huang Y., Zhao T.","7403574436;57452897500;7402268129;","A synchronous prediction method for hourly energy consumption of abnormal monitoring branch based on the data-driven",2022,"Energy and Buildings","260",,"111940","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124626115&doi=10.1016%2fj.enbuild.2022.111940&partnerID=40&md5=ad14ca0e25a290d1be2222251969a4fc","Data quality assurance of building energy consumption monitoring platform plays an important role in building energy consumption management. However, Data collected and transferred to the cloud platform is affected by many factors and shows some missing values and outliers in the time series. Combining with the correlation of smart meters on branches and regularity of building energy consumption characteristics, this paper proposes a synchronous prediction method for predicting building energy consumption in the secondary branch. In this model, synchronous data feature similarity (SDFS) model is used to find a similar energy consumption feature, Extreme gradient boosting (XGBoost) model is used to train and produce accurate prediction results which are compared with back propagation neural network (BPNN) and adaptive boosting (AdaBoost) and maximum distance outlier correction (MDOC) model can further correct the prediction results. Taking the daily energy consumption of the primary branch with a smart meter in the building energy consumption monitoring platform as the test object, the predition results of VRF energy consumption show that the MAE, MAPE, RMSE, and CVRMSE are 1.150, 0.142, 1.511, and 0.132 respectively, which is much lower than BPNN and AdaBoost. This study explores a novel feature mining method for historical data and an integrated model for outlier recognition and correction which significantly improves the accuracy of prediction results. Moreover, after correlation verification, the prediction model can be widely applied in building distribution system with sub-meter system which improves the data utilization rate of building energy management system. © 2022 Elsevier B.V.","Data correction; Data mining; Energy consumption; Feature selection","Adaptive boosting; Backpropagation; Data mining; Energy conservation; Energy management systems; Forecasting; Information management; Neural networks; Quality assurance; Smart meters; Back-propagation neural networks; Building energy consumption; Data corrections; Data driven; Energy consumption monitoring; Energy-consumption; Features selection; In-buildings; Monitoring platform; Prediction methods; Energy utilization",Article,Scopus,2-s2.0-85124626115
"Mishra K., Basu S., Maulik U.","57202200899;57207486409;6603607810;","Graft: A graph based time series data mining framework",2022,"Engineering Applications of Artificial Intelligence","110",,"104695","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124612112&doi=10.1016%2fj.engappai.2022.104695&partnerID=40&md5=b502909492f4e8ee41cca9043ea7b6d1","Rapid technology integration causes a high dimensional time series data accumulation in multiple domains and applying the classical data mining tools and techniques becomes a challenging task. Hence, the time series data representation have gained popularity over the years, which ease the task of mining, analysis and visualization. Graph based representation is one such emerging tool in which the time series data is represented as nodes and edges of graph. The current graph based representation is designed either to mine the motif or discords from a single time series or cluster the time series where each node represents a time series sample. Such representation technique causes information loss and also no further analysis could be performed other than clustering. To address these challenges, we propose a unique graph representation for time series dataset that works on multiple domains. Novelty of the graph representation is that it is unique for multiple time series and it acts as a framework for whole time series clustering, temporal pattern extraction from each cluster and temporally dependent rare event discovery. A new research direction for the proposed graph based framework is shown. Comparative analysis reveal the superiority of the proposed framework particularly as a clustering technique. The key contributions of the paper are: (i) transformation strategy of time series database from time domain to graph structure in topological domain (ii) time series clustering using path level analysis (iii) identification of temporally dependent co-occurring patterns (iv) rare event detection using component level analysis © 2022 Elsevier Ltd","Clustering; Directed and undirected graphs; Eigen values; Graph based networks; Time series similarity","Data mining; Data visualization; Directed graphs; Graphic methods; Time domain analysis; Time series analysis; Undirected graphs; Clusterings; Eigen-value; Graph based network; Graph-based; Graph-based representations; Multiple domains; Time series similarity; Time-series data; Times series; Undirected graph; Time series",Article,Scopus,2-s2.0-85124612112
"Murri R., Masciocchi C., Lenkowicz J., Fantoni M., Damiani A., Marchetti A., Sergi P.D.A., Arcuri G., Cesario A., Patarnello S., Antonelli M., Bellantone R., Bernabei R., Boccia S., Calabresi P., Cambieri A., Cauda R., Colosimo C., Crea F., De Maria R., De Stefano V., Franceschi F., Gasbarrini A., Landolfi R., Parolini O., Richeldi L., Sanguinetti M., Urbani A., Zega M., Scambia G., Valentini V., the Gemelli against Covid Group","7006313086;57203810504;57194679683;7006554507;55620912500;57221601253;57222029290;57226082813;7006465542;57219864227;7102393593;7005836718;57447252400;57217786181;7102418853;6603182894;7006356239;7006169189;35371766500;57226065469;7006192748;7005989850;55512984500;7006420209;7003869202;7003874268;57209097150;55198663000;26650262700;7102799259;7006177042;","A real-time integrated framework to support clinical decision making for covid-19 patients",2022,"Computer Methods and Programs in Biomedicine","217",,"106655","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124421101&doi=10.1016%2fj.cmpb.2022.106655&partnerID=40&md5=47260f29b0df76385e922514de0a4c26","Background: The COVID-19 pandemic affected healthcare systems worldwide. Predictive models developed by Artificial Intelligence (AI) and based on timely, centralized and standardized real world patient data could improve management of COVID-19 to achieve better clinical outcomes. The objectives of this manuscript are to describe the structure and technologies used to construct a COVID-19 Data Mart architecture and to present how a large hospital has tackled the challenge of supporting daily management of COVID-19 pandemic emergency, by creating a strong retrospective knowledge base, a real time environment and integrated information dashboard for daily practice and early identification of critical condition at patient level. This framework is also used as an informative, continuously enriched data lake, which is a base for several on-going predictive studies. Methods: The information technology framework for clinical practice and research was described. It was developed using SAS Institute software analytics tool and SAS® Vyia® environment and Open-Source environment R ® and Python ® for fast prototyping and modeling. The included variables and the source extraction procedures were presented. Results: The Data Mart covers a retrospective cohort of 5528 patients with SARS-CoV-2 infection. People who died were older, had more comorbidities, reported more frequently dyspnea at onset, had higher D-dimer, C-reactive protein and urea nitrogen. The dashboard was developed to support the management of COVID-19 patients at three levels: hospital, single ward and individual care level. Interpretation: The COVID-19 Data Mart based on integration of a large collection of clinical data and an AI-based integrated framework has been developed, based on a set of automated procedures for data mining and retrieval, transformation and integration, and has been embedded in the clinical practice to help managing daily care. Benefits from the availability of a Data Mart include the opportunity to build predictive models with a machine learning approach to identify undescribed clinical phenotypes and to foster hospital networks. A real-time updated dashboard built from the Data Mart may represent a valid tool for a better knowledge of epidemiological and clinical features of COVID-19, especially when multiple waves are observed, as well as for epidemic and pandemic events of the same nature (e. g. with critical clinical conditions leading to severe pulmonary inflammation). Therefore, we believe the approach presented in this paper may find several applications in comparable situations even at region or state levels. Finally, models predicting the course of future waves or new pandemics could largely benefit from network of DataMarts. © 2022",,"Behavioral research; Clinical research; Data integration; Data mining; Decision making; Diseases; Hospitals; Knowledge based systems; Metadata; Open source software; Open systems; Patient treatment; Python; SARS; Software prototyping; Centralised; Clinical decision making; Clinical practices; Data mart; Healthcare systems; Integrated frameworks; Patient data; Predictive models; Real- time; Real-world; Hospital data processing",Article,Scopus,2-s2.0-85124421101
"Molinos-Senante M., Maziotis A.","35765339200;57204311254;","Prediction of the efficiency in the water industry: An artificial neural network approach",2022,"Process Safety and Environmental Protection","160",,,"41","48",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124385115&doi=10.1016%2fj.psep.2022.02.012&partnerID=40&md5=de8b7b2bd13cfa22172585cdaa84366d","The measurement of efficiency of water utilities has been traditionally carried out using econometric methods or linear programming techniques. Alternatively, in this study a data mining non-parametric method is used, such as an artificial neural network (ANN) approach, to predict the efficiency of several water companies in England and Wales. The further use of a regression tree model allowed us to visualize and quantify the impact of operating characteristics on efficiency. The average efficiency score for the water industry was 0.411. Average scores for water only companies and water and sewerage companies were 0.210 and 0.626, respectively. Only one water company was identified as being fully efficient. This indicates that most of the English and Welsh water companies need to make substantial improvements in their managerial practices to catch-up with the most efficient ones in the industry. Several operating characteristics such as water leakage, water taken from different sources and population density were found to influence efficiency. The percentage of water leakage was identified as the most relevant operational variable influencing the efficiency of water companies. The findings of our study aim to support benchmarking analysis in regulated industries and to get a better insight on what drives efficiency. © 2022 The Institution of Chemical Engineers","Artificial neural networks; Data envelopment analysis; Efficiency measurement; Environmental variables; Regression tree; Water utilities","Data envelopment analysis; Data mining; Digital storage; Forestry; Leakage (fluid); Linear programming; Neural networks; Population statistics; Trees (mathematics); Artificial neural network approach; Efficiency measurement; Environmental variables; Measurements of; Operating characteristics; Regression trees; Water companies; Water industries; Water leakage; Water utility; Efficiency",Article,Scopus,2-s2.0-85124385115
"Qin J., Wang J., Li Q., Fang S., Li X., Lei L.","57216879768;55742573400;57443590900;57225696364;56451244300;56597438400;","Differentially private frequent episode mining over event streams",2022,"Engineering Applications of Artificial Intelligence","110",,"104681","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124231015&doi=10.1016%2fj.engappai.2022.104681&partnerID=40&md5=4e59d76730348ee118c188f36b81bd00","Frequent episode mining is a wide range framework of data mining from sequential data with many applications, which is a totally short-ordered collection of event-types and unearths temporal correlations without information loss over event streams. While offering substantial benefits, directly releasing frequent episodes to the public will enormously threaten the individual's privacy. However, there is little work so far concentrating on privately frequent episode mining. In this paper, we investigate the privacy problem in mining frequent episodes from event streams due to continuous releases in successive windows and propose a real-time differentially private frequent episode mining algorithm over event streams to avoid the privacy leakage with ω-event privacy guarantee. To obtain private frequent episodes, we propose a sample-based perturbation approach, which improves the accuracy of selecting frequent episodes based on sampling databases. To reduce the privately mining time and avoid repeatedly privacy budget allocation to coincident window of adjacent releases as much as possible, we present an incremental perturbation approach according to the judgment in dissimilarity calculation mechanism. Meanwhile, in order to protect data collected from any ω successive timestamps over event streams, we employ an adaptive ω-event privacy mechanism on the basis of the dynamicity of episodes. Finally, experimental results on real-world datasets demonstrate the effectiveness and efficiency of our algorithm. © 2022 Elsevier Ltd","Differential privacy; Event streams; Frequent episode; Privacy preservation; Real-time data mining","Budget control; Differential privacies; Event streams; Event Types; Frequent episode minings; Frequent episodes; Perturbation approach; Privacy preservation; Real-time data mining; Sequential data; Temporal correlations; Data mining",Article,Scopus,2-s2.0-85124231015
"Zhang H., Cheng J., Zhang L., Li Y., Zhang W.","57221296360;57199449820;57222508162;55907838900;56119460300;","H2GNN: Hierarchical-Hops Graph Neural Networks for Multi-Robot Exploration in Unknown Environments",2022,"IEEE Robotics and Automation Letters","7","2",,"3435","3442",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124214723&doi=10.1109%2fLRA.2022.3146912&partnerID=40&md5=dd8a10a1c36d38c17167e14ce87f61fd","Multi-robot coarse-to-fine exploration in unknown environments makes great sense in many application fields like search and rescue. For different stages of the task, robots need to extract information from the environment discriminately, which can improve their decision-making capability. To this end, we present the Hierarchical-Hops Graph Neural Networks (H2GNN) to enable robots to targetedly integrate the key information of the graph-represented environment, which distinguishes the importance of information from different hops around robots based on the multi-head attention mechanism. And in order to improve the efficiency of cooperation, we utilize multi-agent reinforcement learning (MARL) to help robots to learn collaborative strategies implicitly. We conduct experiments to verify our proposed method in a simulation environment, and the experimental results demonstrate that the H2GNN significantly improves the multi-robot exploration performance in unknown environments. © 2016 IEEE.","Autonomous exploration; Graph neural networks; Multi-robot system","Decision making; Fertilizers; Graph neural networks; Industrial robots; Job analysis; Multi agent systems; Multipurpose robots; Reinforcement learning; Robot learning; Autonomous exploration; Features extraction; Graph neural networks; Multi-robot exploration; Multi-robot systems; Multirobots; Robot kinematics; Robot sensing system; Task analysis; Unknown environments; Data mining",Article,Scopus,2-s2.0-85124214723
"Uronen L., Salanterä S., Hakala K., Hartiala J., Moen H.","57194025115;6701804966;55307771500;7005510609;25655389700;","Combining supervised and unsupervised named entity recognition to detect psychosocial risk factors in occupational health checks",2022,"International Journal of Medical Informatics","160",,"104695","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124195123&doi=10.1016%2fj.ijmedinf.2022.104695&partnerID=40&md5=5dc4fd77abea51e186599e295cbdff09","Introduction: In occupational health checks the information about psychosocial risk factors, which influence work ability, is documented in free text. Early detection of psychosocial risk factors helps occupational health care to choose the right and targeted interventions to maintain work capacity. In this study the aim was to evaluate if we can automate the recognition of these psychosocial risk factors in occupational health check electronic records with natural language processing (NLP). Materials and methods: We compared supervised and unsupervised named entity recognition (NER) to detect psychosocial risk factors from health checks’ documentation. Occupational health nurses have done these records. Results: Both methods found over 60% of psychosocial risk factors from the records. However, the combination of BERT-NER (supervised NER) and QExp (query expansion/paraphrasing) seems to be more suitable. In both methods the most (correct) risk factors were found in the work environment and equipment category. Conclusion: This study showed that it was possible to detect risk factors automatically from free-text documentation of health checks. It is possible to develop a text mining tool to automate the detection of psychosocial risk factors at an early stage. © 2022 The Author(s)","Health check; Occupational health; Psychosocial risk factors; Text mining, named entity recognition","Character recognition; Data mining; Industrial hygiene; Natural language processing systems; Occupational risks; Electronic records; Free texts; Health checks; Named entity recognition; Occupational health; Psychosocial risk factors; Risk factors; Text mining, named entity recognition; Work abilities; Work capacities; Health risks; article; documentation; human; mining; natural language processing; occupational health; occupational health nursing; risk assessment; risk factor; work environment",Article,Scopus,2-s2.0-85124195123
"Ishii Y., Hayakawa K., Koide S., Chikaraishi M.","57201553862;57443859600;57444436200;26031259900;","Entropy Tucker model: Mining latent mobility patterns with simultaneous estimation of travel impedance parameters",2022,"Transportation Research Part C: Emerging Technologies","137",,"103559","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124177628&doi=10.1016%2fj.trc.2022.103559&partnerID=40&md5=8521ffa3f268c96ca6779dfd24eef605","With the rapid increase in the availability of passive data in the field of transportation, combining machine learning with transportation models has emerged as an important research topic in recent years. This study proposes an entropy Tucker model that integrates (1) a Tucker decomposition technique, i.e., an existing machine learning method, and (2) an entropy maximizing model, i.e., an existing model used for modeling trip distribution in the field of transportation. In addition, an optimization algorithm is presented to empirically identify the proposed model. The proposed model provides a solid theoretical foundation for the machine learning method, substantially improves prediction performance, and provides richer behavioral implications through empirical parameter estimation of travel impedance. We conducted a case study using public transit smart card data. The results showed that the proposed model improves the prediction performance and interpretability of the results compared to the conventional nonnegative Tucker decomposition technique. Further, we empirically confirmed that the travel impedance varies with the origin–destination pair, time of the day, and day of the week. Finally, we discussed how embedding the theoretical foundations of transport modeling into machine learning methods can facilitate the use of various passive data in wider practical contexts of transport policy decision making. © 2022 The Author(s)","Data-driven; Entropy maximizing model; Knowledge-driven; Mobility pattern; Smart card data; Tensor decomposition","Data mining; Decision making; Machine learning; Parameter estimation; Smart cards; Data driven; Decomposition technique; Entropy maximizing model; Knowledge-driven; Machine learning methods; Mobility pattern; Smart card data; Tensor decomposition; Theoretical foundations; Travel impedances; Entropy",Article,Scopus,2-s2.0-85124177628
"da Costa N.L., de Sá Alves M., de Sá Rodrigues N., Bandeira C.M., Oliveira Alves M.G., Mendes M.A., Cesar Alves L.A., Almeida J.D., Barbosa R.","57191824729;57280532500;57280155500;7801356701;57210199273;7102005557;57442247700;55667453700;8938657100;","Finding the combination of multiple biomarkers to diagnose oral squamous cell carcinoma – A data mining approach",2022,"Computers in Biology and Medicine","143",,"105296","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124169435&doi=10.1016%2fj.compbiomed.2022.105296&partnerID=40&md5=c06fdd32f0d90d42273f8b067b650b03","Data mining has proven to be a reliable method to analyze and discover useful knowledge about various diseases, including cancer research. In particular, data mining and machine learning algorithms to study oral squamous cell carcinoma (OSCC), the most common form of oral cancer, is a new area of research. This malignant neoplasm can be studied using saliva samples. Saliva is an important biofluid that must be used to verify potential biomarkers associated with oral cancer. In this study, first, we provide an overview of OSSC diagnoses based on machine learning and salivary metabolites. To our knowledge, this is the first study to apply advanced data mining techniques to diagnose OSCC. Then, we give new results of classification and feature selection algorithms used to identify potential salivary biomarkers of OSCC. To accomplish this task, we used the filter feature selection random forest importance algorithm and a wrapper methodology to evaluate the importance of metabolites obtained from gas chromatography mass-spectrometry (GC-MS) in the context of differentiation of OSCC and the control group. Salivary samples (n = 68) were collected for the control group, and the OSCC group were from patients matched for gender, age, and smoking habit. The classification process occurred based on Random Forest (RF) classification algorithm along with 10-cross validation. The results showed that glucuronic acid, maleic acid, and batyl alcohol can classify the samples with an area under the curve (AUC) of 0.91 versus an AUC of 0.76 using all 51 metabolites analyzed. The methodology used in this study can assist healthcare professionals and be adopted to discover diagnostic biomarkers for other diseases. © 2022","Data mining; Feature selection; Machine learning; Metabolites; Oral squamous cell carcinoma; Salivary biomarkers","Biomarkers; Biomolecules; Body fluids; Decision trees; Diagnosis; Diseases; Feature extraction; Filtration; Gas chromatography; Glucose; Machine learning; Mass spectrometry; Metabolites; Random forests; Areas under the curves; Cancer research; Control groups; Features selection; Machine learning algorithms; Multiple biomarkers; Oral cancer; Oral squamous cell carcinomata; Reliable methods; Salivary biomarkers; Data mining",Article,Scopus,2-s2.0-85124169435
"Zhou S., Wang X., ster M., Li B., Ye C., Zhang Z., Wang C., Bu J.","57204471479;57221484284;57440822900;57441325400;57441024400;57206280981;56017252400;55684269900;","Direction-Aware User Recommendation Based on Asymmetric Network Embedding",2022,"ACM Transactions on Information Systems","40","2","3466754","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124107292&doi=10.1145%2f3466754&partnerID=40&md5=889285c316074ac571ac26f85dc26533","User recommendation aims at recommending users with potential interests in the social network. Previous works have mainly focused on the undirected social networks with symmetric relationship such as friendship, whereas recent advances have been made on the asymmetric relationship such as the following and followed by relationship. Among the few existing direction-aware user recommendation methods, the random walk strategy has been widely adopted to extract the asymmetric proximity between users. However, according to our analysis on real-world directed social networks, we argue that the asymmetric proximity captured by existing random walk based methods are insufficient due to the inbalance in-degree and out-degree of nodes.To tackle this challenge, we propose InfoWalk, a novel informative walk strategy to efficiently capture the asymmetric proximity solely based on random walks. By transferring the direction information into the weights of each step, InfoWalk is able to overcome the limitation of edges while simultaneously maintain both the direction and proximity. Based on the asymmetric proximity captured by InfoWalk, we further propose the qualitative (DNE-L) and quantitative (DNE-T) directed network embedding methods, capable of preserving the two properties in the embedding space. Extensive experiments conducted on six real-world benchmark datasets demonstrate the superiority of the proposed DNE model over several state-of-the-art approaches in various tasks. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.","graph neural networks; random walk; User recommendation","Data mining; Graph neural networks; Random processes; Asymmetric networks; Graph neural networks; In-Degree; Network embedding; Random Walk; Random walk strategies; Real-world; Recommendation methods; Symmetrics; User recommendations; Network embeddings",Article,Scopus,2-s2.0-85124107292
"Qiu R., Huang Z., Chen T., Yin H.","57211938334;57201920068;57196727070;55007318200;","Exploiting Positional Information for Session-Based Recommendation",2022,"ACM Transactions on Information Systems","40","2","3473339","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124097136&doi=10.1145%2f3473339&partnerID=40&md5=b205306098b041cea82882aa504c9781","For present e-commerce platforms, it is important to accurately predict users' preference for a timely next-item recommendation. To achieve this goal, session-based recommender systems are developed, which are based on a sequence of the most recent user-item interactions to avoid the influence raised from outdated historical records. Although a session can usually reflect a user's current preference, a local shift of the user's intention within the session may still exist. Specifically, the interactions that take place in the early positions within a session generally indicate the user's initial intention, while later interactions are more likely to represent the latest intention. Such positional information has been rarely considered in existing methods, which restricts their ability to capture the significance of interactions at different positions. To thoroughly exploit the positional information within a session, a theoretical framework is developed in this paper to provide an in-depth analysis of the positional information. We formally define the properties of forward-awareness and backward-awareness to evaluate the ability of positional encoding schemes in capturing the initial and the latest intention. According to our analysis, existing positional encoding schemes are generally forward-aware only, which can hardly represent the dynamics of the intention in a session. To enhance the positional encoding scheme for the session-based recommendation, a dual positional encoding (DPE) is proposed to account for both forward-awareness and backward-awareness. Based on DPE, we propose a novel Positional Recommender (PosRec) model with a well-designed Position-aware Gated Graph Neural Network module to fully exploit the positional information for session-based recommendation tasks. Extensive experiments are conducted on two e-commerce benchmark datasets, Yoochoose and Diginetica and the experimental results show the superiority of the PosRec by comparing it with the state-of-the-art session-based recommender models. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.","graph neural network; positional encoding; Session-based recommendation","Data mining; Electronic commerce; Encoding (symbols); Graph neural networks; Network coding; 'current; Commerce platforms; E- commerces; Encoding schemes; Graph neural networks; Historical records; Positional encoding; Positional information; Session-based recommendation; User's preferences; Recommender systems",Article,Scopus,2-s2.0-85124097136
"Dai X., Xi Y., Zhang W., Liu Q., Tang R., He X., Hou J., Wang J., Yu Y.","57224688438;57219876417;56108513500;57211289518;50362152900;55235819900;57219876466;55902731900;8723751600;","Beyond Relevance Ranking: A General Graph Matching Framework for Utility-Oriented Learning to Rank",2022,"ACM Transactions on Information Systems","40","2","3464303","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124090918&doi=10.1145%2f3464303&partnerID=40&md5=2d66a81c6371f68490c98d1d1d2ee78d","Learning to rank from logged user feedback, such as clicks or purchases, is a central component of many real-world information systems. Different from human-annotated relevance labels, the user feedback is always noisy and biased. Many existing learning to rank methods infer the underlying relevance of query-item pairs based on different assumptions of examination, and still optimize a relevance based objective. Such methods rely heavily on the correct estimation of examination, which is often difficult to achieve in practice. In this work, we propose a general framework U-rank+ for learning to rank with logged user feedback from the perspective of graph matching. We systematically analyze the biases in user feedback, including examination bias and selection bias. Then, we take both biases into consideration for unbiased utility estimation that directly based on user feedback, instead of relevance. In order to maximize the estimated utility in an efficient manner, we design two different solvers based on Sinkhorn and LambdaLoss for U-rank+. The former is based on a standard graph matching algorithm, and the latter is inspired by the traditional method of learning to rank. Both of the algorithms have good theoretical properties to optimize the unbiased utility objective while the latter is proved to be empirically more effective and efficient in practice. Our framework U-rank+ can deal with a general utility function and can be used in a widespread of applications including web search, recommendation, and online advertising. Semi-synthetic experiments on three benchmark learning to rank datasets demonstrate the effectiveness of U-rank+. Furthermore, our proposed framework has been deployed on two different scenarios of a mainstream App store, where the online A/B testing shows that U-rank+ achieves an average improvement of 19.2% on click-through rate and 20.8% improvement on conversion rate in recommendation scenario, and 5.12% on platform revenue in online advertising scenario over the production baselines. © 2021 Association for Computing Machinery.","examination bias; graph matching; implicit feedback; Learning to rank; position bias; selection bias; utility maximization","Data mining; Learning systems; Examination bias; General graph; Graph matchings; Implicit feedback; Online advertizing; Position bias; Relevance ranking; Selection bias; User feedback; Utility maximizations; Marketing",Article,Scopus,2-s2.0-85124090918
"Vu D.-H.","57205527331;","Privacy-preserving Naive Bayes classification in semi-fully distributed data model",2022,"Computers and Security","115",,"102630","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124075473&doi=10.1016%2fj.cose.2022.102630&partnerID=40&md5=e3d20d30b353b2825715a98a4b132b3a","In recent years, issues of privacy preservation in data mining and machine learning have received more and more attention from the research community. Privacy-preserving data mining and machine learning solutions enable data holders to jointly discover knowledge and valuable information, as well as construct machine learning models without privacy concerns. In this paper, we address the distressing problem of privacy-preservation for a novel data model called the semi-fully distributed setting. Differently from the existing scenarios, each record of the dataset in this data model is composed of three parts, in which the first part is privately kept by a data user, the second one is securely stored by the miner, and the rest is publicly known by both the miner and the data user. For this new data model, we propose a privacy-preserving Naive Bayes classification solution based on secure multi-party computation. Our proposed solution not only achieves a high level of privacy but also guarantees the accuracy of the classification model. The experimental results show that the new proposal is efficient in real-life applications. Furthermore, our pioneering study paves the way for new researches into privacy preservation issues for the semi-fully distributed data model. © 2022 Elsevier Ltd","Data privacy; Distributed computing; Naive Bayes classification; Privacy-preserving data mining and machine learning; Secure multi-party computation","Classification (of information); Data mining; Machine learning; Miners; Data users; Distributed data; Machine learning models; Naive Bayes classification; Privacy preservation; Privacy preserving; Privacy-preserving data mining; Privacy-preserving data mining and machine learning; Research communities; Secure multi-party computation; Privacy-preserving techniques",Article,Scopus,2-s2.0-85124075473
"Zola F., Segurola-Gil L., Bruse J.L., Galar M., Orduna-Urrutia R.","57210337576;57274439100;57441607500;35731257600;57331383000;","Network traffic analysis through node behaviour classification: a graph-based approach with temporal dissection and data-level preprocessing",2022,"Computers and Security","115",,"102632","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124024189&doi=10.1016%2fj.cose.2022.102632&partnerID=40&md5=f7d1fa4f8036aeb63d7f02afc09cebd1","Network traffic analysis is an important cybersecurity task, which helps to classify anomalous, potentially dangerous connections. In many cases, it is critical not only to detect individual malicious connections, but to detect which node in a network has generated malicious traffic so that appropriate actions can be taken to reduce the threat and increase the system's cybersecurity. Instead of analysing connections only, node behavioural analysis can be performed by exploiting the graph information encoded in a connection network. Network traffic, however, is temporal data and extracting graph information without a fixed time scope may only unveil macro-dynamics that are less related to cybersecurity threats. To address these issues, a threefold approach is proposed here: firstly, temporal dissection for extracting graph-based information is applied. As the resulting graphs are typically affected by class imbalance (i.e. malicious nodes are under-represented), two novel graph data-level preprocessing techniques - R-hybrid and SM-hybrid - are introduced, which focus on exploiting the most relevant graph substructures. Finally, a Neural Network (NN) and two Graph Convolutional Network (GCN) approaches are compared when performing node behaviour classification. Furthermore, we compare the node classification performance of these supervised models with traditional unsupervised anomaly detection techniques. Results show that temporal dissection parameters affected classification performance, while the data-level preprocessing strategies reduced class imbalance and led to improved supervised node behaviour classification, outperforming anomaly detection models. In particular, Neural Network (NN) outperformed Graph Convolutional Network (GCN) approaches for two attack families and was less affected by class imbalance, yet one GCN performed best overall. The presented study successfully applies a temporal graph-based approach for malicious actor detection in network traffic data. © 2022 The Author(s)","Deep Learning; Graph Convolutional Networks; Graph imbalance problem; Network analysis; Temporal dissection","Anomaly detection; Convolution; Convolutional neural networks; Cybersecurity; Data mining; Deep learning; Graph theory; Behaviour classification; Convolutional networks; Cyber security; Data level; Deep learning; Graph convolutional network; Graph imbalance problem; Graph-based; Imbalance problem; Temporal dissection; Graphic methods",Article,Scopus,2-s2.0-85124024189
"Tianhao Z., Sh. Majdi H., Olegovich Bokov D., Abdelbasset W.K., Thangavelu L., Su C.-H., Chinh Nguyen H., Alashwal M., Ghazali S.","57437721700;57226554798;56845561300;57208873763;57262401100;16231987300;57363533200;57222374438;57191379755;","Prediction of busulfan solubility in supercritical CO2 using tree-based and neural network-based methods",2022,"Journal of Molecular Liquids","351",,"118630","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123924716&doi=10.1016%2fj.molliq.2022.118630&partnerID=40&md5=b032887c7a01acb6c76c635a495ca543","Drug solubility is a critical parameter in the pharmaceutical industry for developing efficient processes for production of nanomedicine at industrial scale. Several attempts have been made in recent years to investigate and obtain this parameter using various data mining methods, including neural networks. In this study, to reduce the error rate in predicting solubility, three methods including Multi-layer Perceptron (MLP), decision tree, and random forest have been applied to 32 rows of experimental data collected from literature for solubility of a model drug in supercritical CO2. Afterwards, the results of these models are examined and compared with measured data to calibrate and validate the developed models. Finally, the mean squared error improved to 1.77 e −5 in Random Forest Model. MLP and decision tree models mean squared errors are equal to 6.72 e −5 and 3.28 e −5, respectively which is a good result, especially when we can guarantee that the model did not have more problems in predicting the drug solubility and can be used as reliable methods in the pharmaceutical area. © 2022 Elsevier B.V.","Decision tree; Drug; Machine learning; Solubility; Supercritical processing","Carbon dioxide; Data mining; Decision trees; Drug delivery; Forecasting; Machine learning; Mean square error; Medical nanotechnology; Busulfan; Drug; Drug solubility; Mean squared error; Multilayers perceptrons; Network-based; Neural-networks; Supercritical CO 2; Supercritical processing; Tree-based networks; Solubility",Article,Scopus,2-s2.0-85123924716
"Mu T., Wang H., Wang C., Liang Z., Shao X.","57217029103;14061534000;57209508178;57219762184;57435115900;","Auto-CASH: A meta-learning embedding approach for autonomous classification algorithm selection",2022,"Information Sciences","591",,,"344","364",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123843336&doi=10.1016%2fj.ins.2022.01.040&partnerID=40&md5=520855686d9825d2550a992cb99abaa3","With years of development, machine learning algorithms have excellent performance in some tasks of data analysis and data mining. To apply machine learning to new tasks, suitable algorithm and hyperparameters selection techniques, which is known as Combined Algorithm Selection and Hyperparameter optimization problem, are in demand. In the field of data analysis, how to automate the algorithm selection process has become a hot research topic in recent years. Most of the existing approaches are developed under the background of Automated Machine Learning with high time or space complexity. To alleviate the issue, an approach extracts and learns from prior experience based on meta-learning theory named Auto-CASH is proposed in this paper. One of the major drawbacks of existing meta-learning methods is that they rely too much on human expertise to extract and filter knowledge that guides subsequent training. Auto-CASH can automatically select features of tasks by introducing a reinforcement learning strategy. Thus Auto-CASH becomes less dependent on human expertise. Besides, two pruning strategies when processing Hyperparameter Optimization to improve efficiency are firstly proposed. Extensive experiments on classification tasks are conducted and results demonstrate that Auto-CASH outperforms state-of-the-art CASH approaches and popular AutoML systems with less time cost. © 2022 Elsevier Inc.","AutoML; Autonomous algorithm selection; Classification; Deep Q-Network; Hyperparameter optimization; Meta-learning","Data handling; Data mining; Deep learning; Filtration; Learning algorithms; Algorithm selection; Automl; Autonomous algorithm selection; Classification algorithm; Deep Q-network; Embeddings; Human expertise; Hyper-parameter optimizations; Machine learning algorithms; Metalearning; Reinforcement learning",Article,Scopus,2-s2.0-85123843336
"Zeng F.J., Ding L., Chai X.G.","55889456500;57435512600;57435374800;","Urban traffic condition recognition and accessibility prediction based on big data",2022,"Advances in Transportation Studies","56",,,"143","157",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123837298&doi=10.53136%2f979125994790110&partnerID=40&md5=231d009ed7eadf2e9238226326566de6","Traffic congestion, frequent traffic accidents, and air pollution problems in the cities have brought opportunities for the development of intelligent transportation technologies and the data mining of massive traffic spatiotemporal data. Few existing studies have talked about the feature layer data fusion that reflects the main characteristics of the big data of traffic flow time series, and the prediction methods of temporal and spatial accessibility from alternative starting points to destinations in urban traffic spatiotemporal network are pending further research. For these reasons, this paper focused on traffic condition recognition and accessibility prediction based on multi-source big data of urban traffic flow. At first, this paper proposed a method for extracting the features of urban traffic spatiotemporal data, and realized preprocessing and fusion of the obtained data of the traffic flow and average vehicle speed of the road network. Then, a combinatorial optimization algorithm of simulated annealing (SA) and particle swarm optimization (PSO), and the Fuzzy C- Means (FCM) algorithm were employed to perform fuzzy classification on urban traffic conditions, and the urban traffic spatiotemporal network was used to describe the optimization objective function of urban traffic accessibility. Finally, this paper used real cases to verify the effectiveness of the proposed algorithm and the constructed model. © 2022, Aracne Editrice. All rights reserved.","Traffic accessibility; Traffic condition recognition; Urban traffic big data","Combinatorial optimization; Data fusion; Data mining; Forecasting; Fuzzy systems; Motor transportation; Particle swarm optimization (PSO); Simulated annealing; Traffic congestion; Intelligent transportation technologies; Pollution problems; Prediction-based; Spatio-temporal data; Spatiotemporal networks; Traffic accessibilities; Traffic condition recognition; Traffic flow; Urban traffic; Urban traffic big data; Big data",Article,Scopus,2-s2.0-85123837298
"Jia H., Liu Z., Xu C., Chen Z., Zhang X., Xia J., Yu S.L.","7202381397;57210927937;55760735000;57189359895;57239242400;57216392662;57434866100;","Adaptive pressure-driven multi-criteria spatial decision-making for a targeted placement of green and grey runoff control infrastructures",2022,"Water Research","212",,"118126","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123828287&doi=10.1016%2fj.watres.2022.118126&partnerID=40&md5=40b6208d05f1c6bfedea9a0358e943cb","Traditional runoff control measures ignore the spatial imbalance of regional pressures, thereby failing to achieve a site-specific placement for green and grey infrastructure simultaneously. A multi-criterion decision-making framework for runoff control infrastructure spatial planning was therefore developed in this study. The pressure-state-response framework was applied to creatively match the pressure induced adjustment demands with the infrastructure effectiveness. The pressures were quantified from the perspective of environment, economy, and ecology on a grid scale. States were considered as the relative priority of regional pressure adjustment demand in multiple perspectives. Responses were presented as state-targeted green and grey infrastructure placement. Multi-perspective effectiveness of different green and grey infrastructure was simultaneously evaluated at an effective scale of controlling 1 m3/s runoff for comparison. Methods such as data mining, hydrological model simulation, and remote sensing inversion were combined to quantify the regional pressures. The capital investment and ecological impact of infrastructures were quantified from a life cycle perspective. A case study was carried out in Wuhan, China. The study area was clustered by gridded pressure into three regions. In region Ⅰ, ecological and environmental pressure were of higher weight. In region Ⅱ, the environmental pressure was dominant. In region Ⅲ, the ecological pressure took precedence over the environmental and economic constraints. The area ratios of the region Ⅰ, Ⅱ, and Ⅲ were 43%, 36%, and 21% respectively. The result indicated a synergy and spatial heterogeneity of multi-perspective pressures, and further demonstrating that expert experience tends to fail to weigh the multi-function of green and grey infrastructures for coping with the pressures. Results also stated that green infrastructures were more acceptable in areas that aspire to achieve simultaneous runoff control and ecological improvement. The decision-making framework developed in this study can maximize the overall performance by providing targeted infrastructure placement solutions. © 2022 Elsevier Ltd","adaptive pressure-driven adjustment; green and grey infrastructure; multi-criteria decision-making; pressure-state-response framework; spatial planning","Data mining; Decision making; Ecology; Economics; Investments; Life cycle; Remote sensing; Adaptive pressure-driven adjustment; Control infrastructures; Green and gray infrastructure; Multi criteria decision-making; Multicriteria decision-making; Multicriterion decision makings; Pressure-driven; Pressure-state-response framework; Spatial planning; State response; Runoff; adaptive management; decision making; ecological impact; hydrological modeling; life cycle analysis; multicriteria analysis; remote sensing; runoff; spatial planning; article; China; data mining; decision making; ecology; economic aspect; hydrological model; investment; life cycle; multicriteria decision analysis; quantitative analysis; remote sensing; runoff; simulation; city; hydrology; China; Hubei; Wuhan; China; Cities; Hydrology",Article,Scopus,2-s2.0-85123828287
"Somyanonthanakul R., Theeramunkong T.","57095005900;56006507400;","Scenario-based analysis for discovering relations among interestingness measures",2022,"Information Sciences","590",,,"346","385",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123761465&doi=10.1016%2fj.ins.2021.12.121&partnerID=40&md5=6db736783eea5b63d78bf8deb0e98a84","Many interestingness measures have been proposed for mining meaningful association rules among two events in the form of A→B, but their characteristics and semantic similarity relations have not been comprehensively investigated. This paper presents a scenario-based approach for characterizing sixty-one commonly used measures and revealing their relationships in three steps. The first step generates a set of 969 three-probability scenarios, S={s|s=(p(A),p(B),p(A,B))∧p(A),p(B),p(A|B)∈[0,1]∧p(A,B)⩽min(p(A),p(B))}, in consideration of all possible situations in the range of 0.0 to 1.0 with a step of 0.05, excluding infinity and not-a-number cases. In the second step, 937,992 pairs of scenarios are enumerated, and for each scenario pair s1 and s2, the values of a measure (M) of s1 and s2, i.e., M(s1) and M(s2), are compared with the result of greater-than (M(s1)&gt;M(s2)), smaller-than (M(s1)&lt;M(s2)), or equal-to (M(s1)=M(s2)) for characterizing the measure. The final step is based on three types of relations: (1) behavior-based, (2) correlation-based, and (3) association-based similarity relations. The behavior of measures is depicted using nine common algebraic/statistical properties and four special condition properties, i.e., zero, min–max, infinity, and not-a-number of the measures. Similarities among the measures can be examined by grouping measures based on their properties. With three correlation functions, i.e., correlation coefficient, joint entropy, and mutual information, a correlation analysis was performed to discover relations among interestingness measures in the form of dendrograms and clusters with thresholding. Finally, the details of the relations among these interestingness measures are explored with association rule mining. Besides support, confidence, and lift, we propose five types of rules, i.e. same-directionrule (S-rule), opposite-direction rule (O-rule), equal-both rule (E-rule), equal-left rule (EL-rule),and equal-right rule (ER-rule) for a five-gradient comparison of any two measures to outline their similarities and dissimilarities in five directions. © 2022 Elsevier Inc.","Association rule mining; Behavior characterization; Interestingness measures; Scenario-based analysis","Data mining; Semantics; Behavior-based; Behaviour characterization; Interestingness measures; Not a numbers; Property; Scenario-based; Scenario-based analysis; Semantic similarity; Similarity relations; Types of relations; Association rules",Article,Scopus,2-s2.0-85123761465
"Kamimura R.","26643093500;","Cost-forced collective potentiality maximization by complementary potentiality minimization for interpreting multi-layered neural networks",2022,"Neurocomputing","480",,,"234","256",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123711986&doi=10.1016%2fj.neucom.2022.01.027&partnerID=40&md5=ab157ded17062d566741b08c52507594","The present paper aims to propose a new type of information-theoretic method to augment the ratio of collective potentiality to its cost for the interpretation. Collective potentiality represents how many components, such as connection weights or neurons, are collectively used to deal with relations between inputs and outputs, while individual potentiality shows how an individual component potentially contributes to the inputs or outputs. From our viewpoint, the conventional interpretation methods have focused on the individual potentiality of components, ignoring these collective behaviors. Thus, the present paper stresses that neural networks should try to deal with the collective properties of components and to examine the final results from multiple points of view. For implementing this concept of potentiality maximization, we introduce the complementary potentiality minimization, which aims to reduce the strength of larger weights as much as possible, and at the same time, it can be used for increasing the potentiality. The method was applied to two intuitively interpretable data sets, namely, the absenteeism and online shoppers data sets. With both data sets, experimental results confirmed that the method could increase collective potentiality and considerably reduce the corresponding cost. The new method could extract many groups of weights with the same small strength, all of which tried to respond to the coming inputs, while the conventional methods, including the regularization ones, tried to produce a smaller number of stronger connection weights very selectively. In particular, the present method could produce stable compressed weights similar to the original correlation coefficients of the data sets, meaning that simple, independent, and linear relations could be detected, contrary to the presupposed non-linear ones. Thus, the collective interpretation, applied to two business data sets, revealed a possibility that, behind seemingly complicated non-linear relations between inputs and outputs, neural networks with collective potentiality augmentation with a smaller cost can extract the very simple relations for easy interpretation. All complicated relations can possibly be generated, based on those simple ones. © 2022 Elsevier B.V.","Collective potentiality; Complementary potentiality; Cost; Individual potentiality; Interpretation; Multi-layered neural networks","Data mining; Information theory; Multilayer neural networks; Collective potentiality; Complementary potentiality; Connection weights; Data set; Individual potentiality; Input and outputs; Interpretation; Minimisation; Multilayered neural networks; Simple++; Network layers; absenteeism; article; correlation coefficient; nerve cell; physiological stress",Article,Scopus,2-s2.0-85123711986
"Wang J., Zhang X., Liu C.","55926609300;57192118431;42961941300;","Grained matrix and complementary matrix: Novel methods for computing information descriptions in covering approximation spaces",2022,"Information Sciences","591",,,"68","87",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123707976&doi=10.1016%2fj.ins.2022.01.016&partnerID=40&md5=100f2559230e107b847ba17ff142acaf","As two important tools of information descriptions, minimal description (MinD) and maximal description (MaxD) are used to eliminate redundant information when describing an object in covering-based rough sets (CbRSs). Hence, there are many key issues (such as reductions, neighborhoods and approximations) have to do with them in CbRSs. However, it is complicated and time-consuming to use set approaches to compute them in a large cardinal covering approximation space. So the matrix approaches have been used to calculate them by which computers can implement the corresponding calculations easily. Based on the previous research work and inspired by the demand of knowledge discovery under large scale covering information systems, we propose two new matrix methods to calculate MinD and MaxD in CbRSs in this paper. Firstly, a note on the existing methods (matrix approach-1 and approach-2) is presented. Secondly, a new matrix, called grained matrix, is presented. After combining grained matrices and the mean of “sum”, the new matrix approach-1 is presented to calculate MinD and MaxD. Thirdly, three new matrix operations and the notion of complementary matrix are given which can simplify the calculation of computers. Based on new operations, we present the new matrix approach-2 to compute MinD and MaxD. Finally, the computational efficiency of the presented approaches are shown and compared with other methods by experiments on several UCI data sets. © 2022 Elsevier Inc.","keyword: Covering-based rough set; Knowledge discovery; Matrix; Maximal description; Minimal description","Approximation algorithms; Computational efficiency; Data mining; Matrix algebra; % reductions; Approximation spaces; Information descriptions; Key Issues; Keyword: covering-based rough set; matrix; Matrix approach; Maximal description; Minimal description; Novel methods; Rough set theory",Article,Scopus,2-s2.0-85123707976
"Huang Y., Guo K., Xiuwen Yi, Li Z., Li T.","57030782500;57431332300;57431933500;57109495400;7406372548;","Matrix representation of the conditional entropy for incremental feature selection on multi-source data",2022,"Information Sciences","591",,,"263","286",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123701671&doi=10.1016%2fj.ins.2022.01.037&partnerID=40&md5=265c75cd88bd61d50c9348f3f845ee34","In many real applications, the data are always collected from different information sources and are subject to evolve over time. Such data are referred to as dynamic multi-source data. How to efficiently select the informative features from dynamic multi-source data is a challenging problem in data mining. Incremental feature selection with rough sets is an effective method to select features from dynamic data. However, existing methods focus on single-source data and are not suitable for dynamic multi-source data with variations in data sources. To deal with this issue, we present an incremental feature selection method based on the matrix representation of the conditional entropy. We first propose a novel conditional entropy for multi-source data and discuss its properties, including the monotonicity and boundedness. Then, matrix characterization of the conditional entropy is presented by employing the condition and decision relation matrices associated with some matrix operators. Finally, considering the addition and deletion of data sources in multi-source data, we employ the matrix approach to investigate the incremental mechanisms for the computation of the conditional entropy and develop the corresponding incremental feature selection algorithms. Extensive comparative experimental results are obtained to verify the effectiveness and efficiency of the proposed method. © 2022 Elsevier Inc.","Dynamic multi-source data; Feature selection; Incremental larning; Matrix operators; Rough sets","Data mining; Entropy; Matrix algebra; Rough set theory; Conditional entropy; Data-source; Dynamic data; Dynamic multi-source data; Features selection; Incremental larning; Information sources; Matrix representation; Multisource data; Real applications; Feature extraction",Article,Scopus,2-s2.0-85123701671
"Cai S., Li L., Chen J., Zhao K., Yuan G., Sun R., Sosu R.N.A., Huang L.","57193957504;57259555400;57429699100;57203843562;56559288300;8931933200;57209822278;57429519000;","MWFP-outlier: Maximal weighted frequent-pattern-based approach for detecting outliers from uncertain weighted data streams",2022,"Information Sciences","591",,,"195","225",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123644867&doi=10.1016%2fj.ins.2022.01.028&partnerID=40&md5=16ffbcfa135eeb3707b1e65196787b29","Many outlier detection approaches have been proposed for identifying previously unknown outliers, therefore improving the credibility of data. However, previous outlier detection approaches have some problems. First, most approaches were designed for static precise datasets, thus, their detection accuracy is very low when processing uncertain data streams. Second, these approaches considered the importance (aka weight) of each pattern is the same, which could not accurately reflect some actual situations in real life. To solve these problems, we propose an efficient maximal weighted frequent-pattern-based outlier detection approach, called MWFP-Outlier, for accurately detecting potential outliers from uncertain data streams through two phases, namely pattern mining phase and an outlier detection phase. In the pattern mining phase, through fully considering the existential probabilities and weights for each pattern, we propose the MWFP-Mine approach to accurately and efficiently mine maximal weighted frequent patterns based on the designed tree structure, list structure, and pruning strategies. In the outlier detection phase, we design four deviation indices to accurately measure the deviation degree of each transaction, and then the transactions in the top k ranked are identified as potential outliers. Extensive experimental results demonstrate that the MWFP-Outlier approach can accurately detect the outliers from uncertain weighted data streams, as well as uses less time consumption. © 2022 Elsevier Inc.","Data mining; Deviation indices; Maximal weighted frequent patterns; Outlier detection; Uncertain weighted data streams","Anomaly detection; Data handling; Data mining; Pattern recognition; Trees (mathematics); Data stream; Detection approach; Detection phase; Deviation index; Maximal weighted frequent pattern; Outlier Detection; Pattern mining; Uncertain data streams; Uncertain weighted data stream; Weighted data; Statistics",Article,Scopus,2-s2.0-85123644867
"Li T., Wang Z., Zhao W.","57429926200;57386609000;57215935583;","Comparison and application potential analysis of autoencoder-based electricity pattern mining algorithms for large-scale demand response",2022,"Technological Forecasting and Social Change","177",,"121523","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123603587&doi=10.1016%2fj.techfore.2022.121523&partnerID=40&md5=98d368f77ec7143fb89c1fa44fb2c8a7","Studies of electricity consumption behavior patterns (ECBPs) are very important for demand-side management and emission reduction. Most of the existing ECBPs studies have limitations in data volume, algorithm performance and application potential in large-scale data scenario. Based on various autoencoders, this study proposes an ECBPs mining method with better performance and wider application potential. Two indices are constructed for model evaluations. The empirical research results based on the high-frequency power consumption data of residents show that the representation learning of ECBPs through autoencoders can not only effectively reduce the data dimension (reduced by 90%) for pattern mining but also achieve an effect no less than that of pattern mining from the original data, with the difference in pattern aggregation degree between them being only 0.003. Different autoencoders have characteristics in the representation mapping of the original data space. The results also show that the curves decoded by different autoencoders reflect different characteristics suitable for different scenarios. This study solves the dimension disaster problem of ECBPs mining in the context of large-scale data, and provides a better tool for more complex ECBPs based tasks such as multi-energy prosumers modeling and energy system optimization. © 2022","Autoencoder; Clustering; Demand side management; Electricity consumption behavior pattern; Smart grid","Data mining; Data reduction; Demand side management; Electric power transmission networks; Electric utilities; Emission control; Learning systems; Smart power grids; Auto encoders; Behaviour patterns; Clusterings; Electricity consumption behavior pattern; Electricity-consumption; Large scale data; Large-scales; Pattern mining; Pattern mining algorithms; Potential analysis; Electric power utilization",Article,Scopus,2-s2.0-85123603587
"Kanger L., Bone F., Rotolo D., Steinmueller W.E., Schot J.","56741356700;57204960412;36164935100;6602841466;55425822000;","Deep transitions: A mixed methods study of the historical evolution of mass production",2022,"Technological Forecasting and Social Change","177",,"121491","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123573410&doi=10.1016%2fj.techfore.2022.121491&partnerID=40&md5=5e691f85f1c51c00e2b0de227330be0d","Industrial societies contain a range of socio-technical systems fulfilling functions such as the provision of energy, food, mobility, housing, healthcare, finance and communications. The recent Deep Transitions (DT) framework outlines a series of propositions on how the multi-system co-evolution over 250 years of these systems has contributed to several current social and ecological crises. Drawing on evolutionary institutionalism, the DT framework places a special emphasis on the concepts of ‘rules’ and ‘meta-rules’ as coordination mechanisms within and across socio-technical systems. In this paper, we employ a mixed-method approach to provide an empirical assessment of the propositions of the DT framework. We focus on the historical evolution of mass production from the 18th century to the present. Combining a qualitative narrative based on a synthesis of secondary historical literature with a quantitative text mining-based analysis of the corpus of Scientific American (1845–2019), we map the emergence and alignment of rules underpinning mass production. Our study concludes by reflecting on important methodological lessons for the application of text mining techniques to examine large-scale and long-term socio-technical dynamics. © 2022 Elsevier Inc.","Deep transitions; Historical sources; Mass production; Mixed methods; Rules; Socio-technical systems; Text mining","Deep transition; Energy; Historical evolutions; Historical source; Industrial societies; Mass production; Mixed method; Multi systems; Rule; Sociotechnical systems; Data mining; eighteenth century; empirical analysis; historical perspective; historical record; industrial history; qualitative analysis; welfare provision; United States",Article,Scopus,2-s2.0-85123573410
"Li H.-W., Wu Y.-S., Huang Y.","56824551600;55843226200;7501577033;","On the Feasibility of Anomaly Detection with Fine-Grained Program Tracing Events",2022,"Journal of Network and Systems Management","30","2","28","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123343119&doi=10.1007%2fs10922-021-09635-3&partnerID=40&md5=4837449ba6e14d12b6306e300124f6e0","The efficacy of anomaly detection is fundamentally limited by the descriptive power of the input events. Today’s anomaly detection systems are optimized for coarse-grained events of specific types such as system logs and API traces. An attack can evade detection by avoiding noticeable manifestations in the coarse-grained events. Intuitively, we may fix the loopholes by reducing the event granularity, but this brings up two obvious challenges. First, fine-grained events may not have the rich semantics needed for feature construction. Second, the anomaly detection algorithms may not scale for the volume of the fine-grained events. We propose the application profile extractor (APE) that utilizes compression-based sequential pattern mining to generate compact profiles from fine-grained program traces for anomaly detection algorithms. With minimal assumptions on the event semantics, the profile generation are compatible with a wide variety of program traces. In addition, the compact profiles scale anomaly detection algorithms for the high data rate of fine-grained program tracing. We also outline scenarios that justify the need for anomaly detection with fine-grained program tracing events. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Anomaly detection; Application-specific attack; Compression-based sequential pattern mining; Fine-grained program tracing; Program behavior","Application programs; Data mining; Semantics; Signal detection; Anomaly detection; Anomaly-detection algorithms; Application specific; Application-specific attack; Coarse-grained; Compression-based sequential pattern mining; Fine grained; Fine-grained program tracing; Program behavior; Sequential-pattern mining; Anomaly detection",Article,Scopus,2-s2.0-85123343119
"Zhao H., Zhuang H., Wang C., Yang M.","57221706105;57317076700;57422609300;57196041116;","G3DOA: Generalizable 3D Descriptor With Overlap Attention for Point Cloud Registration",2022,"IEEE Robotics and Automation Letters","7","2",,"2541","2548",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123285740&doi=10.1109%2fLRA.2022.3142733&partnerID=40&md5=4198c5b385af335a2a7e3038dbd47d42","Point cloud registration (PCR) is a key problem for robotics, autonomous driving, and other applications. Constructing generalizable 3D descriptors and determining whether a 3D descriptor is in the overlapping area are challenging tasks in PCR. Despite the fast evolution of learning-based 3D descriptors, existing methods are either sensitive to rigid transformation and scenario changes, or can not find the appropriate descriptors in the overlapping area, which means their generalization and descriptive ability are not enough for practice applications. To solve these problems, we propose a novel neural network, named G3DOA, which jointly learns generalizable rotation-invariant 3D descriptors and their overlap scores (representing the probability in the overlapping area), to enhance the generalization ability of the descriptors across different data collected by various laser sensors. To ensure the rotation and scale invariance of the point cloud in the input stage, we estimate the Local Reference Frame (LRF) of local patches and normalize the coordinates. To learn generalizable and distinctive descriptors, we propose a novel Cylindrical LRF convolution module with multi-scaled cylindrical shells and neural layers, which hierarchically encodes and aggregates the geometric information in different cylindrical shells. Moreover, to estimate the probability of whether a point is in the overlapping area, we propose an overlap attention module that extracts co-contextual information between the feature encodings of the two point clouds. The experiments show that G3DOA trained only on an indoor dataset can be efficiently generalized to complex outdoor datasets, and the generalization ability of G3DOA outperforms state-of-the-art learning-based 3D descriptors. 2377-3766 © 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.","Convolution; Data mining; Encoding; Feature extraction; Neural networks; Point cloud compression; Three-dimensional displays","Computer vision; Convolution; Cylinders (shapes); Data mining; Encoding (symbols); Network coding; Shells (structures); Surface measurement; Three dimensional displays; 3d descriptor; AI-based method; Computer vision for automation; Encodings; Features extraction; Neural-networks; Overlapping area; Point cloud compression; Point-clouds; Three-dimensional display; Neural networks",Article,Scopus,2-s2.0-85123285740
"Mirzaie M., Esfandyari H., Tatar A.","57219388060;57201280492;55839820500;","Dew point pressure of gas condensates, modeling and a comprehensive review on literature data",2022,"Journal of Petroleum Science and Engineering","211",,"110072","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123127282&doi=10.1016%2fj.petrol.2021.110072&partnerID=40&md5=7be5d6124b1a7d74cc089b110f887a01","The accurate and in-time prediction of gas condensates dew point pressure (PDew) is of great importance regarding the technical and economic points of view for fluid characterization, reservoir performance calculations, planning the development of gas condensate reservoirs, and design and optimization of production systems. Although the laboratory tests provide the most accurate and reliable results, it is an expensive and time-consuming process sometimes associated with some errors. Artificial intelligence-based methods have emerged as promising tools in different aspects of engineering. In this study, after a thorough analysis of the gas condensate data samples, the application of different intelligent modeling is investigated. A databank of 721 data samples is gathered, and different intelligent methods approaches are used for modeling. The results of three different data mining methods are combined using Committee Machine Intelligent Systems (CMIS) in an attempt to receive more accurate results. Three different methods of arithmetic, geometric, and harmonic approaches are utilized to develop the CMIS model. It was concluded that the harmonic CMIS yields the best predictions by average absolute relative deviation (AARD) and R2 values of 3.456% and 0.9702, respectively. This novel CMIS model could successfully outperform all the developed initial models. Additionally, a literature review showed that the proposed model could outperform the previously published models including artificial intelligence, equation of state, and correlation-based method considering both prediction accuracy and data coverage. © 2022 Elsevier B.V.","CMIS; Correlation; Data mining; Gas condensates; Intelligent modeling; PDew","Equations of state; Forecasting; Gas condensates; Gases; Intelligent systems; Monte Carlo methods; Committee machine intelligent system; Committee machines; Correlation; Data sample; Dew point pressures; Intelligent models; Literature data; Pdew; System models; Time predictions; Data mining",Article,Scopus,2-s2.0-85123127282
"Sinha N.K., Chattopadhyay S.","57197006275;8648886600;","A case study on elimination of premature failure source from manufacture of fluoroelastomer inflatable seals for Sodium-cooled Fast Reactor towards sustainability",2022,"Engineering Failure Analysis","134",,"106039","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123003139&doi=10.1016%2fj.engfailanal.2022.106039&partnerID=40&md5=054bf59e864dab5de11b960b9ce5361b","An innovative model of engineering failure analysis leading to a prospective model of sustainability and decarbonisation, supported by design-material-failure threads, emerge from two defective test inflatable seals (Seals; ∼2 m dia) subjected to laboratory investigations i.e. visual, dimensional, physico-mechanical, material fingerprinting and analysis of variation. The Seals of 500 MW(e) Prototype Fast Breeder Reactor (PFBR) design were industrially manufactured (year: 2007) by cold feed extrusion and continuous cure (CFECC) of a peroxide cured, Viton GBL 200S:600S blend compound, APA-1, 2007. Primary aim was to implement life maximization (global design thread) potential (≥30 y) of advanced polymer architecture (APA) fluoroelastomer (FKM) in reactor inflatable seals (∼6.3 m/∼4.2 m dia) with repeatability and reproducibility by eliminating premature failure source from improper manufacture. Unfeasibility of or unlikely value addition from validation-modelling-simulation-microstructure study necessitated innovative treatment i.e. extracting maximum from the minimalist investigation by maximizing interpretation-data mining-cross verification and utilizing past (validated) FEA results of standard FKM PFBR inflatable-backup seals (life: 10 y) through design-material threads. The treatment was reinforced by reorientation of perspective. Distribution of laboratory data along Seal circumference was interpreted as phenomenological manifestation of thermomechanical-electrochemical interactions in the APA-1, 2007 macromolecular realm, brought about by thermomechanical loads from the progressing CFECC with time. The data was thus utilised to reconstruct the CFECC (2007) in terms of processing parameter variance with respect to a benchmark CFECC study on identical seal design. Incorrect cure index (of APA-1, 2007; global failure thread) was ascertained as the source of premature failure by tracing the origin of departures e.g. tracking the straying of APA-1, 2007 tensile stress-strain behaviour (structure property correlation) and cracking failure limits (global material thread) from the desired (i.e. ≥30 y) along the abstracted echelons (i.e. reconstructing manufacture, molecular-phenomenological interplay) of the CFECC in terms of unacceptable macromolecular transitions. The novel approach and methodology predict new-seal-failure-situation in reactor (by residual stress) and/or continuing operation-safety uncertainty of inflatable seal and reactor (by undercure) from the incorrect cure index. The identified source and template for permanent rectification are crucial for FBR-Gen IV cover gas elastomeric sealing. The applicability extends to other nuclear and inaccessible-hazardous, critical/very critical elastomeric usage (marine, air, space, oilfield, gas etc.) as well. The research findings provide complete description of (APA-FKM) residual stress and tensile stress-strain behaviour by the first principles and an analogy of mechanical-entropy spring, accompanied by a new commentary on structure-property-performance correlation of APA-FKM. The findings are expected to enrich understanding of rubber failure. The failure analysis of a very critical nuclear component grows vastly in connotation using global threads to suggest a prospective conceptual benchmark template for sustainability-decarbonisation-augmentaion i.e. better realization of life maximization potentials in applications by synthesizing improved specifying-manufacturing-quality monitoring with the global threads. This study could provide basis for new research on defect modeling-simulation-validation and encourage formulation-modeling-validation of new constitutive relations, considering incorrect cure index and its effects. The concepts and routes of maximum from minimum, working by first principal, reconstruction, threads and molecular-phenomenological interplay are expected to find wider scope beyond this research. © 2022 Elsevier Ltd","APA FKM; Cold feed extrusion; Factor of safety; Gen IV SFR; Incorrect cure index; Inflatable seal; PFBR; Residual stress; Underdeveloped molecular architecture","Data mining; Failure (mechanical); Residual stresses; Safety factor; Seals; Structural design; Sustainable development; Tensile strength; Advanced polymer architecture FKM; Advanced polymers; Cold feed extrusion; Factors of safeties; Gen IV SFR; Incorrect cure index; Inflatable seal; Molecular architecture; Polymer architecture; Prototype fast breeder reactors; Underdeveloped molecular architecture; Curing",Article,Scopus,2-s2.0-85123003139
"Wyckhuys K.A.G., Zou Y., Wanger T.C., Zhou W., Gc Y.D., Lu Y.","15838302800;57415324200;24377209700;57415168700;28767615700;55351914200;","Agro-ecology science relates to economic development but not global pesticide pollution",2022,"Journal of Environmental Management","307",,"114529","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122949793&doi=10.1016%2fj.jenvman.2022.114529&partnerID=40&md5=fb0e474913cbb9ae872d4182139d5178","Synthetic pesticides are core features of input-intensive agriculture and act as major pollutants driving environmental change. Agroecological science has unveiled the benefits of biodiversity for pest control, but research implementation at the farm-level is still difficult. Here we address this implementation gap by using a bibliometric approach, quantifying how countries' scientific progress in agro-ecology relates to pesticide application regimes. Among 153 countries, economic development does spur scientific innovation but irregularly bears reductions in pesticide use. Some emerging economies bend the Environmental Kuznets curve (EKC) – the observed environmental pollution by a country's wealth – for pesticides and few high-income countries exhibit a weak agro-ecology ‘technique effect’. Our findings support recent calls for large-scale investments in nature-positive agriculture, underlining how agro-ecology can mend the ecological resilience, carbon footprint, and human health impacts of intensive agriculture. Yet, in order to effectively translate science into practice, scientific progress needs to be paralleled by policy-change, farmer education and broader awareness-raising. © 2022 The Authors","Agro-biodiversity; Culturomics; Econometrics; EKC hypothesis; Environmental pollution; Nature-based solutions; Pesticide regulation; Sustainable intensification","pesticide; carbon dioxide; agroecology; biodiversity; economic development; education; global change; Kuznets curve; pest control; pesticide; agricultural management; agricultural waste; Article; bibliometrics; biological pest control; country economic status; data mining; economic development; environmental planning; geographic distribution; global change; Organisation for Economic Co-operation and Development; pesticide spraying; plant ecology; publication; scientific literature; time factor; agriculture; human; pest control; pollution; Agriculture; Carbon Dioxide; Economic Development; Environmental Pollution; Humans; Pest Control; Pesticides",Article,Scopus,2-s2.0-85122949793
"Ayhan T., Uçar T.","57411764100;36165070600;","Determining customer limits by data mining methods in credit allocation process",2022,"International Journal of Electrical and Computer Engineering","12","2",,"1910","1915",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122797193&doi=10.11591%2fijece.v12i2.pp1910-1915&partnerID=40&md5=4a28995fad7661aa170f8fdb79687d55","The demand for credit is increasing constantly. Banks are looking for various methods of credit evaluation that provide the most accurate results in a shorter period in order to minimize their rising risks. This study focuses on various methods that enable the banks to increase their asset quality without market loss regarding the credit allocation process. These methods enable the automatic evaluation of loan applications in line with the sector practices, and enable determination of credit policies/strategies based on actual needs. Within the scope of this study, the relationship between the predetermined attributes and the credit limit outputs are analyzed by using a sample data set of consumer loans. Random forest (RF), sequential minimal optimization (SMO), PART, decision table (DT), J48, multilayer perceptron (MP), JRip, naïve Bayes (NB), one rule (OneR) and zero rule (ZeroR) algorithms were used in this process. As a result of this analysis, SMO, PART and random forest algorithms are the top three approaches for determining customer credit limits. © 2022 Institute of Advanced Engineering and Science. All rights reserved.","Banking; Credit allocation process; Data mining; Machine learning algorithms",,Article,Scopus,2-s2.0-85122797193
"Yavuz Ö.","55371850200;","A data mining analysis of COVID-19 cases in states of United States of America",2022,"International Journal of Electrical and Computer Engineering","12","2",,"1754","1758",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122796022&doi=10.11591%2fijece.v12i2.pp1754-1758&partnerID=40&md5=1ab9e1cb66517b2bb06018955af877f1","Epidemic diseases can be extremely dangerous with its hazarding influences. They may have negative effects on economies, businesses, environment, humans, and workforce. In this paper, some of the factors that are interrelated with COVID-19 pandemic have been examined using data mining methodologies and approaches. As a result of the analysis some rules and insights have been discovered and performances of the data mining algorithms have been evaluated. According to the analysis results, JRip algorithmic technique had the most correct classification rate and the lowest root mean squared error (RMSE). Considering classification rate and RMSE measure, JRip can be considered as an effective method in understanding factors that are related with corona virus caused deaths. © 2022 Institute of Advanced Engineering and Science. All rights reserved.","Coronavirus; COVID-19; Data mining; Epidemic; Machine learning; Pandemic; Quantitative analysis",,Article,Scopus,2-s2.0-85122796022
"Han X., Dell'Aglio D., Grubenmann T., Cheng R., Bernstein A.","57210573023;57211275968;57192396177;7201955416;57212626682;","A framework for differentially-private knowledge graph embeddings",2022,"Journal of Web Semantics","72",,"100696","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122787404&doi=10.1016%2fj.websem.2021.100696&partnerID=40&md5=15678f30729dd2ec36d665a385fee672","Knowledge graph (KG) embedding methods are at the basis of many KG-based data mining tasks, such as link prediction and node clustering. However, graphs may contain confidential information about people or organizations, which may be leaked via embeddings. Research recently studied how to apply differential privacy to a number of graphs (and KG) analyses, but embedding methods have not been considered so far. This study moves a step toward filling such a gap, by proposing the Differential Private Knowledge Graph Embedding (DPKGE) framework. DPKGE extends existing KG embedding methods (e.g., TransE, TransM, RESCAL, and DistMult) and processes KGs containing both confidential and unrestricted statements. The resulting embeddings protect the presence of any of the former statements in the embedding space using differential privacy. Our experiments identify the cases where DPKGE produces useful embeddings, by analyzing the training process and tasks executed on top of the resulting embeddings. © 2021 Elsevier B.V.","Differential privacy; Knowledge graph embeddings","Data mining; Graph embeddings; AS-links; Data mining tasks; Differential privacies; Embedding method; Embeddings; Graph embeddings; Graph-based data minings; Knowledge graph embedding; Knowledge graphs; Link prediction; Knowledge graph",Article,Scopus,2-s2.0-85122787404
"Yang H., He H., Zhang W., Bai Y.","57219401875;55264665100;35263372000;57219401548;","MTGK: Multi-source cross-network node classification via transferable graph knowledge",2022,"Information Sciences","589",,,"395","415",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122641643&doi=10.1016%2fj.ins.2022.01.007&partnerID=40&md5=f7c3482ee847edb1bccb47b5fa22464b","The main idea of multi-source cross-network node classification (CNNC) is to promote the target network's node classification accuracy by borrowing knowledge from multi-source networks. However, the source networks and the target network often have no intersection on the nodes and links. Thus, the main challenge of multi-source CNNC is to select the most transferable source networks and identify network-invariant patterns between networks to promote cross-network learning. Therefore, targeting the problem of CNNC, in this study, we introduce a multi-source knowledge transfer method, which aims to use a large number of transferable graph patterns in multiple source networks to assist the target network's unlabeled nodes to be correctly classified. Specifically, we propose to select the most transferable source networks to learn network-invariant common sub-graph patterns and utilize such patterns to build new structural features for the target network. In addition, we devise an iterative classification algorithm, MTGK, which jointly classifies the nodes of the target network based on the attribute features and the transferable structural features of the nodes in multiple source networks. A large number of experiments conducted on real-world social networks and citation networks demonstrate that MTGK outperforms the most advanced methods in CNNC performance. © 2022","Graph mining; Multi-source transfer learning; Networked data; Node classification","Classification (of information); Data mining; Graph theory; Iterative methods; Cross networks; Graph mining; Multi-source transfer learning; Multi-Sources; Network invariants; Network node; Networked datum; Node classification; Source Transfer; Transfer learning; Knowledge management",Article,Scopus,2-s2.0-85122641643
"Liu W., Wang J.","57211829419;57211089817;","Recursive elimination current algorithms and a distributed computing scheme to accelerate wrapper feature selection",2022,"Information Sciences","589",,,"636","654",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122613897&doi=10.1016%2fj.ins.2021.12.086&partnerID=40&md5=93ca56dfcd3cd6d9d8835721e1703ecf","Feature selection (FS) for classification tasks in machine learning and data mining has attracted significant attention. Recently, increasing metaheuristic optimization algorithms have been applied to solve wrapper FS problems. Nevertheless, the algorithms that improve existing optimizers inevitably bring higher complexity and require higher computational cost in wrapper mode. In this work, we present recursive elimination current (REC) algorithms, a novel set of algorithms for wrapper FS, which consists of the simplest feature subset representation, an ingenious structure enlightened by the recursion technique in computer science and necessary components. To some extent, the proposed algorithms, recurrent REC (REC2) and distributed REC (DiREC), dispose of the issues, including but not limited to keeping diversity of population and scalability for high-dimensionality, which are often discussed in metaheuristics-based FS research. Thereinto, DiREC is a distributed computing scheme proposed to accelerate the FS process by distributing the tasks to different computing units in a cluster of computers. A series of experiments were carried out on several representative benchmark datasets, such as Madelon from UCI machine learning repository, using REC2 and DiREC with various numbers of logical processors. It is demonstrated that the proposed algorithms perform efficiently in wrapper mode, and the distributed computing scheme is effective and yields computational time savings. © 2021 Elsevier Inc.","Computer cluster; Distributed computing; Nesting principle; Recursion technique; Wrapper feature selection","Cluster computing; Clustering algorithms; Computational efficiency; Data mining; Machine learning; Optimization; 'current; Classification tasks; Computer clusters; Computing scheme; Features selection; Metaheuristic optimization; Nesting principle; Recursion technique; Recursions; Wrapper feature selection; Feature extraction",Article,Scopus,2-s2.0-85122613897
"González-Fernández E., Álvarez-López S., Garrido A., Fernández-González M., Rodríguez-Rajo F.J.","57210175709;57214588817;57219158480;57203178126;57405541600;","Data mining assessment of Poaceae pollen influencing factors and its environmental implications",2022,"Science of the Total Environment","815",,"152874","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122509824&doi=10.1016%2fj.scitotenv.2021.152874&partnerID=40&md5=8cf713a7a250309a5acb14abf3e8a472","Poaceae pollen is highly allergenic, with a marked contribution to the pollen worldwide allergy prevalence. Pollen counts are defined by the species present in the considered area, although year-to-year oscillations may be triggered by different parameters, among which are weather conditions. Due to the predominant role of Poaceae pollen in the allergenicity in urban green areas, the aim of this study was the analysis of pollen trends and the influence of meteorology to forecast relevant variations in airborne pollen levels. The study was carried out during the 1993–2020 period in Ourense, in NW Iberian Peninsula. We used a volumetric Lanzoni VPPS 2000 trap for recording Poaceae airborne pollen grains, and meteorological daily data were obtained from the Galician Institute for Meteorology and Oceanography. The main indexes of the pollen season and their trends were calculated. A correlation analysis and ‘C5.0 Decision Trees and Rule-Based Models’ data mining algorithm were applied to determine the influence of meteorological conditions on pollen levels. We detected atmospheric Poaceae pollen during 139 days on average, mainly from April to August. The mean pollen grains amount recorded during the pollen season was 4608 pollen grains, with the pollen maximum peak of 276 pollen/m3 on 27 June. We found no statistically significant trends and slight slopes for the seasonal indexes, similarly to previous Poaceae studies in the same region. The calculated C5.0 model offered defined results, indicating that the combination of mean temperature above 17.46 °C and sunlight exposure higher than 12.7 h is conductive to significantly high pollen levels. The obtained results make possible the identification of risk moments during the pollen season for the activation of protective measures for sensitized population to grass pollen. © 2022","Poaceae pollen; Pollen trends; Urban environment; ‘C5.0 Decision Trees and Rule-Based Models’ algorithm","Data mining; Meteorology; Risk assessment; Model algorithms; Poacea pollen; Poaceae; Pollen grains; Pollen trend; Rule-based models; Tree-based model; Trees-based models; Urban environments; ‘c5.0 decision tree and rule-based model’ algorithm; Decision trees; algorithm; allergy; data mining; environmental factor; pollen; urban area; algorithm; allergenicity; article; correlation analysis; data mining; decision tree; grain; grass pollen; meteorology; nonhuman; oceanography; Poaceae; season; sun exposure; urban area; data mining; Poaceae; pollen; pollen allergy; Galicia [Spain]; Iberian Peninsula; Orense [Galicia]; Spain; allergen; Allergens; Data Mining; Poaceae; Pollen; Rhinitis, Allergic, Seasonal; Seasons",Article,Scopus,2-s2.0-85122509824
"Donyatalab Y., Kutlu Gündoğdu F., Farid F., Seyfi-Shishavan S.A., Farrokhizadeh E., Kahraman C.","57214100358;57205296891;57258418800;57217735227;57214066586;7003388495;","Novel spherical fuzzy distance and similarity measures and their applications to medical diagnosis",2022,"Expert Systems with Applications","191",,"116330","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122505438&doi=10.1016%2fj.eswa.2021.116330&partnerID=40&md5=152c9bad2e8899f2bcb375095c0f1f90","Spherical fuzzy sets (SFSs) have recently become more popular in various fields. It was proposed as a generalization of picture fuzzy sets and Pythagorean fuzzy sets in order to deal with uncertainty and fuzziness information. The similarity measure is one of the beneficial tools to determine the degree of similarity between objects. It has many crucial application areas such as decision making, data mining, medical diagnosis, and pattern recognition. In the short time since their first appearance, some different distance and similarity measures of SFSs have been proposed, but they are limited through the literature. In this study, some novel distances and similarity measures of spherical fuzzy sets are presented. Then, we propose the Minkowski, Minkowski-Hausdorff, Weighted Minkowski and Weighted Minkowski-Hausdorff distances for SFSs. In addition, trigonometric and f-similarity measures are developed based on the proposed distances in this paper. The newly defined similarity measures are applied to medical diagnosis problem for COVID-19 virus and results are discussed. A comparative study of new similarity measures was established and some advantages of the proposed work are discussed. © 2021 Elsevier Ltd","F-similarity measures; Minkowski distance; Minkowski-Hausdorff distance; Similarity measures; Spherical fuzzy sets; Trigonometric similarity measures","Data mining; Decision making; Fuzzy sets; Medical problems; Pattern recognition; Spheres; Viruses; F-similarity measure; Hausdorff distance; Minkowski; Minkowski's distance; Minkowski-hausdorff distance; Similarity measure; Spherical fuzzy set; Trigonometric similarity measure; Diagnosis",Article,Scopus,2-s2.0-85122505438
"Ruipérez-Valiente J.A., Staubitz T., Jenner M., Halawa S., Zhang J., Despujol I., Maldonado-Mahauad J., Montoro G., Peffer M., Rohloff T., Lane J., Turro C., Li X., Pérez-Sanagustín M., Reich J.","56013933500;56237871000;57215927060;57363955500;56595432900;56741514800;57190294959;22433430900;56568211100;57190375445;12761693700;18435401600;24450664300;23393559900;56144824600;","Large scale analytics of global and regional MOOC providers: Differences in learners’ demographics, preferences, and perceptions",2022,"Computers and Education","180",,"104426","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122502949&doi=10.1016%2fj.compedu.2021.104426&partnerID=40&md5=289b7b33bbe635320d2a1b499019b5f0","Massive Open Online Courses (MOOCs) remarkably attracted global media attention, but the spotlight has been concentrated on a handful of English-language providers. While Coursera, edX, Udacity, and FutureLearn received most of the attention and scrutiny, an entirely new ecosystem of local MOOC providers was growing in parallel. This ecosystem is harder to study than the major players: they are spread around the world, have less staff devoted to maintaining research data, and operate in multiple languages with university and corporate regional partners. To better understand how online learning opportunities are expanding through this regional MOOC ecosystem, we created a research partnership among 15 different MOOC providers from nine countries. We gathered data from over eight million learners in six thousand MOOCs, and we conducted a large-scale survey with more than 10 thousand participants. From our analysis, we argue that these regional providers may be better positioned to meet the goals of expanding access to higher education in their regions than the better-known global providers. To make this claim we highlight three trends: first, regional providers attract a larger local population with more inclusive demographic profiles; second, students predominantly choose their courses based on topical interest, and regional providers do a better job at catering to those needs; and third, many students feel more at ease learning from institutions they already know and have references from. Our work raises the importance of local education in the global MOOC ecosystem, while calling for additional research and conversations across the diversity of MOOC providers. © 2022 The Authors","Cultural factors; Distance learning; Educational data mining; Equity; Large scale analytics; Learning analytics; Massive open online courses","Data mining; E-learning; Ecosystems; Population statistics; Surveys; Cultural factors; Distance-learning; Educational data mining; English languages; Equity; Large-scale analytics; Learning analytic; Massive open online course; Media attention; Research data; Students",Article,Scopus,2-s2.0-85122502949
"Liu J., Ye Z., Yang X., Wang X., Shen L., Jiang X.","7410105611;57214469533;57209607350;57405719500;57405222600;8865540600;","Efficient strategies for incremental mining of frequent closed itemsets over data streams",2022,"Expert Systems with Applications","191",,"116220","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122501168&doi=10.1016%2fj.eswa.2021.116220&partnerID=40&md5=6ca3b07e76577a1bb4b2049b22144144","Mining frequent closed itemsets over data streams is an important data mining problem. Mining data streams is more challenging than mining static data because of the nature of data streams, including high arrival rate, massive volume of incoming data, and concept drift. The existing algorithms for mining frequent closed itemsets over data streams suffer from scalability and efficiency bottlenecks. This paper proposes a novel algorithm for mining frequent closed itemsets over data streams both for the sliding window model and for the landmark model. An indexed prefix closed itemset tree is proposed for compressing all closed itemsets and for quick searching of closed itemsets, and novel search strategies are proposed to prune the search space in updating the set of closed itemsets. The proposed algorithm outperforms the state-of-the-art intersection-based algorithms, CICLAD, ConPatSet, and CloStream, by several times to 2 orders of magnitude in efficiency, and also outperforms the state-of-the-art pattern enumeration algorithm, Moment, by up to 2 orders of magnitude over data streams with large windows and sparse data streams. The proposed algorithm is also superior in scalability. © 2021 Elsevier Ltd","Closed itemsets; Data mining; Data streams; Frequent itemsets; Knowledge discovery","Efficiency; Scalability; Closed itemsets; Data mining problems; Data stream; Efficient strategy; Frequent closed itemsets; Frequent itemset; Incremental mining; Orders of magnitude; State of the art; Static datum; Data mining",Article,Scopus,2-s2.0-85122501168
"Sun L., Wang K., Xu L., Zhang C., Balezentis T.","55625030300;57219027019;57397351200;55557005000;37062856100;","A time-varying distance based interval-valued functional principal component analysis method – A case study of consumer price index",2022,"Information Sciences","589",,,"94","116",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122327300&doi=10.1016%2fj.ins.2021.12.113&partnerID=40&md5=9c063968332ebe449a67a7bb51542c34","Functional principal component analysis (FPCA) is an extension of conventional principal component analysis (PCA) that allows the processing of functional data. Besides the reduction in dimensionality that is inherent to PCA, FPCA relies on fewer assumptions and offers a greater ability to visualize the functional data. Thus, FPCA can be used in, for example, social, economic, and medical research. However, the existing FPCA methods are sensitive to outliers, and underperform when extracting features from interval-valued functional data. At the same time, the existing PCA methods for interval-valued functional data suffer from inconsistency in the interpretation of the principal components, and substantial information loss. Therefore, this paper proposes an interval-valued functional principal component analysis (IFPCA) method based on the time-varying distance function. The time-varying distance function containing information on the midpoint and radius is constructed to mitigate information loss. The novel IFPCA method is also able to solve the problem of the inconsistent interpretation of the principal components. The effectiveness of the method is verified by considering the case of the consumer price index. © 2021 Elsevier Inc.","Feature extraction; Functional principal component analysis; Interval-valued functional data; Multivariate statistics; Time-varying distance function","Data handling; Data mining; Principal component analysis; Distance functions; Features extraction; Functional datas; Functional principal component analysis; Interval-valued; Interval-valued functional data; Multivariate statistics; Principal component analysis method; Time varying; Time-varying distance function; Multivariant analysis",Article,Scopus,2-s2.0-85122327300
"Hou W., Hou L., Zhao S., Liu W.","57200503460;57397474000;57398223900;57198618927;","A hybrid data-driven robust optimization approach for unit commitment considering volatile wind power",2022,"Electric Power Systems Research","205",,"107758","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122297905&doi=10.1016%2fj.epsr.2021.107758&partnerID=40&md5=7a78646384d039fe56946e01dcace964","This paper presents a data-driven hybrid method combining kernel density estimation (KDE) and Wasserstein metric to solve unit commitment problems considering volatile wind power. The proposed approach adopts KDE to deduce the underlying probability distribution function (PDF) of wind power forecasting errors, and utilizes the confidence interval of the estimated PDF to construct the support of uncertainties. Wasserstein metric is applied to measure the distances among distributions belonging to the established support. We take the PDF estimated by KDE as the center, and take a certain Wasserstein distance, so as to form a Wasserstein ball of probability distributions, namely ambiguity set. The unit commitment plan is obtained by minimizing the expected cost with regard to the worst-case distribution in the ambiguity set. As a fusion of Wasserstein technique and KDE, the proposed method inherits good abilities of conventional Wasserstein-based method, such as statistics, tractability, extensibility and data mining capacity. More importantly, since the PDF estimated by KDE can converge to the true distribution with the increase of sample data, the proposed method can effectively further narrow the conservatism of the model without losing robustness. Computational results on IEEE 57-bus, 118-bus and 145-bus systems from MATPOWER 6.0 numerically demonstrate the favorable features of the proposed method. © 2021","Data-driven; kernel density estimation; robust optimization; uncertainty; unit commitment","Data mining; Distribution functions; Optimization; Uncertainty analysis; Weather forecasting; Wind power; Ambiguity set; Data driven; Hybrid datum; Kernel Density Estimation; Optimization approach; Probability distribution functions; Robust optimization; Uncertainty; Unit Commitment; Wasserstein metric; Statistics",Article,Scopus,2-s2.0-85122297905
"Gul S., Räbiger S., Saygın Y.","57393122300;56444378100;6603336081;","Context-based extraction of concepts from unstructured textual documents",2022,"Information Sciences","588",,,"248","264",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122054942&doi=10.1016%2fj.ins.2021.12.056&partnerID=40&md5=14e5e78dba6886e0e1b513cf250d14cb","Summarizing a collection of unstructured textual documents, e.g., lecture slides or book chapters, by extracting the most relevant concepts helps learners realize connections among these concepts. However, to accomplish this goal existing methods neglect the context in which concepts are extracted - because a concept might be irrelevant in one context, but relevant in another one. To that end we propose a novel unsupervised method for extracting the relevant concepts from a collection of unstructured textual documents assuming that the documents are related to a certain topic. Our two-step method first identifies candidate concepts from the textual documents, then infers the context information for the input documents and finally ranks them with respect to the inferred context. In the second step this context information is enriched with more abstract information to improve the ranking process. In the experiments we demonstrate that our method outperforms seven supervised and unsupervised approaches on five datasets and is competitive on the other two. Furthermore, we release three new benchmark datasets that were created from books in the educational domain. Our code and datasets are available at: https://github.com/gulsaima/COBEC. © 2021","Concept extraction; Keyword extraction; Knowledge base; Unsupervised learning","Data mining; Extraction; Unsupervised learning; Concept extraction; Context information; Context-based; Keywords extraction; Lecture slides; Ranking process; Textual documents; Two step method; Unsupervised approaches; Unsupervised method; Knowledge based systems",Article,Scopus,2-s2.0-85122054942
"Dai B., Wang F., Chang Y.","57388519400;14627932600;57307371200;","Multi-objective economic load dispatch method based on data mining technology for large coal-fired power plants",2022,"Control Engineering Practice","121",,"105018","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121910796&doi=10.1016%2fj.conengprac.2021.105018&partnerID=40&md5=d182786c40876bde61fde5c6c63e4c84","Coal-fired power plants occupy a dominant position in the power system and are the key sector of energy consumption and pollutant emission. A fallacious load dispatching scheme for the power plant will have a negative impact on energy use and emission reduction. This paper proposes a multi-objective economic load dispatch method for the coal-fired power plant based on data mining technology. The core of the method is to mine the optimal decision-making samples from the offline database to guide the online economic load dispatching, according to the load demand of the power grid. Thus, in the big data environment, a hierarchical clustering and retrieval strategy based on the fuzzy c-means clustering algorithm is proposed to construct the offline database, which can lower the online calculation amount of sample retrieval while balancing the retrieval accuracy. Moreover, the multiple attribute decision-making method, which comprehensively considers multiple objectives, is used to mining the optimal decision-making samples from the offline database to guide the load dispatching scheme for the power plant. The database maintenance strategy is utilized to optimize and update the offline database. Finally, the proposed method is applied to an on-duty coal-fired power plant to verify its effectiveness. The results show that the data-mining-based method can achieve the plant-level optimal load dispatching while meeting the actual requirements of the grid. © 2021 Elsevier Ltd","Coal-fired power plants; Data mining; Economic load dispatch; Hierarchical clustering retrieval","Clustering algorithms; Coal fired power plant; Coal fueled furnaces; Data mining; Database systems; Decision making; Electric load dispatching; Electric power plant loads; Electric power transmission networks; Emission control; Energy utilization; Fossil fuel power plants; Mining; Data mining technology; Economic load dispatch; Hier-archical clustering; Hierarchical Clustering; Hierarchical clustering retrieval; Load dispatching; Multi objective; Offline; Optimal decision making; Power; Coal",Article,Scopus,2-s2.0-85121910796
"Wu Y., Yuan Z., Li Y., Guo L., Fournier-Viger P., Wu X.","15844651200;57388354100;56822819000;57388754300;14048484800;57283041900;","NWP-Miner: Nonoverlapping weak-gap sequential pattern mining",2022,"Information Sciences","588",,,"124","141",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121902335&doi=10.1016%2fj.ins.2021.12.064&partnerID=40&md5=d9443bfadef447e0d031598790d52719","Nonoverlapping sequential pattern mining (SPM) is a type of SPM with gap constraints that can mine valuable information in sequences. One of the disadvantages of nonoverlapping SPM is that any characters can match with gap constraints. Hence, there can be a significant difference between the trend of a pattern and those of its occurrences. To tackle this issue, we propose nonoverlapping weak-gap sequential pattern (NWP) mining, where characters are divided into two types: weak and strong. This allows discovering frequent patterns more accurately by limiting the gap constraints to match only weak characters. To discover NWPs, we propose NMP-Miner which involves two key steps: support calculation and candidate pattern generation. To efficiently calculate the support of candidate patterns, depth-first search and backtracking strategies based on a simplified Nettree structure are adopted, which effectively reduce the time and space complexities of the algorithm. Moreover, a pattern join approach is applied to effectively reduce the number of candidate patterns. The experimental results show that NWP-Miner is more efficient than other competitive algorithms. More importantly, the case study of time series shows that NWP-Miner can effectively filter out noise patterns and discover more meaningful patterns. Algorithms and datasets can be downloaded from https://github.com/wuc567/Pattern-Mining/tree/master/NWP-Miner. © 2021 Elsevier Inc.","Depth-first search; Frequent pattern; Gap constraint; Nettree structure; Sequential pattern mining; Time series","Data mining; Filtration; Miners; Candidate patterns; Depth first; Depth-first search; Frequent pattern; Gap constraint; Nettree structure; Nonoverlapping; Sequential patterns; Sequential-pattern mining; Times series; Time series",Article,Scopus,2-s2.0-85121902335
"Liu J., Xu Y., Wang L.","55588480600;57226071755;57376303200;","Fault information mining with causal network for railway transportation system",2022,"Reliability Engineering and System Safety","220",,"108281","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121334512&doi=10.1016%2fj.ress.2021.108281&partnerID=40&md5=f42697d35a11bcd8e1d764bedd8fed51","Various sensors implemented in the railway transportation system brings opportunities in improving its safety and challenges in fault information mining. Extracting effective and synthetic fault-specific information from the over-rich data is one of the key challenges. The classical feature dimension reduction methods are mostly based on the statistical correlation among variables. Considering the cause-effect relationship may reflect the true influence of one variable on the other, this paper proposes three unsupervised feature extraction methods based on causal network. Precisely, after discovering the causal network among the monitoring variables in a rail transportation system, principle components related to the specific fault are extracted from the causal strength matrix or the full causal strength matrix constructed from the causal network. In comparison with the state-of-art correlation-based feature reduction methods, the effectiveness of the proposed methods is verified on two public datasets and a real dataset considering high-speed train braking system. In addition, the intrinsic working mechanism of the proposed methods is analyzed with respect to the constructed causal network, which improves the interpretability of the fault detection and diagnosis. © 2021 Elsevier Ltd","Causal network; Causal strength; Fault information mining; Feature extraction; Railway transportation system","Correlation methods; Data mining; Extraction; Fault detection; Feature extraction; Railroad cars; Railroad transportation; Causal network; Causal strength; Fault information mining; Faults information; Features extraction; Information mining; matrix; Railway transportation; Railway transportation system; Transportation system; Railroads",Article,Scopus,2-s2.0-85121334512
"Friederich J., Francis D.P., Lazarova-Molnar S., Mohamed N.","57222155699;57195680806;23393330900;57373358500;","A framework for data-driven digital twins for smart manufacturing",2022,"Computers in Industry","136",,"103586","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121259264&doi=10.1016%2fj.compind.2021.103586&partnerID=40&md5=80477776811b2969943e95e4b98e670b","Adoption of digital twins in smart factories, that model real statuses of manufacturing systems through simulation with real time actualization, are manifested in the form of increased productivity, as well as reduction in costs and energy consumption. The sharp increase in changing customer demands has resulted in factories transitioning rapidly and yielding shorter product life cycles. Traditional modeling and simulation approaches are not suited to handle such scenarios. As a possible solution, we propose a generic data-driven framework for automated generation of simulation models as basis for digital twins for smart factories. The novelty of our proposed framework is in the data-driven approach that exploits advancements in machine learning and process mining techniques, as well as continuous model improvement and validation. The goal of the framework is to minimize and fully define, or even eliminate, the need for expert knowledge in the extraction of the corresponding simulation models. We illustrate our framework through a case study. © 2021 The Author(s)","Data-driven; Digital twin; Machine learning; Process mining; Reconfigurable manufacturing; Smart factory","Data mining; E-learning; Energy utilization; Life cycle; Manufacture; Real time systems; % reductions; Data driven; Energy-consumption; Increased productivity; Process mining; Real- time; Reconfigurable manufacturing; Simulation model; Smart factory; Smart manufacturing; Machine learning",Article,Scopus,2-s2.0-85121259264
"Caldeira J., Brito e Abreu F., Cardoso J., dos Reis J.P.","23090205600;6602691646;55429578600;57193424852;","Unveiling process insights from refactoring practices",2022,"Computer Standards and Interfaces","81",,"103587","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121237248&doi=10.1016%2fj.csi.2021.103587&partnerID=40&md5=3277f0b61ca520cf76e503ce941f956b","Context: Software comprehension and maintenance activities, such as refactoring, are said to be negatively impacted by software complexity. The methods used to measure software product and processes complexity have been thoroughly debated in the literature. However, the discernment about the possible links between these two dimensions, particularly on the benefits of using the process perspective, has a long journey ahead. Objective: To improve the understanding of the liaison of developers’ activities and software complexity within a refactoring task, namely by evaluating if process metrics gathered from the IDE, using process mining methods and tools, are suitable to accurately classify different refactoring practices and the resulting software complexity. Method: We mined source code metrics from a software product after a quality improvement task was given in parallel to (117) software developers, organized in (71) teams. Simultaneously, we collected events from their IDE work sessions (320) and used process mining to model their processes and extract the correspondent metrics. Results: Most teams using a plugin for refactoring (JDeodorant) reduced software complexity more effectively and with simpler processes than the ones that performed refactoring using only Eclipse native features. We were able to find moderate correlations (≈43%) between software cyclomatic complexity and process cyclomatic complexity. Using only process driven metrics, we computed ≈30,000 models aiming to predict the type of refactoring method (automatic or manual) teams had used and the expected level of software cyclomatic complexity reduction after their work sessions. The best models found for the refactoring method and cyclomatic complexity level predictions, had an accuracy of 92.95% and 94.36%, respectively. Conclusions: We have demonstrated the feasibility of an approach that allows building cross-cutting analytical models in software projects, such as the one we used for detecting manual or automatic refactoring practices. Events from the development tools and support activities can be collected, transformed, aggregated, and analyzed with fewer privacy concerns or technical constraints than source code-driven metrics. This makes our approach agnostic to programming languages, geographic location, or development practices, making it suitable for challenging contexts, such as, in modern global software development where many projects adopt agile methodologies, and low/no code platforms. Initial findings are encouraging, and lead us to suggest practitioners may use our method in other development tasks, such as, defect analysis and unit or integration tests. © 2021 Elsevier B.V.","Refactoring practices; Software complexity; Software development process mining; Software process complexity","Computer software maintenance; Data mining; Forecasting; Software design; Cyclomatic complexity; Process complexity; Process mining; Refactoring practice; Refactorings; Software complexity; Software development process; Software development process mining; Software process; Software process complexity; Integrodifferential equations",Article,Scopus,2-s2.0-85121237248
"Martin N., Van Houdt G., Janssenswillen G.","56559224800;57209773919;57190124753;","DaQAPO: Supporting flexible and fine-grained event log quality assessment",2022,"Expert Systems with Applications","191",,"116274","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121233364&doi=10.1016%2fj.eswa.2021.116274&partnerID=40&md5=1d85e88b227161fa3c4802b3aa2ec995","Process mining can provide valuable insights in business processes using an event log containing process execution data. Despite the significant potential of process mining to support the analysis and improvement of processes, the reliability of process mining outcomes depends on the quality of the event log. Real-life logs typically suffer from various data quality issues. Consequently, thorough event log quality assessment is required before applying process mining algorithms. This paper introduces DaQAPO, the first R-package which supports flexible and fine-grained event log quality assessment. It provides a rich set of tests to identify a wide range of event log quality issues, while having sufficient flexibility to allow the detection of context-specific quality issues. © 2021 Elsevier Ltd","Data quality; Event log; Event log quality; Event log quality assessment; Process mining; R","Reliability analysis; Data quality; Event log quality; Event log quality assessment; Event logs; Log quality; Process mining; Quality assessment; R; Data mining",Article,Scopus,2-s2.0-85121233364
"Wang T., Liu R., Qi G.","57200190771;35069849300;57368963600;","Multi-classification assessment of bank personal credit risk based on multi-source information fusion",2022,"Expert Systems with Applications","191",,"116236","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121011427&doi=10.1016%2fj.eswa.2021.116236&partnerID=40&md5=5aad4ad332690deb9c23d5724cf8d19f","There have been many studies on machine learning and data mining algorithms to improve the effect of credit risk assessment. However, there are few methods that can meet its universal and efficient characteristics. This paper proposes a new multi-classification assessment model of personal credit risk based on the theory of information fusion (MIFCA) by using six machine learning algorithms. The MIFCA model can simultaneously integrate the advantages of multiple classifiers and reduce the interference of uncertain information. In order to verify the MIFCA model, dataset collected from a real data set of commercial bank in China. Experimental results show that MIFCA model has two outstanding points in various assessment criteria. One is that it has higher accuracy for multi-classification assessment, and the other is that it is suitable for various risk assessments and has universal applicability. In addition, the results of this research can also provide references for banks and other financial institutions to strengthen their risk prevention and control capabilities, improve their credit risk identification capabilities, and avoid financial losses. © 2021 Elsevier Ltd","D-S evidence theory; Information fusion; Multi-classification assessment; Personal credit risk","Classification (of information); Data mining; Information fusion; Learning algorithms; Losses; Machine learning; Credit risk assessment; Credit risks; D S evidence theory; Data mining algorithm; Multi-classification; Multi-classification assessment; Multi-source information fusion; Personal credit; Personal credit risk; Risk-based; Risk assessment",Article,Scopus,2-s2.0-85121011427
"Perišić A., Jung D.Š., Pahor M.","57221839188;56233465400;7006261689;","Churn in the mobile gaming field: Establishing churn definitions and measuring classification similarities",2022,"Expert Systems with Applications","191",,"116277","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120734992&doi=10.1016%2fj.eswa.2021.116277&partnerID=40&md5=ffb9f5497807e6a5958a46a29131cd20","Churn prediction gained attention across different application fields, both in the business and academic world, and a variety of sophisticated churn prediction models have already been built. One of the most important issues with churn is the lack of a good definition. This is especially prominent in non-contractual business settings where a definition of a churner needs to be stated prior to building the churn prediction model. The churn definition statement is highly subjective and will necessarily influence the model and the resulting classification. This study raises the problem of the churn definition statement and proposes four groups of churn definitions relying on user activity or user engagement. Although proposed definitions are established within the field of freemium mobile games, they can be applied to various non-contractual business settings, as they are related exclusively to user behavior. When applied to real data, different definitions will yield inconsistent classifications. Consequently, the second problem addressed in this research is evaluating similarities between churn definitions. The methodology for evaluating the similarity of churn definitions and churn definition groups is proposed following the principle that definitions applied on a set of users are more similar if they induce more similar churn classifications. Similarities in churn definitions and corresponding classifications are evaluated by applying the Jaccard similarity coefficient and its k-adic formulation. Comparing similarities between definition groups is a more challenging task since within-group heterogeneity must be taken into account. To deal with this problem this paper proposes a modified 2-group k-adic Jaccard similarity coefficient that can be applied in different fields, beyond just churn classification. Proposed churn definitions and similarity measures are applied on a real dataset as a case study that further illustrates the developed methodology and its applications in real-life problems. © 2021 Elsevier Ltd","Churn definition; Game data mining; Resemblance measures; Similarity coefficient","Behavioral research; Forecasting; Application fields; Churn definition; Churn predictions; Game data minings; Jaccard similarity coefficients; Mobile gaming; Non-contractual business; Prediction modelling; Resemblance measures; Similarity coefficients; Data mining",Article,Scopus,2-s2.0-85120734992
"Wu Y., Zhang Q., Hu Y., Sun-Woo K., Zhang X., Zhu H., jie L., Li S.","57356597300;57356882800;57356950900;57356521400;57356668700;57356668800;57356739600;57356813600;","Novel binary logistic regression model based on feature transformation of XGBoost for type 2 Diabetes Mellitus prediction in healthcare systems",2022,"Future Generation Computer Systems","129",,,"1","12",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120156892&doi=10.1016%2fj.future.2021.11.003&partnerID=40&md5=4bb9ccdca2820f3c1103942b974406d2","The rapidly increasing incidence of Diabetes Mellitus (DM) has shown that DM is a serious disease that endangered human life in all parts of the world. The late stage of Type-II DM (T2DM) in particular is accompanied by complex complications. Healthcare systems with various data mining algorithms can help the endocrinologist to find whether patients have diabetes in the early detection of T2DM. In the present research, a novel and efficient binary logistic regression (BLR) is proposed founding on feature transformation of XGBoost (XGBoost-BLR) for accurately predicting the specific type of T2DM, and making the model adaptive to more than one dataset. In order to raise the identification ratio, the databases are executed by series of preprocessing procedures which include removing outliers, normalization, and missing value processing. We select features that have a more significant effect on the results by χ2 test (CST). Then, the selected features are projected into high-dimensional feature space by XGBoost. Finally, the high-dimensional features generated can be modeled by the BLR application. The proposed XGBoost-BLR achieved a 94% and 98% identification rate for diabetes prediction in Pima Indians Diabetes Database (PIDD) and Early-Stage Diabetes Risk Prediction Database (ESDRPD). © 2021","Binary logistic regression; Feature transformation; Healthcare systems; Type 2 diabetes mellitus; XGBoost; XGBoost-binary logistic regression","Data mining; Database systems; Health care; Regression analysis; Binary logistic regression; Binary logistic regression models; Diabetes mellitus; Feature transformations; Healthcare systems; Model-based OPC; Type 2 diabetes mellitus; Xgboost; Xgboost-binary logistic regression; Forecasting",Article,Scopus,2-s2.0-85120156892
"Kumar Ganti P., Naik H., Kanungo Barada M.","57346825800;57226430663;57346504800;","Environmental impact analysis and enhancement of factors affecting the photovoltaic (PV) energy utilization in mining industry by sparrow search optimization based gradient boosting decision tree approach",2022,"Energy","244",,"122561","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119408764&doi=10.1016%2fj.energy.2021.122561&partnerID=40&md5=7e31808b30faecd551387618929d44da","This paper proposes a hybrid technique to recover the efficiency of solar photovoltaic (PV) energy system from the environmental impacts. The proposed technique is the combination of both sparrow search algorithm and gradient boosting decision tree; thus it is named SSA-GBDT method. The purpose of the proposed technique is to improve the efficiency of solar PV energy system and maximization of power removal from PV arrays. The PV module voltage, current and power is measured by SSA and it creates possible database offline. Database with electric parameters is utilized to develop the model online using GBDT. The data set contains some parameters like particle size and dust weight input and maximal power value output variables. Then, the proposed technique is implemented on the MATLAB/Simulink platform and the performance is compared with existing techniques. The performance of PV under normal condition, dust accumulation condition, water drops condition and partial shading conditions are the considered cases. In the cases, photovoltaic reference irradiance and temperature, PV current, voltage and generated power, active and reactive power, grid current and voltage, inverter power is also evaluated. The efficiency comparison of PV power for solution processes like ANN, GBDT, SSA and proposed system are also analyzed. © 2021 Elsevier Ltd","Environmental impacts; Gradient boosting decision tree; Mining industry; Solar photovoltaic system; Sparrow search algorithm","Data mining; Dust; Energy utilization; Environmental impact; Learning algorithms; Particle size; Particle size analysis; Photovoltaic cells; Solar concentrators; Solar power generation; Condition; Gradient boosting; Gradient boosting decision tree; Performance; Photovoltaic energy systems; Power; Search Algorithms; Solar photovoltaic energies; Solar photovoltaic system; Sparrow search algorithm; Decision trees",Article,Scopus,2-s2.0-85119408764
"Cao Q., Zanni-Merk C., Samet A., Reich C., Beuvron F.D.B.D., Beckmann A., Giannetti C.","57207364529;26666437200;57213097859;24344999900;57221812774;56257130300;55638096000;","KSPMI: A Knowledge-based System for Predictive Maintenance in Industry 4.0",2022,"Robotics and Computer-Integrated Manufacturing","74",,"102281","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118864321&doi=10.1016%2fj.rcim.2021.102281&partnerID=40&md5=11b770e995f3ccc7fde2ac174ef644d4","In the context of Industry 4.0, smart factories use advanced sensing and data analytic technologies to understand and monitor the manufacturing processes. To enhance production efficiency and reliability, statistical Artificial Intelligence (AI) technologies such as machine learning and data mining are used to detect and predict potential anomalies within manufacturing processes. However, due to the heterogeneous nature of industrial data, sometimes the knowledge extracted from industrial data is presented in a complex structure. This brings the semantic gap issue which stands for the lack of interoperability among different manufacturing systems. Furthermore, as the Cyber-Physical Systems (CPS) are becoming more knowledge-intensive, uniform knowledge representation of physical resources and real-time reasoning capabilities for analytic tasks are needed to automate the decision-making processes for these systems. These requirements highlight the potential of using symbolic AI for predictive maintenance. To automate and facilitate predictive analytics in Industry 4.0, in this paper, we present a novel Knowledge-based System for Predictive Maintenance in Industry 4.0 (KSPMI). KSPMI is developed based on a novel hybrid approach that leverages both statistical and symbolic AI technologies. The hybrid approach involves using statistical AI technologies such as machine learning and chronicle mining (a special type of sequential pattern mining approach) to extract machine degradation models from industrial data. On the other hand, symbolic AI technologies, especially domain ontologies and logic rules, will use the extracted chronicle patterns to query and reason on system input data with rich domain and contextual knowledge. This hybrid approach uses Semantic Web Rule Language (SWRL) rules generated from chronicle patterns together with domain ontologies to perform ontology reasoning, which enables the automatic detection of machinery anomalies and the prediction of future events’ occurrence. KSPMI is evaluated and tested on both real-world and synthetic data sets. © 2021 Elsevier Ltd","Chronicle mining; Industry 4.0; Knowledge-based system; Ontology reasoning; Predictive maintenance","Data mining; Decision making; Embedded systems; Engineering education; Industry 4.0; Interoperability; Knowledge based systems; Knowledge representation; Life cycle; Machine learning; Machinery; Maintenance; Ontology; Predictive analytics; Search engines; Artificial intelligence technologies; Chronicle mining; Data analytics; Domain ontologies; Hybrid approach; Industrial datum; Knowledge-based systems; Manufacturing process; Ontology reasonings; Predictive maintenance; Real time systems",Article,Scopus,2-s2.0-85118864321
"Hu J., Xing Y., Han M., Wang F., Zhao K., Che X.","57302022500;57208399491;57285068600;57199195626;18435554700;25648807400;","Nonnegative matrix tri-factorization based clustering in a heterogeneous information network with star network schema",2022,"Tsinghua Science and Technology","27","2",,"386","395",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116355877&doi=10.26599%2fTST.2020.9010049&partnerID=40&md5=cc8560c9312058c19f264c01d8d1bcbb","Heterogeneous Information Networks (HINs) contain multiple types of nodes and edges; therefore, they can preserve the semantic information and structure information. Cluster analysis using an HIN has obvious advantages over a transformation into a homogenous information network, which can promote the clustering results of different types of nodes. In our study, we applied a Nonnegative Matrix Tri-Factorization (NMTF) in a cluster analysis of multiple metapaths in HIN. Unlike the parameter estimation method of the probability distribution in previous studies, NMTF can obtain several dependent latent variables simultaneously, and each latent variable in NMTF is associated with the cluster of the corresponding node in the HIN. The method is suited to co-clustering leveraging multiple metapaths in HIN, because NMTF is employed for multiple nonnegative matrix factorizations simultaneously in our study. Experimental results on the real dataset show that the validity and correctness of our method, and the clustering result are better than that of the existing similar clustering algorithm. © The author(s) 2022.","Clustering; Data mining; Heterogeneous information network; Nonnegative matrix tri-factorization","Cluster analysis; Clustering algorithms; Data mining; Information services; Matrix algebra; Probability distributions; Semantics; Based clustering; Clustering results; Clusterings; Heterogeneous information; Heterogeneous information network; Information networks; Latent variable; Non-negative matrix; Nonnegative matrix tri-factorization; STAR network; Factorization",Article,Scopus,2-s2.0-85116355877
"Zheng W., Li M., Lin Z., Zhang Y.","55834259400;57222606243;56374335000;57280429700;","Leveraging tourist trajectory data for effective destination planning and management: A new heuristic approach",2022,"Tourism Management","89",,"104437","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116132151&doi=10.1016%2fj.tourman.2021.104437&partnerID=40&md5=b1360ff62f27bdbc5d1b822522d0aa0f","Understanding tourist movements provides insights for destination planning, service design and marketing. The key challenge is to develop a tool that can capture the value in the tourist mobility data. This study presents a new heuristic approach that combines adaptive spatial clustering with frequent pattern mining to improve the performance and efficiency of trajectory data analytics. The aim is to fully leverage the semantic information in the tourist data and the duration that tourists stay at an attraction. Anonymous mobile positioning data from 741 tourists to one of China's leading destinations are used to illustrate the application of the new analytical approach. The results reveal a four-level destination spatial structure ranging from core to peripheral areas. The findings provide practical implications for facilitating intra-destination cooperation and optimizing destination resource allocation and service design. © 2021 Elsevier Ltd","Core-peripheral; Data mining; Frequent pattern mining; Intra-destination cooperation; Time-space; Tourist mobility","core-periphery relations; data mining; heuristics; planning practice; tourism management; tourist behavior; tourist destination; trajectory; China",Article,Scopus,2-s2.0-85116132151
"Zhang C., Du Z., Yang Y., Gan W., Yu P.S.","57260719000;57218418849;57221313968;57223999014;57260831800;","On-Shelf Utility Mining of Sequence Data",2022,"ACM Transactions on Knowledge Discovery from Data","16","2","21","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115030599&doi=10.1145%2f3457570&partnerID=40&md5=7931211b7daa3e74e6b2f3fce72712e3","Utility mining has emerged as an important and interesting topic owing to its wide application and considerable popularity. However, conventional utility mining methods have a bias toward items that have longer on-shelf time as they have a greater chance to generate a high utility. To eliminate the bias, the problem of on-shelf utility mining (OSUM) is introduced. In this article, we focus on the task of OSUM of sequence data, where the sequential database is divided into several partitions according to time periods and items are associated with utilities and several on-shelf time periods. To address the problem, we propose two methods, OSUM of sequence data (OSUMS) and OSUMS+, to extract on-shelf high-utility sequential patterns. For further efficiency, we also design several strategies to reduce the search space and avoid redundant calculation with two upper bounds time prefix extension utility (TPEU) and time reduced sequence utility (TRSU). In addition, two novel data structures are developed for facilitating the calculation of upper bounds and utilities. Substantial experimental results on certain real and synthetic datasets show that the two methods outperform the state-of-the-art algorithm. In conclusion, OSUMS may consume a large amount of memory and is unsuitable for cases with limited memory, while OSUMS+ has wider real-life applications owing to its high efficiency. © 2021 Association for Computing Machinery.","data mining; On-shelf utility mining; sequence data; utility mining","Mining; High-efficiency; Limited memory; Real-life applications; Sequential database; Sequential patterns; State-of-the-art algorithms; Synthetic datasets; Utility mining; Efficiency",Article,Scopus,2-s2.0-85115030599
"Ding M., Wang T., Wang X.","57260718900;57221130052;55961737500;","Establishing Smartphone User Behavior Model Based on Energy Consumption Data",2022,"ACM Transactions on Knowledge Discovery from Data","16","2","25","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115026982&doi=10.1145%2f3461459&partnerID=40&md5=cd58cf3b8e20c308986c738db875318f","In smartphone data analysis, both energy consumption modeling and user behavior mining have been explored extensively, but the relationship between energy consumption and user behavior has been rarely studied. Such a relationship is explored over large-scale users in this article. Based on energy consumption data, where each users' feature vector is represented by energy breakdown on hardware components of different apps, User Behavior Models (UBM) are established to capture user behavior patterns (i.e., app preference, usage time). The challenge lies in the high diversity of user behaviors (i.e., massive apps and usage ways), which leads to high dimension and dispersion of data. To overcome the challenge, three mechanisms are designed. First, to reduce the dimension, apps are ranked with the top ones identified as typical apps to represent all. Second, the dispersion is reduced by scaling each users' feature vector with typical apps to unit g.,""1 norm. The scaled vector becomes Usage Pattern, while the g.,""1 norm of vector before scaling is treated as Usage Intensity. Third, the usage pattern is analyzed with a two-layer clustering approach to further reduce data dispersion. In the upper layer, each typical app is studied across its users with respect to hardware components to identify Typical Hardware Usage Patterns (THUP). In the lower layer, users are studied with respect to these THUPs to identify Typical App Usage Patterns (TAUP). The analytical results of these two layers are consolidated into Usage Pattern Models (UPM), and UBMs are finally established by a union of UPMs and Usage Intensity Distributions (UID). By carrying out experiments on energy consumption data from 18,308 distinct users over 10 days, 33 UBMs are extracted from training data. With the test data, it is proven that these UBMs cover 94% user behaviors and achieve up to 20% improvement in accuracy of energy representation, as compared with the baseline method, PCA. Besides, potential applications and implications of these UBMs are illustrated for smartphone manufacturers, app developers, network providers, and so on. © 2021 Association for Computing Machinery.","Data mining; smartphone energy consumption; user behavior modeling","Behavioral research; Dispersions; Energy utilization; Smartphones; Vectors; Clustering approach; Energy consumption datum; Energy consumption model; Energy representations; Hardware components; Intensity distribution; User behavior modeling; User behavior patterns; Data reduction",Article,Scopus,2-s2.0-85115026982
"Hidalgo J.I.G., Santos S.G.T.C., Barros R.S.M.","57197746309;56805114500;57211168902;","Dynamically Adjusting Diversity in Ensembles for the Classification of Data Streams with Concept Drift",2022,"ACM Transactions on Knowledge Discovery from Data","16","2","31","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115011467&doi=10.1145%2f3466616&partnerID=40&md5=d8262f3ee7f1cba5b878a79c9902ab6d","A data stream can be defined as a system that continually generates a lot of data over time. Today, processing data streams requires new demands and challenging tasks in the data mining and machine learning areas. Concept Drift is a problem commonly characterized as changes in the distribution of the data within a data stream. The implementation of new methods for dealing with data streams where concept drifts occur requires algorithms that can adapt to several scenarios to improve its performance in the different experimental situations where they are tested. This research proposes a strategy for dynamic parameter adjustment in the presence of concept drifts. Parameter Estimation Procedure (PEP) is a general method proposed for dynamically adjusting parameters which is applied to the diversity parameter (?) of several classification ensembles commonly used in the area. To this end, the proposed estimation method (PEP) was used to create Boosting-like Online Learning Ensemble with Parameter Estimation (BOLE-PE), Online AdaBoost-based M1 with Parameter Estimation (OABM1-PE), and Oza and Russell's Online Bagging with Parameter Estimation (OzaBag-PE), based on the existing ensembles BOLE, OABM1, and OzaBag, respectively. To validate them, experiments were performed with artificial and real-world datasets using Hoeffding Tree (HT) as base classifier. The accuracy results were statistically evaluated using a variation of the Friedman test and the Nemenyi post-hoc test. The experimental results showed that the application of the dynamic estimation in the diversity parameter (?) produced good results in most scenarios, i.e., the modified methods have improved accuracy in the experiments with both artificial and real-world datasets. © 2021 Association for Computing Machinery.","classification; concept drift; data stream; Ensemble; online learning","Adaptive boosting; Classification (of information); Data mining; Data streams; Learning systems; Adjusting parameters; Base classifiers; Classification ensembles; Classification of data; Dynamic estimation; Dynamic parameters; Estimation methods; Real-world datasets; Parameter estimation",Article,Scopus,2-s2.0-85115011467
"Wang G., Cong G., Zhang Y., Hai Z., Ye J.","55875640600;8987190900;56275712300;55508907200;7403237682;","A Synopsis Based Approach for Itemset Frequency Estimation over Massive Multi-Transaction Stream",2022,"ACM Transactions on Knowledge Discovery from Data","16","2","29","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115008473&doi=10.1145%2f3465238&partnerID=40&md5=45918c0d50b472717ffc521f79c82968","The streams where multiple transactions are associated with the same key are prevalent in practice, e.g., a customer has multiple shopping records arriving at different time. Itemset frequency estimation on such streams is very challenging since sampling based methods, such as the popularly used reservoir sampling, cannot be used. In this article, we propose a novel k-Minimum Value (KMV) synopsis based method to estimate the frequency of itemsets over multi-transaction streams. First, we extract the KMV synopses for each item from the stream. Then, we propose a novel estimator to estimate the frequency of an itemset over the KMV synopses. Comparing to the existing estimator, our method is not only more accurate and efficient to calculate but also follows the downward-closure property. These properties enable the incorporation of our new estimator with existing frequent itemset mining (FIM) algorithm (e.g., FP-Growth) to mine frequent itemsets over multi-transaction streams. To demonstrate this, we implement a KMV synopsis based FIM algorithm by integrating our estimator into existing FIM algorithms, and we prove it is capable of guaranteeing the accuracy of FIM with a bounded size of KMV synopsis. Experimental results on massive streams show our estimator can significantly improve on the accuracy for both estimating itemset frequency and FIM compared to the existing estimators. © 2021 Association for Computing Machinery.","close frequent itemset mining; Data stream mining; downward-closure estimator; itemset frequency estimation; k-minimum value synopsis; massive multi-transaction stream data","Computer science; Data mining; Bounded size; Downward closure properties; Frequent itemset mining; Minimum value; Multi-transactions; Multiple transactions; Reservoir samplings; Sampling-based method; Frequency estimation",Article,Scopus,2-s2.0-85115008473
"Saúde J., Ramos G., Boratto L., Caleiro C.","46461962700;56354034800;35106859100;6602228219;","A Robust Reputation-Based Group Ranking System and Its Resistance to Bribery",2022,"ACM Transactions on Knowledge Discovery from Data","16","2","26","","",,4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115007825&doi=10.1145%2f3462210&partnerID=40&md5=49123b45d3a6d3468d0556ef1463e18e","The spread of online reviews and opinions and its growing influence on people's behavior and decisions boosted the interest to extract meaningful information from this data deluge. Hence, crowdsourced ratings of products and services gained a critical role in business and governments. Current state-of-the-art solutions rank the items with an average of the ratings expressed for an item, with a consequent lack of personalization for the users, and the exposure to attacks and spamming/spurious users. Using these ratings to group users with similar preferences might be useful to present users with items that reflect their preferences and overcome those vulnerabilities. In this article, we propose a new reputation-based ranking system, utilizing multipartite rating subnetworks, which clusters users by their similarities using three measures, two of them based on Kolmogorov complexity. We also study its resistance to bribery and how to design optimal bribing strategies. Our system is novel in that it reflects the diversity of preferences by (possibly) assigning distinct rankings to the same item, for different groups of users. We prove the convergence and efficiency of the system. By testing it on synthetic and real data, we see that it copes better with spamming/spurious users, being more robust to attacks than state-of-the-art approaches. Also, by clustering users, the effect of bribery in the proposed multipartite ranking system is dimmed, comparing to the bipartite case. © 2021 Association for Computing Machinery.","briebery; clustering; data mining; graph algorithms for the web; multipartite graphs; Ranking systems; reputation-based ranking systems","Computational complexity; Clustering users; Kolmogorov complexity; Personalizations; Products and services; Ranking system; State of the art; State-of-the-art approach; Synthetic and real data; Spamming",Article,Scopus,2-s2.0-85115007825
"Wei K., Li T., Huang F., Chen J., He Z.","55955057200;57254113600;57196054434;55925522600;57254228200;","Cancer classification with data augmentation based on generative adversarial networks",2022,"Frontiers of Computer Science","16","2","162601","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114677792&doi=10.1007%2fs11704-020-0025-x&partnerID=40&md5=a175f9acf23a7b89fdd98ab649399f3b","Accurate diagnosis is a significant step in cancer treatment. Machine learning can support doctors in prognosis decision-making, and its performance is always weakened by the high dimension and small quantity of genetic data. Fortunately, deep learning can effectively process the high dimensional data with growing. However, the problem of inadequate data remains unsolved and has lowered the performance of deep learning. To end it, we propose a generative adversarial model that uses non target cancer data to help target generator training. We use the reconstruction loss to further stabilize model training and improve the quality of generated samples. We also present a cancer classification model to optimize classification performance. Experimental results prove that mean absolute error of cancer gene made by our model is 19.3% lower than DC-GAN, and the classification accuracy rate of our produced data is higher than the data created by GAN. As for the classification model, the classification accuracy of our model reaches 92.6%, which is 7.6% higher than the model without any generated data. © 2022, Higher Education Press.","cancer data analysis; data mining; deep learning; generative adversarial networks","Clustering algorithms; Decision making; Deep learning; Diseases; Adversarial networks; Cancer classification; Classification accuracy; Classification models; Classification performance; Data augmentation; High dimensional data; Mean absolute error; Classification (of information)",Article,Scopus,2-s2.0-85114677792
"Zantow K., Yu J., Ye G., Xi Y., Liao X.","15057359200;55682165300;36643131500;36138969900;56486699100;","An Integrated Model for User Innovation Knowledge Based on Super-Network",2022,"IEEE Transactions on Engineering Management","69","2",,"399","408",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105581097&doi=10.1109%2fTEM.2020.2981423&partnerID=40&md5=feae06513fdfdcd043c9f9e2bb9aa263","Purpose: This article aims to provide an integrated model and methodology for discovering valuable innovation knowledge and creative users in online innovative communities. Design/methodology/approach: The super-network integration approach is applied in constructing the user's innovation knowledge super-network (UIKSN) model based on knowledge fragments discovered from user-generated content (UGC) data using text mining methods. The social network analysis (SNA) methodology is then used to analyze the KSN model. An empirical research is conducted with China's Xiaomi online community to illustrate the UIKSN model and the SNA methodology. Findings: Compared with current methods, the proposed KSN model and SNA methodology are demonstrated to be more effective and valid in discovering valuable innovation knowledge including valuable innovations, innovation trends, creative users and their knowledge background, etc. Research limitations/implications: The results of KSN modeling and analysis are inevitably influenced by the performance of text mining methods. Practical implications: More and more companies start to adopt online community user's innovations in their new product developments. However, it is difficult to discover users' opinions and innovations from the UGC due to its tremendous volume. Therefore, the KSN model and SNA methodology presented in this article are helpful for companies to manage online communities and to utilize users' innovations effectively and efficiently. Originality/value: The article provides two main contributions in the study of online communities: 1) a well-justified model (users' innovation KSN, UIKSN) to explore online users' contributions; 2) a new and more effective methodology to discover and analyze valuable innovations and innovative users. © 1988-2012 IEEE.","Integrated model; super-network; user innovation knowledge","Data mining; Knowledge based systems; Online systems; User profile; Creatives; Integrated modeling; Mining methods; Network models; Social Network Analysis; Super-network; Text-mining; User innovation; User innovation knowledge; User-generated; Social networking (online)",Article,Scopus,2-s2.0-85105581097
"Muhlroth C., Grottke M.","57201290217;15623138200;","Artificial Intelligence in Innovation: How to Spot Emerging Trends and Technologies",2022,"IEEE Transactions on Engineering Management","69","2",,"493","510",,5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085759965&doi=10.1109%2fTEM.2020.2989214&partnerID=40&md5=bed5a33f586f45423067a8cefe0b3b0d","Firms apply strategic foresight in technology and innovation management to detect discontinuous changes early, to assess their expected consequences, and to develop a future course of action enabling superior company performance. For this purpose, an ever-increasing amount of data has to be collected, analyzed, and interpreted. Still, a major part of these activities is performed manually, which requires high investments in various resources. To support these processes more efficiently, this article presents an artificial-intelligence-based data mining model that helps firms spot emerging topics and trends at a higher level of automation than before. Its modular structure consists of components for query generation, data collection, data preprocessing, topic modeling, topic analysis, and visualization, combined in such a way that only a minimum amount of manual effort is required during its initial set up. The approach also incorporates self-adaptive capabilities, allowing the model to automatically update itself once new data has become available. The model parameterization is based on latest research in this area, and its threshold parameter is learnt during supervised training using a training data set. We have applied our model to an independent test data set to verify its effectiveness as an early warning system. By means of a retrospective analysis, we show in three case studies that our model is able to identify emerging technologies prior to their first publication in the Gartner Hype Cycle for Emerging Technologies. Based on our findings, we derive both theoretical and practical implications for the technology and innovation management of firms, and we suggest future research opportunities to further advance this field. © 1988-2012 IEEE.","Artificial intelligence (AI); computer-aided foresight; corporate foresight; innovation management; machine learning; strategic decision making; strategic foresight; technology management; trend detection","Artificial intelligence; Data acquisition; Data visualization; Engineering research; Statistical tests; Early Warning System; Emerging technologies; Level of automations; Model parameterization; Research opportunities; Retrospective analysis; Self adaptive capabilities; Supervised trainings; Data mining",Article,Scopus,2-s2.0-85085759965
"Yang Y., Yang H., Liu Z., Yuan Y., Guan X.","57218478874;57461912600;55714815900;55067776300;57392292000;","Fall detection system based on infrared array sensor and multi-dimensional feature fusion",2022,"Measurement: Journal of the International Measurement Confederation","192",,"110870","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125011666&doi=10.1016%2fj.measurement.2022.110870&partnerID=40&md5=602731774cabc5f0f3601eec83d0ec87","In recent years, with the acceleration of the aging of the population, the safety of the elderly living alone has attracted great attention, and the falls have become one of the main factors leading to elderly casualties. In order to obtain a high precision and low cost fall detection system for the elderly, a fall detection system based on infrared array sensor and multi-dimensional feature fusion is proposed in this paper. First, we propose a new data acquisition method using infrared array sensor, which effectively enlarges the detection area. Then the personnel positioning is performed before fall detection, which can ensure real-time detection while reducing computational complexity. In addition, a sliding window algorithm is developed and four representative features of a fall are extracted from the collected data, which is fitful to the online detection. Among them, the four characteristics include the change in the center of mass of the falling process, the change in the speed, the change in the area of the person, and the change in variance. Finally, based on the refined features, the support vector machine (SVM) classifier is introduced to identify falls and improve the classification accuracy. The experimental results validate that the proposed fall detection system shows good fall detection accuracy and great practicability. © 2022","Fall detection; Feature extraction; Infrared array sensor; SVM algorithm","Data acquisition; Data mining; Feature extraction; Support vector machines; Array sensors; Detection system; Fall detection; Features extraction; Features fusions; Infrared array sensor; Infrared arrays; Living alone; Multi dimensional; Support vector machines algorithms; Fall detection",Article,Scopus,2-s2.0-85125011666
"You S., Zheng Q., Chen B., Xu Z., Lin Y., Gan M., Zhu C., Deng J., Wang K.","57191984347;57193336154;57209180394;57454972300;57194798718;55303069800;57208151542;7402613376;56203568900;","Identifying the spatiotemporal dynamics of forest ecotourism values with remotely sensed images and social media data: A perspective of public preferences",2022,"Journal of Cleaner Production","341",,"130715","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124696307&doi=10.1016%2fj.jclepro.2022.130715&partnerID=40&md5=b6b6098d6795d2ee3cfb630033c48e29","Ecotourism, as a cultural ecosystem service, is an essential part of rural economic growth. Developing feasible methods and indicators for ecotourism values is vital to exploring the interactions between public preferences and ecosystems, thus providing information for sustainable ecosystem management. However, forest ecotourism value assessments at the regional scale still lack scientific frameworks covering the spatiotemporal dimension. Here, we proposed a framework to assess forest ecotourism values and explore the contributions of public preferences, selecting a typical and abundant forest area, Lin'an County, China. First, we applied a text mining approach on social comments to identify key variables related to public preferences. Then, we mapped these variables with remote sensing and GIS data and incorporated them with a Maxent model. This model revealed the spatiotemporal patterns of forest ecotourism value and its interaction with public preferences, over two periods, i.e., 2010–2014 and 2015–2019. Our results showed that albeit with a minor change magnitude in the entirety of ecotourism values (1–3% overall decrease), about 34% and 44% of forests were observed as significant increases and decreases in ecotourism values, respectively. A large number of increases were concentrated in the village scenic areas, while decreases were observed in planted areas. Such notable changes were mainly attributable to changes in accessibility variables with 67% of contributions in 2010–2014 and 72% in 2015–2019. Variables related to human perception and vision (forest attribute and color diversity) also played a part. Our findings infer that local rural-building policies have an evident influence on the distribution and intensity of forest ecotourism. Our proposed framework provides references for evaluating the response and effectiveness of local policies and to make proactive efforts for forest protection, restoration and management. © 2022 Elsevier Ltd","Aesthetic; Cultural ecosystem service; Maxent; Recreation; Text mining","Conservation; Data mining; Forestry; Remote sensing; Rural areas; Social networking (online); Cultural ecosystem service; Eco-tourisms; Ecosystem services; Esthetic; Maxent; Public preferences; Recreation; Remotely sensed images; Social media datum; Spatio-temporal dynamics; Ecosystems",Article,Scopus,2-s2.0-85124696307
"Derval G., Schaus P.","57196052613;23089426500;","Maximal-Sum submatrix search using a hybrid contraint programming/linear programming approach",2022,"European Journal of Operational Research","297","3",,"853","865",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114637156&doi=10.1016%2fj.ejor.2021.06.008&partnerID=40&md5=8ca155f23a2cbea2ae0f676474922f17","A Maximal-Sum Submatrix (MSS) maximizes the sum of the entries corresponding to the Cartesian product of a subset of rows and columns from an original matrix (with positive and negative entries). Despite being NP-hard, this recently introduced problem was already proven to be useful for practical data-mining applications. It was used for identifying bi-clusters in gene expression data or to extract a submatrix that is then visualized in a circular plot. The state-of-the-art results for MSS are obtained using an advanced Constraint Programing approach that combines a custom filtering algorithm with a Large Neighborhood Search. We improve the state-of-the-art approach by introducing new upper bounds based on linear and mixed-integer programming formulations, along with dedicated pruning algorithms. We experiment on both synthetic and real-life data, and show that our approach outperforms the previous methods. © 2021 Elsevier B.V.","Combinatorial optimization; Constraint programming; Linear relaxation; Maximum-sum submatrix","Combinatorial optimization; Constraint theory; Data mining; Gene expression; Integer programming; Satellites; Cartesian Products; Data mining applications; Linear relaxations; Linear-programming; matrix; Maximum-sum submatrix; Negative Entries; NP-hard; Programming: linear; Submatrix; Constraint programming",Article,Scopus,2-s2.0-85114637156
"Carlier J., Berardinelli D., Montanari E., Sirignano A., Di Trana A., Busardò F.P.","55236843400;57451301400;36461449100;6603477826;57209004091;55931641600;","3F-α-pyrrolydinovalerophenone (3F-α-PVP) in vitro human metabolism: Multiple in silico predictions to assist in LC-HRMS/MS analysis and targeted/untargeted data mining",2022,"Journal of Chromatography B: Analytical Technologies in the Biomedical and Life Sciences","1193",,"123162","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124512599&doi=10.1016%2fj.jchromb.2022.123162&partnerID=40&md5=044cd17485c7baba78a136f6c2014dff","Synthetic cathinones (SCs) constitute a heterogenous class of new psychoactive substances (NPS), structurally related to cathinone. SCs represent the widest NPS class, second to synthetic cannabinoids, accounting for approximately 160 different analogues with substitution at the phenyl group, the amine group, or the alkyl chain. In 2020, α-pyrrolidonophenone analogues were the most trafficked SCs, and were involved in many fatalities and intoxication cases. In particular, 3F-α-pyrrolidinovalerophenone (3F-α-PVP) was the cause of the highest number of SC-related fatal intoxications in Sweden in 2018. Minor structural modifications are used to avoid legal controls and analytical detection, but may also induce different toxicological profile. Therefore, the identification of specific markers of consumption is essential to discriminate SCs in clinical and forensic toxicology. In this study, we assessed 3F-α-PVP metabolic profile. 3F-α-PVP was incubated with 10-donor-pooled human hepatocytes, LC-HRMS/MS analysis, and software-assisted data mining. This well-established workflow was completed by in silico metabolite predictions using three different freeware. Ten metabolites were identified after 3 h incubation, including hydrogenated, hydroxylated, oxidated, and N-dealkylated metabolites. A total of 51 phase I and II metabolites were predicted, among which 7 were detected in the incubations. We suggest 3F-α-PVP N-butanoic acid, 3F-α-PVP pentanol, and 3F-α-PVP 2-ketopyrrolidinyl-pentanol as specific biomarkers of 3F-α-PVP consumption. This is the first time that an N-ethanoic acid is detected in the metabolic pathway of a pyrrolidine SC, demonstrating the importance of a dual targeted/untargeted data mining strategy. © 2022","3F-α-PVP; In silico metabolite prediction; In vitro human hepatocyte metabolism; LC-HRMS/MS; New psychoactive substances; Synthetic cathinone","Biomolecules; Forecasting; Metabolism; Metabolites; 3f-α-pyrrolydinovalerophenone; Cathinone; Human hepatocytes; In silico metabolite prediction; In vitro human hepatocyte metabolism; In-silico; In-vitro; LC-HRMS/MS; New psychoactive substance; Synthetic cathinone; Data mining",Article,Scopus,2-s2.0-85124512599
"Liu X., Sun H., Han S., Han S., Niu S., Qin W., Sun P., Song D.","57204795636;57437675500;57193326707;57219936519;57437384600;57211406824;57211405123;55983062800;","A data mining research on office building energy pattern based on time-series energy consumption data",2022,"Energy and Buildings","259",,"111888","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123923735&doi=10.1016%2fj.enbuild.2022.111888&partnerID=40&md5=3c89b69e9879cb1cc597552852fdcf42","The purpose of this paper is to study the energy usage pattern of office building all year round using time-series energy consumption data. This investigated pattern could uncover the energy utilization issue availing building energy efficiency implementation. Past researches principally focused on the total energy condition instead of time-series in terms of time cycle. This paper implements the innovative artificial intelligent algorithms to perform the data mining target via cluster analysis and association rule discovery between different types of energy. Official energy building models provide the studied database. The result shows that k-shape and apriori algorithm could successfully obtain the energy using pattern hidden dataset. There is significant dispensable energy wastage after working time with office building since the long leave course happening after 18:00. The main subentry energy determining total energy is different in disparate stages. Moreover, cooling energy primarily manipulates the total energy in most of time indicating more than 80% degree in terms of confidence level. Conclusion illustrates that this workflow could successfully detect power load profile features and find the unreasonable issues with energy using. Combining above cluster and association analysis outcome contributes to the energy adjustment in each period more precision. © 2022 Elsevier B.V.","Algorithm; Association; Cluster; Data mining; Energy profile; Time-series data","Cluster analysis; Clustering algorithms; Energy efficiency; Energy utilization; Office buildings; Time series; Building energy; Cluster; Energy; Energy consumption datum; Energy patterns; Energy profile; Energy usage; Time-series data; Times series; Total energy; Data mining",Article,Scopus,2-s2.0-85123923735
"Zhou Z., Wang C., Feng Y., Chen D.","57420988800;56017252400;57204938737;57203682220;","JointE: Jointly utilizing 1D and 2D convolution for knowledge graph embedding",2022,"Knowledge-Based Systems","240",,"108100","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123253028&doi=10.1016%2fj.knosys.2021.108100&partnerID=40&md5=8cced2b242c4b080ae563a2733d1cd8d","Knowledge graph embedding is a popular method to predict missing links for knowledge graphs by projecting entities and relations into continuous low-dimension embeddings. Some recent embedding models employ translation-based operations to learn the representations of entities and relations with shallow and linear structures, and others leverage neural networks, especially convolution neural networks, to embed the entities and relations with deep and non-linear structures. However, shallow and linear models limit the extraction capacity of the latent knowledge while deep and non-linear models lead to the overabundance of parameters and the loss of surface and explicit knowledge. In this paper, we propose JointE, which utilizes 1D and 2D convolution operations jointly to alleviate these issues effectively. More specifically, we utilize 2D convolution operations to facilitate the interactions between entities and relations, thereby capturing the latent knowledge sufficiently. To reduce the number of parameters significantly, we innovatively construct 2D convolution filters from internal embeddings rather than using external filters which costs plenty of redundant parameters. Furthermore, we appropriately employ 1D convolution filters over input embeddings to extract the surface and explicit knowledge and preserve it by element-wise addition. Experimental evaluation on five benchmark datasets demonstrates that our model outperforms all other state-of-the-art convolution-based models and simultaneously enhances the parameter efficiency. © 2022 Elsevier B.V.","Convolution networks; Knowledge graph; Knowledge graph embedding","Data mining; Graph embeddings; Knowledge graph; 2-D convolution; Convolution filters; Convolution network; Embeddings; Explicit knowledge; Graph embeddings; Knowledge graph embedding; Knowledge graphs; Learn+; Convolution",Article,Scopus,2-s2.0-85123253028
"Yu Y., Wang W., Wu N., Liu H., Shao M.","57205726238;57416523300;56998065800;57415770200;57280343400;","IISD: Integrated Interaction Subgraph Detection for event mining",2022,"Knowledge-Based Systems","240",,"108080","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123011825&doi=10.1016%2fj.knosys.2021.108080&partnerID=40&md5=d5bdbeed4ebe830462da2ab0fd2c53d8","Event detection on networks is an important research task in data mining. Most previous methods usually detect a particular type of event that satisfies the predefined rules in the model. However, few methods consider human expert's interests during the detection process to discover unexplored events. In this paper, we regard the interactive event detection as the anomalous subgraph detection on attributed networks, named Integrated Interaction Subgraph Detection (IISD), where events are treated as anomalous connected subgraphs on the network. The core of our method is automatically identifying events by evaluating the abnormality of the subgraph and integrating the human expert's interaction simultaneously. Specifically, we first define the human expert's interaction and recommended interaction domain (i.e., the subgraph and neighbor vertices), which are used to conduct interactive operations based on the human expert's interests. Afterwards, we propose an efficient subgraph detection algorithm that iteratively integrates human expert's feedback and updates the recommended interaction domain. In this way, our method could retrieve the most anomalous subgraph on the network as the final event, which could contain the potential unexplored information due to the continuously optimized interaction of human experts. We have conducted extensive experiments on two real-world datasets and proved that our algorithm could achieve better performance compared with several competitive baselines. Moreover, the case study shows that our method could detect global abnormal events effectively. © 2022 Elsevier B.V.","Anomalous subgraph detection; Global abnormal event; Interactive event detection","Data mining; Anomalous subgraph detection; Detection process; Event mining; Events detection; Global abnormal event; Human expert; Integrated interaction; Interaction domain; Interactive event detection; Subgraphs; Iterative methods",Article,Scopus,2-s2.0-85123011825
"Zou Y., Chou C.-A.","57415829700;35955527200;","A combinatorial optimization approach for multi-label associative classification",2022,"Knowledge-Based Systems","240",,"108088","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122974703&doi=10.1016%2fj.knosys.2021.108088&partnerID=40&md5=784b3136eba7424f34ec7512d13b520e","Mining associations between variables corresponding to multiple class labels (or outcomes) is prevalent in various applied domains, such as medical diagnosis, text mining, e-commence, and social behavior analysis. While most associative classification algorithms have been developed to discover association rules of binary variables for single-label classification problem, there are limited methods designed for the problem, called multi-label classification, that accounts for multi-labels. In this study, we consider the multi-label classification problem as a multi-class classification problem and formulate it as a 0–1 integer optimization model. We then leverage combinatorial optimization and association rule techniques to solve this hard problem. More specifically, we propose a ranking metric for selecting and aggregating stronger rules to form an optimal multi-label classifier. The computational results for multiple real applications show that our algorithm is able to identify significant association rules between key variables and multiple labels, and in turn achieves a competitive classification performance compared to state-of-the-art machine learning methods such as logistic regression, decision trees, and random forest. Moreover, we design a user-interface tool interfaced with the developed algorithm and demonstrate a medical diagnosis problem to predict multiple high-risk subgroups during emergency care in practice. © 2021 Elsevier B.V.","Associative classification; Combinatorial optimization; Interpretable data mining; Machine learning","Association rules; Classification (of information); Combinatorial optimization; Decision trees; Integer programming; Machine learning; Associative classification; Behavior analysis; Class labels; Interpretable data mining; Mining associations; Multi label associative classifications; Multiple class; Optimization approach; Social behaviour; Text-mining; Data mining",Article,Scopus,2-s2.0-85122974703
"Schumacher B.L., Yost M.A., Burchfield E.K., Allen N.","57218367495;57094707200;56518310300;57203306838;","Water in the West: Trends, production efficiency, and a call for open data",2022,"Journal of Environmental Management","306",,"114330","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122687747&doi=10.1016%2fj.jenvman.2021.114330&partnerID=40&md5=2d9f2cb9dfc7346b650dca22dd34e4f9","Climate change is projected to transform US agriculture, particularly in places reliant on limited irrigation water resources. As water demand and scarcity increase simultaneously over the coming decades, water managers and growers will need to optimize water use on their irrigated lands. Understanding how growers maintain high yields in arid, water stressed places, while conserving water, is of key importance for the future of US agriculture in the West. We explore water use management and trends in irrigated agriculture in the Western US using operator-level USDA-NASS Farm and Ranch Irrigation Survey/Irrigation and Water Management Survey data aggregated for the first time to the county-scale. In this exploration, we build the first county-level, openly accessible dataset linking farm(er) characteristics to irrigation behaviors in the West. We find notable spatial and temporal variability in Western irrigation practices, with neighboring counties exhibiting large differences in efficiency, water use, and crop yields, as well as in the sources of information, scheduling methods, and technological improvements employed. To produce effective management initiatives in the West, we call for the express and open dissemination of USDA irrigation data at sub-state scales. These data will contribute to our understanding of irrigated production and could support a pathway that will prepare growers for a more resilient agricultural future. © 2021","Data dissemination; Irrigation; USDA Farm and ranch irrigation surveys/irrigation and water management surveys; Water use","tap water; water; well water; data set; irrigation; water demand; water management; water use; agricultural land; alfalfa; Article; climate change; computer simulation; confidential information; crop management; data analysis; data availability; data mining; decision making; decision making task; grain; harvest; human; information processing; irrigation (agriculture); open source software; production efficiency; productivity; qualitative research; silage; trend study; water availability; water conservation; water insecurity; water management; water supply; wheat; agriculture; irrigation (agriculture); water supply; United States; Agricultural Irrigation; Agriculture; Climate Change; Water; Water Supply",Article,Scopus,2-s2.0-85122687747
"Men H., Liu M., Shi Y., Xia X., Wang T., Liu J., Liu Q.","8366126000;57223438611;57190755116;57357614600;57346480900;54948000000;57219039970;","Interleaved attention convolutional compression network: An effective data mining method for the fusion system of gas sensor and hyperspectral",2022,"Sensors and Actuators B: Chemical","355",,"131113","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120317059&doi=10.1016%2fj.snb.2021.131113&partnerID=40&md5=c916395ff3e622087320678ee506e028","The combination of multi-instrument can comprehensively show the overall attributes of sample from different information sources. However, multi-sensor data fusion brings some redundant information and reduces the recognition accuracy. Therefore, a new deep learning model, namely interleaved attention convolutional compression network (IACCN), is proposed to realize the identification of rice quality in six storage periods under different storage conditions. Firstly, electronic nose (e-nose) and hyperspectral technology are used to get the gas information and spectral information. Secondly, the interleaved attention convolution block (IACB) is proposed in the IACCN to realize the information interaction between the e-nose and hyperspectral data, improve the parameter utilization and extract important features. Finally, knowledge distillation (KD) is introduced to improve the detection performance and stability of the model. Compared with other deep learning methods, IACCN shows a better classification performance and good stability. In conclusion, IACCN is an effective data mining method to improve the classification ability of the fusion system and provides the technology to monitor the rice quality. © 2021 Elsevier B.V.","Data fusion; Electronic nose; Hyperspectral; Interleaved attention convolution block; Knowledge distillation; Rice","Data mining; Deep learning; Digital storage; Distillation; Electronic nose; Sensor data fusion; Data mining methods; Fusion systems; Gas-sensors; HyperSpectral; Information sources; Interleaved attention convolution block; Knowledge distillation; Multi sensors data fusion; Rice; Rice qualities; Convolution",Article,Scopus,2-s2.0-85120317059
"Park J., Kim Y., Na K., Youn B.D., Chen Y., Zuo M.J., Bae Y.-C.","56424215400;57202143829;57199156276;7005009209;57045544000;7007129577;24466421500;","An image-based feature extraction method for fault diagnosis of variable-speed rotating machinery",2022,"Mechanical Systems and Signal Processing","167",,"108524","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119013774&doi=10.1016%2fj.ymssp.2021.108524&partnerID=40&md5=c3ebfa32d5a10ac24f9e93047ebf1056","This paper proposes a new feature extraction method using time–frequency image data for fault diagnosis of variable-speed rotating machinery. Time-frequency representation (TFR) is widely used to analyze time-varying behaviors of rotating machinery. Recently, methods have been developed to extract fault-related features from TFR image data. However, these methods can be only applied to in-phase TFR image data, or have limited sensitivity because they cannot utilize the characteristics of faults in rotating machinery. Therefore, the research outlined in this paper proposes a new fault feature for rotating machinery under variable-speed conditions. The proposed feature enhances sensitivity by exploiting faulty behaviors in the TFR image data. Two experimental case studies are presented to demonstrate the performance of the proposed method: a planetary gearbox and a spur gearbox. From the results, we conclude that the proposed method shows higher fault sensitivity than the previous image-based features, while showing consistent behavior under different phases of TFR image data. © 2021 Elsevier Ltd","Fault diagnosis; Prognostics and health management; Rotating machines; Time-frequency analysis; Vibration signal","Data mining; Extraction; Failure analysis; Fault detection; Feature extraction; Gears; Image enhancement; Vibration analysis; Faults diagnosis; Feature extraction methods; Image data; Image-based features; Prognostic and health management; Rotating machine; Time-frequency Analysis; Time-frequency representations; Variable speed; Vibration signal; Rotating machinery",Article,Scopus,2-s2.0-85119013774
"Zhang Z., Huang J., Tan Q.","56340063400;55628526066;57221320548;","Association Rules Enhanced Knowledge Graph Attention Network",2022,"Knowledge-Based Systems","239",,"108038","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122803463&doi=10.1016%2fj.knosys.2021.108038&partnerID=40&md5=a7d4476a5341327e2f063eb5f0c976b7","Embedding knowledge graphs into continuous vector spaces has recently attracted increasing interest in knowledge base completion. However, in most existing embedding methods, only fact triplets are utilized, and logical rules have not been thoroughly studied for the knowledge base completion task. To overcome the problem, we propose an association rules enhanced knowledge graph attention network (AR-KGAN). In this paper, triplets and logical rules are jointly modeled in the proposed unified framework to achieve more predictive entity and relation embeddings. Association rules and corresponding correlation degrees between them can be automatically obtained according to our designed mining algorithm. The major component of AR-KGAN is an encoder of an effective neighborhood aggregator, which addresses the problems by aggregating neighbors with both association rules based and graph based attention weights. The decoder enables AR-KGAN to be translational between entities and relations while keeping the superior link prediction performance. Then, the global loss is minimized over both atomic and complex formulas to achieve the embedding task. In this manner, we learn embeddings compatible with triplets and association rules, which are certainly more predictive for knowledge acquisition and inference. The results show that the proposed AR-KGAN model achieves significant and consistent improvements over state-of-the-art methods on three benchmark datasets. © 2021","Association rules; Embedding propagation; Graph attention network; High-order neighborhood; Knowledge Graphs; Knowledge inference","Association rules; Data mining; Graph embeddings; Graphic methods; Vector spaces; Embedding propagation; Embeddings; Graph attention network; High-order; High-order neighborhood; Higher-order; Knowledge graphs; Knowledge inference; Logical rules; Neighbourhood; Knowledge graph",Article,Scopus,2-s2.0-85122803463
"Lin W.-C., Tsai C.-F., Zhong J.R.","56205754000;7404966986;57409330500;","Deep learning for missing value imputation of continuous data and the effect of data discretization",2022,"Knowledge-Based Systems","239",,"108079","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122686867&doi=10.1016%2fj.knosys.2021.108079&partnerID=40&md5=8ede661ed8f0ba04a4120199d32d57b2","Often real-world datasets are incomplete and contain some missing attribute values. Furthermore, many data mining and machine learning techniques cannot directly handle incomplete datasets. Missing value imputation is the major solution for constructing a learning model to estimate specific values to replace the missing ones. Deep learning techniques have been employed for missing value imputation and demonstrated their superiority over many other well-known imputation methods. However, very few studies have attempted to assess the imputation performance of deep learning techniques for tabular or structured data with continuous values. Moreover, the effect on the imputation results when the continuous data need to be discretized has never been examined. In this paper, two supervised deep neural networks, i.e., multilayer perceptron (MLP) and deep belief networks (DBN), are compared for missing value imputation. Moreover, two differently ordered combinations of data discretization and imputation steps are examined. The results show that MLP and DBN significantly outperform the baseline imputation methods based on the mean, KNN, CART, and SVM, with DBN performing the best. On the other hand, when considering the discretization of continuous data, the order in which the two steps are combined is not the most important, but rather, the chosen imputation algorithm. That is, the final performance is much better when using DBN for imputation, regardless of whether discretization is performed in the first or second step, than the other imputation methods. © 2021 Elsevier B.V.","Data discretization; Data science; Deep learning; Machine learning; Missing value imputation","Data mining; Deep neural networks; Learning algorithms; Support vector machines; Continuous data; Data discretization; Deep belief networks; Deep learning; Discretizations; Imputation methods; Learning techniques; Missing value imputation; Multilayers perceptrons; Performance; Multilayer neural networks",Article,Scopus,2-s2.0-85122686867
"Deng P., Zhang F., Li T., Wang H., Horng S.-J.","23388525600;57408032600;7406372548;56018441400;35585485600;","Biased unconstrained non-negative matrix factorization for clustering",2022,"Knowledge-Based Systems","239",,"108040","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122630151&doi=10.1016%2fj.knosys.2021.108040&partnerID=40&md5=6950d11e65451562dc272261469c40f1","Clustering remains a challenging research hotspot in data mining. Non-negative matrix factorization (NMF) is an effective technique for clustering, which aims to find the product of two non-negative low-dimensional matrices that approximates the original matrix. Since the matrices must satisfy the non-negative constraints, the Karush–Kuhn–Tucker conditions need to be used to obtain the update rules for the matrices, which limits the choice of update methods. Moreover, this method has no learning rate and the updating process is completely dependent on the data itself. In addition, the two low-dimensional matrices in NMF are randomly initialized, and the clustering performance of the model is reduced. To address these problems, this paper proposes a biased unconstrained non-negative matrix factorization (BUNMF) model, which integrates the l2 norm and adds bias. Specifically, BUNMF uses a non-linear activation function to make elements of the matrices to remain non-negative, and converts the constrained problem into an unconstrained problem. The matrices are renewed by sequentially updating the matrices’ elements using stochastic gradient descent to obtain an update rule with a learning rate. Furthermore, the BUNMF model is constructed by three different activation functions and their iteration update algorithms are given through detailed reasoning. Finally, experimental results on eight public datasets show the effectiveness of the proposed model. © 2021 Elsevier B.V.","Clustering; Non-negative matrix factorization; Stochastic gradient descent; Unconstrained regularization","Chemical activation; Data mining; Gradient methods; Learning algorithms; Matrix algebra; Stochastic systems; Clusterings; Factorization model; Learning rates; Low dimensional; matrix; Non negatives; Nonnegative matrix factorization; Regularisation; Stochastic gradient descent; Unconstrained regularization; Non-negative matrix factorization",Article,Scopus,2-s2.0-85122630151
"Huynh H.M., Nguyen L.T.T., Pham N.N., Oplatková Z.K., Yun U., Vo B.","56039325700;55195057700;57225757647;15043128400;57388239400;35147075900;","An efficient method for mining sequential patterns with indices",2022,"Knowledge-Based Systems","239",,"107946","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122511875&doi=10.1016%2fj.knosys.2021.107946&partnerID=40&md5=452075817c2d77e84c02b7d7f776aab6","In recent years, mining informative data and discovering hidden information have become increasingly in demand. One of the popular means to achieve this is sequential pattern mining, which is to find informative patterns stored in databases. Its applications cover different areas and many methods have been proposed. Recently, pseudo-IDLists were proposed to improve both runtime and memory usage in the mining process. However, the idea cannot be directly used for sequential pattern mining as it only works on clickstream patterns, a more distinct type of sequential pattern. We propose adaptations and changes to the original idea to introduce SUI (Sequential pattern mining Using Indices). Comparing SUI with two other state-of-the-art algorithms on six test databases, we show that SUI has effective and efficient performance and memory usage. © 2021 Elsevier B.V.","Data-IDList; Pseudo-IDList; Sequential pattern mining; Vertical format","Data-idlist; Hidden information; Informative patterns; ITS applications; Mining process; Mining sequential patterns; Pseudo-idlist; Runtime and memory usage; Sequential-pattern mining; Vertical format; Data mining",Article,Scopus,2-s2.0-85122511875
"Cebral-Loureda M., Tamés-Muñoz E., Hernández-Baqueiro A.","57226353919;57462936500;57462615500;","The Fertility of a Concept: A Bibliometric Review of Human Flourishing",2022,"International Journal of Environmental Research and Public Health","19","5","2586","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125108920&doi=10.3390%2fijerph19052586&partnerID=40&md5=5ed23263bce7608a09f3d0dc8bb1f8ab","Human flourishing is a thriving concept, whose use has greatly increased among academic researchers from a variety of fields, from the arts and humanities and psychology to the social and environmental sciences and economics. To better understand the concept’s success, this work proposes a bibliometric review, in which statistical methods and data mining were used to analyze 1829 documents, chosen from the Scopus database by searching the term “human flourishing”. Through cluster and network analyses, the study shows the concept’s evolution and composition, as well as its current tensions and trends, in which the predominantly psychological approach is being compensated with social concerns and the search for justice. Furthermore, the concept’s strong philosophical roots provide it with abstract richness and great fertility, which can be seen in keywords, such as virtue or eudaimonia. This bibliometric review proved to be useful for this type of study, despite the limitations imposed by the characteristics of the Scopus database itself. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Bibliometrics; Cluster analysis; Data mining; Human flourishing; Network analysis; Scopus; Virtue","article; bibliometrics; cluster analysis; controlled study; data mining; fertility; human; human experiment; justice; network analysis; Scopus; systematic review; tension",Article,Scopus,2-s2.0-85125108920
"Shahbazi Z., Byun Y.-C.","57212241946;8897891700;","Knowledge Discovery on Cryptocurrency Exchange Rate Prediction Using Machine Learning Pipelines",2022,"Sensors","22","5","1740","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125071881&doi=10.3390%2fs22051740&partnerID=40&md5=98bc2b9a9d51ed9b098cee044bbe364d","The popularity of cryptocurrency in recent years has gained a lot of attention among researchers and in academic working areas. The uncontrollable and untraceable nature of cryptocurrency offers a lot of attractions to the people in this domain. The nature of the financial market is non-linear and disordered, which makes the prediction of exchange rates a challenging and difficult task. Predicting the price of cryptocurrency is based on the previous price inflations in research. Various machine learning algorithms have been applied to predict the digital coins’ exchange rate, but in this study, we present the exchange rate of cryptocurrency based on applying the machine learning XGBoost algorithm and blockchain framework for the security and transparency of the proposed system. In this system, data mining techniques are applied for qualified data analysis. The applied machine learning algorithm is XGBoost, which performs the highest prediction output, after accuracy measurement performance. The prediction process is designed by using various filters and coefficient weights. The cross-validation method was applied for the phase of training to improve the performance of the system. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Blockchain; Cryptocurrency; Exchange rate prediction; XGBoost","Blockchain; Data mining; Electronic money; Filtration; Finance; Learning algorithms; Machine learning; Block-chain; Exchange rates; Exchange rates prediction; Machine learning algorithms; Machine-learning; Non linear; Performance; Price inflation; Working areas; Xgboost; Forecasting",Article,Scopus,2-s2.0-85125071881
"Wersin P., Mazurek M., Gimmi T.","6602671486;7103058119;6603899125;","Porewater chemistry of Opalinus Clay revisited: Findings from 25 years of data collection at the Mont Terri Rock Laboratory",2022,"Applied Geochemistry","138",,"105234","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124705630&doi=10.1016%2fj.apgeochem.2022.105234&partnerID=40&md5=2eda0c697e71afd9722c5960288e01f7","The characterisation of porewater chemistry in nanoporous clayrocks is a difficult task. Appropriate extraction methods that have been developed fairly recently and the Mont Terri Rock Laboratory (Switzerland) have played a pioneer role in this regard. During the last 25 years high-quality data from the Opalinus Clay have been acquired. Notably, since the early synthesis of Pearson et al. (2003) a considerable number of newer data from borehole waters and waters extracted from drillcores have been generated. In this study, borehole, squeezing, leaching and cation exchange data were critically evaluated in order to derive a consistent porewater chemistry database across the formation. The results underline that the porewater composition is not constant but exhibits a regular change towards the formation boundaries. This is explained by diffusive exchange between the Na–Cl type porewater and the two bounding freshwater aquifers. Furthermore, the porewater is constrained by cation exchange, carbonate mineral and celestite equilibria. Major solute data obtained from borehole waters and squeezed waters are broadly consistent, although the latter exhibit somewhat more scatter. Overall, the knowledge on porewaters at the Mont Terri Rock Laboratory has been significantly improved. In particular, this regards the spatial profiles of major elements besides Cl, and better constraints on exchanger composition and pH/pCO2 conditions. © 2022 The Authors","Clayrock; Database; Mont Terri project; Opalinus Clay; Porewater chemistry","Aquifers; Data mining; Hydrogeology; Ion exchange; Positive ions; Borehole water; Cation exchanges; Clayrocks; Data collection; Extraction method; Mont terri project; Nano-porous; Opalinus clay; Pore waters; Pore-water chemistry; Rocks; aquifer; borehole; celestine; clay; ion exchange; laboratory method; leaching; porewater; Jura [Switzerland]; Mont Terri; Switzerland",Article,Scopus,2-s2.0-85124705630
"Shao Z., Yuan S., Xu J., Wang Y.","57216560954;57205187114;57226192963;7601486903;","A statistical feature data mining framework for constructing scholars’ career trajectories in academic data",2022,"Applied Soft Computing","118",,"108550","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124390804&doi=10.1016%2fj.asoc.2022.108550&partnerID=40&md5=c1c6024d6957e9d3813a2db1b7cb11c5","Temporal and spatial information about scholars can be found in their academic papers. Examining the footprints of scholars’ careers can help us understand the course of their growth, potential collaborations for future research, and trends in the flow of talent. Although a great deal of research has been conducted in related fields, the challenge of accurately constructing scholars’ career trajectories from redundant and noisy academic data is far from resolved. To address this problem, a unified framework called ATrajRN that employs AMiner academic data is proposed for the first time. To accurately obtain scholars’ geographic location information from their research achievements, this study introduces an algorithm called Positioning based on Academic Achievements of Scholars (PAAS), which aims to make the most of academic data and the characteristics of different maps. To avoid the interference of data redundancy, this paper proposes a statistical feature-based method to find the most reliable career trajectories by some state-of-the-art approaches. To restore the continuously scholars’ career trajectories, this paper offers the trajectory generation algorithm based on the output from the previous step. Experiments and systematic analysis shows that the proposed novel method could achieve approximately 80% accuracy – an increase of approximately 10% – manifestly outperform the baseline method. Lastly, based on this work, we develop a system for understanding scholars’ trajectories through analysis and visualization, and we investigate the migration characteristics of typical scholar groups. © 2022 Elsevier B.V.","ATrajRN; Deep learning; Noisy data; Redundant data","Data mining; Deep learning; ATrajRN; Data mining frameworks; Deep learning; Feature data; Noisy data; Redundant data; Spatial informations; Statistical features; Temporal and spatial; Temporal information; Trajectories",Article,Scopus,2-s2.0-85124390804
"Kalezhi J., Chibuluma M., Chembe C., Chama V., Lungo F., Kunda D.","35111507000;57197730344;57191954406;57202324037;57445681700;8674731100;","Modelling Covid-19 infections in Zambia using data mining techniques",2022,"Results in Engineering","13",,"100363","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124287851&doi=10.1016%2fj.rineng.2022.100363&partnerID=40&md5=24afd3e1efdcc214ceddad7c56a41ab0","The outbreak of Covid-19 pandemic has been declared a global health crisis by the World Health Organization since its emergence. Several researchers have proposed a number of techniques to understand how the pandemic affects the populations. Reported among these techniques are data mining models which have been successfully applied in a wide range of situations before the advent of Covid-19 pandemic. In this work, the researchers have applied a number of existing data mining methods (classifiers) available in the Waikato Environment for Knowledge Analysis (WEKA) machine learning library. WEKA was used to gain a better understanding on how the epidemic spread within Zambia. The classifiers used are J48 decision tree, Multilayer Perceptron and Naïve Bayes among others. The predictions of these techniques are compared against simpler classifiers and those reported in related works. © 2022 The Authors","Coronavirus; COVID-19; J48 algorithm; Multilayer perceptron; Naïve Bayes; WEKA","Classifiers; Data mining; Decision trees; Multilayer neural networks; Multilayers; Coronaviruses; COVID-19; Data-mining techniques; Global health; Health crisis; J48 algorithm; Knowledge analysis; Multilayers perceptrons; Naive bayes; Waikato environment for knowledge analyse; Coronavirus",Article,Scopus,2-s2.0-85124287851
"Xue L., Jiang H., Zhao Y., Wang J., Wang G., Xiao M.","57445820500;57191585282;57224075252;57208264573;57445386200;18234453600;","Fault diagnosis of wet clutch control system of tractor hydrostatic power split continuously variable transmission",2022,"Computers and Electronics in Agriculture","194",,"106778","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124270324&doi=10.1016%2fj.compag.2022.106778&partnerID=40&md5=9475e3f1533f10bc1134adf7bf2f35fd","A hydrostatic power split continuously variable transmission (CVT) for a tractor often has multiple ranges to improve transmission efficiency. However, a failure of the wet-clutch control system will lead to interruption of tractor power and even endanger driving safety. To improve the reliability of CVT tractors, methods for diagnosing faults in the wet-clutch control system were studied. A test bench was built and the clutch engagement pressure was measured under different fault modes. For these time series data, statistics such as the mean, variance, kurtosis etc. were selected as features in the analysis. An improved Gaussian naive Bayes algorithm based on a time window as well as principal component analysis were used to classify the different fault modes of the clutch control system. Finally, the performance of the algorithm was analyzed. A test with multiple faults was run, as was a comparison with traditional algorithms. By optimizing the time window for data interception, the classification accuracy of the Gaussian naive Bayes algorithm for normal operation reached 97% and reached 100% for the fault modes. The average accuracy and recall rate of fault diagnosis were 98.2% and 89.4%, respectively, which are better than the results for a support vector machine, the k-nearest neighbors algorithm, or the decision tree algorithm. Importantly, the results show that the clutch pressure fluctuations during the shift by a tractor CVT can be used for the fault diagnosis of a clutch control system. © 2022 Elsevier B.V.","Continuously variable transmission; Fault diagnosis; Gaussian naive Bayes; Principal component analysis; Tractor","Data mining; Failure analysis; Gaussian distribution; Support vector machines; Time series analysis; Tractors (agricultural); Tractors (truck); Trees (mathematics); Variable speed transmissions; Clutch control; Continuously variable transmission; Fault modes; Faults diagnosis; Gaussian naive baye; Gaussians; Naive bayes; Principal-component analysis; Tractor; Wet Clutches; Principal component analysis; algorithm; control system; hydrostatics; principal component analysis; transportation safety",Article,Scopus,2-s2.0-85124270324
"Sun G., Zeng Q., Zhou J.-X.","57203962719;7401806648;31767769800;","Machine learning coupled with mineral geochemistry reveals the origin of ore deposits",2022,"Ore Geology Reviews","142",,"104753","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124244192&doi=10.1016%2fj.oregeorev.2022.104753&partnerID=40&md5=d60047c934d3cfa095e6663a8911214e","As geosciences enter the era of big data, machine learning (ML) that is successful in big data, is now contributing to solving problems in the geosciences, yet there have been few applications in economic geology. This paper highlights the effectiveness of ML-based methods coupled with mineral geochemistry in revealing the origin of the Qingchengzi Pb–Zn ore field in China, which are either metamorphosed sedimentary exhalative (SEDEX) or magmatic–hydrothermal fluid related deposits. Laser ablation–inductively coupled plasma–mass spectrometry (LA–ICP–MS) pyrite trace elements coupled with decision tree (DT), K-nearest neighbors (KNN), and support vector machine (SVM) algorithms were applied to train the classification models. Testing of the DT, KNN, and SVM classifiers yielded accuracies of 98.2%, 96.4%, and 93.6%, respectively. The trained classifiers predict that the strata-bound and vein-type ore bodies at Qingchengzi ore field have a magmatic–hydrothermal origin, with DT, KNN, and SVM values of 100%, 97.4%, and 97.4%. In situ δ34S values of pyrite from strata-bound and vein-type ore bodies are 4.04‰ to 9.10‰ and 6.31‰ to 9.29‰, respectively, slightly higher than those of magmatic intrusions. In situ Pb isotopic ratios plot on the upper crust curve and yield two-stage model ages that are younger than metamorphic events in the region. Principal component (PC) analysis was used to determine the formation of the two types of mineralization. Pyrite from vein-type ore bodies (Py1) has lower contents of PC1 elements (Cu, Zn, Ge, Ag, Cd, Sn, Sb, and Pb) and higher contents of PC2 elements (Co, Ni, and Se) compared with pyrite from strata-bound ore bodies (Py2). Combined with previous fluid inclusion data, the vein-type ore bodies are inferred to have formed at higher temperatures than the strata-bound ore bodies. This study presents three visual classifiers to discriminate metamorphosed SEDEX and magmatic–hydrothermal Pb–Zn deposits. The prediction of classifiers and in situ S–Pb isotopic compositions suggest that the Qingchengzi Pb–Zn deposits have a magmatic–hydrothermal origin. The results demonstrate the effective application of ML-based methods to examine the origin of ore deposits. © 2022","In situ S–Pb isotopes; Machine learning; Pb–Zn deposits; Pyrite trace elements; Qingchengzi","Antimony compounds; Big data; Copper compounds; Data mining; Deposits; Germanium compounds; Isotopes; Laser ablation; Lead compounds; Mass spectrometry; Mineralogy; Nearest neighbor search; Principal component analysis; Selenium compounds; Sulfur compounds; Support vector machines; Tin compounds; Trace elements; Zinc compounds; Geosciences; In situ S–pb isotope; Nearest-neighbour; Orebodies; Origin of ore deposits; Pb isotopes; Pb-zn deposits; Pyrite trace element; Qingchengzi; Traces elements; Decision trees",Article,Scopus,2-s2.0-85124244192
"Munoz-Gama J., Martin N., Fernandez-Llatas C., Johnson O.A., Sepúlveda M., Helm E., Galvez-Yanjari V., Rojas E., Martinez-Millana A., Aloini D., Amantea I.A., Andrews R., Arias M., Beerepoot I., Benevento E., Burattin A., Capurro D., Carmona J., Comuzzi M., Dalmas B., de la Fuente R., Di Francescomarino C., Di Ciccio C., Gatta R., Ghidini C., Gonzalez-Lopez F., Ibanez-Sanchez G., Klasky H.B., Prima Kurniati A., Lu X., Mannhardt F., Mans R., Marcos M., Medeiros de Carvalho R., Pegoraro M., Poon S.K., Pufahl L., Reijers H.A., Remy S., Rinderle-Ma S., Sacchi L., Seoane F., Song M., Stefanini A., Sulis E., ter Hofstede A.H.M., Toussaint P.J., Traver V., Valero-Ramon Z., Weerd I.V.D., van der Aalst W.M.P., Vanwersch R., Weske M., Wynn M.T., Zerbato F.","36603094000;56559224800;57204151458;14822120300;7005859415;54791996100;57443006200;25636372400;49964119000;18436111500;57209475028;7402135994;56549516300;57205727294;57202441413;38361002000;6506017456;22333712700;23007964200;57191161172;8608714800;25928227700;35193494400;14037272400;6701842843;57194203846;55834617200;54395554500;57443391700;56597133700;56007915000;23100144700;7103197520;57196937103;57210642326;57444355400;55815410500;6603060277;57216526078;24376814800;57192331744;8535281700;57198324241;57189324421;56611749600;7004086310;7005429149;36947073600;57209300772;57338998600;7007153024;36168586600;23394109900;8964287000;56641828600;","Process mining for healthcare: Characteristics and challenges",2022,"Journal of Biomedical Informatics","127",,"103994","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124238268&doi=10.1016%2fj.jbi.2022.103994&partnerID=40&md5=c2a0b11bd9775b3f16b1ccd1f9fa6e46","Process mining techniques can be used to analyse business processes using the data logged during their execution. These techniques are leveraged in a wide range of domains, including healthcare, where it focuses mainly on the analysis of diagnostic, treatment, and organisational processes. Despite the huge amount of data generated in hospitals by staff and machinery involved in healthcare processes, there is no evidence of a systematic uptake of process mining beyond targeted case studies in a research context. When developing and using process mining in healthcare, distinguishing characteristics of healthcare processes such as their variability and patient-centred focus require targeted attention. Against this background, the Process-Oriented Data Science in Healthcare Alliance has been established to propagate the research and application of techniques targeting the data-driven improvement of healthcare processes. This paper, an initiative of the alliance, presents the distinguishing characteristics of the healthcare domain that need to be considered to successfully use process mining, as well as open challenges that need to be addressed by the community in the future. © 2022 Elsevier Inc.","Healthcare; Process mining","Diagnosis; Health care; Machinery; Business Process; Case-studies; Diagnostic process; Healthcare process; It focus; Mining techniques; Organizational process; Process mining; Process-oriented; Treatment process; Data mining; adult; article; attention; data science; human; mining",Article,Scopus,2-s2.0-85124238268
"Kiguchi M., Saeed W., Medi I.","57444355600;57225183678;57200159615;","Churn prediction in digital game-based learning using data mining techniques: Logistic regression, decision tree, and random forest",2022,"Applied Soft Computing","118",,"108491","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124184184&doi=10.1016%2fj.asoc.2022.108491&partnerID=40&md5=268194ae79113b42523e18ef78c37a37","Educational Technology (EdTech) is an industry that integrates education and technology advances. Digital game-based learning (DGBL) is one of the narrowed-down categories of EdTech. One of the common issues in the EdTech market is the higher churn rate. However, because the DGBL market is still in the early stage, few studies related to marketing perspectives exist. Besides, the approach in education or online gaming industries can be only partially applicable to DGBL. A popular approach for addressing a higher churn rate is churn prediction. By using a dataset from a Japanese company providing DGBL services, this work proposes an approach for the combination of defining churn and churn prediction for DGBL. This work has three objectives. First, determining churn in DGBL by comparing the recency and the addition of average and two standard deviations of user inactive time. Second, clarifying the churn rate of the Japanese service, which became evident as 56.77% by using the newly created churn definition. Third, developing a churn prediction model by comparing logistic regression (LR), decision tree, and random forest models. Feature selection, dataset split ratio comparison, and hyperparameter tuning were conducted to achieve better predictions. Based on the results, LR scored the highest AUC of 0.9225 and an F1-score of 0.9194. These results are on the higher side comparing with the past churn prediction studies in online gaming and education industries. As a consequence, the results indicate the effectiveness of the proposed approach for churn determination and prediction in DGBL. © 2022 Elsevier B.V.","Churn determination; Churn prediction; Digital game-based learning; Educational Technology; Machine learning","Commerce; Computer games; Data mining; E-learning; Educational technology; Forecasting; Logistic regression; Machine learning; Random forests; Churn determination; Churn predictions; Churn rates; Data-mining techniques; Digital game-based learning; Japanese company; Logistics regressions; On-line gaming; Random forests; Technology advances; Decision trees",Article,Scopus,2-s2.0-85124184184
"Weinan E., Zhou Y.","57194935723;57440206400;","A Mathematical Model for Universal Semantics",2022,"IEEE Transactions on Pattern Analysis and Machine Intelligence","44","3",,"1124","1132",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124051475&doi=10.1109%2fTPAMI.2020.3022533&partnerID=40&md5=4aab4f3e2ed5a643948de7c84b037af8","We characterize the meaning of words with language-independent numerical fingerprints, through a mathematical analysis of recurring patterns in texts. Approximating texts by Markov processes on a long-range time scale, we are able to extract topics, discover synonyms, and sketch semantic fields from a particular document of moderate length, without consulting external knowledge-base or thesaurus. Our Markov semantic model allows us to represent each topical concept by a low-dimensional vector, interpretable as algebraic invariants in succinct statistical operations on the document, targeting local environments of individual words. These language-independent semantic representations enable a robot reader to both understand short texts in a given language (automated question-answering) and match medium-length texts across different languages (automated word translation). Our semantic fingerprints quantify local meaning of words in 14 representative languages across five major language families, suggesting a universal and cost-effective mechanism by which human languages are processed at the semantic level. Our protocols and source codes are publicly available on https://github.com/yajun-zhou/linguae-naturalis-principia-mathematica. © 1979-2012 IEEE.","hitting time; question answering; recurrence time; Recurring patterns in texts; semantic model; word translation","Cost effectiveness; Data mining; Knowledge based systems; Markov processes; Natural language processing systems; Translation (languages); Hitting time; Language independents; Mathematical analysis; Question Answering; Recurrence time; Recurring pattern in text; Semantic fields; Semantic modelling; Time-scales; Word translation; Semantics; article; controlled study; human; human experiment; knowledge base; linguistics; Markov chain; mathematical analysis; reading; robotics; semantics; topical drug administration",Article,Scopus,2-s2.0-85124051475
"Xie Q., Cleverly J., Moore C.E., Ding Y., Hall C.C., Ma X., Brown L.A., Wang C., Beringer J., Prober S.M., Macfarlane C., Meyer W.S., Yin G., Huete A.","56047297500;6603208467;56411439600;56096020100;56019637200;55836408900;57195411327;57203538268;7006146499;6602365069;7004457568;57439628600;56091801700;7006114701;","Land surface phenology retrievals for arid and semi-arid ecosystems",2022,"ISPRS Journal of Photogrammetry and Remote Sensing","185",,,"129","145",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124039784&doi=10.1016%2fj.isprsjprs.2022.01.017&partnerID=40&md5=8d6d95628f94faec017d4fb38117cd33","Land surface phenology (LSP) plays a critical role in the regulation of photosynthesis, evapotranspiration, and energy fluxes. Significant progress has been made in extracting LSP information over large areas using satellite data, yet LSP retrievals remain a challenge over vast arid and semi-arid ecosystems because of sparse greenness, high variability and the lack of distinct annual patterns; for example, the MODerate Imaging Spectrometer (MODIS) Land Cover Dynamics Product MCD12Q2 that provides LSP metrics globally often failed to provide LSP information in these ecosystems. In this study, we used a modified threshold algorithm to extract LSP timing metrics, including the start, peak, and end of growing seasons, using the 16-day composite Enhanced Vegetation Index (EVI) time series from MODIS data. We applied this regionally customized algorithm across all arid and semi-arid climate regions of Australia (75% of the continental land area) encompassing shrublands, grasslands, savannas, woodlands, and croplands, extracting LSP metrics annually from 2003 to 2018, with up to two (phenology) seasons accounted for in each year. Our algorithm yielded an average of 64.9% successful rate of retrieval (proportion of pixels with retrieved LSP metrics) across 16 years in Arid and Semi-arid AUStralia (AS-AUS), which was a significant increase compared to the 14.5% rate of retrieval yielded in our study area by the global product and the major cause of the different performances between these two approaches was the different EVI amplitude restrictions utilized to avoid spurious peaks (i.e. EVI amplitude ≥ 0.1 used by the global product and peak EVI ≥ time series average EVI used by our algorithm). Gross primary productivity (GPP) measurements at OzFlux eddy covariance (EC) tower sites were used to cross-compare with the presence/absence of growing seasons detected by our algorithm, and 97% of our retrieved seasons matched with those extracted using EC data. Preliminary tests at five OzFlux sites showed that our algorithm was robust to view angle-induced sensitivity of the input data and showed similar performance when using EVI data calculated using MODIS Nadir BRDF-Adjusted Reflectance product. Our retrieved LSP metrics revealed that vegetation growth in arid ecosystems is highly irregular and can occur at any time of the year, more than once in a year, or can skip a year. The proportion of pixels with two growing seasons was found to be correlated with the average annual precipitation of the study area (p < 0.01), providing an estimation approach of LSP via rainfall. Our study improves the detection and measurement of vegetation phenology in arid and semi-arid regions by improving the spatial extend of LSP retrievals, which contributes to studies on LSP variations and dryland ecosystem resilience to climate change. More evaluation is planned for future work to assess and further improve the accuracy of the retrieved LSP metrics. © 2022 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Arid and semi-arid ecosystems; EVI; Gross primary productivity; Land surface phenology; MODIS; TERN OzFlux","Data mining; Ecosystems; Evapotranspiration; Photosynthesis; Phytoplankton; Radiometers; Spectrometers; Surface measurement; Arid and semi-arid ecosystem; Australia; Enhanced vegetation index; Gross primary productivity; Land surface phenology; Moderate imaging spectrometers; Semi-arid ecosystems; Study areas; TERN ozflux; Times series; Time series; accuracy assessment; amplitude; arid region; ecosystem resilience; eddy covariance; estimation method; growing season; MODIS; phenology; pixel; precipitation (climatology); primary production; semiarid region; Australia",Article,Scopus,2-s2.0-85124039784
"Serdyukov A.S.","42561716200;","Ground-roll extraction using the Karhunen-Loeve transform in the time-frequency domain",2022,"Geophysics","87","2",,"A19","A24",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123958214&doi=10.1190%2fgeo2021-0453.1&partnerID=40&md5=743ceecccf66458ea63b982acef50041","Ground-roll suppression is critical for seismic reflection data processing. Many standard methods, i.e., f-k fan filtering, fail when spatially aliased surface wave interference is present in the data. Spatial aliasing is a common problem; receiver spacing is often not dense enough to extract wavenumbers of low-velocity surface waves. It has long been known that the Karhunen-Loeve (KL) transform can be used to suppress aliased ground roll. However, the ground roll should be flattened before suppression, which is challenging due to the dispersion of surface wave velocities. I propose to solve this problem via the time-frequency domain. I apply the S-transform, which was previously shown to perform well in the multichannel analysis of surface waves. A simple complex-valued constant phase shift is a suitable model of surface wave propagation in common-frequency S-transform gathers. Therefore, it is easy to flatten the corresponding S-transform narrow-band frequency surface wave packet and extract it from the data by principal component analysis of the corresponding complex-valued data-covariance matrix. As the result, our S-transform KL (SKL) method filters the aliased ground roll without damaging the reflection amplitudes. The advantages of SKL filtering have been confirmed by synthetic- and field-data processing. © 2022 Society of Exploration Geophysicists.","common shot; KL transform; reduced-rank filtering; signal processing; surface wave","Covariance matrix; Data handling; Data mining; Filtration; Frequency domain analysis; Mathematical transformations; Principal component analysis; Signal processing; Wave propagation; Aliased; Common shot; Complex-valued; Reduced-rank filtering; S-transforms; Seismic reflection data; Signal-processing; Spatial aliasing; Time frequency domain; Wave numbers; Surface waves",Article,Scopus,2-s2.0-85123958214
"Sivanantham V., Sangeetha V., Alnuaim A.A., Hatamleh W.A., Anilkumar C., Hatamleh A.A., Sweidan D.","57221273710;57212722606;55834987700;57224119502;57371770600;55881612200;57343545200;","Quantile correlative deep feedforward multilayer perceptron for crop yield prediction",2022,"Computers and Electrical Engineering","98",,"107696","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123889462&doi=10.1016%2fj.compeleceng.2022.107696&partnerID=40&md5=19c0be9cce4776d0eaf89a81aea635f8","Crop yield prediction is an essential one in agriculture. Crop yield protection is the science and practice of handling plant diseases, weeds, and other pests. Accurate information regarding the crop yield history is essential for making decisions regarding agricultural risk management. Many research studies have been undertaken for identifying crop productivity using various data mining techniques. However, the prediction accuracy of crop yields was not improved with minimum time consumption. To overcome the issues, a novel Quantile Regressive Empirical correlative Functioned Deep FeedForward Multilayer Perceptron Classification (QRECF-DFFMPC) Method is proposed for crop yield prediction. QRECF-DFFMPC Method comprises three layers such as input and output layer with one or more hidden layers. The input layer of deep neural learning receives several features and data from the dataset and then sent it to the hidden layer 1. In that layer, Empirical Orthogonal Function is used to select the relevant features with the help of orthogonal basis functions. After that, Quantile regression is used in the hidden layer 2 to analyze the features and produce the regression value for every data point. Then, the regression value of data points is sent to the output layer for improving the prediction accuracy and reducing the time complexity. Experimental evaluation is carried out on factors such as prediction accuracy, precision, and prediction time for several data points and the number of features. The result shows that the proposed technique enhanced the prediction accuracy and precision by 6% and 9% and reduces the prediction time by 32%, as compared to existing works. © 2022","Crop yield prediction; Deep feedforward multi-layer perceptron; Empirical orthogonal function; Quantile regression; Sigmoid activation function","Crops; Data mining; Decision making; Deep neural networks; Forecasting; Multilayers; Orthogonal functions; Regression analysis; Risk management; Crop yield; Crop yield prediction; Deep feedforward multi-layer perceptron; Empirical Orthogonal Function; Feed forward; Multilayers perceptrons; Prediction accuracy; Quantile regression; Sigmoid activation function; Yield prediction; Multilayer neural networks",Article,Scopus,2-s2.0-85123889462
"Horsman G.","55366646200;","Defining principles for preserving privacy in digital forensic examinations",2022,"Forensic Science International: Digital Investigation","40",,"301350","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123870013&doi=10.1016%2fj.fsidi.2022.301350&partnerID=40&md5=84cdffc6ca1629706a90a9fe54d2c4f3","As digital devices play an increasing role in criminal investigations, where in some cases the data they contain may describe events where few other sources of information exist, there is an increasing concern regarding potential privacy invasion caused by their examination. ICO, 2020, the Information Commissioner's Office called for the greater evaluation and scrutiny of data extraction and examination processes deployed by investigating authorities with regard to smartphone data. In doing so, a light was shone on the wider issues and balancing act of maintaining data privacy whilst still allowing for an effective investigation to be conducted by law enforcement. This article proposes a set of ten Privacy-Preserving Data Processing Principles (PPDPP) for consideration by those conducting the digital forensic extraction and examination of data from a digital device. These principles define conduct that is indicative of privacy-preserving, where it is encouraged that those undertaking device investigations demonstrate evidence of adherence to the spirit of them. © 2022","Digital forensic; Investigation; Law enforcement; Privacy","Computer crime; Computer forensics; Data mining; Digital devices; E-learning; Electronic crime countermeasures; Law enforcement; Privacy-preserving techniques; Criminal investigation; Data extraction; Examination process; Extraction process; Forensic examinations; Investigation; Privacy; Privacy invasions; Privacy preserving; Sources of informations; Extraction",Article,Scopus,2-s2.0-85123870013
"Yu Y., Zong N., Wen A., Liu S., Stone D.J., Knaack D., Chamberlain A.M., Pfaff E., Gabriel D., Chute C.G., Shah N., Jiang G.","57211907673;55205153900;57203248254;57196071898;57211270553;57435554000;57409785200;55922283700;36975473900;7006581202;35249316000;7401706799;","Developing an ETL tool for converting the PCORnet CDM into the OMOP CDM to facilitate the COVID-19 data integration",2022,"Journal of Biomedical Informatics","127",,"104002","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123846607&doi=10.1016%2fj.jbi.2022.104002&partnerID=40&md5=9b0041f6e213cc1ccfdf8b2bdbf149bd","Objective: The large-scale collection of observational data and digital technologies could help curb the COVID-19 pandemic. However, the coexistence of multiple Common Data Models (CDMs) and the lack of data extract, transform, and load (ETL) tool between different CDMs causes potential interoperability issue between different data systems. The objective of this study is to design, develop, and evaluate an ETL tool that transforms the PCORnet CDM format data into the OMOP CDM. Methods: We developed an open-source ETL tool to facilitate the data conversion from the PCORnet CDM and the OMOP CDM. The ETL tool was evaluated using a dataset with 1000 patients randomly selected from the PCORnet CDM at Mayo Clinic. Information loss, data mapping accuracy, and gap analysis approaches were conducted to assess the performance of the ETL tool. We designed an experiment to conduct a real-world COVID-19 surveillance task to assess the feasibility of the ETL tool. We also assessed the capacity of the ETL tool for the COVID-19 data surveillance using data collection criteria of the MN EHR Consortium COVID-19 project. Results: After the ETL process, all the records of 1000 patients from 18 PCORnet CDM tables were successfully transformed into 12 OMOP CDM tables. The information loss for all the concept mapping was less than 0.61%. The string mapping process for the unit concepts lost 2.84% records. Almost all the fields in the manual mapping process achieved 0% information loss, except the specialty concept mapping. Moreover, the mapping accuracy for all the fields were 100%. The COVID-19 surveillance task collected almost the same set of cases (99.3% overlaps) from the original PCORnet CDM and target OMOP CDM separately. Finally, all the data elements for MN EHR Consortium COVID-19 project could be captured from both the PCORnet CDM and the OMOP CDM. Conclusion: We demonstrated that our ETL tool could satisfy the data conversion requirements between the PCORnet CDM and the OMOP CDM. The outcome of the work would facilitate the data retrieval, communication, sharing, and analysis between different institutions for not only COVID-19 related project, but also other real-world evidence-based observational studies. © 2022 Elsevier Inc.",,"Data acquisition; Data mining; Interoperability; Mapping; Monitoring; Common data model; Concepts mappings; Data conversion; Information loss; Large-scales; Mapping accuracy; Mapping process; Observational data; Real-world; Surveillance task; Data integration; accuracy; adolescent; adult; aged; Article; body height; body mass; body weight; child; controlled study; coronavirus disease 2019; data integration; demography; diastolic blood pressure; disease surveillance; electronic health record; feasibility study; female; human; infant; major clinical study; male; middle aged; national health organization; newborn; preschool child; school child; standardization; systolic blood pressure; task performance; young adult",Article,Scopus,2-s2.0-85123846607
"De Roock E., Martin N.","57435141900;56559224800;","Process mining in healthcare – An updated perspective on the state of the art",2022,"Journal of Biomedical Informatics","127",,"103995","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123833655&doi=10.1016%2fj.jbi.2022.103995&partnerID=40&md5=7606c38380537fe10fa1da68d6e50218","Process mining is the research domain focusing on the development of innovative methods to gather insights from event logs. It has been used for various use cases within the healthcare domain with the ambition to instigate evidence-based process improvement. Over the past years, the research interest in process mining in healthcare has been increasing. This paper presents the results of an extensive systematic literature review on process mining in healthcare in which 263 papers have been reviewed. Besides providing the most recent overview of literature and the extensive number of reviewed papers, we complement existing reviews by considering three novel review dimensions: (i) the process mining project stages, (ii) the involvement of domain expertise, and (iii) the Key Performance Indicators (KPI) considered during the process mining analysis. Orthogonal to these three novel dimensions, we also highlight the evolution of the research domain by considering time trends within the review dimensions. The review generates new perspectives on process mining in healthcare as a research domain. For instance, process redesign is rarely part of a process mining project, domain experts are mostly asked for validating insights, and less than half of the published papers considers one or more specific KPIs to direct their analysis. © 2022","Event logs; Healthcare; Literature review; Process mining","Benchmarking; Health care; Event logs; Evidence-based; Healthcare domains; Innovative method; Literature reviews; Mining projects; Process Improvement; Process mining; Research domains; State of the art; Data mining; article; human; key performance indicator; mining; systematic review",Article,Scopus,2-s2.0-85123833655
"Yang Y., He T., Feng Y., Liu S., Xu B.","57194544399;57433837000;41861285900;35243051900;7404589262;","Mining Python fix patterns via analyzing fine-grained source code changes",2022,"Empirical Software Engineering","27","2","48","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123797036&doi=10.1007%2fs10664-021-10087-1&partnerID=40&md5=fec2f421d4efd4f90b87555151f448db","Many code changes are inherently repetitive, and researchers employ repetitiveness of the code changes to generate bug fix patterns. Automatic Program Repair (APR) can automatically detect and fix bugs, thus helping developers to improve the quality of software products. As a critical component of APR, software bug fix patterns have been revealed by existing studies to be very effective in detecting and fixing bugs in different programming languages (e.g., Java/C++); yet the fix patterns proposed by these studies can not be directly applied to improve Python programs because of syntactic incompatibilities and lack of analysis of dynamic features. In this paper, we proposed a mining approach to identify fix patterns of Python programs by extracting fine-grained bug-fixing code changes. We first collected bug reports from GitHub repository and employed the abstract syntax tree edit distance to cluster similar bug-fixing code changes to generate fix patterns. We then evaluated the effectiveness of these fix patterns by applying them to single-hunk bugs in two benchmarks (BugsInPy and QuixBugs). The results show that 13 out of 101 real bugs can be fixed without human intervention; that is, the generated bug patch is identical or semantically equivalent with developer’s patches. Also, we evaluated the fix patterns in the wild. For each complex bug, 15% of the bug code could be fixed, and 37% of the bug code could be matched by fix patterns. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Bug fix changes; Fix pattern; Pattern mining; Program repair","C++ (programming language); Codes (symbols); Computer software; Data mining; Program debugging; Repair; Syntactics; Trees (mathematics); Automatic programs; Bug fix change; Bug fixes; Bug-fixing; Code changes; Fine-grained source code changes; Fix pattern; Pattern mining; Program repair; Quality of softwares; Python",Article,Scopus,2-s2.0-85123797036
"Jia C., Ma J., He M., Su Y., Zhang Y., Yu Q.","57204239500;56945878400;57433006000;57433292800;55924899200;57204704964;","Motion primitives learning of ship-ship interaction patterns in encounter situations",2022,"Ocean Engineering","247",,"110708","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123792938&doi=10.1016%2fj.oceaneng.2022.110708&partnerID=40&md5=dce0e11c06de5abd4219ef6084bdb10d","Understanding the ship encounter situations is a tough task, since the uncertain motions of encountering ships and the long-lasting frequent interactions make the encounter processes dynamic and stochastic. If we can decompose these complex encounter processes into motion primitives, which represent the elementary interaction patterns, an encounter can be more easily understood and identified. In this study, we propose a two-stream Long Short-Term Memory-based autoencoder (LSTM-based AE) approach to extract motion primitives from multi-dimensional encounter data without specific rules and prior knowledge. This approach leverages the sequential representation ability of LSTM to learn the temporal dependencies of each encountering ship motion. Then, we developed a fusion gate named dynamic artificial potential field (DAPF) to fuse the outputs of two LSTMs and generated the high-level representation needed to capture the spatiotemporal relationships of the interactions. After that, through a clustering method, the motion primitives were automatically extracted to describe various interaction patterns. The effectiveness of this approach was validated by naturalistic encounter data. The low reconstruction errors of the LSTM-based AE demonstrated that high-level representations captured the relationships of the interactions in both temporal and spatial dimensions. Case studies demonstrated that the utilization of motion primitives provided a semantically interpretable technique to analyze the interaction patterns and encounter process, which is conducive to situation awareness and decision-making for developing intelligent ships. © 2022 Elsevier Ltd","Encounter situation; Motion primitives; Representation learning; Ship-ship interaction; Two-stream LSTM","Data mining; Decision making; Long short-term memory; Stochastic systems; Auto encoders; Encounter situation; Interaction pattern; Long lasting; Motion primitives; Representation learning; Ship encounters; Ship-ship interaction; Two-stream; Two-stream LSTM; Ships",Article,Scopus,2-s2.0-85123792938
"Basharat Z., Khan K., Jalal K., Ahmad D., Hayat A., Alotaibi G., Al Mouslem A., Aba Alkhayl F.F., Almatroudi A.","56641793600;57353203300;57353318600;57352875000;57353203400;57371627300;57310993300;57224950345;55948213200;","An in silico hierarchal approach for drug candidate mining and validation of natural product inhibitors against pyrimidine biosynthesis enzyme in the antibiotic-resistant Shigella flexneri",2022,"Infection, Genetics and Evolution","98",,"105233","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123783068&doi=10.1016%2fj.meegid.2022.105233&partnerID=40&md5=9b3efb86ea29d8472839de21b17fb94a","Shigella flexneri is the main causative agent of the communicable diarrheal disease, shigellosis. It is estimated that about 80–165 million cases and > 1 million deaths occur every year due to this disease. S. flexneri causes dysentery mostly in young children, elderly and immunocompromised patients, all over the globe. Recently, due to the emergence of S. flexneri antibiotic resistance strains, it is a dire need to predict novel therapeutic drug targets in the bacterium and screen natural products against it, which could eliminate the curse of antibiotic resistance. Therefore, in current study, available antibiotic-resistant genomes (n = 179) of S. flexneri were downloaded from PATRIC database and a pan-genome and resistome analysis was conducted. Around 5059 genes made up the accessory, 2469 genes made up the core, and 1558 genes made up the unique genome fraction, with 44, 34, and 13 antibiotic-resistant genes in each fraction, respectively. Core genome fraction (27% of the pan-genome), which was common to all strains, was used for subtractive genomics and resulted in 384 non-homologous, and 85 druggable targets. Dihydroorotase was chosen for further analysis and docked with natural product libraries (Ayurvedic and Streptomycin compounds), while the control was orotic acid or vitamin B13 (which is a natural binder of this protein). Dynamics simulation of 50 ns was carried out to validate findings for top-scored inhibitors. The current study proposed dihydroorotase as a significant drug target in S. flexneri and 4-tritriacontanone & patupilone compounds as potent drugs against shigellosis. Further experiments are required to ascertain validity of our findings. © 2022 The Authors","Dihydroorotase; Natural products; Pan-genomics; Resistome; Shigella flexneri","4 tritriacontanone; aminoglycoside; antiinfective agent; ayurvedic drug; cephalosporin derivative; dihydroorotase; epothilone B; ketone; natural product; orotic acid; pyrimidine inhibitor; streptomycin; tetracycline; unclassified drug; antibiotic resistance; Article; bacterial gene; bacterial genome; bacterial strain; computer model; controlled study; core genome fraction; data mining; drug candidate mining; drug potency; drug safety; drug screening; genome; genomics; human; human cell; hydrogen bond; hydrophobicity; molecular dynamics; nonhuman; Shigella flexneri; shigellosis; validation study",Article,Scopus,2-s2.0-85123783068
"Dupuis A., Dadouchi C., Agard B.","57433093900;57203683671;57203235538;","Predicting crop rotations using process mining techniques and Markov principals",2022,"Computers and Electronics in Agriculture","194",,"106686","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123778488&doi=10.1016%2fj.compag.2022.106686&partnerID=40&md5=2ae2ee466c167d697839a36de87b82fa","Meeting an increasing demand for food while preserving the environment is one of the most important challenges of the 21st century. To meet this challenge, conservation agriculture can rely on the age-old practice of crop rotation. The objective of this article is to develop a methodology for predicting and visualizing crop rotations, supporting discussions between agronomists and producers. Based on crop history data, the 6-phase methodology, uses Markov chains for the prediction of the N most likely crops grown in year n + 1. Process mining and Directly-Follows Graphs (DFG) enables modelling and visualization of the results. Generalisation and filtering operations highlight the frequent behaviors of producers. Applied to analyse the crop history of 10,376 fields from 409 field crop farms in Quebec, Canada, the methodology is competitive with the performance of various recurrent neural networks (LSTM, RNN, GRU) with a successful prediction rate that exceeds 90%, while allowing for an intelligibility of results and a relative computational simplicity. © 2022 Elsevier B.V.","Agriculture 4.0; Crop rotation; Machine learning; Markov Chains; Process mining","Crops; Data visualization; Forecasting; Long short-term memory; Markov processes; Rotation; Visualization; Agriculture 4.0; Conservation agricultures; Crop rotation; Field crops; Filtering operations; Generalisation; History data; Mining techniques; Most likely; Process mining; Data mining; agronomy; crop plant; crop rotation; detection method; methodology; visualization; Canada; Quebec [Canada]",Article,Scopus,2-s2.0-85123778488
"Özaydın E., Fışkın R., Uğurlu Ö., Wang J.","57433836500;56784332900;55759517600;57218486122;","A hybrid model for marine accident analysis based on Bayesian Network (BN) and Association Rule Mining (ARM)",2022,"Ocean Engineering","247",,"110705","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123775040&doi=10.1016%2fj.oceaneng.2022.110705&partnerID=40&md5=d68dce30089042fc220e98abd82fdc5d","In order to ensure sustainable maritime safety, studies based on unreported maritime accidents in maritime transport are necessary. Such studies allow the causes of accidents that have not come to light, to be identified and addressed. In this study, the data of unreported occupational accidents on Turkish fishing vessels with a full length of 12 m and above was analysed using both Bayesian network (BN) and Association Rule Mining (ARM) methods. A network structure that summarizes the occurrence of occupational accidents on fishing vessels with the BN method was put forward. The network structure makes it possible to analyse the latent factors, active failures and operational conditions that cause the accident qualitatively and quantitatively. The Predictive Apriori algorithm was used to establish rules for the occurrence of occupational accidents on fishing vessels, taking variables such as day condition, length, sea condition, and ship type into account. These rules provide an understanding of how occupational accidents occur on fishing vessels. In other words, these rules define the minimum requirements for the occurrence of accidents on fishing boats. The developed hybrid model can be used for analysing unreported occupational accidents on fishing vessels. © 2022 Elsevier Ltd","Accident analysis; Association rule mining; Bayesian network; Fishing vessel; Marine accident","Association rules; Bayesian networks; Data mining; Fisheries; Fishing vessels; Occupational risks; Accident analysis; Bayesia n networks; Condition; Hybrid model; Marine accidents; Maritime safety; Network structures; Occupational accident; Rule mining; Safety studies; Accidents; accident; algorithm; fishing vessel; qualitative analysis; quantitative analysis; shipping; sustainability; transportation safety; working conditions; Turkey",Article,Scopus,2-s2.0-85123775040
"Ma S.-C., Xu J.-H., Fan Y.","57197715290;54407563100;7403491920;","Characteristics and key trends of global electric vehicle technology development: A multi-method patent analysis",2022,"Journal of Cleaner Production","338",,"130502","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123680344&doi=10.1016%2fj.jclepro.2022.130502&partnerID=40&md5=9468bcc468c932abd94a5534329edf22","To grasp the key characteristics and trends of the rapid development of electric vehicle (EV) technology and to study the development trajectory and main holders of EV technology, this study analyzes the patents related to EVs from 1970 to 2016. In this study, three types of information technologies—text mining, clustering, and social network analysis—are adopted to collate and analyze patent data. We find that solving the problem of how to safely and quickly charge a battery through a charging facility and distribute the energy to each storage unit is a highly concerning topic in the field of EV technology, involving the technology of batteries, charging facilities, and power control systems. The research frontier of EV technology is wireless charging technology. Finally, the major holders of EV technology in terms of patent holdings and cooperation are analyzed. © 2022","Development trajectory; Electric vehicle; Patent; Social network; Text mining","Charging (batteries); Data mining; Digital storage; Electric vehicles; Power control; Secondary batteries; Social sciences computing; Charging facilities; Development trajectory; Key characteristics; Key trends; Multi methods; Patent analysis; Social network; Technology development; Text-mining; Vehicle technology; Patents and inventions",Article,Scopus,2-s2.0-85123680344
"Geronikolou S.A., Il Takan I., Pavlopoulou A., Mantzourani M., Chrousos G.P.","56431220200;57442542400;21739721300;6602273535;36051235900;","Thrombocytopenia in COVID.19 and vaccine.induced thrombotic thrombocytopenia",2022,"International Journal of Molecular Medicine","49","3","35","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123669286&doi=10.3892%2fijmm.2022.5090&partnerID=40&md5=93fa88ff506fb7be02a241683f8118ec","The highly heterogeneous symptomatology and unpredictable progress of COVID.19 triggered unprecedented intensive biomedical research and a number of clinical research projects. Although the pathophysiology of the disease is being progressively clarified, its complexity remains vast. Moreover, some extremely infrequent cases of thrombotic thrombocytopenia following vaccination against SARS.CoV.2 infection have been observed. The present study aimed to map the signaling pathways of thrombocytopenia implicated in COVID.19, as well as in vaccine.induced thrombotic thrombocytopenia (VITT). The biomedical literature database, MEDLINE/PubMed, was thoroughly searched using artificial intelligence techniques for the semantic relations among the top 50 similar words (>0.9) implicated in COVID.19.mediated human infection or VITT. Additionally, STRING, a database of primary and predicted associations among genes and proteins (collected from diverse resources, such as documented pathway knowledge, high.throughput experimental studies, cross.species extrapolated information, automated text mining results, computationally predicted interactions, etc.), was employed, with the confidence threshold set at 0.7. In addition, two interactomes were constructed: i) A network including 119 and 56 nodes relevant to COVID.19 and thrombocytopenia, respectively; and ii) a second network containing 60 nodes relevant to VITT. Although thrombocytopenia is a dominant morbidity in both entities, three nodes were observed that corresponded to genes (AURKA, CD 46 and CD19) expressed only in VITT, whilst ADAM10, CDC20, SHC1 and STXBP2 are silenced in VITT, but are commonly expressed in both COVID.19 and thrombocytopenia. The calculated average node degree was immense (11.9 in COVID.19 and 6.43 in VITT), illustrating the complexity of COVID.19 and VITT pathologies and confirming the importance of cytokines, as well as of pathways activated following hypoxic events. In addition, PYCARD, NLP3 and P2RX7 are key potential therapeutic targets for all three morbid entities, meriting further research. This interactome was based on wild.type genes, revealing the predisposition of the body to hypoxia.induced thrombosis, leading to the acute COVID.19 phenotype, the elong.COVID syndromef, and/or VITT. Thus, common nodes appear to be key players in illness prevention, progression and treatment. © 2022 Spandidos Publications. All rights reserved.","Artificial intelligence; Autoimmunity; COVID.19; Cytokine storm; Enzymes; HLA system; Interactions network; Lymphocytes; Natural language processing; SARS.CoV.2; Thrombocytopenia; Vaccine.induced thrombotic thrombocytopenia","ADAM10 endopeptidase; aurora A kinase; CD19 antigen; cell cycle protein 20; inflammasome; membrane cofactor protein; protein NLP3; purinergic P2X7 receptor; SARS-CoV-2 vaccine; SHC transforming protein 1; syntaxin; syntaxin binding protein 2; unclassified drug; Article; coronavirus disease 2019; data mining; disease control; disease course; disease predisposition; drug safety; drug targeting; employment status; high throughput technology; human; long COVID; pathology; pathophysiology; prediction; protein expression; publication; signal transduction; thrombocytopenia; thrombosis; thrombotic thrombocytopenic purpura",Article,Scopus,2-s2.0-85123669286
"Nishi K., Fujibuchi T., Yoshinaga T.","57220177922;26428166700;57220176455;","Development and evaluation of the effectiveness of educational material for radiological protection that uses augmented reality and virtual reality to visualise the behaviour of scattered radiation",2022,"Journal of Radiological Protection","42","1","011506","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123651948&doi=10.1088%2f1361-6498%2fac3e0a&partnerID=40&md5=7ed2067fd67aa66b030cb1aad78fab19","Understanding the behaviour of scattered radiation is important for learning appropriate radiation protection methods, but many existing visualisation systems for radiation require special devices, making it difficult to use them in education. The purpose of this study was to develop teaching material for radiation protection that can help visualise the scattered radiation with augmented reality (AR) and virtual reality (VR) on a web browser, develop a method for using it in education and examine its effectiveness. The distribution of radiation during radiography was calculated using Monte Carlo simulation, and teaching material was created. The material was used in a class for department of radiological technology students and its influence on motivation was evaluated using a questionnaire based on the evaluation model for teaching materials. In addition, text mining was used to evaluate impressions objectively. Educational material was developed that can be used in AR and VR for studying the behaviour of scattered radiation. The results of the questionnaire showed that the average value of each item was more than four on a five-point scale, indicating that the teaching material attracted the interest of users. Through text mining, it could be concluded that there was improved understanding of, and confidence in, radiation protection. © 2022 Society for Radiological Protection. Published on behalf of SRP by IOP Publishing Limited. All rights reserved.","augmented reality; education in radiation protection; Monte-Carlo simulation; scattered radiation; virtual reality","Augmented reality; Data mining; E-learning; Education computing; Monte Carlo methods; Radiation; Radiation protection; Surveys; Virtual reality; Education in radiation protection; Educational materials; It in educations; Protection methods; Radiological protection; Scattered radiations; Special devices; Teaching materials; Text-mining; Visualization system; Intelligent systems; article; augmented reality; comparative effectiveness; education; human; human experiment; mining; Monte Carlo method; motivation; questionnaire; radiation protection; radiography; radiology; teaching; virtual reality; web browser; computer simulation; Augmented Reality; Computer Simulation; Humans; Radiation Protection; Teaching Materials; Virtual Reality",Article,Scopus,2-s2.0-85123651948
"Dąbrowski J., Letier E., Perini A., Susi A.","57196376910;57204376747;7003847681;9038664800;","Analysing app reviews for software engineering: a systematic literature review",2022,"Empirical Software Engineering","27","2","43","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123235870&doi=10.1007%2fs10664-021-10065-7&partnerID=40&md5=04640bb9448d79abbdb058c8439a8a1d","App reviews found in app stores can provide critically valuable information to help software engineers understand user requirements and to design, debug, and evolve software products. Over the last ten years, a vast amount of research has been produced to study what useful information might be found in app reviews, and how to mine and organise such information as efficiently as possible. This paper presents a comprehensive survey of this research, covering 182 papers published between 2012 and 2020. This survey classifies app review analysis not only in terms of mined information and applied data mining techniques but also, and most importantly, in terms of supported software engineering activities. The survey also reports on the quality and results of empirical evaluation of existing techniques and identifies important avenues for further research. This survey can be of interest to researchers and commercial organisations developing app review analysis techniques and to software engineers considering to use app review analysis. © 2021, The Author(s).","App store analysis; Mining app reviews; Mining software repository; Software engineering; Systematic literature review; User feedback","Data mining; Product design; Program debugging; Quality control; Surveys; App store analyse; App stores; Design debug; Mining app review; Mining software; Mining software repository; Software repositories; Systematic literature review; User feedback; User requirements; Application programs",Article,Scopus,2-s2.0-85123235870
"Funkner A.A., Yakovlev A.N., Kovalchuk S.V.","57192820292;57419249600;55382199400;","Surrogate-assisted performance prediction for data-driven knowledge discovery algorithms: Application to evolutionary modeling of clinical pathways",2022,"Journal of Computational Science","59",,"101562","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123215479&doi=10.1016%2fj.jocs.2022.101562&partnerID=40&md5=e49ddc3e445070929caff790da3387c9","The paper proposes and investigates an approach for surrogate-assisted performance prediction of data-driven knowledge discovery algorithms. The approach is based on the identification of surrogate models for prediction of the target algorithm's quality and performance. The proposed approach was implemented and investigated as applied to an evolutionary algorithm for discovering clusters of interpretable clinical pathways in electronic health records of patients with acute coronary syndrome. Several clustering metrics and execution time were used as the target quality and performance metrics respectively. An analytical software prototype based on the proposed approach for the prediction of algorithm characteristics and feature analysis was developed to provide a more interpretable prediction of the target algorithm's performance and quality that can be further used for parameter tuning. © 2022 Elsevier B.V.","Clinical pathway; Evolutionary algorithms; Knowledge discovery; Multi-objective optimization; Parameter tuning; Predictive modeling; Surrogate modeling","Clustering algorithms; Data mining; Forecasting; Multiobjective optimization; Parameter estimation; Software prototyping; Clinical pathways; Data driven; Discovery algorithm; Evolutionary models; Multi-objectives optimization; Parameters tuning; Performance; Performance prediction; Predictive models; Surrogate modeling; Evolutionary algorithms",Article,Scopus,2-s2.0-85123215479
"Ashraf M., Abdelkader T., Rady S., Gharib T.F.","57206234300;25928995600;34870497700;6602973637;","TKN: An efficient approach for discovering top-k high utility itemsets with positive or negative profits",2022,"Information Sciences","587",,,"654","678",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123209596&doi=10.1016%2fj.ins.2021.12.024&partnerID=40&md5=fc37ef669bd6f5dabbcf8b6cac07d7bd","Top-k high utility itemsets (HUIs) mining permits discovering the required number of patterns - k, without having an optimal minimum utility threshold (i.e., minimum profit). Multiple top-k HUIs mining algorithms have been introduced with interesting results. However, these algorithms focus mainly on mining patterns from positive profit datasets, while few preliminary studies can handle datasets with negative profits. Moreover, conventional top-k HUI mining algorithms, that are meant for exploring positive profit datasets, perform poorly when mining top-k HUIs on highly dense and large datasets. In this paper, we propose TKN (efficiently mining Top-K HUIs with positive or Negative profits) which employs generalized and adaptive techniques to mine both positive and negative profit datasets effectively. The proposed approach adopts transactional projection and merging mechanisms to decrease the dataset traversing cost. Furthermore, several pruning and threshold elevating ideas are utilized to significantly narrow the exploration space. To highlight the reliability of the devised TKN, a series of extensive comparisons were conducted using two versions of six real datasets. The obtained results reveal that TKN is clearly superior in finding the required number of patterns, whether on positive or negative profit datasets, compared to the current cutting-edge competitors. © 2021 Elsevier Inc.","Data mining; Negative unit profits; Pattern growth approach; Pattern mining; Threshold raising strategies; Top-k high utility itemsets","Data mining; Large dataset; High utility itemset minings; High utility itemsets; Highly dense; Mining algorithms; Negative unit profit; Pattern growth; Pattern growth approach; Pattern mining; Threshold raising strategy; Top-k high utility itemset; Profitability",Article,Scopus,2-s2.0-85123209596
"Bai J., Hager W.W., Zhang H.","56337105900;7202227743;57196154282;","An inexact accelerated stochastic ADMM for separable convex optimization",2022,"Computational Optimization and Applications","81","2",,"479","518",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122854000&doi=10.1007%2fs10589-021-00338-8&partnerID=40&md5=6ad30beec702185134539ec9b218a9a4","An inexact accelerated stochastic Alternating Direction Method of Multipliers (AS-ADMM) scheme is developed for solving structured separable convex optimization problems with linear constraints. The objective function is the sum of a possibly nonsmooth convex function and a smooth function which is an average of many component convex functions. Problems having this structure often arise in machine learning and data mining applications. AS-ADMM combines the ideas of both ADMM and the stochastic gradient methods using variance reduction techniques. One of the ADMM subproblems employs a linearization technique while a similar linearization could be introduced for the other subproblem. For a specified choice of the algorithm parameters, it is shown that the objective error and the constraint violation are O(1 / k) relative to the number of outer iterations k. Under a strong convexity assumption, the expected iterate error converges to zero linearly. A linearized variant of AS-ADMM and incremental sampling strategies are also discussed. Numerical experiments with both stochastic and deterministic ADMM algorithms show that AS-ADMM can be particularly effective for structured optimization arising in big data applications. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Accelerated gradient method; Accelerated stochastic ADMM; AS-ADMM; Big data; Complexity; Convex optimization; Inexact stochastic ADMM; Separable structure","Big data; Convex optimization; Data mining; Functions; Gradient methods; Linearization; Accelerated gradient method; Accelerated stochastic ADMM; Accelerated stochastic alternating direction method of multiplier; Alternating directions method of multipliers; Complexity; Convex optimisation; Gradient's methods; Inexact stochastic ADMM; Separable structure; Stochastics; Stochastic systems",Article,Scopus,2-s2.0-85122854000
"Ma Y., Xie Z., Li G., Ma K., Huang Z., Qiu Q., Liu H.","57411465900;57412408100;57411597000;57385490300;57411466000;57203591589;57412277000;","Text visualization for geological hazard documents via text mining and natural language processing",2022,"Earth Science Informatics","15","1",,"439","454",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122831697&doi=10.1007%2fs12145-021-00732-0&partnerID=40&md5=f0d94475bef57eb12dfac45c31c19919","An increasing number of geological hazard documents about the mechanism and occurrence process of geological disasters contain unstructured geoscientific data that are not fully utilized. Text mining and visualization techniques offer opportunities to leverage this wealth of data and extract valuable information from dense, abstract geological disaster reports to quickly focus on the core information in geological reports and improve the efficiency of report usage. In this research, a flow framework for the automatic extraction of key information and its transformation to a simple and intuitive form for managers/researchers to quickly navigate, understand and make more informed decisions based on the key information are described. To automatically extract key information from text, an optimized term frequency-inverse document frequency algorithm is proposed to analyze text characteristics. The important information extracted from a case study document is demonstrated using a word cloud. Co-occurrence network analysis is used to present key content from geological reports and describe the correlations between words. We use the dependency grammar technique to extract triads of geological report text information and we visualize them using knowledge graphs. The results show that text visualization analysis can be used to identify the types and locations of geological disasters in reports, highlight key information from survey reports as an auxiliary public resource, and more rapidly analyze the key contents of a large number of geological disaster survey reports. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Geological disaster report; Natural language processing; Text mining; Text visualization analysis","data mining; geological hazard; knowledge; network analysis; visualization",Article,Scopus,2-s2.0-85122831697
"Huang H., Long R., Chen H., Sun K., Li Q.","57412392000;8396729800;57050551500;57411580600;57193156463;","Exploring public attention about green consumption on Sina Weibo: Using text mining and deep learning",2022,"Sustainable Production and Consumption","30",,,"674","685",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122804066&doi=10.1016%2fj.spc.2021.12.017&partnerID=40&md5=ebfdac03a6ae887b6b9083850c4169b3","Achieving the goal of carbon neutrality and carbon peak as scheduled puts forward new demands for the green transition of low-carbon lifestyle in Chinese society. In-depth practice of green consumption (GC) behavior can effectively promote the supply-side and consumption-side emission reduction work, but the phenomenon of “high awareness, low practice” is widespread in GC. The causes of consumers' low practice of GC need to be analyzed from the perspective of time and space from the actual media data. Furthermore, this process assists policymakers and stakeholders to understand the general attitude of the public towards GC, clarifying the propagation path of public emotions and the source of negative emotions. Based on the data from Sina Weibo, this paper applied text mining, a hybrid model of convolutional neural network and long and short-term memory neural network to analyze the public's attention, sentiment tendency and hot topics on GC. The results show that the vast majority of the Chinese public has a positive attitude toward GC; women and economically developed regions are more concerned about GC; the drivers of positive public sentiment toward GC include environmental awareness education, air pollution prevention and control, and online shopping; high green product prices, excessive time costs, chaotic sharing economy and one-size-fits-all solutions lead to negative public sentiment toward GC. By providing public sentiment analysis of GC, this research would assist decision-makers to understand the dissemination mechanism of public will in social media and clarify targeted solutions, which is of great significance for policy formulation and improvement. © 2021","CNN-LSTM; Green consumption; Public attitudes; Social media; Topic analysis","Air pollution; Backpropagation; Carbon; Convolutional neural networks; Costs; Data mining; Decision making; Economics; Emission control; Long short-term memory; Social networking (online); Carbon carbons; Carbon neutralities; CNN-LSTM; Green consumption; Public attitudes; Public sentiments; Sina-weibo; Social media; Text-mining; Topic analysis; Sentiment analysis",Article,Scopus,2-s2.0-85122804066
"Chen D., Wang P., Pan R., Zha C., Fan J., Li S., Cheng K.","12780050200;57203738828;37097919700;57194519697;57408695100;57218879994;57408528300;","Process optimization of selective laser melting 316L stainless steel by a data-driven nonlinear system",2022,"Welding in the World","66","3",,"409","422",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122664713&doi=10.1007%2fs40194-021-01205-0&partnerID=40&md5=0c815b6899d27a2df53cc9d5a2f38031","Selective laser melting (SLM) provides a great degree of design freedom, but because the forming process is accompanied by multi-scale and multi-physics phenomena, the computational cost of modelling the forming process remains quite high. The cost of testing and simulation can be reduced by creating a model based on a nonlinear system, fitting the nonlinear function between input and output, and choosing the best option for forming quality. In this study, a nonlinear system was created by integrating data mining and statistical inference technologies, and a mix of simulation and experiment was utilized to create a specimen with the desired properties. Firstly, the density of 316L stainless steel specimens formed by SLM with various layer thicknesses was obtained, the important parameters impacting density were examined, and the effect of layer thickness on specimen density was reported. Furthermore, the nonlinear neural network optimization system was built through training and testing in order for the trained network to forecast the nonlinear function’s output. Finally, the system is utilized to forecast density in high-efficiency moulding mode, and the test results match the anticipated data well. © 2021, International Institute of Welding.","Data-drive; Neural network; Nonlinear system; Relative density; Selected laser melting","Austenitic stainless steel; Data mining; Digital storage; Functions; Melting; Neural networks; Optimization; Selective laser melting; 316 L stainless steel; Data driven; Data drives; Laser melting; Layer thickness; Neural-networks; Nonlinear functions; Relative density; Selected laser melting; Selective laser melting; Nonlinear systems",Article,Scopus,2-s2.0-85122664713
"Liao S.-H., Widowati R., Puttong P.","7401923068;57223238647;57188964855;","Data mining analytics investigate Facebook Live stream users’ behaviors and business models: The evidence from Thailand",2022,"Entertainment Computing","41",,"100478","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122650708&doi=10.1016%2fj.entcom.2022.100478&partnerID=40&md5=7b3cfcb69ad7786f36c1355409849b52","Live streaming refers to the technology of leisure, entertainment or communication that broadcasts instant images/video on the Internet with the rise of online audio and video platforms in the modern society. Due to the growth in the popularity and market potential of live streaming, some studies have emphasized how streaming transfers from its audio and video broadcasting role of social media/social network leisure and entertainment usage to social commerce and business development. Thus, this research examines the behaviors of various users of FB Live in Thailand through an empirical survey research. There are 3081 valid questionnaire data. This study develops data mining analytic methods, including association rules and K-means clustering analysis, based on a relational data database development. This study shows how the FB Live study and social media/network analytics results can obtain streamers/users knowledge patterns and rules investigating different users’ profiles and behaviors to develop social commerce and business models in Thailand. © 2022 Elsevier B.V.","Business model; Data analytics; Facebook Live; Live streaming; Social media/social network","Behavioral research; Commerce; Data mining; K-means clustering; Leisure; Media streaming; Surveys; User profile; Business models; Data analytics; Facebook; Facebook live; Live streaming; Social business; Social commerces; Social media; Social medium/social network; Thailand; Social networking (online)",Article,Scopus,2-s2.0-85122650708
"Dong Y., Zhao J., Floricioiu D., Krieger L.","57188588591;57214842225;6507664989;57196035339;","Automatic calving front extraction from digital elevation model-derived data",2022,"Remote Sensing of Environment","270",,"112854","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122627133&doi=10.1016%2fj.rse.2021.112854&partnerID=40&md5=e10a7b19f4a46d76c075ade5b34da7d0","Calving Front (CF) is an important parameter to analyse ice sheet dynamics and to measure glacier mass balance. DEM products covering polar regions are critical remote sensing data sources to provide fundamental terrain information for glaciological studies. It is necessary to provide accurate CFs for DEM specific applications, such as mass balance calculation or the substitution of water values with geoid values in the DEM editing process. However, much less attention is paid to automatically delineate CFs from DEM data. In this study, we propose a DEM-based Automatic CF Extraction method (DACE) to efficiently extract the CFs from DEM data. In DACE, a height-sensitive terrain feature is designed to enhance the contrast between the ice sheet and the ocean by combining elevation and roughness information. To improve the CF extraction performance, a two-category image classification based on game theory is proposed that considers the spatial consistency of the feature image created. For validation, DACE was applied to DEM products generated from the single-pass SAR interferometry mission TanDEM-X (TDM) at different posting sizes (12 and 90 m) and to the optical photogrammetry-based Reference Elevation Model of Antarctica (REMA) with a 2 m posting. The proposed algorithm can achieve a CF extraction accuracy of better than 14, 20, and 70 m for the 2-m REMA, 12- and 90-m TDM DEMs, respectively, when compared with the manually delineated CFs. The experimental results demonstrate that the proposed algorithm can effectively extract the CFs from DEM data. DACE can be used to replace water values and edit the DEMs themselves. The CFs extracted from the DEM data can also be used for glacier mass balance calculation with the DEM-based geodetic method and for the temporal analysis of CF changes when multi-temporal DEM data are available. © 2021 Elsevier Inc.","Calving Front; Digital Elevation Model (DEM); Height-sensitive terrain feature; Image classification; REMA; Remote sensing; TanDEM-X DEM","Data mining; Digital instruments; Extraction; Game theory; Geomorphology; Glaciers; Image enhancement; Landforms; Remote sensing; Surveying; Antarctica; Calfing front; Digital elevation model; Elevation models; Height-sensitive terrain feature; Images classification; Reference elevation model of antarctica; Remote-sensing; TanDEM-X; TanDEM-X digital elevation model; Terrain features; Image classification; algorithm; digital elevation model; glacier mass balance; ice sheet; image classification; interferometry; remote sensing; synthetic aperture radar; TanDEM-X; temporal analysis; Antarctica",Article,Scopus,2-s2.0-85122627133
"Shen Y., Song Z., Kusiak A.","57222014024;35321474100;26643635700;","Enhancing the generalizability of predictive models with synergy of data and physics",2022,"Measurement Science and Technology","33","3","034002","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122626087&doi=10.1088%2f1361-6501%2fac3944&partnerID=40&md5=638fd6fc809182a942c0d6ebf3745e0d","Wind farms require predictive models for predictive maintenance. There is a need to predict values of non-observable parameters beyond ranges reflected in available data. A predictive model developed for one machine many not perform well for another similar machine. This is usually due to a lack of generalizability of data-driven models. To increase the generalizability of predictive models, this research integrates data mining with first-principle knowledge. Physics-based principles are combined with machine learning algorithms through feature engineering, strong rules and divide-and-conquer. The proposed synergy concept is illustrated with a wind turbine blade icing prediction and achieves significant predictive accuracy across different turbines. The proposed process should be widely accepted by wind energy predictive maintenance practitioners because of its simplicity and efficiency. Furthermore, the testing scores of KNN, CART and DNN algorithms are increased by 44.78%, 32.72% and 9.13%, respectively, with our proposed process. We demonstrate the importance of embedding physical principles within the machine learning process, and also highlight an important point that the need for more complex machine learning algorithms in industrial big data mining is often much less than it is in other applications, making it essential to incorporate physics and follow the 'less is more' philosophy. © 2021 IOP Publishing Ltd.","industrial big data; machine learning; physical principle; predictive model; process engineering; wind turbine blade icing","Big data; Data mining; Learning algorithms; Machine learning; Turbine components; Wind power; Wind turbines; Industrial big data; Machine learning algorithms; Non-observable; Observable parameters; Physical principles; Predictive maintenance; Predictive models; Wind farm; Wind turbine blade icing; Wind turbine blades; Turbomachine blades",Article,Scopus,2-s2.0-85122626087
"Adavoudi Jolfaei A., Mala H., Zarezadeh M.","57200001383;35185355900;57204722221;","EO-PSI-CA: Efficient outsourced private set intersection cardinality",2022,"Journal of Information Security and Applications","65",,"102996","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122613948&doi=10.1016%2fj.jisa.2021.102996&partnerID=40&md5=a227b378410d974611bc97ad9f50a892","Private set intersection cardinality (PSI-CA) is a useful cryptographic primitive for many data analysis techniques, e.g. in genomic computations and data mining. In the last few years, several classical multi-party PSI-CA protocols have been designed where parties jointly compute the PSI-CA and at the end of the protocol none of them learns more than their private input sets and the output. The computation complexity of these multi-party protocols is quadratic in the size of the input sets and linear in the number of the parties involved in the protocol. In addition, the communication complexity scales quadratically as the number of parties increases. With the advent of cloud computing, it is now necessary to gain the benefits of the computation and storage capabilities of the cloud for outsourcing private input sets and PSI-CA computation. For the first time, in this paper, we design an efficient outsourced private set intersection cardinality named EO-PSI-CA in the multi-party setting. This protocol computes PSI-CA by employing the Bloom filter (BF) technique and the exponential ElGamal cryptosystem over encrypted Bloom filters. In our protocol, two or more parties outsource their private input sets to the cloud and finally one of the parties requests the EO-PSI-CA value. Due to the use of Bloom filter, the size of the parties’ sets is independent of each other, and the computational and communication complexity of each party is independent of the total number of parties. We formally prove the security of our protocol in the semi-honest adversarial model and we claim that our scheme addresses the intersection size hiding. On a more positive note, our EO-PSI-CA is the first in its kind with linear complexity supporting outsourcing in a multi-party setting. © 2021","Additive homomorphic encryption; Bloom filter; Outsourcing; Private set intersection cardinality; Secure computation; Size hiding","Computational complexity; Data mining; Data structures; Digital storage; Filtration; Outsourcing; Additive homomorphic encryption; Bloom filters; Cardinalities; Ho-momorphic encryptions; Homomorphic-encryptions; Input set; Private set intersection cardinality; Secure computation; Set intersection; Size hiding; Cryptography",Article,Scopus,2-s2.0-85122613948
"Lai X., Zhang S., Mao N., Liu J., Chen Q.","49861520200;57211147871;57404893800;55850202300;57404893900;","Kansei engineering for new energy vehicle exterior design: An internet big data mining approach",2022,"Computers and Industrial Engineering","165",,"107913","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122539330&doi=10.1016%2fj.cie.2021.107913&partnerID=40&md5=5eb1cdde636d5540f517cd582e14e9c5","New energy vehicles (NEVs) such as electronic cars represent a major trend in the automobile industry, where most their exterior designs still follow those of convention fuelled vehicles (FVs). It is important to investigate whether NEV users have unique requirements that differ from those of traditional users. Kansei engineering is a practical tool for perceptual demand analysis. However, the conventional method requires questionnaires or surveys to perform limited data collection. In this study, we utilised massive internet data to collect user Kansei requirements for NEV exterior design. The Scrapy crawler was adopted for data collection and a bidirectional long short-term memory, conditional random field, and multilayer perceptron framework was developed for text mining. To quantify design features and Kansei image scores, a hybrid Apriori + structural equation model (SEM) system is proposed, where the data-driven Apriori algorithm can explore the hidden relationships in big user generated comments, while the SEM model captures the users’ behaviour and decision procedure so that to provide interpretable results. In addition, the association rules mined from user comments by Apriori can facilitate the specification of a complicated SEM model, substantially reducing the modelling and calibration effort. Goodness-of-fit results suggest that the proposed model outperforms conventional models. A case study on 1805 automobiles, 287 brands, and 369105 comments was conducted and the results suggest that some design features that would increase the Kansei image scores for conventional FVs may have the opposite effect on NEVs. Discussions on engineering and managerial insights are presented and the discovered rules and relationships are employed to develop a design-aided system. © 2021 Elsevier Ltd","Automobile exterior; Deep learning; Kansei engineering; Structural equation model; Text mining","Automobiles; Automotive industry; Big data; Data mining; Deep learning; Surveys; Apriori; Automobile exterior; Data collection; Deep learning; Design features; Exterior designs; Kansei Engineering; Kansei images; New energy vehicles; Structural equation models; Data acquisition",Article,Scopus,2-s2.0-85122539330
"Monti K., Bachi K., Gray M., Mahajan V., Sweeney G., Oprescu A.M., Munjal K.G., Hurd Y.L., Lim S.","56958326100;55201184800;57222477913;57403424900;57402910000;57195542673;6505884249;7005112959;57218618834;","Data mining-based clinical profiles of substance use-related emergency department utilizers",2022,"American Journal of Emergency Medicine","53",,,"104","111",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122481084&doi=10.1016%2fj.ajem.2021.12.059&partnerID=40&md5=40768f3c0aca01e721fac2abd0ed845e","Objective: Substance-use is a prevalent presentation to the emergency department (ED); however, the clinical characterization of patients who are treated and discharged without admission for further treatment is under-investigated. The study aims to define and characterize the clinical profiles of this patient population. Methods: Patients' presentations were examined by clinical data mining (chart review) of ED records of substance use-related events of individuals discharged without admission for further treatment. Records (N = 199) from three major hospitals in New York City from March and June 2017 were randomly sampled with primary diagnosis of alcohol, opioid-related and other psychoactive substance-use presentations. Qualitative thematic coding of clinical presentation with inter-rater reliability was performed. Quantitative distinctive validity tested independence through Pearson's chi-squared and analysis of variance using Fisher's F-test. Results: Six distinct clinical profiles were identified, including, High Utilizers (chronically intoxicated with comorbid health conditions) (36.7%), Single Episode (20.1%), Service Request (14.1%), Altered Mental Status (13.6%), Overdose (9.0%), and Withdrawal (7.5%). The profiles differed (p < 0.05) in age, housing status, payor, mode of arrival, referral source, index visit time, prescribed treatment, triage acuity level, psychiatric history, and medical history. Differences (p < 0.05) between groups across clinical profiles in age and pain level at triage were observed. Conclusions: The identified clinical profiles represent the broad spectrum and complex nature of substance use-related ED utilization, highlighting critical factors of psychosocial and mental-health comorbidities. These findings provide a preliminary foundation to support person-centered interventions to decrease substance use-related ED utilization and to increase engagement/linkage of patients to addiction treatment. © 2022","Clinical data mining; Clinical profiles; Emergency department; High utilizers; Mixed methods research; Substance use","alcohol; lapretolimod; opiate; psychotropic agent; adult; aged; alcohol consumption; altered state of consciousness; Article; clinical feature; clinical study; comorbidity; cross-sectional study; data mining; electronic health record; emergency health service; emergency ward; female; follow up; health care personnel; health care utilization; hospital admission; hospital discharge; housing; human; ICD-10; major clinical study; male; medical history; medical record review; mental health; middle aged; pilot study; prescription; quantitative analysis; retrospective study; substance use; withdrawal syndrome",Article,Scopus,2-s2.0-85122481084
"Jestratijevic I., Maystorovich I., Vrabič-Brodnjak U.","57214321068;57403156500;50360904400;","The 7 Rs sustainable packaging framework: Systematic review of sustainable packaging solutions in the apparel and footwear industry",2022,"Sustainable Production and Consumption","30",,,"331","340",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122476361&doi=10.1016%2fj.spc.2021.12.013&partnerID=40&md5=18951cba354abc1b337320c6a6023033","The apparel and footwear industry creates large amounts of packaging waste that filter through the entire value chain. Although retailers are making efforts to reduce packaging waste, there is a lack of understanding of the sustainable packaging solutions already available. To address this knowledge gap, this research provides a framework that highlights new theoretical and practical advances in sustainable packaging. The study follows the principles of systematic review research methodology. A data-mining approach was utilized to execute the website search. Thematic content clustering was applied for iterative data analysis. The sample included 478 international retail brands advertising sustainable packaging solutions. The key findings of the study indicate that the 7R's approach might be considered a gradual pathway towards sustainable packaging where the proposed framework includes mutually inclusive and complementary approaches. Such a holistic framework can aid retailers in establishing new or revised criteria to advance sustainable packaging solutions. © 2021 Institution of Chemical Engineers","Packaging; Recycle; Reduce; Refuse; Repurpose; Retail; Rethink; Reuse; Rot","Data mining; Filtration; Iterative methods; Packaging; Apparel industry; Packaging solutions; Reduce; Refuse; Repurpose; Rethink; Reuse; Rot; Sustainable packaging; Systematic Review; Sales",Article,Scopus,2-s2.0-85122476361
"Wu H., Luo X., Zhou M., Rawa M.J., Sedraoui K., Albeshri A.","57212467482;36460171200;7403506743;55290678700;6507325355;36617092600;","A PID-incorporated Latent Factorization of Tensors Approach to Dynamically Weighted Directed Network Analysis",2022,"IEEE/CAA Journal of Automatica Sinica","9","3",,"533","546",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122361420&doi=10.1109%2fJAS.2021.1004308&partnerID=40&md5=4e05e5b1a8965eb5558f2c8fd5bd5591","A large-scale dynamically weighted directed network (DWDN) involving numerous entities and massive dynamic interaction is an essential data source in many big-data-related applications, like in a terminal interaction pattern analysis system (TIPAS). It can be represented by a high-dimensional and incomplete (HDI) tensor whose entries are mostly unknown. Yet such an HDI tensor contains a wealth knowledge regarding various desired patterns like potential links in a DWDN. A latent factorization-of-tensors (LFT) model proves to be highly efficient in extracting such knowledge from an HDI tensor, which is commonly achieved via a stochastic gradient descent (SGD) solver. However, an SGD-based LFT model suffers from slow convergence that impairs its efficiency on large-scale DWDNs. To address this issue, this work proposes a proportional-integral-derivative (PID)-incorporated LFT model. It constructs an adjusted instance error based on the PID control principle, and then substitutes it into an SGD solver to improve the convergence rate. Empirical studies on two DWDNs generated by a real TIPAS show that compared with state-of-the-art models, the proposed model achieves significant efficiency gain as well as highly competitive prediction accuracy when handling the task of missing link prediction for a given DWDN. © 2014 Chinese Association of Automation.","Big data; high dimensional and incomplete (HDI) tensor; latent factorization-of-tensors (LFT); machine learning; missing data; optimization; proportional-integral-derivative (PID) controller","Artificial intelligence; Big data; Data mining; Efficiency; Factorization; Learning systems; Optimization; Proportional control systems; Stochastic models; Stochastic systems; Three term control systems; Two term control systems; Directed network; High dimensional and incomplete tensor; High-dimensional; Higher-dimensional; Latent factorization-of-tensor; Missing data; Optimisations; Proportional-integral-derivatives controllers; Stochastic gradient descent; Tensor model; Tensors",Article,Scopus,2-s2.0-85122361420
"Zhong B., Wu H., Xiang R., Guo J.","23975246400;57205511865;57224466677;57245460600;","Automatic Information Extraction from Construction Quality Inspection Regulations: A Knowledge Pattern-Based Ontological Method",2022,"Journal of Construction Engineering and Management","148","3","04021207","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122359623&doi=10.1061%2f%28ASCE%29CO.1943-7862.0002240&partnerID=40&md5=dc829dc9d8c2dbb6ad9333f6136fbccb","Quality compliance checking is essential to ensure construction quality, the prerequisite for which is information extraction from construction quality inspection regulations (CQIRs). Due to the inclusion of multiple qualitative constraints, complex syntax, semantic structures, and exceptions, extracting constraint information from CQIR automatically is difficult. To address the research gap, a knowledge pattern-based ontological method was developed to extract constraint information automatically from CQIR. The entire study process was guided by design science. To begin, knowledge patterns of three typical types of construction quality constraints were investigated to identify constraint elements and their semantic relationships, namely construction procedure constraints, product quality attribute constraints, and resource selection constraints. Then an ontology model was developed to represent these knowledge patterns by defining concepts and properties based on identified constraint elements and semantic relations. Based on the proposed ontology model, Java Annotation Patterns Engine (JAPE) rules were encoded to extract constraint information from CQIR. Finally, a prototype system was created to validate the proposed method, using text data from five mandatory regulations of groundwork and foundation construction. Experimental results demonstrated the theoretical feasibility of the presented method in automatically extracting constraints from CQIR. © 2021 American Society of Civil Engineers.","Construction quality; Information extraction; Knowledge pattern modelling; Ontology; Quality compliance checking","Compliance control; Data mining; Information retrieval; Quality control; Semantics; Automatic information extraction; Constraint information; Construction quality; Information extraction; Knowledge pattern modeling; Knowledge patterns; Ontology model; Ontology's; Quality compliance checking; Quality inspection; Ontology",Article,Scopus,2-s2.0-85122359623
"Lu W., Tao C., Li H., Qi J., Li Y.","57218829148;35235290700;57189334346;57211483444;55969097400;","A unified deep learning framework for urban functional zone extraction based on multi-source heterogeneous data",2022,"Remote Sensing of Environment","270",,"112830","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122326577&doi=10.1016%2fj.rse.2021.112830&partnerID=40&md5=f939c418d85cb0ed22cd11d0b9c32cc6","Remote sensing imagery (RSI) and point of interest (POI) are two complementary data for urban functional zone (UFZ) extraction. However, current methods only use single data or just simply fuse the features extracted from these two data, which may not fully exploit their complementary strength. To solve this problem, in this paper, we propose a unified deep learning framework containing two modules to jointly use RSI and POIs for UFZ extraction. In the first module, the complementary feature learning and fusing module, two coupled convolutional neural networks (CNNs) are used to learn the visual feature from RSI and social feature from POIs, respectively. Specifically, to apply CNN on discrete POIs, we convert them into a hierarchical distance heatmap, and employ CNN with the attention mechanism to extract the co-occurrence relationship of POIs in a specific UFZ for social feature representation. Afterward, we ensemble these two coupled CNNs by a feature attention based fusion strategy, to fuse the complementary features learned from RSI and POIs with adaptively learned weights in an end-to-end manner. In the second module, the UFZ spatial relationship modeling module, different from previous methods that only consider features in single UFZ, we design a spatial relation learning network, which can aggregate both local and long-range between-UFZ spatial relationship for UFZ classification. Experiments on three urban regions demonstrate that the proposed framework can take full advantage of visual, social, and spatial features learned from both RSI and POIs, and thus achieve more satisfactory result than the methods using single data or simple fusion strategy. Furthermore, we analyze the impacts of several factors on UFZ extraction, including the contributions of different category of POIs on UFZ extraction improvement, the synergy mechanism of RSI and POIs for the classification of UFZs, and the influence of using different mapping unit for UFZ mapping. The insights distilled from this study can potentially help researchers use heterogeneous data for UFZ extraction. The source codes are available at: https://github.com/GeoX-Lab/UnifiedDL-UFZ-extraction © 2021 Elsevier Inc.","Deep learning; Point of interest; Remote sensing; Urban functional zone","Convolutional neural networks; Data mining; Deep learning; Extraction; Mapping; Complementary features; Convolutional neural network; Deep learning; Functional zones; Heterogeneous data; Learning frameworks; Point of interest; Remote sensing imagery; Remote-sensing; Urban functional zone; Remote sensing; algorithm; data set; mapping method; satellite imagery; urban region",Article,Scopus,2-s2.0-85122326577
"Ahmed U., Srivastava G., Djenouri Y., Lin J.C.-W.","57204392052;57202588447;55652929600;56449520400;","Knowledge graph based trajectory outlier detection in sustainable smart cities",2022,"Sustainable Cities and Society","78",,"103580","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122261652&doi=10.1016%2fj.scs.2021.103580&partnerID=40&md5=9b89852f30f022f3290242966e1b7d3d","Graph-based intelligent systems are emerging in the field of transportation systems. Knowledge graphs help to provide semantic and interconnectivity capabilities to the intelligent transportation system. In this paper, we propose a graph-based method for detecting outliers in the trajectory. Normal and outlier graphs are constructed using directed weighted graphs. Then, comparison with source and target graphs leading to vectors is performed. The similarity measure is used, which measures common nodes and edges. The features are then used by the machine learning based algorithm to classify the trajectory. Instead of a manually tuned parameter, the tree-based pipeline optimization method selects the best classifier and its hyperparameter. Then, the tuned model is compared with the traditional algorithms, i.e., random forest, decision tree, Naïve Bayes, and KNN. To evaluate the system under real conditions, an experiment is performed on a real dataset of trajectories. The results show that the graph-based method performs well and achieves an F-value of 0.81, while the optimization model achieves an F-value of 0.87. The graph-based model matched with the learning method helps to detect outliers and deviation points of the trajectory with high precision. © 2021 Elsevier Ltd","Data mining; Outlier detection; Road traffic management; Smart city application; Trajectory analysis","Data mining; Decision trees; Directed graphs; Graphic methods; Intelligent systems; Machine learning; Semantics; Smart city; Statistics; Trajectories; F values; Graph-based; Graph-based methods; Interconnectivity; Knowledge graphs; Outlier Detection; Road traffic management; Smart city application; Trajectory analysis; Transportation system; Anomaly detection",Article,Scopus,2-s2.0-85122261652
"He Y., Chu Y., Song Y., Liu M., Shi S., Chen X.","57309183300;57396770300;55910275200;57396131000;57396234600;57396665600;","Analysis of design strategy of energy efficient buildings based on databases by using data mining and statistical metrics approach",2022,"Energy and Buildings","258",,"111811","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122239125&doi=10.1016%2fj.enbuild.2021.111811&partnerID=40&md5=ad32e23ea3ac3d7175309b5bcb9bb7ba","This research aimed to study the design strategies of abundant existing Energy-Efficient Buildings (EEBs) by analyzing correlations between the Climate Parameters (CPs) and Building Parameters (BPs). A database of near zero energy buildings was established and tailored data-mining programs were developed to collect data. Statistical test, data distribution visualization, and correlation analysis were implemented using customized statistical metrics programs to reveal the trends, preferences and potentials of the design strategies summarized as follows: (1) Climate-adaptable design of exterior form was paid more attention to than envelope system, and the design optimization focused on the adjustment of building shape coefficients. (2) The humidity and average high temperature were the most influential factors. (3) The design strategy of envelope system focused on adapting to cold climates, while hot climates were little considered. (4) For EEBs in China, correlations between the CPs and BPs were higher than buildings in the other countries. © 2021 Elsevier B.V.","Data mining; Databases; Design strategy analysis; Energy efficient buildings; Statistical metrics","Architectural design; Data visualization; Database systems; Energy efficiency; Zero energy buildings; Building parameters; Climate parameters; Data distribution; Design strategies; Design strategy analyse; Energy efficient building; Existing energies; Mining projects; Statistical metric; Test data; Data mining",Article,Scopus,2-s2.0-85122239125
"Yu L., Cao F.","57194025874;36960346200;","Weighted matrix-object data clustering guided by matrix-object distributions",2022,"Engineering Applications of Artificial Intelligence","109",,"104612","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122213157&doi=10.1016%2fj.engappai.2021.104612&partnerID=40&md5=2fb2273db859e57742c337cb1a74a1a5","In data mining, the input of most algorithms is a data set in which each example is a feature vector. However, in many real applications an example usually contains multiple feature vectors and its observed classification is the responsibility of all feature vectors. We call this example kind matrix-object. Some existing clustering algorithms for matrix-object data fail to consider contributions of attributes to clusters, which may degrade clustering solutions due to less discriminative attributes. Some existing clustering algorithms for the data in which each example is a vector consider the contributions but encounter difficulties in handling matrix-object data. For matrix-object data, ordered and cross matrix-object distributions may exist in a cluster and cause different ways of measuring qualities of clusters. In this paper, we propose a weighted matrix-object data clustering algorithm guided by matrix-object distributions. We define cluster and matrix-object compactness respectively for the two distributions to measure qualities of clusters. The bigger the compactness is, the higher the quality is. So the proposed algorithm utilizes the compactness to assign a weight to each attribute for each cluster and maximizes weighted cluster and matrix-object compactness to find the optimal weight and the final clustering partition. Furthermore, a regular term about weight is added to the objective function to make more higher discriminative attributes participate in the optimization. Experimental results on real data have shown the effectiveness of the proposed algorithm. Compared with previous clustering algorithms, the proposed algorithm improves the clustering performance and enhances the interpretability of clustering results. © 2021 Elsevier Ltd","Cluster compactness; Clustering; Matrix-object; Matrix-object compactness; Weighted","Cluster analysis; Clustering algorithms; Data mining; Vectors; Cluster compactness; Clusterings; Features vector; matrix; Matrix-object; Matrix-object compactness; Object data; Object distribution; Weighted; Matrix algebra",Article,Scopus,2-s2.0-85122213157
"Balasubramanian K., Ananthamoorthy N.P., Ramya K.","57188969156;55123205600;57212691573;","Prediction of neuro-degenerative disorders using sunflower optimisation algorithm and Kernel extreme learning machine: A case-study with Parkinson’s and Alzheimer’s disease",2022,"Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine","236","3",,"438","453",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122129595&doi=10.1177%2f09544119211060989&partnerID=40&md5=c49cd46cf15ebd5e4c2bf43c2ae5f96a","Parkinson’s and Alzheimer’s Disease are believed to be most prevalent and common in older people. Several data-mining approaches are employed on the neuro-degenerative data in predicting the disease. A novel method has been built and developed to diagnose Alzheimer’s (AD) and Parkinson’s (PD) in early stages, which includes image acquisition, pre-processing, feature extraction and selection, followed by classification. The challenge lies in selecting the optimal feature subset for classification. In this work, the Sunflower Optimisation Algorithm (SFO) is employed to select the optimal feature set, which is then fed to the Kernel Extreme Learning Machine (KELM) for classification. The method is tested on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) and local dataset for AD, the University of California, Irvine (UCI) machine learning repository and the Istanbul dataset for PD. Experimental outcomes have demonstrated a high accuracy level in both AD and PD diagnosis. For AD diagnosis, the highest classification rate is obtained for the AD versus NC classification using the ADNI dataset (99.32%) and local dataset (98.65%). For PD diagnosis, the highest accuracy of 99.52% and 99.45% is achieved on the UCI and Istanbul datasets, respectively. To show the robustness of the method, the method is compared with other similar methods of feature selection and classification with 10-fold cross-validation (CV) and with unseen data. The method proposed has an excellent prospect, bringing greater convenience to clinicians in making a better solid decision in clinical diagnosis of neuro-degenerative diseases. © IMechE 2021.","Alzheimer’s; feature selection; KELM; Parkinson’s; SFO","Classification (of information); Computer aided diagnosis; Data mining; Knowledge acquisition; Learning algorithms; Machine learning; Neuroimaging; Optimization; Alzheimer; Alzheimer’s; Features selection; High-accuracy; Istanbul; Kernel extreme learning machine; Optimization algorithms; Parkinson’s; Sunflower optimization algorithm; University of California; Feature extraction; aged; algorithm; Alzheimer disease; cognitive defect; human; nuclear magnetic resonance imaging; Parkinson disease; sunflower; Aged; Algorithms; Alzheimer Disease; Cognitive Dysfunction; Helianthus; Humans; Magnetic Resonance Imaging; Parkinson Disease",Article,Scopus,2-s2.0-85122129595
"Taoufik N., Boumya W., Elmoubarki R., Elhalil A., Achak M., Abdennouri M., Barka N.","57201685688;57028458100;55887221800;56705570200;24491484300;35502887200;13205747400;","Experimental design, machine learning approaches for the optimization and modeling of caffeine adsorption",2022,"Materials Today Chemistry","23",,"100732","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121922333&doi=10.1016%2fj.mtchem.2021.100732&partnerID=40&md5=af8c85a6727a8e83a06c1a2d7f08efd7","In the current research, the sorption of caffeine on fresh and calcined Cu–Al layered double hydroxide was comparatively studied based on adsorption parameters, adsorption kinetics, and adsorption isotherm. Response surface methodology (RSM), support vector machine (SVM) and artificial neural network (ANN), as data mining methods, were applied to develop models by considering various operating variables. Different characterization methods were exploited to conduct a comprehensive analysis of the characteristics of HDL in order to acquire a thorough understanding of its structural and functional features. The Langmuir model was employed to accurately describe the maximum monolayer adsorption capacity for calcined sample (qmax) of 152.99 mg/g mg/g with R2 = 0.9977. The pseudo-second order model precisely described the adsorption phenomenon (R2 = 0.999). The thermodynamic analysis also reveals a favorable and spontaneous process. The ANN model predicts adsorption efficiency result with R2 = 0.989. The five-fold cross-validation was achieved to evaluate the validity of the SVM. The predication results revealed approximately 99.9% accuracy for test datasets and 99.63% accuracy for experiment data. Moreover, ANOVA analysis employing the central composite design-response surface methodology (CCD-RSM) indicated a good agreement between the quadratic equation predictions and the experimental data, which results in R2 of 0.9868 and the highest removal percentages in optimized step were obtained for RSM (pH 5.05, mass of adsorbent 20 mg, time of 72 min, and caffeine concentrations of 22 mg/L). On the whole, the findings confirm that the proposed machine learning models provided reliable and robust computer methods for monitoring and simulating the adsorption of pollutants from aqueous solutions by Cu–Al–LDH. © 2021 Elsevier Ltd","ANN; Emerging compounds; Layered double hydroxide; RSM; Sequestration; SVM","Adsorption; Aluminum compounds; Caffeine; Calcination; Data mining; Design of experiments; Monolayers; Neural networks; Surface properties; Thermoanalysis; 'current; Adsorption kinetics; Adsorption parameters; Emerging compound; Layered-double hydroxides; Machine learning approaches; Optimisations; Response-surface methodology; Sequestration; Support vectors machine; Support vector machines",Article,Scopus,2-s2.0-85121922333
"Vučetić M., Brokešová Z., Hudec M., Pastoráková E.","54685491800;56049743800;8575283100;56048977700;","Financial literacy and psychological disaster preparedness: applicability of approach based on fuzzy functional dependencies",2022,"Information Processing and Management","59","2","102848","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121820483&doi=10.1016%2fj.ipm.2021.102848&partnerID=40&md5=e8414531e3952767d71739c246bf6126","Recent catastrophes and crises emphasize the necessity of an individual's and household's preparedness for coping with these events. Previous research focuses mainly on short-term preparation and immediate response to catastrophes. The ex ante approach to managing long-term effects can rarely be found in literature, especially with an emphasis on financial preparedness. In this work, we use a soft computing method for analysing the social/psychological effects and implications. Specifically, we focus on data mining and linguistic interpretation of the effects of financial literacy on psychological preparedness for disasters, using fuzzy functional dependencies (FFDs). FFDs determine influences between features in datasets and examine neighbouring categorical answers with a certain degree of similarity, which overcomes indecisiveness when marking a categorical answer. Based on the surveyed data, we show that FFDs offer an approach for discovering additional patterns and explaining those patterns linguistically. Our results are as follows: financial literacy significantly influences anxiety, lack of control, nervousness, and fear of death (in order of intensity). Using this approach, we conclude that being better financially prepared can help in long-term disaster risk management. Financial preparedness is also helpful in mitigating psychological risks related to disasters. This paper provides results that can support the designing knowledge management solutions in order to improve psychological disaster preparedness. © 2021 Elsevier Ltd","Disaster preparedness; Emotions; Financial literacy; Fuzzy functional dependencies; Linguistic interpretation of dependencies; Summarized information","Data mining; Disaster prevention; Finance; Knowledge management; Linguistics; Risk management; Soft computing; Disaster preparedness; Emotion; Ex antes; Financial literacy; Fuzzy functional dependencies; Linguistic interpretation of dependency; Long-term effects; Research focus; Soft computing methods; Summarized information; Disasters",Article,Scopus,2-s2.0-85121820483
"Li H., Gao Y., Liu J., Zhang J., Li C.","35148279700;57384136100;42761838200;57382836600;57383927700;","Semi-supervised graph regularized nonnegative matrix factorization with local coordinate for image representation",2022,"Signal Processing: Image Communication","102",,"116589","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121642444&doi=10.1016%2fj.image.2021.116589&partnerID=40&md5=c36ad84cbf35d19233f26e46075bee2d","Nonnegative matrix factorization(NMF) is a powerful image representation algorithm in pattern recognition and data mining. However, the traditional NMF does not utilize any label information, or fail to guarantee the sparse parts-based representation. In this paper, we put forward a semi-supervised local coordinate NMF (SLNMF) algorithm, which incorporate the available label information and the local coordinate constraint into NMF. Particularly, SLNMF makes the learned coefficients sparse by adding the coordinate constraint, and enhance the discriminative ability of different classes by using the label information constraint. Furthermore, in order to extract the geometric structure of the data space, we propose a new semi-supervised graph regularized NMF with local coordinate constraint (SGLNMF) method, which incorporates the graph regularization into SLNMF to enhance the discriminative abilities of data representations. SGLNMF not only reveals the intrinsic geometrical information of the data space, but also takes into account the local coordinate constraint and the label information. Clustering experiments on several standard image datasets demonstrate the effectiveness of our proposed SLNMF and SGLNMF methods compared to the state-of-the-art methods. © 2021","Graph regularization; Local coordinate; Nonnegative matrix factorization; Semi-supervised learning","Data mining; Image representation; Matrix factorization; Pattern recognition; Supervised learning; Data space; Discriminative ability; Graph regularization; Image representations; Label information; Local coordinate; Nonnegative matrix factorization; Part-based representation; Regularisation; Semi-supervised graphs; Matrix algebra",Article,Scopus,2-s2.0-85121642444
"Yang B., Pan L., Zhu H., Sun S., Zhang Q.","55584795282;57222626179;57222621353;57222626329;35797856200;","Spatiotemporal correlation analysis of the dynamic response of supertall buildings under ambient wind conditions",2022,"Structural Design of Tall and Special Buildings","31","4","e1914","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121595389&doi=10.1002%2ftal.1914&partnerID=40&md5=10c110a614258c8e496b3560153bd28e","The structural health monitoring systems of supertall buildings are typical big data systems with abundant and high-dimensional data. Correlation analysis is a vital technique for data mining and analysis. This study aims to reveal the spatial and temporal correlation of 4 years of structural health monitoring data from the 632-m-high Shanghai Tower under normal wind and typhoon conditions. In this paper, the root mean square (RMS) value of the acceleration response is used to characterize the structural vibration intensity. The maximal information coefficient (MIC) and Pearson product-moment correlation coefficient (PPMCC) are adopted to measure the spatial correlations along Shanghai Tower under different wind conditions. The temporal correlation of structural vibration is investigated with kernel density estimation and the density-based spatial clustering of applications with noise (DBSCAN) algorithm. The results show that the structural vibrations of Shanghai Tower in different spatial positions are correlated. The correlation of the structural vibrations of the same floor is at a high level under both normal wind and typhoons, while that of different floors rises with increasing mean wind speed. Furthermore, the MIC is more stable than the PPMCC in measuring this spatial correlation. Based on temporal correlation analysis, a circadian rhythm tendency of the structural vibration intensity of Shanghai Tower on the daily scale is identified, and it is verified that the structural response is temporally correlated. These findings can provide a structural health state estimation index based on the correlation coefficient and provide a basis for the screening of modeling data. © 2021 John Wiley & Sons Ltd.","clustering algorithm; correlation analysis; structural health monitoring; supertall building; wind effect","Correlation methods; Data mining; Floors; Hurricanes; Modal analysis; Structural dynamics; Structural health monitoring; Vibration analysis; Wind effects; Correlation analysis; Information coefficient; Maximal information; Pearsons product moment correlation coefficients; Spatial correlations; Structural vibrations; Super tall buildings; Temporal correlations; Vibration intensity; Wind conditions; Clustering algorithms",Article,Scopus,2-s2.0-85121595389
"Tung N.T., Nguyen L.T.T., Nguyen T.D.D., Fourier-Viger P., Nguyen N.-T., Vo B.","57226721646;55195057700;57203865437;57377977600;7403180310;35147075900;","Efficient mining of cross-level high-utility itemsets in taxonomy quantitative databases",2022,"Information Sciences","587",,,"41","62",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121458287&doi=10.1016%2fj.ins.2021.12.017&partnerID=40&md5=ef5c3346c7bead2e8f736695d6698fc3","In contrast to frequent itemset mining (FIM) algorithms that focus on identifying itemsets with high occurrence frequency, high-utility itemset mining algorithms can reveal the most profitable sets of items in transaction databases. Several algorithms were proposed to perform the task efficiently. Nevertheless, most of them ignore the item categorizations. This useful information is provided in many real-world transaction databases. Previous works, such as CLH-Miner and ML-HUI Miner were proposed to solve this limitation to discover cross-level and multi-level HUIs. However, the CLH-Miner has a long runtime and high memory usage. To address these drawbacks, this study extends tight upper bounds to propose effective pruning strategies. A novel algorithm named FEACP (Fast and Efficient Algorithm for Cross-level high-utility Pattern mining) is introduced, which adopts the proposed strategies to efficiently identify cross-level HUIs in taxonomy-based databases. It can be seen from a thorough performance evaluation that FEACP can identify useful itemsets of different abstraction levels in transaction databases with high efficiency, that is up to 8 times faster than the state-of-the-art algorithm on the tested sparse databases and up to 177 times on the tested dense databases. FEACP reduces memory usage by up to half over the CLH-Miner algorithm. © 2021 Elsevier Inc.","Cross-level itemsets; Data mining; Hierarchical database; High-utility itemsets; Taxonomy","Data mining; Database systems; Miners; Cross levels; Cross-level itemset; Frequent itemset mining; Hierarchical database; High utility itemsets; Itemset; Memory usage; Mining algorithms; Quantitative database; Transaction database; Taxonomies",Article,Scopus,2-s2.0-85121458287
"Hao Y., Jiang S., Yu F., Zeng W., Wang X., Yang X.","57373758200;57203764370;35365764700;8893081800;13305942200;8912567500;","Linear dynamic fuzzy granule based long-term forecasting model of interval-valued time series",2022,"Information Sciences","586",,,"563","595",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121277993&doi=10.1016%2fj.ins.2021.12.007&partnerID=40&md5=60d1e5f950dea34399fa17cf46179bc1","The studies on modeling and analysis of time series based on fuzzy granulation have shown that fuzzy granulation is an effective approach in data mining of time series. However, the investigation of fuzzy granulation of interval-valued time series (ITS) and its applications has just begun in recent years with appearance of few research results. Different from the existing studies, this paper carried out the investigation of fuzzy granulation of ITS in interval number space instead of real number space. Two distinguished concepts, namely static fuzzy information granules and dynamic fuzzy information granules of ITS, are proposed firstly. Then the approaches for constructing a static fuzzy information granule and a dynamic fuzzy information granule are designed respectively. After that, two fuzzy granulation methods for ITS based on the above two approaches are presented in the framework of interval analysis under the guidance of the principle of justifiable granularity. Based on the specific proposed fuzzy granule which is called linear dynamic fuzzy granule, a long-term forecasting model for ITS was developed with the aid of artificial neural network. Experiments conducted on several ITS from stock markets with different dynamic characteristics showed the outperformance of the proposed long-term forecasting model. © 2021 Elsevier Inc.","Fuzzy information granules; Interval number space; Interval-valued time series; Linear dynamic fuzzy granulation; Long-term forecasting","Data mining; Forecasting; Granulation; Neural networks; Time series analysis; Fuzzy granulation; Fuzzy information; Fuzzy information granule; Interval number; Interval number space; Interval-valued; Interval-valued time series; Linear dynamic fuzzy granulation; Linear dynamics; Long-term forecasting; Times series; Time series",Article,Scopus,2-s2.0-85121277993
"Yilmaz T., Ulusoy Ö.","57301506500;35516816100;","Understanding security vulnerabilities in student code: A case study in a non-security course",2022,"Journal of Systems and Software","185",,"111150","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121237420&doi=10.1016%2fj.jss.2021.111150&partnerID=40&md5=587d2b064c8740345d8d2a2644cc29ae","Secure coding education is quite important for students to acquire the skills to quickly adapt to the evolving threats towards the software they are expected to create once they graduate. Educators are also more aware of this situation and incorporate teaching security in their respective fields. An effective application of this is only possible by cultivating the teaching and learning perspectives. Understanding the security awareness and practice of students is useful as an initial step to create a baseline for teaching methods and content. In this paper, we first survey to investigate how students approach security and what could motivate them to learn and apply security practices. Then, we analyze the source code for 6 semesters of coding assignments for 2 tasks using a source code vulnerability analysis tool. In our analysis, we report the types of vulnerabilities and various aspects between them while incorporating the effect of student grades. We then explore the lexical and structural features of security in student code using data analysis and machine learning. For the lexical analysis, we build a classifier to extract informative features and for the structural analysis, we utilize Syntax Trees to represent code and perform clustering in terms of structural features where clusters themselves yield different vulnerability levels. © 2021","Data mining; Secure coding education; Source code analysis; Vulnerability analysis","Codes (symbols); Computer programming languages; Structural analysis; Students; Teaching; Trees (mathematics); Case-studies; Secure coding; Secure coding education; Security Practice; Security vulnerabilities; Source code analysis; Source codes; Structural feature; Teaching and learning; Vulnerability analysis; Data mining",Article,Scopus,2-s2.0-85121237420
"Tang D., Chen J., Wang X., Zhang S., Yan Y.","56707980200;57211343791;57212769588;57222489273;57211338597;","A new detection method for LDoS attacks based on data mining",2022,"Future Generation Computer Systems","128",,,"73","87",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120999771&doi=10.1016%2fj.future.2021.09.039&partnerID=40&md5=a0b1b5c59f389a23041ecac6d0d58469","The serving capabilities of networks are reduced by low-rate denial of service (LDoS) attacks that periodically send high-intensity pulse data flows. This type of attack shows a harmful effect similar to that of traditional DoS attacks, but their attack modes differ greatly. The high concealment of LDoS attacks makes it extremely difficult for traditional DoS detection methods to detect LDoS attacks. Meanwhile, the state-of-art detection methods for LDoS attacks have low-efficiency and resource-intensive and time complexity issues. We propose a novel detection method with analysis of abnormal network traffic under LDoS attacks that combines data mining technology. The judgement benchmarks were also established. The results from the experimental simulation on the simulated environment, physical environment and public datasets prove that the developed method can effectively detect LDoS attacks with optimal detection cost and low complexity, and has a high accuracy, a low false-negative rate, and a low false-positive rate. © 2021 Elsevier B.V.","Abnormal traffic; Adaptive exponentially weighted moving average (AEWMA) control chart; Data mining; Frequency distribution; Histogram; Low-rate denial of service (LDoS)","Complex networks; Denial-of-service attack; Flowcharting; Abnormal traffic; Adaptive exponentially weighted moving average control chart; Denial of Service; Denialof- service attacks; Detection methods; Exponentially weighted moving average control charts; Frequency distributions; High intensity pulse; Low rates; Low-rate denial of service; Data mining",Article,Scopus,2-s2.0-85120999771
"Smith E., Papadopoulos D., Braschler M., Stockinger K.","57224807287;57195630441;6603066610;6603748288;","LILLIE: Information extraction and database integration using linguistics and learning-based algorithms",2022,"Information Systems","105",,"101938","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120964492&doi=10.1016%2fj.is.2021.101938&partnerID=40&md5=5a89b2ea4efbc5e9f8167be1abcf3f9c","Querying both structured and unstructured data via a single common query interface such as SQL or natural language has been a long standing research goal. Moreover, as methods for extracting information from unstructured data become ever more powerful, the desire to integrate the output of such extraction processes with “clean”, structured data grows. We are convinced that for successful integration into databases, such extracted information in the form of “triples” needs to be both (1) of high quality and (2) have the necessary generality to link up with varying forms of structured data. It is the combination of both these aspects, which heretofore have been usually treated in isolation, where our approach breaks new ground. The cornerstone of our work is a novel, generic method for extracting open information triples from unstructured text, using a combination of linguistics and learning-based extraction methods, thus uniquely balancing both precision and recall. Our system called LILLIE (LInked Linguistics and Learning-Based Information Extractor) uses dependency tree modification rules to refine triples from a high-recall learning-based engine, and combines them with syntactic triples from a high-precision engine to increase effectiveness. In addition, our system features several augmentations, which modify the generality and the degree of granularity of the output triples. Even though our focus is on addressing both quality and generality simultaneously, our new method substantially outperforms current state-of-the-art systems on the two widely-used CaRB and Re-OIE16 benchmark sets for information extraction. We have made our code publicly available to facilitate further research. © 2021 The Authors","Data integration; Information extraction; Machine learning for database systems","Balancing; Benchmarking; Data mining; Database systems; Information retrieval; Information use; Linguistics; Machine learning; Natural language processing systems; Query processing; Search engines; Databases integrations; Information extraction; Learning-based algorithms; Machine learning for database system; Natural languages; Query interfaces; Research goals; SQL languages; Structured data; Unstructured data; Data integration",Article,Scopus,2-s2.0-85120964492
"Wang S., Xiao S., Zhu W., Guo Y.","36601774500;57315896900;8985800200;56463184700;","Multi-view fuzzy clustering of deep random walk and sparse low-rank embedding",2022,"Information Sciences","586",,,"224","238",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120860093&doi=10.1016%2fj.ins.2021.11.075&partnerID=40&md5=dfb28f885cb6ce3aa83bd01ceb60026c","Multi-view clustering aims to improve the learning performance by exploiting discriminative information from heterogeneous data sources. It has been capturing growing research attention due to its wide utilization in the areas of data mining and computer vision. However, the full use of complementarity and consistency from multi-view data is still a challenging task. Here, we propose a novel multi-view fuzzy clustering, which adopts the joint learning of deep random walk and sparse low-rank embedding. First, a deep random walk is employed to acquire a robust similarity matrix of data points and convert fuzzy membership matrix learning to adaptive graph learning. Second, the adaptive graph is restricted with sparse low-rank constraints, which ensures its strong discriminative ability and effective cluster assignments. Third, by exploiting the sparse low-rank property, the multi-view fuzzy clustering problem is formulated as optimizing a regularized graph adjacency matrix with distance metric learning and spectral norm minimization simultaneously. The distance metric learning is embedded to scatter all data points so that any two samples are projected onto a low-dimensional subspace with a large margin. Finally, an efficient algorithm is developed for the formulated problem and its convergence is also guaranteed. In order to demonstrate the superiority of the proposed method, extensive experiments are conducted by comparing state-of-the-arts on real-world databases. © 2021 Elsevier Inc.","Deep random walk; Distance metric learning; Fuzzy clustering; Machine learning; Multi-view clustering","Data mining; Deep learning; Embeddings; Fuzzy clustering; Graph theory; Matrix algebra; Datapoints; Deep random walk; Distance Metric Learning; Embeddings; Heterogeneous data sources; Learning performance; Multi-view clustering; Multi-view datum; Multi-views; Random Walk; Random processes",Article,Scopus,2-s2.0-85120860093
"Zhang C., Guo W., Yao X., Xia J., Zhang Z., Li J., Chen H., Lin L.","57226446339;57199262818;57367752800;57368376900;57263870100;57216663981;56507963400;55202887200;","Database mining and animal experiment-based validation of the efficacy and mechanism of Radix Astragali (Huangqi) and Rhizoma Atractylodis Macrocephalae (Baizhu) as core drugs of Traditional Chinese medicine in cancer-related fatigue",2022,"Journal of Ethnopharmacology","285",,"114892","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120850410&doi=10.1016%2fj.jep.2021.114892&partnerID=40&md5=163b95119203aaa73045f64ad4232168","Ethnopharmacological relevance: In China, Traditional Chinese medicine (TCM) is often used as the main therapy for cancer-related fatigue (CRF). However, there is limited evidence to prove its therapeutic effect and mechanism. Aim of the study: We aimed to provide a basis for the therapeutic effect of TCM for CRF. Materials and methods: We performed a meta-analysis to investigate the efficacy of TCM treatment for CRF. Through frequency statistics and association rule mining, we screened the core Chinese medicine components, Astragalus mongholicus Bunge., root (Radix astragali, Huangqi) and Atractylodes macrocephala Koidz., rhizome (Rhizoma atractylodis macrocephalae, Baizhu). We then used animal experiments to verify the effectiveness of these two TCMs and changes in related indicators in mice. Relevant molecular mechanisms were explored through network pharmacological analysis. Results: Twenty-four randomised control trials (RCTs) involving 1865 patients were included in the meta-analysis. TCM produced more positive effects on CRF than standard therapy alone. Radix astragali and Rhizoma atractylodis macrocephalae, as the core drug pair for the treatment of CRF, enhanced the physical fitness of mice; reduced abdominal circumference, level of inflammatory factors, and tumour weight; and increased body weight and blood sugar. Network pharmacology analysis showed that the mechanism of action of Radix astragali and Rhizoma atractylodis macrocephalae on CRF mainly involved compounds, such as quercetin, kaempferol and luteolin, acting through multiple targets, such as Protein kinase B α (AKT1), Tumour necrosis factor (TNF), and Interleukin-6 (IL-6). These molecules regulate cytokines, cancer signalling, and metabolic pathways and confer an anti-CRF effect. Conclusions: TCM may be a promising therapy to relieve CRF in cancer patients. Our research may provide a reference for the clinical application of TCM for treating CRF. © 2021 Elsevier B.V.","Cancer; Fatigue; Mechanism; meta-Analysis; Traditional Chinese medicine","herbaceous agent; huangqi decoction; animal; complication; data mining; fatigue; human; meta analysis; neoplasm; reproducibility; Animals; Data Mining; Drugs, Chinese Herbal; Fatigue; Humans; Neoplasms; Reproducibility of Results",Article,Scopus,2-s2.0-85120850410
"Peng S., Yang Z., Ling B.W.-K., Chen B., Lin Z.","57191199826;24336506200;57210406493;16177239100;57366709400;","Dual semi-supervised convex nonnegative matrix factorization for data representation",2022,"Information Sciences","585",,,"571","593",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120779329&doi=10.1016%2fj.ins.2021.11.045&partnerID=40&md5=8f25afbad2cc6d34f3b1c64dd47fad2d","Semi-supervised nonnegative matrix factorization (NMF) has received considerable attention in machine learning and data mining. A new semi-supervised NMF method, called dual semi-supervised convex nonnegative matrix factorization (DCNMF), is proposed in this paper for fully using the limited label information. Specifically, DCNMF simultaneously incorporates the pointwise and pairwise constraints of labeled samples as dual supervisory information into convex NMF, which results in a better low-dimensional data representation. Moreover, DCNMF imposes the nonnegative constraint only on the coefficient matrix but not on the base matrix. Consequently, DCNMF can process mixed-sign data, and hence enlarge the range of applications. We derive an efficient alternating iterative algorithm for DCNMF to solve the optimization, and analyze the proposed DCNMF method in terms of the convergence and computational complexity. We also discuss the relationships between DCNMF and several typical NMF based methods. Experimental results illustrate that DCNMF outperforms the related state-of-the-art NMF methods on nonnegative and mixed-sign datasets for clustering applications. © 2021 Elsevier Inc.","clustering; Convex nonnegative matrix factorization; data representation; Semi-supervised learning","Computational efficiency; Data mining; Iterative methods; Matrix factorization; Supervised learning; Clusterings; Convex nonnegative matrix factorization; Data representations; Factorization methods; Label information; Non negatives; Nonnegative matrix factorization; Pairwise constraints; Point wise; Semi-supervised; Matrix algebra",Article,Scopus,2-s2.0-85120779329
"Gao Q., Liu C., Li Y., Du Y., Yue G., Liu B.","57215685064;56288854700;57211571395;16021438800;57363375400;57196892373;","Mining Co-Occurrence Patterns among Deep Road Distresses Using Association Rule Analysis",2022,"Journal of Transportation Engineering Part B: Pavements","148","1","04021078-1","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120606092&doi=10.1061%2fJPEODX.0000328&partnerID=40&md5=da4b71bf2d5ee11fe3cbfe9cb173190d","Co-occurrence patterns among different deep road distresses (distresses below the road surface) play a pivotal role in road maintenance. It is essential for the sustainable development of road performance and draws much attention from road maintenance departments. However, current work mainly focused on the rapid detection and development evaluation of pavement distress. Few studies shed light on the relationship among them. In this paper, over 200 km of field tests were conducted on 87 sections of the highways in China by ground-penetrating radar (GPR). Based on the distress detection results, the association rule mining algorithm Apriori was applied to explore the co-occurrence pattern among 13 types of deep road distress. Results indicate a significant correlation among light distresses (distresses with light degree), and between light distress and severe distress (distresses with moderate and heavy degree). Light distress has an average 53% probability of accompanying or inducing other distress, which is supposed to be maintained in time to prevent the road from further deterioration. Light and severe distress have a 36% probability of co-occurrence. However, the relationship among severe distresses is proved to be weak. Compared with the external environment, the interaction between different distresses is also a considerable inducement for pavement performance deterioration. The study provides a new perspective on the generation mechanism of deep road distress, which can further help the authorities optimize the maintenance schedule. © 2021 American Society of Civil Engineers.","Association rule mining; Co-occurrence pattern; Deep road distress; Ground-penetrating radar (GPR); Road maintenance schemes","Association rules; Data mining; Deterioration; Geological surveys; Geophysical prospecting; Ground penetrating radar systems; Pavements; Sustainable development; Association rule analysis; Co-occurrence pattern; Deep road distress; Ground Penetrating Radar; Ground-penetrating radar; Maintenance schemes; Road maintenance; Road maintenance scheme; Road performance; Road surfaces; Maintenance; algorithm; ground penetrating radar; machine learning; maintenance; pavement; road transport; China",Article,Scopus,2-s2.0-85120606092
"Dokuz A.S.","46061138200;","Weighted spatio-temporal taxi trajectory big data mining for regional traffic estimation",2022,"Physica A: Statistical Mechanics and its Applications","589",,"126645","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120491109&doi=10.1016%2fj.physa.2021.126645&partnerID=40&md5=f7a75ad8267f4a22cd6d03ec3035170b","The estimation of traffic conditions in cities is becoming essential to establish a sustainable transportation system and to help traffic management authorities plan the traffic of cities. Recently, taxi trajectory big datasets are being collected during taxi drivers are routing around the cities. Taxi trajectory datasets provide behavioral information about the city residents, urban flows of the taxi passengers, and infrastructure for traffic condition estimation. This study aims to estimate regional traffic velocity of New York City using New York taxi trajectory dataset. A new method is proposed that uses weighted spatio-temporal trajectory big data mining approach and scores each region of the cities in terms of traffic velocity. A new algorithm is proposed, namely Regional Traffic Velocity Estimation (RTVE) algorithm, which uses proposed regional spatio-temporal velocity estimation method and experimentally evaluated using New York taxi trajectory dataset. Experimental results show that each region in New York have different velocity and usage characteristics in terms of hourly and daily analyses. Also, borough-level analyses are performed that reveal knowledge about the boroughs of New York. The estimated regional traffic velocity of cities based on taxi trajectory datasets would provide a decision support system for decision-makers in terms of regional hourly and daily evaluation of cities with cost-free and widespread city traffic dataset. © 2021 Elsevier B.V.","Big data mining; Regional traffic condition monitoring; Regional traffic velocity estimation; Taxi trajectory dataset; Weighted spatio-temporal pattern mining","Artificial intelligence; Big data; Condition monitoring; Data mining; Decision making; Decision support systems; Taxicabs; Velocity; Big data mining; New York; Regional traffic condition monitoring; Regional traffic velocity estimation; Spatiotemporal patterns; Taxi trajectory dataset; Temporal pattern minings; Traffic conditions; Velocity estimation; Weighted spatio-temporal pattern mining; Trajectories",Article,Scopus,2-s2.0-85120491109
"Li M., Shi L., Wang Y., Wang J., Wang Q., Hu J., Peng X., Liao W., Pi G.","57207884211;37061770900;57216957551;57360580100;57360730600;57219759997;57220049744;57220051676;57220049872;","Automated data function extraction from textual requirements by leveraging semi-supervised CRF and language model",2022,"Information and Software Technology","143",,"106770","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120487992&doi=10.1016%2fj.infsof.2021.106770&partnerID=40&md5=c1f2c458d096a124ba1cbdae23650aae","Context: Function Point Analysis (FPA) provides an objective, comparative measure for size estimation in the early stage of software development. When practicing FPA, analysts typically abide by the following steps: data function (DF) extraction, transactional function extraction, function type classification and adjustment factor determination. However, due to lack of approach and tool support, these steps are usually conduct by human efforts in practice. Related approaches can hardly be applied in the FPA due to the following three challenges, i.e., FPA rule-driven extraction, domain-specific parsing, and expensive labeled resources. Objective: In this paper, we aim to automate the extraction of DFs, which is the starting and fundamental step in FPA. Method: We propose an automated approach named DEX to extract data functions from textual requirements. Specifically, DEX introduces the popularly-used conditional random field (CRF) model to predict the boundary of a data function. Besides, DEX employs the bootstrapping-based algorithm and DF-oriented language model to further boost the performance. Results: We evaluate DEX from two aspects: evaluation on a real industrial dataset and a manual review by domain experts. The evaluation on the real industrial dataset shows that DEX could achieve 80% precision, 84% recall, and 82% F1, and outperforms three state-of-the-art baselines. The expert review suggests that DEX could increase 16% precision and 13% recall, compared with those produced by engineers. Conclusion: DEX could achieve promising results under a small number of labeled requirements and outperform the state-of-the-art approaches. Moreover, DEX could help engineers produce more accurate and complete DFs in the industrial environment. © 2021 Elsevier B.V.","Bootstrapping; Conditional random field; Function point analysis; Language model; Size estimation","Computational linguistics; Data mining; Function evaluation; Random processes; Software design; Automated data; Bootstrapping; Data functions; Function extraction; Function point analysis; Language model; Random field model; Random languages; Semi-supervised; Size estimation; Extraction",Article,Scopus,2-s2.0-85120487992
"Ahmed M.M., Palaniswamy T.","57215537412;57203907867;","A novel TMGWO–SLBNC-based multidimensional feature subset selection and classification framework for frequent diagnosis of breast lesion abnormalities",2022,"International Journal of Intelligent Systems","37","3",,"2131","2162",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120458986&doi=10.1002%2fint.22768&partnerID=40&md5=15c1c186f07f9b902779556580091509","The selection of optimal subset of features from high-dimensional data sets still remains a major challenge during breast cancer detection and categorization. There exist several research works regarding optimal feature subset selection from high-dimensional data sets, but the obtained results are not satisfying when multidimensional data sets (MDDs) are employed in large amount during disease analysis. In this article, an effective feature subset selection and classification method suitable for MDD is proposed. At first, the important and distinct features are extracted from the mammogram images using a Deep Neural Network with wrapper-based extraction technique. Then, a novel two-phase mutation strategy integrated with grey wolf optimizer algorithm is employed for selecting the most relevant feature subsets. Finally, a learning-based semilazy Bayesian network classifier with parallel implementation is proposed for the precise categorization of the breast cancer stages. The proposed method is executed in MATLAB platform and analyzed using mammogram images taken from MAMMOSET database. The proposed method is likened with three state-of-the-art existing feature subset selection and classification approaches for validating the efficiency of the proposed approach. For Data set 1, the proposed method shows an accuracy of 90%, 92% and 98%, which is better than the existing methods taken for comparison. Also for Data set 2, the proposed method shows an accuracy of 97%, 91% and 94%, which is much better than the accuracy achieved by the existing approaches. Thus, the proposed approach outperforms the compared existing approaches by providing better precision, recall, F-measure, specificity and accuracy. © 2021 Wiley Periodicals LLC.",,"Bayesian networks; Classification (of information); Computer aided diagnosis; Data mining; Deep neural networks; Diseases; Feature extraction; Mammography; Set theory; X ray screens; Bayesian network classifiers; Breast Cancer; Data set; Gray wolf optimization; Gray wolves; Multi-dimensional datasets; Optimisations; Semilazy bayesian network classifier; Two phase; Two-phase mutation; Clustering algorithms",Article,Scopus,2-s2.0-85120458986
"Huang A., Zhang Y., Peng J., Chen H.","55863695800;57359566800;35794147100;57359809200;","Application of Informetrics on Financial Network Text Mining Based on Affective Computing",2022,"Information Processing and Management","59","2","102822","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120410652&doi=10.1016%2fj.ipm.2021.102822&partnerID=40&md5=af469e9e32b8c3b283d2763509fe9a72","With the rapid development of internet, text data is becoming richer, but most part of them is unstructured. So compared to statistics data, the text data is more difficult to be utilized. How to apply the informetrics on financial network text mining is a supplement to the traditional research methods of finance. The paper tries to forecast exchange rate volatility through informetrics on financial network text mining by means of affective computing. We find that if the amount of informetrics on network is used during predicting, only the peak and valley values of its volatility and are synchronous with the volatility of exchange rate. While the volatility of emotional intensity of words of informetrics on network in text data can accurately predict not only the drastic volatility of exchange rate, but also the moderate volatility. © 2021 Elsevier Ltd","affective computing; Distributed computing; informetrics; SparkR","Data mining; Forecasting; Affective Computing; Exchange rate volatilities; Exchange rates; Financial networks; Informetrics; Research method; Sparkr; Statistic data; Text data; Text-mining; Finance",Article,Scopus,2-s2.0-85120410652
"Jiang H., Lei Z., Rao Y., Xie H., Wang F.L.","57218712756;57358515800;55753260800;57219619828;57358624000;","Parallel dynamic topic modeling via evolving topic adjustment and term weighting scheme",2022,"Information Sciences","585",,,"176","193",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120378789&doi=10.1016%2fj.ins.2021.11.060&partnerID=40&md5=46ed7fe24e58b3e40838039cc0b6b4f2","The parallel Hierarchical Dirichlet Process (pHDP) is an efficient topic model which explores the equivalence of the generation process between Hierarchical Dirichlet Process (HDP) and Gamma-Gamma-Poisson Process (G2PP), in order to achieve parallelism at the topic level. Unfortunately, pHDP loses the non-parametric feature of HDP, i.e., the number of topics in pHDP is predetermined and fixed. Furthermore, under the bootstrap structure of pHDP, the topic-indiscriminate words are of high probabilities to be assigned to different topics, resulting in poor qualities of the extracted topics. To achieve parallelism without sacrificing the non-parametric feature of HDP, in addition to improve the quality of extracted topics, we propose a parallel dynamic topic model by developing an adjustment mechanism of evolving topics and reducing the sampling probabilities of topic-indiscriminate words. Both supervised and unsupervised experiments on benchmark datasets show the competitive performance of our model. © 2021 Elsevier Inc.","Dynamic topic model; Parallel gibbs sampling; Term weighting scheme","Data mining; Statistics; Dynamic topic models; Generation process; Gibbs sampling; Hierarchical Dirichlet process; Nonparametrics; Parallel dynamics; Parallel gibbs sampling; Poisson process; Term weighting scheme; Topic Modeling; Benchmarking",Article,Scopus,2-s2.0-85120378789
"Hadanny A., Shouval R., Wu J., Gale C.P., Unger R., Zahger D., Gottlieb S., Matetzky S., Goldenberg I., Beigel R., Iakobishvili Z.","56604238400;36653102500;57194400210;35837808000;7202736674;7004143560;57357378200;35429334200;8856587000;9637772600;6603020069;","Machine learning-based prediction of 1-year mortality for acute coronary syndrome✰",2022,"Journal of Cardiology","79","3",,"342","351",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120343286&doi=10.1016%2fj.jjcc.2021.11.006&partnerID=40&md5=6c38b247a37bc5ea93bc92eddd35c8c5","Background: Clinical risk assessment with quantitative formal risk scores may add to intuitive physician risk assessment and are advised by the international guidelines for the management of acute coronary syndrome (ACS) patients. Most previous studies have used the binary regression/classification approach (dead/alive) for long-term mortality post-ACS, without considering the time-to-event as in survival analysis. The use of machine learning (ML)-based survival models has yet to be validated. The primary objective was to compare survival prediction performance of 1-year mortality following ACS of two newly developed ML-based models [random survival forest (RSF) and deep learning (DeepSurv)] with the traditional Cox-proportional hazard (CPH) model. The secondary objective was external validation of the findings. Methods: This was a retrospective, supervised learning data mining study based on the Acute Coronary Syndrome Israeli Survey (ACSIS) and the Myocardial Ischemia National Audit Project (MINAP). The ACSIS data were divided to train/test in a 70/30 fashion. Next, the models were externally validated on the MINAP data. Harrell's C-index, inverse probability of censoring weighting (IPCW), and the Brier-score were used for models’ performance comparison. Results: RSF performed best among the three models, with Harrell's C-index on training and testing sets reaching 0.953 and 0.924 respectively, followed by CPH multivariate selected model (0.805/0.849), CPH Univariate selected model (0.828/0.806), DeepSurv model (0.801/0.804), and the traditional CPH model (0.826/0.738). The RSF model also had the highest performance on the validation data set with 0.811 for Harrell's C-index, 0.844 for IPCW, and 0.093 for Brier score. The CPH model performance on the validation set had C-index range between 0.689 to 0.790, 0.713 to 0.826 for IPCW, and 0.094 to 0.103 Brier score. Conclusions: RSF survival predictions for long-term mortality post-ACS show improved model performance compared with the classic statistical method. This may benefit patients by allowing better risk stratification and tailored therapy, however further prospective evaluations are required. © 2021","Acute coronary syndrome; Machine learning; Mortality; Outcome; Survival","acute coronary syndrome; adult; Article; Brier score; controlled study; data mining; deep learning; female; harrell's c index; heart muscle ischemia; human; inverse probability of censoring weighting; Israeli; machine learning; major clinical study; male; middle aged; mortality; observational study; proportional hazards model; prospective study; random forest; random survival forest; retrospective study; supervised machine learning; survival prediction; task performance",Article,Scopus,2-s2.0-85120343286
"Yang Y., Li S., Zhang P.","55262412500;54403478700;57198755306;","Data-driven accident consequence assessment on urban gas pipeline network based on machine learning",2022,"Reliability Engineering and System Safety","219",,"108216","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119912543&doi=10.1016%2fj.ress.2021.108216&partnerID=40&md5=b6f7316ed9145bcb49f7eba7b9bb7970","The accidents in Urban Pipeline Network (UPN) may cause enormous economic loss and serious threats to society and environment. The daily operation and maintenance of UPN is usually associated with many aspects of data. How to take full advantage of these multi-source data in combination with advanced data mining techniques to assess the post-event risk of pipeline accidents is of great significance to management of resilient urban systems. This work first summarizes the factors affecting accident consequence of gas UPN and establishes the risk evaluation indicators. A traditional risk assessment model based on the Kent index method and the analytic hierarchy process is then employed to determine the relative risk value of each pipeline. To reduce the dependency on experts’ subjective judgements or calculation of probability events in a Bayes decision procedure, a data-driven model based on graph embedding and clustering algorithm is proposed. The Graph Convolutional Network (GCN) technique is used to extract the topological features of pipeline network as a complement to the common attribute features considering the top pipelines usually bear comparable level of risks. A case study on a real gas pipeline network consisting of more than 6500 pipelines verifies the effectiveness of the proposed model. © 2021 Elsevier Ltd","Accident consequence assessment; Clustering; Data mining; Graph convolutional network; Topological feature; Urban pipeline network","Accidents; Clustering algorithms; Convolution; Data mining; Information management; Risk assessment; Topology; Accident consequence assessment; Accident consequences; Clusterings; Consequence assessment; Convolutional networks; Gas pipeline networks; Graph convolutional network; Pipeline networks; Topological features; Urban pipeline network; Losses",Article,Scopus,2-s2.0-85119912543
"Wu T., Wang X., Qiao S., Xian X., Liu Y., Zhang L.","55210114700;57349652700;16205409900;36447999900;8879913300;57280308700;","Small perturbations are enough: Adversarial attacks on time series prediction",2022,"Information Sciences","587",,,"794","812",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119619905&doi=10.1016%2fj.ins.2021.11.007&partnerID=40&md5=5fa89ffc9318349169f02fc20d1b8abb","Time-series data are widespread in real-world industrial scenarios. To recover and infer missing information in real-world applications, the problem of time-series prediction has been widely studied as a classical research topic in data mining. Deep learning architectures have been viewed as next-generation time-series prediction models. However, recent studies have shown that deep learning models are vulnerable to adversarial attacks. In this study, we prospectively examine the problem of time-series prediction adversarial attacks and propose an attack strategy for generating an adversarial time series by adding malicious perturbations to the original time series to deteriorate the performance of time-series prediction models. Specifically, a perturbation-based adversarial example generation algorithm is proposed using the gradient information of the prediction model. In practice, unlike the imperceptibility to humans in the field of image processing, time-series data are more sensitive to abnormal perturbations and there are more stringent requirements regarding the amount of perturbations. To address this challenge, we craft an adversarial time series based on the importance measurement to slightly perturb the original data. Based on comprehensive experiments conducted on real-world time-series datasets, we verify that the proposed adversarial attack methods not only effectively fool the target time-series prediction model LSTNet, they also attack state-of-the-art CNN-, RNN-, and MHANET-based models. Meanwhile, the results show that the proposed methods achieve a good transferability. That is, the adversarial examples generated for a specific prediction model can significantly affect the performance of the other methods. Moreover, through a comparison with existing adversarial attack approaches, we can see that much smaller perturbations are sufficient for the proposed importance-measurement based adversarial attack method. The methods described in this paper are significant in understanding the impact of adversarial attacks on a time-series prediction and promoting the robustness of such prediction technologies. © 2021 Elsevier Inc.","Adversarial attacks; Adversarial time series; Time-series data; Time-series prediction","Data mining; Deep learning; Forecasting; Image processing; Adversarial attack; Adversarial time series; Attack methods; Performance; Prediction modelling; Real-world; Small perturbations; Time series prediction; Time-series data; Times series; Time series",Article,Scopus,2-s2.0-85119619905
"Li X., Huang T., Cheng K., Qiu Z., Sichao T.","57225920592;57225870296;57191995401;57188589725;57210745118;","Research on anomaly detection method of nuclear power plant operation state based on unsupervised deep generative model",2022,"Annals of Nuclear Energy","167",,"108785","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119438000&doi=10.1016%2fj.anucene.2021.108785&partnerID=40&md5=688b1b50e40457ce3448ffdaa58ed0fc","In the field of traditional industrial control, anomaly detection method is mainly used to identify data items that do not match the normal operation state of the system. The traditional machine learning algorithm needs the transient operation data of normal and accident conditions to identify the abnormal state of nuclear power plant. The transient operation data of nuclear power plant during normal condition is sufficient, but it is lacks of transient operation data in accident conditions. To solve the above problems, an abnormal operation state detection method of nuclear power plant based on unsupervised deep generative model is established by using Variational Auto Encoders (VAE) and Isolation Forest (iForest) in this paper. The biggest advantage of this method is that it can only make use of the normal operation data of the nuclear power plant to make the nuclear power plant control system effectively identify whether the current state of nuclear power plant is normal operation or in accident condition. In the unsupervised deep generative model, VAE is used for data preprocessing, and iForest is used to identify abnormal operation data. Then, the method is verified in seven variable and accident conditions, such as power reduction condition, steam generator tube rupture accident and so on. The verification results show that the anomaly detection method can recognize the current abnormal condition immediately when the accident happens. And the time consumed to identify a group of operation parameters corresponding to the operation state of the nuclear power plant is about 3 ms, which can satisfy the real-time requirements of the control system. Therefore, the anomaly detection method based on unsupervised deep generative model can distinguish the normal or abnormal operation state of the nuclear power plant in real time and effectively, and provide judgment basis for accident classification and subsequent rescue. © 2021","Anomaly detection; Data mining of nuclear power plant; Isolation forest; UnsVupervised deep generative model; Variational auto encoder","Anomaly detection; Control systems; Data mining; Forestry; Learning algorithms; Machine learning; Nuclear energy; Nuclear fuels; Steam generators; Anomaly detection; Anomaly detection methods; Auto encoders; Data mining of nuclear power plant; Generative model; Isolation forest; Normal operations; Operation state; Unsvupervised deep generative model; Variational auto encoder; Nuclear power plants",Article,Scopus,2-s2.0-85119438000
"Zhang S., Lai H., Chen W., Zhang L., Lin X., Xiao R.","53664804100;57346837600;57346837700;57346516600;57198576130;15129160700;","VBLSH: Volume-balancing locality-sensitive hashing algorithm for K-nearest neighbors search",2022,"Information Sciences","587",,,"774","793",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119411527&doi=10.1016%2fj.ins.2021.11.006&partnerID=40&md5=398e9ee118eede03af75f787e55d8c55","K-nearest neighbors search (K-NNS) is a fundamental problem in many areas of machine learning and data mining. In an attempt to solve NNS problems by locality-sensitive hashing (LSH)-based algorithms and avoid the F1-trap, in this paper, we theoretically prove the necessity of maintaining close edges of each dimension, the independence of the partition location and the precision of retrieval. Based on the theories, a new method is proposed for constructing the hash function family. Then, a volume-balancing locality-sensitive hashing (VBLSH) algorithm is implemented. In this algorithm, the problem of constructing the hash function family is transformed into an optimization problem under a fixed number of hypercubes. By solving the optimization problem, different hyperplane intervals are set for each dimension to ensure that the hash functions can evenly segment the data space, where each segmented hypercube has a close volume and close side lengths. Finally, the analysis of the algorithm shows that the VBLSH algorithm has a high index coding efficiency and hash code calculation efficiency. The experimental results show that the VBLSH algorithm can simultaneously take into account the efficiency, precision and recall rate, and avoid the F1-trap; it shows excellent effectiveness on 4 datasets. © 2021 Elsevier Inc.","High-dimension massive data query; Information retrieval; K-nearest neighbors search; Locality-sensitive hashing","Balancing; Data mining; Efficiency; Geometry; Motion compensation; Nearest neighbor search; Data query; Hashing algorithms; Hashing-based algorithms; High-dimension massive data query; Higher dimensions; Hyper-cubes; K nearest neighbor search; Locality sensitive hashing; Massive data; Optimization problems; Hash functions",Article,Scopus,2-s2.0-85119411527
"Hassani H., Ershadi M.J., Mohebi A.","57340348000;34876508300;14834280200;","LVTIA: A new method for keyphrase extraction from scientific video lectures",2022,"Information Processing and Management","59","2","102802","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119204596&doi=10.1016%2fj.ipm.2021.102802&partnerID=40&md5=5616708bb7d769201774c8426dfb5db7","Due to the growth of technology, the expansion of communication infrastructure and crises of COVID-19 pandemic, e-learning and virtual education is expanding. One of the best ways to access and organize these information is indexing using automatic intelligent methods. Indexing requires assigning keywords or keyphrases to each video, to represent its content. The main focus of this research is to propose an approach by which appropriate keyphrases are assigned to scientific video lectures. For this purpose, a new algorithm called LVTIA, Lecture Video Text mining-base Indexing Algorithm, is proposed in which the textual content of video frames along with the text extracted from audio signal are merged together, and a new keyphrase extraction method is proposed. The proposed method considers new local and global features for each candidate phrases, along with a new feature reflecting the occurrence of each phrase in the audio signals or video frames. The method is implemented using five distinct data sets in English and Persian. The results are evaluated based on precision, recall, F1-measure and MAP@K metrics and compared with some of the well-known keyphrase extraction algorithms. Based on the results, the best MAP@K for English videos is related to LVTIA algorithm with the values of, 0.7912, 0.8069, 0.8069 for k=5,10,15, respectively. In addition, LVTIA is able to provide best MAP@K for Persian videos which are 0.6367, 0.6866, 0.6874 for k=5,10,15, respectively. According to Friedman nonparametric statistical test, the performance of different algorithms in precision, recall, F1-measure metrics, are statistically different from LVTIA as well. © 2021","Keyphrase extraction; Keyword extraction; Multimedia indexing; Text mining; Video lecture indexing","Data mining; Indexing (of information); Text processing; Audio signal; Communication crisis; Key-phrase; Key-phrases extractions; Keywords extraction; Multimedia indexing; Persians; Video frame; Video lecture indexing; Video lectures; Extraction",Article,Scopus,2-s2.0-85119204596
"Chen H., Du H., Shi B., Shan W., Hou J.","15062327200;57329439300;57261712600;57216611000;57329053600;","Mechanical properties and strength criterion of clayey sand reservoirs during natural gas hydrate extraction",2022,"Energy","242",,"122526","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118864922&doi=10.1016%2fj.energy.2021.122526&partnerID=40&md5=fed97726e627e09c66abeb6fab0241dd","To study the changes in mechanical properties of hydrate bearing clayey sand reservoirs during the mining process and explain the mechanism from a microscopic perspective, soil samples were artificially configured based on the parameters of SH7B drilling cores in Shenhu area of South China Sea. Natural gas hydrate was formed in the samples, and the mining process was simulated by hydrate decomposition under depressurization. Multistage triaxial tests were carried out, and scanning electron microscopy was conducted on samples after triaxial experiment. The results showed that the shear strengths of samples decreased with the decomposition of hydrate, and the effective cohesion was positively correlated with hydrate saturation degree. An expression for the linear growth of shear strength with effective confining pressure and hydrate saturation degree was established. In addition, hydrate saturation degree was introduced as a parameter into the Mohr-Coulomb criterion to establish a strength criterion applicable to clayey sand reservoirs in the mining process. With the decrease of hydrate saturation degree, the microstructures of samples after shearing were gradually compacted, indicating that the hydrate and soil particles bore the external load together during shear test, which maintained the morphology of pores and improved the shear strength of samples. © 2021 Elsevier Ltd","Methane hydrate bearing sediments; Microstructure; Mining process; Mohr-coulomb strength criterion; Multistage triaxial tests","Bearings (machine parts); Data mining; Gas hydrates; Hydration; Methane; Morphology; Natural gas; Scanning electron microscopy; Soil testing; Soils; Clayey sands; Hydrate bearing sediments; Hydrate saturation; Methane hydrate bearing sediment; Methane hydrates; Mining process; Mohr-Coulomb strength criterion; Multi-stages; Multistage triaxial test; Saturation degree; Microstructure; confining pressure; decomposition; extraction; gas hydrate; mining; saturation; shear strength; China; Pacific Ocean; Shenhu; South China Sea; South China Sea",Article,Scopus,2-s2.0-85118864922
"Sheng H., Qi X.","35291127800;57216250976;","Application of new digital signal processing technology based on distributed cloud computing in electronic information engineering",2022,"Future Generation Computer Systems","128",,,"443","450",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118854027&doi=10.1016%2fj.future.2021.10.032&partnerID=40&md5=ff6e067f8db71ed64d832113c27207ed","The traditional digital signal processing technology in electronic information engineering has many issues, such as redundant data, low data utilization and so on. To end these issues, this paper proposed a new digital signal processing technology based on distributed cloud computing in electronic information engineering. From the data collection, data analysis, data classification, data mining, effective information storage and other aspects of conventional digital signal, through relying on distributed cloud computing method and intelligent gradient tracking algorithms to achieve efficient processing of digital signal, Proportional Integral Derivative (PID) control strategy is used to evaluate the intelligence degree of each link in the digital signal processing technology. This method can realize the adaptive regulation of data collection and storage in the process of digital signal processing, and realize diversified analysis and intelligent matching. Through the distributed cloud computing to achieve the rapid control of the system storage module, so that the database can improve the work efficiency, reduce the power consumption cost of the system in the process of data operation, and improve the efficiency of digital signal processing. The experimental results show that the digital signal processing system based on distributed cloud computing and intelligent gradient tracking algorithms has the advantages of high computing efficiency, high accuracy and good stability. © 2021 Elsevier B.V.","Digital signal processing; Distributed cloud computing; Electronic information engineering; Proportional integral derivative control","Classification (of information); Cloud computing; Data acquisition; Data mining; Digital signal processing; Digital storage; Energy efficiency; Proportional control systems; Tracking (position); Two term control systems; Cloud-computing; Data collection; Distributed cloud computing; Distributed clouds; Electronic information; Electronic information engineering; Information engineerings; Proportional integral derivative control; Signal processing technologies; Technology-based; Distributed cloud",Article,Scopus,2-s2.0-85118854027
"Rahman M.A., Duradoni M., Guazzini A.","57195672725;57191051069;23090651600;","Identification and prediction of phubbing behavior: a data-driven approach",2022,"Neural Computing and Applications","34","5",,"3885","3894",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118561081&doi=10.1007%2fs00521-021-06649-5&partnerID=40&md5=c97861a6822e7ca231da6f2f30c2b681","Research on Phubbing has received a lot of attention in recent years from the research community. However, the studies conducted are mainly based on linear statistics, which is a very conservative method for data analysis. To overcome this limitation, we adopted a data mining and machine learning-based approach to identify the patterns related to Phubbing behavior. We developed several models on online survey data that we collected for our analysis purposes. The results highlighted that addiction measures fail to predict Phubbing fully. Indeed, Phubbing appeared to be linked in a nonlinear way to both Information and Communication Technology (ICT) measures that do not imply a dysfunctional use of technology and social anxiety. Moreover, the machine learning approach appeared more suitable than traditional linear statistics methods to predict Phubbing, as highlighted by a much higher explained variance. Phubbing is not solely attributable to addiction dynamics. Phubbing is indicated by a series of predictors that cannot be reduced to addiction (e.g., age, social anxiety, ICT services owned). Modeling procedures able to account for nonlinearity are also required to accurately assessing users’ Phubbing levels. The patterns produced by our modeling procedure may help scholars in accounting for phubbing definition, detection, and prediction more accurately. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Accuracy; Addiction; Classification; Decision tree; Phubbing","Data mining; Forecasting; Machine learning; Accuracy; Addiction; Data-driven approach; Information and Communication Technologies; Modeling procedure; Online surveys; Phubbing; Research communities; Social anxieties; Survey data; Decision trees",Article,Scopus,2-s2.0-85118561081
"Jangra S., Toshniwal D.","57216562396;8683737500;","Efficient algorithms for victim item selection in privacy-preserving utility mining",2022,"Future Generation Computer Systems","128",,,"219","234",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118132128&doi=10.1016%2fj.future.2021.10.008&partnerID=40&md5=1ca27e824a74cd4ab6455060b4d8fb4a","High-utility itemset mining has evolved as an essential and captivating research topic. It aims to extract the patterns/itemsets having high utility value; hence, they are called high utility itemsets (HUIs). From a business perspective, a utility can be the benefit associated with the sale of a particular item or the usefulness or satisfaction that a customer experiences from a product. The economic utilities are helpful to evaluate the drivers behind a customer's purchase decision. The advances in information technology have enabled us to access the datasets related to various domains like health care, stock market, market-basket, education and bioinformatics. Companies strive to increase the utility value of their products and share their customer's transactions data to extract high utility patterns to achieve global customer insights. However, this can lead to massive security and privacy risk if their competitors misuse the patterns that can disclose their confidential information. Privacy-preserving utility mining (PPUM) is a branch of privacy-preserving data mining (PPDM) that presents various algorithms which intend to hide sensitive high utility itemsets (SHUIs) and maintain a balance between utility-maximizing and privacy-preserving. In this paper, two SHUIs hiding algorithms, MinMax and Weighted, are proposed with three variants of each algorithm. Experiments on various datasets show that proposed algorithms perform better than the existing SHUIs hiding algorithms as fewer distortions of non-sensitive knowledge occur. This study uses six performance evaluating metrics to assess the proposed algorithms against compared algorithms. © 2021 Elsevier B.V.","Data sanitization; High utility itemsets; Knowledge hiding technique; Privacy-preserving utility mining; Sensitive pattern","Commerce; Customer satisfaction; Data mining; Data privacy; Data sanitization; High utility itemsets; Item selection; Knowledge hiding technique; Privacy preserving; Privacy-preserving utility mining; Sanitization; Sensitive pattern; Utility mining; Utility values; Sales",Article,Scopus,2-s2.0-85118132128
"Le H.L., Neri F., Triguero I.","57219281973;8729046500;36601691900;","SPMS-ALS: A Single-Point Memetic structure with accelerated local search for instance reduction",2022,"Swarm and Evolutionary Computation","69",,"100991","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118102792&doi=10.1016%2fj.swevo.2021.100991&partnerID=40&md5=574831c4ae817e7864c42416d72c9648","Real-world optimisation problems pose domain specific challenges that often require an ad-hoc algorithmic design to be efficiently addressed. The present paper investigates the optimisation of a key stage in data mining, known as instance reduction, which aims to shrink the input data prior to applying a learning algorithm. Performing a smart selection or creation of a reduced number of samples that represent the original data may become a complex large-scale optimisation problem, characterised by a computationally expensive objective function, which has been often tackled by sophisticated population-based metaheuristics that suffer from a high runtime. Instead, by following the Ockham's Razor in Memetic Computing, we propose a Memetic Computing approach that we refer to as fast Single-Point Memetic Structure with Accelerated Local Search (SPMS-ALS). Using the k-nearest neighbours algorithm as base classifier, we first employ a simple local search for large-scale problems that exploits the search logic of Pattern Search, perturbing an n-dimensional vector along the directions identified by its design variables one by one. This point-by-point perturbation mechanism allows us to design a strategy to re-use most of the calculations previously made to compute the objective function of a candidate solution. The proposed Accelerated Local Search is integrated within a single-point memetic framework and coupled with a resampling mechanism and a crossover. A thorough experimental analysis shows that SPMS-ALS, despite its simplicity, displays an excellent performance which is as good as that of the state-of-the-art while reducing up to approximately 85% of the runtime with respect to any other algorithm that performs the same number of function calls. © 2021","Classification; Data science; Instance reduction; Memetic algorithm; Ockham'S razor in memetic computing; Pattern search","Classification (of information); Computation theory; Data mining; Data reduction; Local search (optimization); Nearest neighbor search; Structural design; Structural optimization; % reductions; Instance reduction; Local search; Memetic; Memetic algorithms; Memetic computing; Ockham'S razor in memetic computing; Optimization problems; Pattern search; Single point; Population statistics",Article,Scopus,2-s2.0-85118102792
"Tékouabou S.C.K., Alaoui E.A.A., Chabbar I., Toulni H., Cherif W., Silkan H.","57215085805;57204093522;57217050802;55842206000;56288579200;36998623600;","Optimizing the early glaucoma detection from visual fields by combining preprocessing techniques and ensemble classifier with selection strategies",2022,"Expert Systems with Applications","189",,"115975","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117908294&doi=10.1016%2fj.eswa.2021.115975&partnerID=40&md5=969a8725f5448e2d78b8428bd45841d5","Artificial Intelligence is booming and many issues of research are being explored to improve technical performance in health systems. But also making them suitable for targeted medical practices. Their cost must also be justified by real added value for medical practitioners and patients. Extracting accurate information from datasets usually comes up against the amount of data and its distribution, which greatly affect the performance of the classifiers. Unbalanced classes or insignificant data features do not provide information for classifiers. Medical data like those of visual field (VF) most suffer from these problems. These factors limit the performance of individual classifiers. However, ensemble methods such as the bagging classifier (BC) can overcome these limitations and return good performances. BC is simple to process and very favorable to the combination with dynamic/static selection strategies (BC-DS/SS) which considerably improves its performance. By remaining sensitive to the problem of data distribution, this combination requires a fusion with pre-processing techniques such as feature selection and data rebalancing to be efficient. Thus, combining pre-processing techniques with the BC-DS/SS ensemble classifiers would allow to extract more accurate information from VF datasets. The stake of this classifier combining pre-processing techniques and ensemble methods with selection strategies named C2PEMS2 (C2 relates to Classifier Combining, PEM refers to Pre-processing and Ensemble Methods and S2 refers to Selection Strategies) consists of: (1) optimizing the performances while reducing the over-fitting, (2) saving in processing time and more importantly (3) predicting more efficiently the targeted class which often is the minority in unbalanced datasets. The experiments of our approach on VF datasets allowed to predict early glaucoma with greater efficiency compared to the state of the art. © 2021 Elsevier Ltd","Bagging; Dynamic selection; Ensemble classifier; Features selection; Glaucoma; Static selection; Unbalanced data; Visual fields","Classification (of information); Data mining; Feature extraction; Medical problems; Vision; Bagging; Dynamic selection; Ensemble methods; Ensemble-classifier; Features selection; Performance; Pre-processing techniques; Static selection; Unbalanced data; Visual fields; Ophthalmology",Article,Scopus,2-s2.0-85117908294
"Pei H., Yang B., Liu J., Chang K.C.-C.","56719189500;57200878440;35230430700;7407034942;","Active Surveillance via Group Sparse Bayesian Learning",2022,"IEEE Transactions on Pattern Analysis and Machine Intelligence","44","3",,"1133","1148",,20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117854437&doi=10.1109%2fTPAMI.2020.3023092&partnerID=40&md5=56f62fece24f67cf4914d0055c0fa545","The key to the effective control of a diffusion system lies in how accurately we could predict its unfolding dynamics based on the observation of its current state. However, in the real-world applications, it is often infeasible to conduct a timely and yet comprehensive observation due to resource constraints. In view of such a practical challenge, the goal of this work is to develop a novel computational method for performing active observations, termed active surveillance, with limited resources. Specifically, we aim to predict the dynamics of a large spatio-temporal diffusion system based on the observations of some of its components. Towards this end, we introduce a novel measure, the boldsymbolgamma γ value, that enables us to identify the key components by means of modeling a sentinel network with a row sparsity structure. Having obtained a theoretical understanding of the boldsymbolgamma γ value, we design a backward-selection sentinel network mining algorithm (SNMA) for deriving the sentinel network via group sparse Bayesian learning. In order to be practically useful, we further address the issue of scalability in the computation of SNMA, and moreover, extend SNMA to the case of a non-linear dynamical system that could involve complex diffusion mechanisms. We show the effectiveness of SNMA by validating it using both synthetic datasets and five real-world datasets. The experimental results are appealing, which demonstrate that SNMA readily outperforms the state-of-the-art methods. © 1979-2012 IEEE.","automatic relevance determination; diffusion; dynamical systems; Epidemic dynamics; sensor deployment","Data mining; Diffusion; Dynamics; Linear control systems; 'current; Automatic relevance determination; Bayesian learning; Diffusion systems; Epidemic dynamics; Group sparse; Mining algorithms; Sensors deployments; Sparse bayesian; Unfoldings; Dynamical systems; algorithm; article; Bayesian learning; diffusion; mining; theoretical study",Article,Scopus,2-s2.0-85117854437
"Erfanian P.M.A.Y., Cami B.R., Hassanpour H.","57310658900;57203168058;56919429700;","An evolutionary event detection model using the Matrix Decomposition Oriented Dirichlet Process",2022,"Expert Systems with Applications","189",,"116086","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117827793&doi=10.1016%2fj.eswa.2021.116086&partnerID=40&md5=6fdc273f1795be52a9195af4b35f3dbb","With the huge expansion of user generated content on social networks, event detection has emerged as a major challenge and source of knowledge discovery. This knowledge is employed in different applications such as recommender systems, crisis management systems, and decision support systems. Dynamicity, overlapping, and evolutionary behavior are the most important issues in event detection. This paper proposes a novel evolutionary model for event detection to capture the dynamism and evolving behavior of events. The proposed method uses a matrix decomposition technique and a Dirichlet Process to detect events and handle their dynamicity. This model consists of two components, namely preliminary event detection and event evolvement tracking. The former component extracts preliminary events from the available data using the matrix decomposition method. Then, subsequent data is employed into a Non-Parametric Bayesian Network, namely Dirichlet Process Mixture Model to evolve the preliminary events. During the evolvement process, data may migrate between extracted events or new events may be discovered. The experimental results and comparisons with several recently developed approaches show the superiority of the proposed approach, and its ability to capture the evolutionary behavior of events over time. © 2021","Event detection; Event evolution; Incremental clustering; Social network analysis; Topic modeling","Artificial intelligence; Bayesian networks; Data mining; Matrix algebra; Detection models; Dirichlet process; Event evolutions; Events detection; Evolutionary events; Incremental clustering; Matrix decomposition; Social Network Analysis; Topic Modeling; User-generated; Decision support systems",Article,Scopus,2-s2.0-85117827793
"Sardari S., Nakisa B., Rastgoo M.N., Eklund P.","57222106052;56156792800;56481533900;7101693123;","Audio based depression detection using Convolutional Autoencoder",2022,"Expert Systems with Applications","189",,"116076","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117727084&doi=10.1016%2fj.eswa.2021.116076&partnerID=40&md5=e660e337e27824d6b45ab2a1aec73662","Depression is a serious and common psychological disorder that requires early diagnosis and treatment. In severe episodes the condition may result in suicidal thoughts. Recently, the need for building an effective audio-based Automatic Depression Detection (ADD) system has sparked the interest of the research community. To date, most of the reported approaches to recognize depression rely on hand-crafted feature extraction for audio data representation. They combine wide variety of audio-related features to improve the classification performance. However, combining many hand-crafted features including relevant and less-relevant can enlarge the feature space which can lead to high-dimensionality issues as not all the features would carry significant information regarding depression. Having high number of features can make the pattern recognition more difficult and increase the risk of overfitting. To overcome these limitations, an audio-based framework of depression detection which includes an adaptation of a deep learning (DL) technique is proposed to automatically extract the highly relevant and compact feature set. This proposed framework uses an end-to-end Convolutional Neural Network-based Autoencoder (CNN AE) technique to learn the highly relevant and discriminative features from raw sequential audio data, and hence to detect depressed people more accurately. In addition, to address the sample imbalance problem we use a cluster-based sampling technique which highly reduces the risk of bias towards the major class (non-depressed). To evaluate the performance and effectiveness of the proposed pipeline, we perform the experiments on Distress Analysis Interview Corpus-Wizard of Oz (DAIC-WOZ) dataset and compare them with the hand-crafted feature extraction methods and other outstanding studies in this domain. The results show that proposed method outperforms other well-known audio-based ADD models with at least 7% improvement in F-measure for classifying depression. © 2021 Elsevier Ltd","Audio depression detection; Convolutional Autoencoder; Early depression detection; Semi-supervised learning","Convolutional neural networks; Data mining; Deep learning; Diagnosis; Extraction; Feature extraction; Audio data; Audio depression detection; Audio-based; Auto encoders; Condition; Convolutional autoencoder; Early depression detection; Early diagnosis; Psychological disorders; Relevant features; Convolution",Article,Scopus,2-s2.0-85117727084
"Liu W., Wang Z., Li R., Wu T.","57223964841;57196352047;55491292900;57295032100;","A bibliometric analysis of mountain ecosystem services, 2000–2019",2022,"Environmental Science and Pollution Research","29","11",,"16633","16652",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117026383&doi=10.1007%2fs11356-021-16766-2&partnerID=40&md5=2f88d4b3f68a3852b8f3af9f6ef9cf46","Research on mountain ecosystem services (MES) under the influence of climate change and human activities has gradually become the focus of academic attention in recent years. Here, this study analyzes the research hotspots and frontiers of this field based on metrics including main research forces, core journals and papers, research hotspots and topics by using the methods of bibliometrics and text mining. The results revealed the following: (1) the number of papers is increasing rapidly in recent years. From 2015 to 2019, 929 papers were published, with an average of 185 papers per year. But the average cited times of those papers is declining, dropped from 6.01 in 2016 to 4.2 in 2019. The USA, UK, and China rank the top three of the number of papers. Univ Maryland, Univ Oxford and Univ Wisconsin have the greatest influence, with an average of more than 77 citations per paper; (2) The most cited journals are PNAS, WETLANDS, ECOLOGY, AND SOCIETY, which are cited 191.54, 53.91, and 40.00 respectively. Most papers were published in OA journals including SUSTAINABILITY, WATER, Forests since 2017. Ten core papers undertaking knowledge transfer in this field have been identified; (3) analysis of the keywords found a new trend of integration of natural science and humanities. In two development stages of 2000–2014 and 2015–2019, the research hotspots mainly focused on mountain water resources, forest resources, land resources and the impact of climate change and human activities, and there are obvious differences and characteristics in different stages. The hotspot worthy of attention in the near future is the assessment of mountain ecosystem services capacity and value. This is the first comprehensive visualization and analysis of the research hotspots and trends of mountain ecosystem services. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Bibliometrics; Ecological protection; Ecosystem services; Mountain ecosystem; Research hotspots; Sustainable management","bibliography; climate change; data mining; ecosystem service; environmental protection; forest resource; human activity; mountain region; research work; sustainability; water resource; China; United Kingdom; United States; bibliometrics; ecology; ecosystem; forest; human; publication; Bibliometrics; Ecology; Ecosystem; Forests; Humans; Publications",Article,Scopus,2-s2.0-85117026383
"Ujager F.S., Mahmood A., Usman M., Rathore M.S.","37036340000;57197949015;57207835774;57282528200;","A Hierarchical Energy Conservation Framework (HECF) of Wireless Sensor Networks by Temporal Association Rule Mining for Smart Buildings",2022,"Egyptian Informatics Journal","23","1",,"137","147",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116251229&doi=10.1016%2fj.eij.2021.09.001&partnerID=40&md5=ab36dbb056dcd46541c68c9de43279ed","The challenge of extending the sensor's energy consumption is a key research issue in Wireless Sensor Networks (WSNs). Recently, association rule mining has proven to be a potential candidate to prolong the lifetime of sensor nodes in WSN. However, temporal correlations of the contextual values are not taken into account which is useful for the sensors to conserve their energy. Similarly, association rules mining at different tiers of the network has not been considered to reduce the number of transmission messages, by avoiding redundant data which is the major cause of the energy drain of sensors. In this paper, a novel Hierarchical Energy Conservation Framework (HECF) is proposed which aims to conserver's energy at each layer of a network by using the Hierarchical Temporal Association Rule Mining in multistory buildings. In hierarchal setup, each floor of the building can conserve energy locally at the local sink and conserve entire network energy at the global sink by using temporal association rule mining at different tiers of the network. The HECF is ideal for large multistory buildings where energy conservation is a major issue along with effective monitoring and system performance. The result shows that HECF outperformed other classical energy conversation methods such as LEACH-C and RR-Schedule-Buffer in terms of energy consumption. It extends 16% network lifetime, also 20% less number of messages during data transmission, which is a remarkable improvement for sensors energy conservation. © 2022","Energy conservation; Hierarchal temporal association rule mining and Frequent patterns; Wireless Sensor Network (WSN)","Association rules; Data mining; Energy conservation; Historic preservation; Network layers; Sensor nodes; Tall buildings; Energy; Energy-consumption; Hierarchal temporal association rule mining and frequent pattern; Multistory building; Number of transmissions; Research issues; Rule mining; Temporal association rule minings; Temporal correlations; Wireless sensor network; Energy utilization",Article,Scopus,2-s2.0-85116251229
"McGuire S.S., Gazley B., Majerus A.C., Mullan A.F., Clements C.M.","57216965215;57279062100;57211856033;57219096588;7007017269;","Impact of the COVID-19 pandemic on workplace violence at an academic emergency department",2022,"American Journal of Emergency Medicine","53",,,"285.e1","285.e5",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116045805&doi=10.1016%2fj.ajem.2021.09.045&partnerID=40&md5=145f2b3fbed3a75c5de9f7b7562b7f98","Study objectives: COVID-19 brought unique challenges; however, it remains unclear what effect the pandemic had on violence in healthcare. The objective of this study was to identify the impact of the pandemic on workplace violence at an academic emergency department (ED). Methods: This mixed-methods study involved a prospective descriptive survey study and electronic medical record review. Within our hospital referral region (HRR), the first COVID-19 case was documented on 3/11/2020 and cases peaked in mid-November 2020. We compared the monthly HRR COVID-19 case rate per 100,000 people to the rate of violent incidents per 1000 ED visits. Multidisciplinary ED staff were surveyed both pre/early-pandemic (April 2020) and mid/late-pandemic (December 2020) regarding workplace violence experienced over the prior 6-months. The study was deemed exempt by the Mayo Clinic Institutional Review Board. Results: There was a positive association between the monthly HRR COVID-19 case rate and rate of violent ED incidents (r = 0.24). Violent incidents increased overall during the pandemic (2.53 incidents per 1000 visits) compared to the 3 months prior (1.13 incidents per 1000 visits, p < .001), as well as compared to the previous year (1.24 incidents per 1000 patient visits, p < .001). Survey respondents indicated a higher incidence of assault during the pandemic, compared to before (p = .019). Discussion: Incidents of workplace violence at our ED increased during the pandemic and there was a positive association of these incidents with the COVID-19 case rate. Our findings indicate health systems should prioritize employee safety during future pandemics. © 2021 Elsevier Inc.","COVID-19; Pandemic; Staff safety; Workplace violence","Article; assault; bite; body fluid; controlled study; coronavirus disease 2019; descriptive research; electronic medical record; emergency ward; female; harassment; human; incidence; major clinical study; male; medical record review; pandemic; prospective study; punching; safety; scratching; staff safety; university hospital; verbal hostility; weapon; workplace violence; adult; case report; chi square distribution; crime victim; data mining; health care personnel; hospital emergency service; middle aged; organization and management; prevention and control; psychology; questionnaire; rehabilitation; university hospital; workplace violence; Academic Medical Centers; Adult; Chi-Square Distribution; COVID-19; Crime Victims; Data Mining; Emergency Service, Hospital; Female; Health Personnel; Humans; Male; Middle Aged; Prospective Studies; Surveys and Questionnaires; Workplace Violence",Article,Scopus,2-s2.0-85116045805
"Badawy M., Alqahtani F., Hafez H.","57205334115;57193701162;57210604711;","Identifying the risk factors affecting the overall cost risk in residential projects at the early stage",2022,"Ain Shams Engineering Journal","13","2","101586","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115971077&doi=10.1016%2fj.asej.2021.09.013&partnerID=40&md5=22b5fc2e8f512377ae6891fbfe65c90e","Many previous studies have developed models for estimating the total cost, whether in the planning stage or the early stage of the project. However, models for estimating the overall risk were proposed in the planning stage only. This paper identifies the factors affecting the overall risk in residential projects at the early stage. The 43 risk factors at the planning stage were identified using a Delphi technique. Experts summarize the 43 risk factors into four factors that can be used to predict the overall risk in the early stage of the project. A multilayer perceptron model with one hidden layer was proposed. The mean absolute error rate for the proposed model was 10%. Risk factors can be used to develop a model to predict the impact of overall risk on project cost at the early stage. The developed model helps stakeholders decide whether the project should continue or be terminated. © 2021","Artificial Neural Network (ANN); Data mining; Multilayer perceptron; Overall risk; Residential projects","Housing; Multilayer neural networks; Multilayers; Risk perception; Artificial neural network; Delphi technique; Developed model; Hidden layers; Multilayers perceptrons; Overall costs; Overall risk; Planning stages; Residential projects; Risk factors; Data mining",Article,Scopus,2-s2.0-85115971077
"Keyvanpour M.R., Barani Shirzad M., Mahdikhani L.","24476189600;57170244800;57209637949;","WARM: a new breast masses classification method by weighting association rule mining",2022,"Signal, Image and Video Processing","16","2",,"481","488",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111652098&doi=10.1007%2fs11760-021-01989-0&partnerID=40&md5=16d5ddcb8a666fb7ad27ead64248fc22","Breast cancer is the growth of a malignant tumor in the breast. The incidence of this disease in women has increased significantly in recent years. Currently, early detection is an important factor in cancer treatment. The most effective method for early detection is through mammography’s images. The computer-aided diagnosis systems are essential to help searching for suspicious signs, or classifying lesions in benign or malignant types. In this paper, a new method is designed for mass detection and classification based on weighted association rule mining (WARM). The main purpose of this study is to focus on the segmentation and classification and to provide a solution to optimize the accuracy of detection and classification of masses in mammography images to classify the masses in mammography images into two classes, benign and malignant. The results show proposed model in terms of accuracy, sensitivity and specificity achieved superior in comparison with several baselines. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Association rule mining; Breast cancer; Mammography; Mass classification; Mass detection","Association rules; Data mining; Diseases; Image segmentation; Breast Cancer; Classification methods; Computer aided diagnosis systems; Malignant tumors; Mammography images; Mass detection; Sensitivity and specificity; Weighted association rules; Computer aided diagnosis",Article,Scopus,2-s2.0-85111652098
"Muthusamy D., Rakkimuthu P.","56448906700;57226146972;","Steepest deep bipolar Cascade correlation for finger-vein verification",2022,"Applied Intelligence","52","4",,"3825","3845",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110660094&doi=10.1007%2fs10489-021-02619-5&partnerID=40&md5=12460e8632280b563e0ea096b36daca5","Finger-vein verification is a considerable problem to be addressed in a biometric system. A lot of research works have been intended for finger-vein authentication with aid of diverse data mining algorithms. But, the verification accuracy using conventional algorithms was minimal. Also, the time complexity involved during the finger-vein verification was maximal. To overcome the above drawbacks, Steepest Deep Bipolar Cascade Correlative Machine Learning (SDBCCML) technique is proposed. The proposed SDBCCML technique is designed to efficiently perform the finger-vein verification process when considering the large size of the dataset as input. The proposed SDBCCML technique contains three main components namely input, hidden, and output units for effective finger-vein authentication. The input unit in the proposed SDBCCML technique takes a number of finger vein images as input and then sent it to the hidden units. The proposed SDBCCML technique employs more numbers of hidden units with aiming to deeply learn the input finger vein images and thereby find the significant vein features by using the Gabor filter. Subsequently, the discovered vein features at hidden units are forwarded to the output unit. In the proposed SDBCCML technique, the output unit applies a bipolar activation function that compares the extracted vein features with pre-stored templates in the dataset. After that, the output unit gives the verification result. If the output unit result is +1, then the input finger vein image is classified as an authorized person. If the output unit result is −1, then the input finger vein image is classified as an unauthorized person. Thus, the main contribution of the proposed SDBCCML technique increases the authentication performance of finger-vein with higher accuracy and minimal time. The simulation of the proposed SDBCCML technique is conducted using metrics such as accuracy, time complexity, error rate, F-Score, and space complexity for a diverse number of finger-vein images. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Bipolar activation function; Cascade correlation; Deep learning; Finger-vein verification; Residual error","Authentication; Biometrics; Data mining; Filtration; Gabor filters; Large dataset; Activation functions; Biometric systems; Conventional algorithms; Data mining algorithm; Number of fingers; Space complexity; Verification process; Verification results; Image classification",Article,Scopus,2-s2.0-85110660094
"Ishita S.Z., Ahmed C.F., Leung C.K.","57202948234;24337639400;7402612526;","New approaches for mining regular high utility sequential patterns",2022,"Applied Intelligence","52","4",,"3781","3806",,5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110302475&doi=10.1007%2fs10489-021-02536-7&partnerID=40&md5=fff3412b8ad5928a7f6bdbe6bd504607","Regular pattern mining has been emerged as one of the promising sub-domains of data mining by discovering patterns with regular occurrences throughout a complete database. In contrast, utility-based pattern mining considers non-binary frequencies of items along with their importance values, and hence reveals more significance than traditional frequent pattern mining. Though regular patterns carry interesting knowledge, considering the utility values of the patterns would unveil more interesting and practical information. In sequence databases, the task of mining regular high utility patterns is more useful and challenging. In the recent time of big data, handling the incremental nature of databases to avoid mining from scratch when new updates appear, will bring effective results in a lot of applications. Moreover, databases can be dynamically updated in the form of data streams where new batches of data are added to the database at a higher rate. A window consisting of several recent batches can be of great interest to some end-users. To address all these important problems, here, we first introduce the concept of regular high utility sequential patterns and develop an algorithm for mining these patterns from static databases. Afterwards, we extend our algorithm to mine regular high utility sequential patterns from incremental databases and sliding-window based data streams. These two approaches produce approximate results in order to generate our intended patters faster and thus boost the performance. Extensive performance analyses of all the algorithms are observed over several real-life datasets and impressive results are found compared to the existing research. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Data mining; Data streams; High utility pattern; Incremental databases; Regular pattern; Sequential pattern","Data streams; Database systems; Approximate results; Frequent pattern mining; Incremental database; Performance analysis; Real life datasets; Sequence database; Sequential patterns; Sliding window-based; Data mining",Article,Scopus,2-s2.0-85110302475
"Singh K., Kumar R., Biswas B.","56449820100;57226848514;56343485400;","High average-utility itemsets mining: a survey",2022,"Applied Intelligence","52","4",,"3901","3938",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110297284&doi=10.1007%2fs10489-021-02611-z&partnerID=40&md5=bf871a36aaa743edf130b799938d6196","HUIM (High utility itemsets mining) is a sub-division of data mining dealing with the task to obtain promising patterns in the quantitative datasets. A variant of HUIM is to discover the HAUIM (High average-utility itemsets mining) where average-utility measure is used to obtain the utility of itemsets. HAUIM is the refined version of FIM (Frequent itemset mining) problem and has various applications in the field of market basket analysis, bio-informatics, text mining, network traffic analysis, product recommendation and e-learning among others. In this paper, we provide a comprehensive survey of the state-of-the-art methods of HAUIM to mine the HAUIs (High average-utility itemsets) from the static and dynamic datasets since the induction of the HAUIM problem. We discuss the pros and cons of each category of mining approaches in detail. The taxonomy of HAUIM is presented according to the mining approaches. Finally,various extensions, future directions and research opportunities of HAUIM algorithms are discussed. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Data mining; Frequent itemset; High average-utility itemsets; High utility itemsets; Pattern mining; Static and dynamic datasets; Utility mining","Surveys; Average utilities; Frequent itemset mining; High utility itemsets; Market basket analysis; Network traffic analysis; Product recommendation; Research opportunities; State-of-the-art methods; Text mining",Article,Scopus,2-s2.0-85110297284
"İlhan N., Demir Yetiş A., Yeşilnacar M.İ., Atasoy A.D.S.","57225702375;55993860800;9636041000;24553864900;","Predictive modelling and seasonal analysis of water quality indicators: three different basins of Şanlıurfa, Turkey",2022,"Environment, Development and Sustainability","24","3",,"3258","3292",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108645973&doi=10.1007%2fs10668-021-01566-y&partnerID=40&md5=ff46271e19827d91a1eb7e4093de34d7","The objective of this paper is twofold. First; we demonstrate the application of data mining techniques to predict quality indicators (TDS, Hardness, Na, Cl, SO4) of groundwater data measured in three different basins of Şanlıurfa. The determination of the potable water classes was predicted using six well-known classifiers with respect to the concentrations of five groundwater quality indicators collected from a total of 1240 sampling points in these basins. Among six data mining algorithms (KNN, J48, Naive Bayes, ANN, JRip and Random Forest), JRip and J48 performed best with 99% accuracy to correctly predict the water quality class at all stations. Second; we studied the effect of seasonal variation on the level of contamination and the physico-chemical properties. According to the seasonal average F-measure values of the classifiers, only the three worst water quality classes (C4, C5, C6) were observed in the Harran Plain. There was a similar seasonal class distribution with C4 and C5 classes in Sarım–Karataş and C5 and C6 classes in Ceylanpınar Plain. The highest contamination was detected in the summer period. When compared in terms of chemical quality indicators, the groundwater situation in Ceylanpınar Plain is better than Harran Plain. The groundwater quality conditions in Sarım–Karataş are substantially similar to Ceylanpınar Plain. According to the results of Kappa statistics, Random Forest, J48 and JRip value results were “Nearly Perfect”, while for Naive Bayes, means indicate “Moderate” to “Significant” level. © 2021, The Author(s), under exclusive licence to Springer Nature B.V.","Data mining; Seasonal analysis; Water quality prediction","data mining; drinking water; groundwater; physicochemical property; sampling; seasonal variation; water quality; Harran; Harran Plain; Turkey; Turkey; Urfa",Article,Scopus,2-s2.0-85108645973
"Qiao B., Zou Z., Huang Y., Fang K., Zhu X., Chen Y.","56037855600;57222316134;57222320566;36760794900;56033548300;55721078200;","A joint model for entity and relation extraction based on BERT",2022,"Neural Computing and Applications","34","5",,"3471","3481",,5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102244346&doi=10.1007%2fs00521-021-05815-z&partnerID=40&md5=e6176af5c20726db2f84b6b57e583cf0","In recent years, as the knowledge graph has attained significant achievements in many specific fields, which has become one of the core driving forces for the development of the internet and artificial intelligence. However, there is no mature knowledge graph in the field of agriculture, so it is a great significance study on the construction technology of agricultural knowledge graph. Named entity recognition and relation extraction are key steps in the construction of knowledge graph. In this paper, based on the joint extraction model LSTM-LSTM-Bias brought in BERT pre-training language model to proposed a agricultural entity relationship joint extraction model BERT-BILSTM-LSTM which is applied to the standard data set NYT and self-built agricultural data set AgriRelation. Experimental results showed that the model can effectively extracted the relationship between agricultural entities and entities. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.","Agricultural knowledge graph; BERT; Joint extraction; Named entity recognition; Relation extraction","Agricultural robots; Agriculture; Extraction; Knowledge representation; Long short-term memory; Construction technologies; Driving forces; Entity-relationship; Extraction model; Joint modeling; Knowledge graphs; Named entity recognition; Relation extraction; Data mining",Article,Scopus,2-s2.0-85102244346
"Şahin M., Yurdugül H.","57040542700;6505489581;","Learners’ Needs in Online Learning Environments and Third Generation Learning Management Systems (LMS 3.0)",2022,"Technology, Knowledge and Learning","27","1",,"33","48",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092543252&doi=10.1007%2fs10758-020-09479-x&partnerID=40&md5=613b3b696fe8cdb12cdde6af269fae6f","Learning Management Systems are web-based systems in which learners can interact with content/learning resources and assessments, as well as other learners and instructors. LMSs have been widely used especially since the beginning of the information age. In the context of this study, the aim was to determine the expectations and needs of the learners, who are considered to be one of the most important stakeholders of the LMSs. An open-ended questionnaire and a semi-structured interview form prepared by the researchers were used as data collection tools. Content analysis was performed to analyze open-ended questions and interview data. According to the findings it was seen that learners want more entertaining and self-monitoring environments, especially with the elements of gamification. It was also seen that the learning environments have reporting and predictive capability on student achievement. Learners’ needs and expectations match with third-generation learning management systems. The third-generation learning management systems can be developed through educational data mining and learning analytics. Within the scope of this research, the learner expectations and needs were discussed in the context of the third generation learning management systems, intervention and types of intervention. © 2020, Springer Nature B.V.","Intelligent learning management systems; Learner needs; Learning analytics; Learning management systems","Computer aided instruction; Data mining; E-learning; Information management; Online systems; Surveys; Data collection tools; Educational data mining; Learning environments; Learning management system; Online learning environment; Open-ended questionnaire; Predictive capabilities; Semi structured interviews; Learning systems",Article,Scopus,2-s2.0-85092543252
"Alturki S., Hulpuș I., Stuckenschmidt H.","57219204070;36188149800;6603168933;","Predicting Academic Outcomes: A Survey from 2007 Till 2018",2022,"Technology, Knowledge and Learning","27","1",,"275","307",,4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091684595&doi=10.1007%2fs10758-020-09476-0&partnerID=40&md5=89db3c04c50dd4052b85c0398fcfd52f","The tremendous growth of educational institutions’ electronic data provides the opportunity to extract information that can be used to predict students’ overall success, predict students’ dropout rate, evaluate the performance of teachers and instructors, improve the learning material according to students’ needs, and much more. This paper aims to review the latest trends in predicting students’ performance in higher education. We provide a comprehensive background for understanding Educational Data Mining (EDM). We also explain the measures of determining academic success and highlight the strengths and weaknesses of the most common data mining (DM) tools and methods used nowadays. Moreover, we provide a rich literature review of the EDM work that has been published during the past 12 years (2007–2018) with focus on the prediction of academic performance in higher education. We analyze the most commonly used features and methods in predicting academic achievement, and highlight the benefits of the mostly used DM tools in EDM. The results of this paper could assist researchers and educational planners who are attempting to carry out EDM solutions in the domain of higher education as we highlight the type of features that the previous researches found to have significant impact on the prediction, as well as the benefits and drawbacks of the DM methods and tools used for predicting academic outcomes. © 2020, The Author(s).","Academic achievement; Educational data mining; Higher education; Prediction","Forecasting; Students; Academic achievements; Academic performance; Educational data minings (EDM); Educational institutions; Extract informations; Learning materials; Literature reviews; Tools and methods; Data mining",Article,Scopus,2-s2.0-85091684595
"Santana Í., Plastino A., Rosseti I.","57214600712;56187378700;15837970600;","Improving a state-of-the-art heuristic for the minimum latency problem with data mining",2022,"International Transactions in Operational Research","29","2",,"959","986",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078857012&doi=10.1111%2fitor.12774&partnerID=40&md5=ba1611a58cb1af233d1ca68789aec027","Recently, hybrid metaheuristics have become a trend in operations research. A successful example combines the Greedy Randomized Adaptive Search Procedures (GRASP) and data mining techniques, where frequent patterns found in high-quality solutions can lead to an efficient exploration of the search space, along with a significant reduction of computational time. In this paper, a GRASP-based state-of-the-art heuristic for the minimum latency problem is improved by means of data mining techniques. Computational experiments showed that the hybrid heuristic with data mining was able to match or improve the solution quality for a large number of instances, together with a substantial reduction of running time. Besides, 32 new best-known solutions are introduced to the literature. To support our results, statistical significance tests, analyses over the impact of mined patterns, comparisons based on running time as stopping criterion, and time-to-target plots are provided. © 2020 The Authors. International Transactions in Operational Research © 2020 International Federation of Operational Research Societies","data mining; GRASP; hybrid metaheuristics; minimum latency problem","Heuristic algorithms; Operations research; Computational experiment; GRASP; Greedy randomized adaptive search procedure; High-quality solutions; Hybrid metaheuristics; Minimum latency problems; Statistical significance test; Substantial reduction; Data mining",Article,Scopus,2-s2.0-85078857012
"Zhang M., Wang G., Ren L., Li J., Deng K., Zhang B.","55704451900;57375583200;57375891400;57196717102;35301568100;55828149215;","METoNR: A meta explanation triplet oriented news recommendation model",2022,"Knowledge-Based Systems","238",,"107922","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121361169&doi=10.1016%2fj.knosys.2021.107922&partnerID=40&md5=af851de79415d697fa716bec607f97da","Personalized news recommendation is an important task for online news platforms to target user interests and alleviate information overload. Most existing methods leverage news contents to make recommendation due to the importance of contents to distinguish pieces of news. Although Heterogeneous Graph (HG) shows great potential in organizing and exploiting varieties of side information to boost recommendation performance in general recommender systems, and at the same time there exist rich side information in real news recommendation scenario, existing methods lack attention to utilizing HG to exploit multiple kinds of side information to enhance news recommendation accuracy. In addition, they pay no attention to providing understandable explanations to improve user satisfaction. To this end, we propose a meta explanation triplet oriented news recommendation model. Specifically, we first construct an HG to extract high-order relatedness knowledge between users and news from various side information. Then, a dedicated neural network is designed to leverage rich side information and contents in a joint way to make news recommendation. Finally, we provide user-centered and news-centered recommendation explanations for users based on meta explanation triplets. Extensive experiments on two benchmark real-world datasets show that our model could improve news recommendation performance compared with state-of-the-art methods and provide effective explanations. © 2021 Elsevier B.V.","Knowledge extraction; Meta explanation triplet; News recommendation; Recommendation explanation; Supervised multi-task learning","Benchmarking; Data mining; Learning systems; Real time systems; Heterogeneous graph; Knowledge extraction; Meta explanation triplet; Meta-explanation; News recommendation; Personalized news; Recommendation explanation; Recommendation performance; Side information; Supervised multi-task learning; Recommender systems",Article,Scopus,2-s2.0-85121361169
"Altay E.V., Alatas B.","57203221455;10341297500;","Chaos numbers based a new representation scheme for evolutionary computation: Applications in evolutionary association rule mining",2022,"Concurrency and Computation: Practice and Experience","34","5","e6744","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120307247&doi=10.1002%2fcpe.6744&partnerID=40&md5=68c4fa4fc85e4d06d17b0ca451f36d06","In some practical situations, new computational methods are required for appropriately representing systems and their variables with inaccuracies, uncertainties, or variability. The chaos numbers seem to efficiently represent a set or range of values with lower and upper bounds for variables. In this article, a new chaos-enhanced representation scheme that is based on the notion of chaos numbers is proposed for evolutionary optimization methods. As a first application of chaos-based new encoding type, it is integrated into the novel hybrid intelligent optimization method proposed that is adapted as numerical association rules miner for the first time. The proposed hybrid method can also be used for complex types of search and optimization problems. The method is designed as a multiobjective rule miner that simultaneously handles different conflicting objectives and finds the accurate and comprehensible rules automatically. Based on the chaotic encoding, the proposed method easily and effectively adjusts the intervals of the attributes and automatically mines the rules without any preprocess. The performance of the proposed method was tested in real quantitative data sets and results were compared with the other association rule mining methods. According to the obtained results, the proposed method seems promising with respect to different metrics. © 2021 John Wiley & Sons Ltd.",,"Association rules; Encoding (symbols); Miners; Numerical methods; Optimization; Signal encoding; Chaos number; Evolutionary optimization method; Hybrid method; Intelligent optimization method; Lower and upper bounds; Optimisations; Representation schemes; Rule mining; Search problem; Uncertainty; Data mining",Article,Scopus,2-s2.0-85120307247
"Suchithra M.S., Pai M.L.","57204188388;57328860900;","Label ranking-based recommendation system to rank crops for agroecological units",2022,"Concurrency and Computation: Practice and Experience","34","5","e6695","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118889939&doi=10.1002%2fcpe.6695&partnerID=40&md5=55df251f9178e10cc2dc4bbf43fc0621","Data mining and machine learning techniques help to predict suitable crops in ranking order for a given location with its soil nutrient status. We proposed a tuned bagging-based K-nearest neighbor ensemble label ranker and it is used to predict the ranked crops for density-based spatial clustering of applications with noise (DBSCAN) based compressed crop-ranked agricultural data. This ensemble integrates the commonly used Borda rank aggregation method, but there is a possibility to improve the performance of our proposed algorithm by selecting the most suitable rank aggregation based on the improvement in the label ranked dataset. The results of the study show that Copeland performed better than Borda aggregation with a 6.65% improvement in the performance of our algorithm, while it is 5.16% for Borda. The voting rule selection (VRS) is also integrated into our study with dataset-level and instance-level learning on crop-ranked datasets. VRS and Copeland aggregation methods shared the first rank position with 46.14% and 45.67% improvement in the ranked datasets. The instance-level winning percentage of crop ranked data for VRS, Copeland, and Borda is 61.44%, 56.23%, and 47.81%, respectively. This study has observed that in the case of a tuned K-nearest neighbor label ranking algorithm, the VRS and Copeland voting rule performs better than Borda. © 2021 John Wiley & Sons Ltd.",,"Data mining; Learning systems; Motion compensation; Nearest neighbor search; Aggregation methods; Crop recommendation; Ensemble; Geospatial clustering; Label rankings; Nearest-neighbour; Performance; Rank aggregation; Rule selection; Voting rules; Crops",Article,Scopus,2-s2.0-85118889939
"Shao J., Gao Q., Wang H.","57450942400;57450942500;57450045800;","Online Learning Behavior Feature Mining Method Based on Decision Tree",2022,"Journal of Cases on Information Technology","24","5",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124570933&doi=10.4018%2fJCIT.295244&partnerID=40&md5=601ef917b53c5d54239ff6e8d37fa168","This research mainly discusses the design of an online learning behavior feature mining method based on decision tree. Data collection is the real-time collection of online learning behavior data from distance learning websites. OWC (office web component) technology is used to draw real-time charts on the page. Online learning students are selected as the research object, and the student’s system log data and questionnaire data are selected. When combining the pre-pruning method and the post-pruning method to make decisions after the tree is pruned, the same source data is used to adjust, test, and evaluate the decision tree model. The evaluation process to generate a complete decision tree is completed by the c4.5tree algorithm in C4.5, which can be named with a suffix of .names. The type definition file is used to record the type of each attribute item or the range of possible values. In the study, the prediction accuracy rate of predicting learning effect based on “online learning behavior” reached more than 66%. © 2022 IGI Global. All rights reserved.","Decision tree; Feature mining; Learning behavior; Learning planning; Online learning","Data acquisition; Data mining; E-learning; Learning systems; Mining; Data collection; Distance-learning; Feature mining; Learning behavior; Learning planning; Learning websites; Mining methods; Online learning; Pruning methods; Real-time collection; Decision trees",Article,Scopus,2-s2.0-85124570933
"Xu N.","57450219300;","Research on Value-Added Effect of Venture Capital on Enterprises Based on Data Mining Technology",2022,"Journal of Cases on Information Technology","24","5",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124528647&doi=10.4018%2fJCIT.295245&partnerID=40&md5=93e9fbe74f669f7ee276ca8fb8ed6d27","Based on data mining technology, this paper introduces venture capital, which is an external strategic investor factor, and takes the maximization of enterprise value as the goal, and discusses the characteristics of the optimal ownership structure, the optimal shareholding ratio, and the ways to optimize the ownership structure under the participation of venture capital, so as to accurately increase the value of enterprises and grasp the role of venture capital in the development of start-up enterprises. Venture capital enterprises will have rich content, which is mainly reflected in financial returns. That is to say, if enterprises withdraw from investment after venture capital, they will have direct benefits. In the current environment of relatively scarce technical resources, enterprises should change the situation of passively accepting venture capital to provide financial capital and actively seek venture capital, which can satisfy the complementary resources and easily reach mutual trust, so as to achieve win-win cooperation. © 2022 IGI Global. All rights reserved.","Enterprise value; Value-added effect; Venture capital","Data mining; Structural optimization; 'current; Data mining technology; Enterprise values; Financial capital; Financial returns; Ownership structure; Shareholdings; Technical resources; Value-added effect; Venture Capital; Investments",Article,Scopus,2-s2.0-85124528647
"Duan W., Wang X., Cheng S., Wang R.","57201362227;56868501800;7404685897;57217033330;","Regional division and influencing mechanisms for the collaborative control of PM2.5 and O3 in China: A joint application of multiple mathematic models and data mining technologies",2022,"Journal of Cleaner Production","337",,"130607","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123245027&doi=10.1016%2fj.jclepro.2022.130607&partnerID=40&md5=4d0fd49f778671dfc097d59311b83bd5","Nationwide O3 has been deteriorating and shows an obvious spatial aggregation effect (SAE), while there are still many cities not meeting Chinese national standards for PM2.5, demonstrating the urgency for the collaborative control of PM2.5 and O3. This study adopted multiple mathematic models and data mining technologies, including Moran's I (MI), the self-organizing map (SOM), the distributed lag nonlinear model (DLNM), multivariate meta-analysis, and univariable multivariate meta-regression, and aimed to explore the spatio-temporal trends and influencing mechanisms of PM2.5 and O3 in different regions. Results revealed that PM2.5 and O3 showed nonlinear and lagged associations with meteorology and precursors and relatively large spatial heterogeneity existed in the influencing mechanisms. The eight clusters, divided with SOM based on air pollution, can explain a substantial part of spatial heterogeneity in influencing mechanisms, which means influencing mechanisms are more consistent in regions with similar pollution characteristics. PM2.5 and O3 heavily polluted regions showed strong SAE according to Moran's I index (LMI), and showed sensitive responses to meteorology and precursors according to meta-analysis of DLNM. Results also suggested that simultaneously mitigating PM2.5 and O3 showed a promising long-term prospect, and that NOx reduction should be strengthened in PM2.5 dominated months and lightened in O3 dominated months at current O3-NOx-VOC regime. This study, with multi-technology fusion, provides systematic understanding of PM2.5 and O3 pollution in China and scientifically backed support for the next-stage collaborative control. © 2022 Elsevier Ltd","DLNM; Influencing mechanisms; Meta-analysis; PM2.5 and O3; Regional division; SOM","Data mining; Meteorology; Multivariant analysis; Nitrogen oxides; Pollution control; Self organizing maps; Collaborative control; Data mining technology; Distributed lag nonlinear model; Influencing mechanisms; Mathematics model; Meta-analysis; Non-linear modelling; PM 2.5; PM2.5 and O3; Regional divisions; Conformal mapping",Article,Scopus,2-s2.0-85123245027
"Barros D.B., Cardoso S.M., Oliveira E., Brentan B., Ribeiro L.","57219557264;57219556250;57219560825;55898255600;55521096700;","Using data mining techniques to isolate chemical intrusion in water distribution systems",2022,"Environmental monitoring and assessment","194","3",,"203","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124922386&doi=10.1007%2fs10661-022-09867-z&partnerID=40&md5=df7930ea01bfbebea05a38a9c9450b63","The security of water distribution systems has become the subject of an increasing volume of research over the last decade. Data analysis and machine learning are linked to hydraulic and quality modeling for improving the capacity of water utilities to save lives when faced with the contamination of water networks. This research applies k-nearest neighbor and random forest algorithms to estimate the location of contamination sources at near-real time. Epanet and Epanet-MSX software are used to simulate intrusions of pesticide into water distribution system and the interaction with compounds already present in water bulk. Different pesticide concentrations are considered in the simulations, and chlorine monitoring occurs through placed quality sensors. The results show that random forest can localize [Formula: see text] of contamination scenarios, while the KNN algorithm found [Formula: see text]. Finally, an assessment of contamination spread is made for a better understanding of the impacts of non-localized contamination. © 2022. The Author(s), under exclusive licence to Springer Nature Switzerland AG.","Contamination sources location; K-nearest neighbor algorithm; Random forest algorithm; Water-supply","water; data mining; environmental monitoring; procedures; water quality; water supply; Data Mining; Environmental Monitoring; Water; Water Quality; Water Supply",Article,Scopus,2-s2.0-85124922386
"Sharma R.P., Ramesh D., Pal P., Tripathi S., Kumar C.","57215412090;54405698700;56323427700;57194049493;55787390500;","IoT-Enabled IEEE 802.15.4 WSN Monitoring Infrastructure-Driven Fuzzy-Logic-Based Crop Pest Prediction",2022,"IEEE Internet of Things Journal","9","4",,"3037","3045",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124587313&doi=10.1109%2fJIOT.2021.3094198&partnerID=40&md5=b89890bc25a062efbabef1b0a2c5fd23","Precision agriculture, as the future of farming technology, addresses challenges faced by farmers by data mining of information collected through IoT-enabled crop monitoring infrastructures. The identification of crop disease is one of the widely studied challenges. Crop diseases cannot be accurately predicted by merely analyzing individual disease causes. This proposal presents a fuzzy-logic-based pest prediction mechanism assisting in beforehand preparedness for potential pest prevention. The experiments are performed for pests in rice and millet crops. The data mining over samples collected in a cropping cycle revealed the plausible correlation between temperature, relative humidity, and rainfall with pest breeding. The data collected through IoT monitoring infrastructure is analyzed for the ambient breeding condition of the pest. These conditions are then employed to design the knowledge base of the fuzzy system. More specifically, the linguistic variables of the fuzzy membership function are optimized using a genetic algorithm for close prediction of pest breeding in given environmental conditions. The proposal verified that the weather factors have a strong impact on the occurrence of pests and diseases, and the fuzzy-logic-based pest prediction through IoT application development services will help farmers to take precautionary measures beforehand. © 2014 IEEE.","Fuzzy logic; Genetic algorithm (GA); Pest prediction; Precision agriculture; WSN","Computer circuits; Crops; Data mining; Forecasting; Genetic algorithms; IEEE Standards; Internet of things; Knowledge based systems; Membership functions; Precision agriculture; Condition; Crop disease; Crop monitoring; Disease cause; Fuzzy-Logic; Genetic algorithm; Ieee 802.15.4/zigbee; Pest prediction; Precision Agriculture; WSN; Fuzzy logic",Article,Scopus,2-s2.0-85124587313
"Chen Y.-C., Wu H.-Y., Chang C.-W., Liao P.-C.","57219255098;57211350070;54797520600;57154840100;","Post-Deconvolution MS/MS Spectra Extraction with Data-Independent Acquisition for Comprehensive Profiling of Urinary Glucuronide-Conjugated Metabolome",2022,"Analytical Chemistry","94","6",,"2740","2748",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124461400&doi=10.1021%2facs.analchem.1c03557&partnerID=40&md5=2350b97d61a717fb0e5d1e8a3d187565","Conjugation reactions are of critical significance in human metabolism. Identification of these conjugated metabolites is still challenging. Here, we propose a strategy, post-deconvolution MS/MS spectra extraction with data-independent acquisition (PDMS2E-DIA), to comprehensively profile the glucuronide-conjugated metabolome. PDMS2E-DIA enables the identification of conjugated and unconjugated metabolite pairs through neutral loss filtering combined with a significant change in abundance after the deconjugation reaction. Purified DIA MS/MS spectra were constructed by extracting MS/MS fragments shared between spectra derived from conjugated and unconjugated metabolites. The feasibility of this approach was first demonstrated by the identification of two glucuronide-conjugated metabolite standards spiked in urine samples. For human urine samples, 479 features were structurally annotated as potential glucuronide-conjugated metabolites, resulting in the identification of 211 metabolites. Fragment peaks derived from interferents were found to be removed by PDMS2E-DIA, which increased about 6 times the number of identified urine metabolites compared with those calculated from raw DIA deconvoluted MS/MS spectra. This approach was found to have great potential for identifying glucuronide-conjugated metabolites, as well as other kinds of chemical conjugations. © 2022 The Authors. Published by American Chemical Society",,"Data mining; Extraction; Metabolites; American Chemical Society; Conjugated metabolites; Deconjugation; Deconvolutions; Glucuronides; Human metabolisms; Metabolomes; Neutral loss; Spectra's; Urine sample; Biomolecules",Article,Scopus,2-s2.0-85124461400
"Lee T., Yoon S., Won K.","57403563800;56459313100;57403905800;","Delta-T-based operational signatures for operation pattern and fault diagnosis of building energy systems",2022,"Energy and Buildings","257",,"111769","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122423050&doi=10.1016%2fj.enbuild.2021.111769&partnerID=40&md5=3e6ffc49ccde25b9f04ed4f520e1dc9b","Energy consumption patterns are key information for efficiently operating energy systems in buildings or primary energy systems in cities. Nevertheless, operation pattern analysis (e.g., signature analysis), presenting the correlations between system variables, is needed to diagnose inefficient operation or system faults because conventional energy usage patterns have limited insight into the operation patterns inside building energy systems. Thus, a novel operation pattern analysis method is proposed for building energy systems using the clustering-based operational signatures. The novelty of this method is that it is based on delta-T signatures in building energy systems. Because delta-T can be a major representative of operational states, delta-T signature-based clustering can determine both normal and faulty operation. Moreover, delta-T (e.g., water temperature differences between supply and return paths) can be easily measured in a basic sensor deployment in a building automation system. The delta-T signatures can be converted into operational and faulty signatures. These features enable the operation pattern analysis to be applied to building energy systems widely without the need for luxurious sensing systems or building energy management systems. In this study, the proposed method was applied to an operational chilled water cooling system for real multiunit residential buildings. The application shows how the delta-T signature-based clustering method can diagnose operation patterns and various faults. In the target buildings, inefficient operation (accounting for 30.8%) was found in the normal delta-T area, and sensor errors (random, bias, and drift) and abnormal device sequences were captured in the negative or extreme delta-T areas. These patterns and faults could be matched to daily cooling energy usage patterns, thus providing a deeper knowledge of operation. © 2021 Elsevier B.V.","Chilled water cooling system; Delta-T signature; Fault diagnosis; Operation pattern; Operational signature; Unsupervised data mining","Automation; Cooling; Cooling water; Energy utilization; Failure analysis; Fault detection; Intelligent buildings; Random errors; Thermoelectric equipment; Water cooling systems; Building energy systems; Chilled water; Chilled water cooling system; Delta-T signature; Faults diagnosis; Operation patterns; Operational signatures; Unsupervised data; Unsupervised data mining; Water-cooling systems; Data mining",Article,Scopus,2-s2.0-85122423050
"Elkholy A., Nafeh A.E.-S.A., Fahmy F.H.","57204683973;6603403264;56259546700;","Impact of time resolution averaging analysis on integrated photovoltaic with office buildings and grid interaction metrics: Case study",2022,"Energy and Buildings","257",,"111818","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121965927&doi=10.1016%2fj.enbuild.2021.111818&partnerID=40&md5=1ddd849df2d23eb26c7a4e599a9d44da","The matching of the photovoltaic (PV) power generation profile with that of the office building is very vital to improving the self-consumption index and the building's self-sufficiency degree. This study focuses on grid interaction and load matching metrics of office building integrated PV systems. The test system in this study is an 8 kWp grid connected PV system located at the Electronics Research Institute (ERI), Giza, Egypt. Different data on the PV inverter output side and those between the load and the grid are measured at different time steps to investigate the effect of the different time steps on the performance and sizing of the utilized PV array, through estimating the maximum PV power and the load power, as well as the metrics that measure the proper match between the grid interaction and the load. The entire process is based on the application of time series clustering as a data-driven technique in order to examine the average PV and load power profiles. In addition, two technique, which are time series clustering and trailing moving average, are used to compare the resulted hourly average PV output powers for the considered different time steps. Finally, the results of the estimated capacity factor and self-consumption/sufficiency metrics of the considered PV system are indicated to significantly affect the optimal size of the utilized PV array by satisfying the desired requirements of the net zero energy buildings and enhancing the performance of the utilized PV system. © 2021 Elsevier B.V.","Data mining; Grid connected PV systems; Grid interaction; Load matching metrics; Statistics; Time series clustering","Data mining; Office buildings; Time series; Time series analysis; Zero energy buildings; Different time steps; Grid interaction; Grid-connected photovoltaic system; Load matching; Load matching metric; Matching metric; Performance; Photovoltaic arrays; Photovoltaics; Time series clustering; Photovoltaic cells",Article,Scopus,2-s2.0-85121965927
"Mina M., Rezaei M., Sameni A., Ostovari Y., Ritsema C.","57222987699;55251910400;6603210836;56339953500;7005175451;","Predicting wind erosion rate using portable wind tunnel combined with machine learning algorithms in calcareous soils, southern Iran",2022,"Journal of Environmental Management","304",,"114171","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121776912&doi=10.1016%2fj.jenvman.2021.114171&partnerID=40&md5=360bff802155b901754428b1082e4f28","Wind erosion is a critical factor in land degradation worldwide, particularly in arid and semi-arid regions of southern Iran, which have been severely exposed to wind erosion in the recent years due to climate change and land use changes. The main objective of the present study was to predict the wind erosion rate (WER) using easily measurable soil properties combined with some data mining approaches. For this purpose, the WER was measured at 100 locations with different land uses and soil types in the Fars Province, southern Iran using a portable wind tunnel. The WER was predicted by multiple linear regression (MLR), support vector regression (SVR) and decision tree (DT) algorithms using easily measurable soil properties. Results revealed that land use and soil type had significant effect on the WER. The highest mean WER was observed in Entisols with the lowest organic matter (OM), the lowest penetration resistance (PR) and the lowest aggregate mean weight diameter (MWD). Bare lands with the lowest OM and MWD showed the highest WER compared to other land uses. R2 and RMSE of the non-linear regression models developed based on the type of the relationship between the WER and easily measurable soil properties improved by 15% and 12%, respectively, compared to the linear regression model. In both train and test datasets, the SVR and DT models coupled to a genetic algorithm (GA) used for selecting the effective easily measurable soil properties had higher performance than the SVR and DT models using all easily measurable soil properties for predicting WER. With respect to statistical indices, the SVR model with R2 = 0.91 and RMSE = 0.68 g m−2 s−1 outperformed the MLR and DT for predicting the WER. We concluded that combining the SVR with GA could be an applicable and promising method for predicting WER. © 2021 The Authors","Decision tree; Dust emission; Genetic algorithm; Land degradation","calcium carbonate; calcium sulfate; sodium; soil organic matter; calcareous soil; climate change; dust; genetic algorithm; land degradation; machine learning; soil organic matter; soil type; wind erosion; wind turbine; adsorption kinetics; agricultural land; airflow; Article; computer model; controlled study; data classification; data mining; decision tree; decomposition; dispersion; electric conductivity; flocculation; genetic algorithm; gravimetry; hydraulic conductivity; intermethod comparison; learning algorithm; loam soil; machine learning; measurement accuracy; microbial activity; multiple linear regression analysis; particle size; physical resistance; prediction; rangeland; sand; soil chemistry; soil erosion; soil moisture; soil property; soil structure; soil texture; soil water content; support vector machine; surface property; vegetation; weight; wind erosion; climate change; desert climate; Iran; machine learning; soil; Fars; Climate Change; Desert Climate; Iran; Machine Learning; Soil",Article,Scopus,2-s2.0-85121776912
"Huynh U., Le B., Dinh D.-T., Fujita H.","57192376710;27267916200;57381747100;35611951900;","Multi-core parallel algorithms for hiding high-utility sequential patterns",2022,"Knowledge-Based Systems","237",,"107793","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121496624&doi=10.1016%2fj.knosys.2021.107793&partnerID=40&md5=169870b198a4938d138663b65552de44","High-utility sequential pattern mining (HUSPM) can be applied in many applications such as retail, market basket analysis, click-stream analysis, healthcare data analysis, and bioinformatics. HUSPM algorithms discover useful information from data. However, looking at the dark side, the sensitive patterns can also be disclosed by the competitors, who use a HUSPM algorithm on the leaked data. Therefore, high-utility sequential pattern hiding (HUSPH) is used to protect the privacy information from HUSPM algorithms. This paper proposes three algorithms named High Utility Sequential Pattern Hiding Using Pure Array Structure (USHPA), High Utility Sequential Pattern Hiding Using Parallel Strategy (USHP), and High Utility Sequential Pattern Hiding Using Random Distribution Strategy (USHR) for hiding high-utility sequential patterns on quantitative sequence datasets. These algorithms use a proposed data structure named Pattern Utility Set for Hiding (PUSH) to speed up the hiding process. We also introduce a metric called Privacy Factor to evaluate the quality of hiding results. The comparative experiments were conducted on real datasets to evaluate the performance of the proposed algorithms in terms of runtime, memory consumption, scalability, missing cost, and privacy factor. Results show that the proposed algorithms can efficiently sanitize the input datasets, and they outperform the compared algorithms for all metrics. © 2021 Elsevier B.V.","High-utility sequential pattern hiding; High-utility sequential pattern mining; Parallel hiding; Privacy preserving utility mining","Data mining; Privacy-preserving techniques; High-utility sequential pattern hiding; High-utility sequential pattern mining; Multi-core parallels; Parallel hiding; Privacy preserving; Privacy preserving utility mining; Sequential pattern mining algorithm; Sequential patterns; Sequential-pattern mining; Utility mining; Quality control",Article,Scopus,2-s2.0-85121496624
"Kim S., Choi Y.Y., Choi J.-I.","57213196978;57201675245;55800746400;","Impedance-based capacity estimation for lithium-ion batteries using generative adversarial network",2022,"Applied Energy","308",,"118317","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121459938&doi=10.1016%2fj.apenergy.2021.118317&partnerID=40&md5=2707e2eeff47bc26f9939dcb4c57a63a","This paper proposes a fully unsupervised methodology for the reliable extraction of latent variables representing the characteristics of lithium-ion batteries (LIBs) from electrochemical impedance spectroscopy (EIS) data using information maximizing generative adversarial networks. Meaningful representations can be obtained from EIS data even when measured with direct current and without relaxation, which are difficult to express when using circuit models. The extracted latent variables were investigated as capacity degradation progressed and were used to estimate the discharge capacity of the batteries by employing Gaussian process regression. The proposed method was validated under various conditions of EIS data during charging and discharging. The results indicate that the proposed model provides more robust capacity estimations than the direct capacity estimations obtained from EIS, where the mean absolute error and root mean square error are less than 1.74 mAh and 1.87 mAh, respectively, for all operating conditions for lithium-ion coin cells with a nominal capacity of 45 mAh. We demonstrate that the latent variables extracted from the EIS data measured with direct current and without relaxation reliably represent the degradation characteristics of LIBs. © 2021 Elsevier Ltd","Electrochemical impedance spectroscopy; Information maximizing generative adversarial network; Lithium-ion battery; State of health","Data mining; Electric discharges; Electrochemical impedance spectroscopy; Ions; Mean square error; Capacity degradation; Capacity estimation; Circuit modeling; Direct-current; Discharge capacities; Electrochemical-impedance spectroscopies; Information maximizing generative adversarial network; Latent variable; Spectroscopy data; State of health; Lithium-ion batteries; artificial neural network; electrochemical method; estimation method; fuel cell; methodology",Article,Scopus,2-s2.0-85121459938
"Ji A.-B., Zhang J.-J., He X., Zhang Y.-H.","16042846600;57378094300;57377933800;57377622800;","Fixed effects panel interval-valued data models and applications",2022,"Knowledge-Based Systems","237",,"107798","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121446647&doi=10.1016%2fj.knosys.2021.107798&partnerID=40&md5=91a5a71505742e3ac13a96fa6f6749f6","Interval-valued data is a complex data type which can be got by summarizing large datasets, linear regression models for interval-valued data have been widely studied. Panel data models combining cross-section and time series real-valued data have become increasingly popular in economic research and data mining. It is very important to construct the regression models for panel data with uncertainty and range variability. This paper introduces panel data regression model for interval-valued data and constructs three kinds of panel interval-valued data regression models: the centre model of fixed effects panel interval-valued data regression, the min–max model of fixed effects panel interval-valued data regression and its special model, the centre and range model of fixed effects panel interval-valued data regression. Then combining the parameters estimation of interval-valued regression and analysis of covariance for panel data, this paper presents the parameters estimations for three kinds of panel interval-valued data regression models. Finally, our proposed panel interval-valued data regression models are applied in forecasting of Air Quality Index, the experimental evaluation of actual data sets shows the advantages and the performance of our proposed panel interval-valued data models. © 2021 Elsevier B.V.","Forecasting; Interval data analysis; Interval-valued regression; Panel data model","Air quality; Data mining; Large dataset; Quality control; Complex data; Data regression; Fixed effects; Interval data analysis; Interval-valued; Interval-valued data; Interval-valued regression; Panel data; Panel data models; Parameters estimation; Regression analysis",Article,Scopus,2-s2.0-85121446647
"Wei C., Lei M., Chen T., Zhou C., Gu R.","57209821071;24473310600;55760128900;57220751424;57373778600;","Method on site-specific source apportionment of domestic soil pollution across China through public data mining: A case study on cadmium from non-ferrous industries",2022,"Environmental Pollution","295",,"118605","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121267617&doi=10.1016%2fj.envpol.2021.118605&partnerID=40&md5=2df664e05a6ed06141be313d5afcbd68","The lack of emission data of major Cd-emitting enterprises has long limited the source apportionment of soil cadmium (Cd). Non-ferrous metal enterprises (NMEs) contribute the most Cd emissions in China in recent years. We estimated the cumulative Cd emission of 8750 NMEs across China through public data collection and material balance methods for the first time. The results showed that the total Cd emissions were estimated at 133,177 tons, of which 78.68% contributed by zinc primary smelting and mining. The emission hotspots are mainly concentrated in the south of the Yangtze River, such as Nanling Mountain areas, Nanpan River Basin, and Jincheng River Basin, as well as a few parts of the North and Northwest China. Then a significant positive spatial correlation was furtherly detected between NMEs and soil Cd, except for secondary smelting enterprises. Moreover, the hotspots of soil Cd pollution caused by NMEs were identified across China. By promoting the accounting calibrator from annual emission intensity of regional (mainly provincial) scale to the cumulative emission of site-specific enterprise in its entire life cycle, this study realized the finer description of the spatial heterogeneity of Cd emission from non-ferrous industry on a large scale and make it possible to refine the reliability of follow-up site-specific source apportionment, by introducing the emission intensity instead of the enterprise sites density. Finally, a modified approach for the regional source apportionment of soil pollution was proposed to obtain a more realistic and precise drawing. The results pointed out key NMEs subcategories and the affected hotspots which require continuous strengthening of Cd-related rectification. This methodological framework is expected to contribute to the precise management and differential sources control of Cd pollution and can be further extended to other pollutants for the precise targeting of key industries and hotspots during source pollution control in the future. © 2021 Elsevier Ltd","Cadmium pollution; Emission intensity; Non-ferrous industry; Source apportionment; Spatial correlation","Cadmium; Data mining; River pollution; Rivers; Smelting; Soil pollution; Watersheds; Cadmium pollution; Emissions intensity; Hotspots; Non-ferrous industry; Non-ferrous metals; Public data; Site-specific; Soil cadmiums; Source apportionment; Spatial correlations; Soils; cadmium; zinc; cadmium; heavy metal; cadmium; correlation; data mining; emission; soil pollution; source apportionment; spatial analysis; Article; China; cleaning; concentration (parameter); controlled study; data mining; follow up; kernel method; large scale production; life cycle assessment; river; river basin; soil pollution; soil pollution control; soil property; data mining; environmental monitoring; pollution; reproducibility; risk assessment; soil; soil pollutant; China; Jincheng; Nanling; Nanpan River; Shanxi; Yangtze River; Cadmium; China; Data Mining; Environmental Monitoring; Environmental Pollution; Metals, Heavy; Reproducibility of Results; Risk Assessment; Soil; Soil Pollutants",Article,Scopus,2-s2.0-85121267617
"Ning J., Xiao B., Zhong W., Xiao B.","57360242700;36015967000;57361111500;57208663315;","A rapid detection method for the battery state of health",2022,"Measurement: Journal of the International Measurement Confederation","189",,"110502","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120469064&doi=10.1016%2fj.measurement.2021.110502&partnerID=40&md5=55720d7dbb46c88d1cc2d99b35988cc7","The purpose of this paper is to develop a rapid detector for the battery state-of-health (SOH) in field applications. The research focuses on the detection principle and implementation technology of the instrument, which differs from machine learning methods based on data mining and equivalent-circuit model methods based on state-space modeling and parameter estimation. The charge transfer factor and lithium-ion diffusion factor are introduced to represent the battery SOH in the active material and lithium-ion inventory inside the battery respectively. The relationship between the two indicators and battery impedance is established, which is independent of SOC. Two indicators are obtained by measuring the charging current at a particular single frequency point within seconds. The charge current, which comprises a fixed-amplitude DC current and variant AC current is employed to provide a unified comparison base and shortens the measurement time. The rapid detector is implemented on a microcontrol unit with HRPWM technology. © 2021 Elsevier Ltd","Charge transfer factor; Electrochemical impedance spectrum; HRPWM; Lithium-ion diffusion factor; State of health","Battery management systems; Charging (batteries); Data mining; Equivalent circuits; Health; Ions; Learning systems; Lithium; Lithium-ion batteries; Charge transfer factor; Detection methods; Diffusion factor; Electrochemical impedance spectrum; HRPWM; Lithium ion diffusion; Lithium-ion diffusion factor; Rapid detection; State of health; Transfer Factor; Charge transfer",Article,Scopus,2-s2.0-85120469064
"Nguyen H., Le T., Nguyen M., Fournier-Viger P., Tseng V.S., Vo B.","57189987088;55513981400;57211505214;14048484800;6507335623;35147075900;","Mining frequent weighted utility itemsets in hierarchical quantitative databases",2022,"Knowledge-Based Systems","237",,"107709","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120378765&doi=10.1016%2fj.knosys.2021.107709&partnerID=40&md5=34c11b9abe75075b86427d3ccc407b3d","Mining frequent itemsets in traditional databases and quantitative databases (QDBs) has drawn many researchers’ interest. Although many studies have been conducted on this topic, a major limitation of these studies is that they ignore the relationships between items. However, in real-life datasets, items are often related to each other through a generalization/specialization relationship. To consider the relationships and discover a more generalized form of patterns, this study proposes a new concept of mining frequent weighted utility itemsets in hierarchical quantitative databases (HQDBs). In this kind of databases, items are organized in a hierarchy. Using the extended dynamic bit vector structure with large integer elements, two efficient algorithms named MINE_FWUIS and FAST_MINE_FWUIS are developed. The empirical evaluations in terms of processing time between MINE_FWUIS and FAST_MINE_FWUIS are conducted. The experimental results indicate that FAST_MINE_FWUIS is recommended for mining frequent weighted utility itemsets in hierarchical QDBs. © 2021 Elsevier B.V.","Frequent weighted utility itemsets; Hierarchical database; Hierarchical quantitative database; Quantitative database","Data mining; Frequent weighted utility itemset; Generalisation; Hierarchical database; Hierarchical quantitative database; Itemset; Mining frequent itemsets; Quantitative database; Real life datasets; Weighted utility; Database systems",Article,Scopus,2-s2.0-85120378765
"Tian Y., Zhang Z., Xiong J., Chen L., Ma J., Peng C.","55480173200;57226894652;35094715500;54790727300;56237437100;12807482700;","Achieving Graph Clustering Privacy Preservation Based on Structure Entropy in Social IoT",2022,"IEEE Internet of Things Journal","9","4",,"2761","2777",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113256524&doi=10.1109%2fJIOT.2021.3092185&partnerID=40&md5=b0fc356fea523e92f03d9d28fea0e201","Decoding the real structure from the Social Internet-of-Things (SIoT) network with a large-scale noise structure plays a fundamental role in data mining. Protecting private information from leakage in the mining process and obtaining accurate mining results is a significant challenge. To tackle this issue, we present a graph clustering privacy-preserving method based on structure entropy, which combines data mining with the structural information theory. Specially, user private information in SIoT is encrypted by Brakerski-Gentry-Vaikuntanathan (BGV) homomorphism to generate a graph structure in the ciphertext state, the ciphertext graph structure is then divided into different modules by applying a 2-D structural information solution algorithm and a entropy reduction principle node module partition algorithm, and the K -dimensional structural information solution algorithm is utilized to further cluster the internal nodes of the partition module. Moreover, normalized structural information and network node partition similarity are introduced to analyze the correctness and similarity degree of clustering results. Finally, security analysis and theoretical analysis indicate that this scheme not only guarantees the correctness of the clustering results but also improves the security of private information in SIoT. Experimental evaluation and analysis shows that the clustering results of this scheme have higher efficiency and reliability. © 2014 IEEE.","Graph clustering; homomorphic encryption; privacy-preserving method; structural information; structure entropy","Clustering algorithms; Cryptography; Entropy; Graph algorithms; Graph structures; Graph theory; Graphic methods; Information theory; Internet of things; Privacy by design; Reliability analysis; Structural analysis; Clustering results; Experimental evaluation; Privacy preservation; Privacy preserving; Private information; Similarity degree; Solution algorithms; Structural information; Data mining",Article,Scopus,2-s2.0-85113256524
"Yahav G., Weber Y., Duadi H., Pawar S., Fixler D.","57190189967;57449289400;34167709500;57225467909;57203230722;","Classification of fluorescent anisotropy decay based on the distance approach in the frequency domain",2022,"Optics Express","30","4",,"6176","6192",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124492254&doi=10.1364%2fOE.453108&partnerID=40&md5=03943814c53ddb152305db943bbc788a","Frequency-domain (FD) fluorometry is a widely utilized tool to probe unique features of complex biological structures, which may serve medical diagnostic purposes. The conventional data analysis approaches used today to extract the fluorescence intensity or fluorescence anisotropy (FA) decay data suffer from several drawbacks and are inherently limited by the characteristics and complexity of the decay models. This paper presents the squared distance (D2) technique, which categorized samples based on the direct frequency response data (FRD) of the FA decay. As such, it improves the classification ability of the FD measurements of the FA decay as it avoids any distortion that results from the challenged translation into time domain data. This paper discusses the potential use of the D2 approach to classify biological systems. Mathematical formulation of D2 technique adjusted to the FRD of the FA decay is described. In addition, it validates the D2 approach using 2 simulated data sets of 6 groups with similar widely and closely spaced FA decay data as well as in experimental data of 4 samples of a fluorophore-solvent (fluorescein-glycerol) system. In the simulations, the classification accuracy was above 95% for all 6 groups. In the experimental data, the classification accuracy was 100%. The D2 approach can help classify samples whose FA decay data are difficult to extract making FA in the FD a realistic diagnostic tool. The D2 approach offers an advanced method for sorting biological samples with differences beyond the practical temporal resolution limit in a reliable and efficient manner based on the FRD of their time-resolved fluorescence measurements thereby achieving better diagnostic quality in a shorter time. © 2022 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement",,"Anisotropy; Data mining; Diagnosis; Finite difference method; Fluorophores; Time domain analysis; Anisotropy decay; Biological structures; Classification accuracy; Fluorescence anisotropy decay; Fluorescence intensities; Fluorescent anisotropy; Frequency domains; Medical diagnostics; Response data; Unique features; Frequency response",Article,Scopus,2-s2.0-85124492254
"Yang T., Hou B., Cai Z., Wu K., Zhou T., Wang C.","56949453100;57204175359;57207309525;8707899500;56452134400;57226288108;","6Graph: A graph-theoretic approach to address pattern mining for Internet-wide IPv6 scanning",2022,"Computer Networks","203",,"108666","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121932849&doi=10.1016%2fj.comnet.2021.108666&partnerID=40&md5=dd2caa5dea15622825e1f0e90995d6b7","IPv6 target generation is critical in fast IPv6 scanning for Internet-wide surveys and cybersecurity analysis. However, existing techniques generally suffer from low hit rates because the targets are generated from inappropriate address patterns. To address the problem, we propose 6Graph, a graph-theoretic method for IPv6 address pattern mining. It first divides the IPv6 address space into different regions according to the structural information of a set of known addresses. Then, 6Graph maps the addresses of each region into undirected graphs and conducts the density-based graph cutting for address clustering to mine IPv6 address patterns and detect the misclassified addresses iteratively. Besides, we exploit the random IPv6 target generation based on Hamming distance without additional and complicated target selection. Experiments on 11 large-scale candidate datasets show that the address patterns of 6Graph have a higher seed density than the existing methods. Further results over real-world networks indicate that 6Graph can achieve 12.6%–35.8% hit rates on the candidate datasets, which is an 8.8%–275.0% improvement over the state-of-the-art methods in Internet-wide scanning. © 2021","Internet-wide scanning; IPv6; Outlier detection; Pattern mining","Anomaly detection; Graph theory; Graphic methods; Hamming distance; Internet protocols; Iterative methods; Large dataset; Scanning; Address space; Cyber security; Graph theoretic approach; Graph-theoretic methods; Hit rate; Internet-wide scanning; Ipv6; Ipv6 address; Pattern mining; Structural information; Data mining",Article,Scopus,2-s2.0-85121932849
"Zou Y., Cao X., Yang B., Deng L., Xu Y., Dong S., Li W., Wu C., Cao G.","57205714121;57199648388;57221260321;57462609300;57222357789;57462441800;54966542700;57193354386;57069446200;","In Silico Infection Analysis (iSFA) Identified Coronavirus Infection and Potential Transmission Risk in Mammals",2022,"Frontiers in Molecular Biosciences","9",,"831876","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125097887&doi=10.3389%2ffmolb.2022.831876&partnerID=40&md5=11fbd553e0f87332234787c5a6ff38c1","Coronaviruses are a great source of threat to public health which could infect various species and cause diverse diseases. However, the epidemic’s spreading among different species remains elusive. This study proposed an in silico infection analysis (iSFA) system that includes pathogen genome or transcript mining in transcriptome data of the potential host and performed a comprehensive analysis about the infection of 38 coronaviruses in wild animals, based on 2,257 transcriptome datasets from 89 mammals’ lung and intestine, and revealed multiple potential coronavirus infections including porcine epidemic diarrhea virus (PEDV) infection in Equus burchellii. Then, through our transmission network analysis, potential intermediate hosts of five coronaviruses were identified. Notably, iSFA results suggested that the expression of coronavirus receptor genes tended to be downregulated after infection by another virus. Finally, binding affinity and interactive interface analysis of S1 protein and ACE2 from different species demonstrated the potential inter-species transmission barrier and cross-species transmission of SARS-CoV-2. Meanwhile, the iSFA system developed in this study could be further applied to conduct the source tracing and host prediction of other pathogen-induced diseases, thus contributing to the epidemic prevention and control. Copyright © 2022 Zou, Cao, Yang, Deng, Xu, Dong, Li, Wu and Cao.","coronaviruses; COVID-19; data mining; in silico docking; in silico infection analysis",,Article,Scopus,2-s2.0-85125097887
"Zhao J.-J., Zhang Y., Wang X.-C., Wang X., Zhang Q., Lu P., Liu P.-P., Yu Y.-J., Han L., Zhou H.-N., Zheng Q.-X., Fu H.-Y.","57223015369;57373311100;57216994472;57218229184;57204551623;57188991051;57388702800;26030765500;57189607935;56175570700;55505121400;56672321000;","A new platform for untargeted UHPLC-HRMS data analysis to address the time-shift problem",2022,"Analytica Chimica Acta","1193",,"339393","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121909071&doi=10.1016%2fj.aca.2021.339393&partnerID=40&md5=320a5bcc85da6e8a473a615cb08259f7","Substantial deviations in retention times among samples pose a great challenge for the accurate screening and identifying of metabolites by ultrahigh-performance liquid chromatography high-resolution mass spectrometry (UHPLC-HRMS). In this study, a coarse-to-refined time-shift correction methodology was proposed to efficiently address this problem. Metabolites producing multiple fragment ions were automatically selected as landmarks to generate pseudo-mass spectra for a coarse time-shift correction. Refined peak alignment for extracted ion chromatograms was then performed by using a moving window-based multiple-peak alignment strategy. Based on this novel coarse-to-refined time-shift correction methodology, a new comprehensive UHPLC-HRMS data analysis platform was developed for UHPLC-HRMS-based metabolomics. Original datasets were employed as inputs to automatically extract and register features in the dataset and to distinguish fragment ions from metabolites for chemometric analysis. Its performance was further evaluated using complex datasets, and the results suggest that the new platform can satisfactorily resolve the time-shift problem and is comparable with commonly used UHPLC-HRMS data analysis tools such as XCMS Online, MS-DIAL, Mzmine2, and Progenesis QI. The new platform can be downloaded from: http://www.pmdb.org.cn/antdas2tsc. © 2021 Elsevier B.V.","Automatic data analysis; Chemometrics; Time-shift correction; UHPLC-HRMS; Untargeted metabolomics","Chromatographic analysis; Data handling; Data mining; Ion chromatography; Ions; Mass spectrometry; Metabolites; Screening; Automatic data analyse; Chemometrices; Liquid chromatography-high resolution mass spectrometries; Mass spectrometry data analysis; Metabolomics; Time shifts; Time-shift correction; Ultra-high performance liquid chromatographies; Ultrahigh-performance liquid chromatography high-resolution mass spectrometry; Untargeted metabolomic; Biomolecules; article; chemometric analysis; chemometrics; data analysis; mass spectrometry; metabolomics; ultra performance liquid chromatography; high performance liquid chromatography; liquid chromatography; Chemometrics; Chromatography, High Pressure Liquid; Chromatography, Liquid; Data Analysis; Mass Spectrometry",Article,Scopus,2-s2.0-85121909071
"Aboud A., Robinson B.","57202161060;57219724958;","Fraudulent financial reporting and data analytics: an explanatory study from Ireland",2022,"Accounting Research Journal","35","1",,"21","36",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094979999&doi=10.1108%2fARJ-04-2020-0079&partnerID=40&md5=8d31906e64ab4bdd19b8542e7f1cc5ac","Purpose: This paper aims to explore the effectiveness of fraud prevention and detection techniques, including data analytics, machine learning and data mining, and to understand how widespread the use of data analytics is across different sectors and to identify and understand the potential barriers to implementing these techniques to detect and prevent fraud. Design/methodology/approach: A survey was administered to 73 Irish businesses to determine to what extent traditional approach, data mining or text mining are being used to prevent or detect fraudulent financial reporting, and to determine the perception level of their effectiveness. Findings: The study suggests that whilst data analytics is widely used by businesses in Ireland there is an under-utilisation of data analytics as an effective tool in the fight against fraud. The study suggests there are barriers that may be preventing companies from implementing advanced data analytics to detect financial statement fraud and identifies how those barriers may be overcome. Originality/value: In contrast to the majority of literature on big data analytics and auditing, which lacks empirical insight into the diffusion, effectiveness and obstacles of data analytics, this explanatory study contributes by providing useful insights from the field on big data analytics. While the extant auditing literature generally addresses the avenues of big data utilisation in auditing domain, our study explores particularly the use big data analytics as a fraud prevention and detection techniques. © 2020, Emerald Publishing Limited.","Data analytics; Data mining; Fraud detection and prevention; Machine learning",,Article,Scopus,2-s2.0-85094979999
"Wang C., Zheng L.","57401982900;57462478000;","AI-Based Publicity Strategies for Medical Colleges: A Case Study of Healthcare Analysis",2022,"Frontiers in Public Health","9",,"832568","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125087109&doi=10.3389%2ffpubh.2021.832568&partnerID=40&md5=2b14457172db40e667885736239b8111","The health status and cognition of undergraduates, especially the scientific concept of healthcare, are particularly important for the overall development of society and themselves. The survey shows that there is a significant lack of knowledge about healthcare among undergraduates in medical college, even among medical undergraduates, not to mention non-medical undergraduates. Therefore, it is a good way to publicize healthcare lectures or electives for undergraduates in medical college, which can strengthen undergraduates' cognition of healthcare and strengthen the concept of healthcare. In addition, undergraduates' emotional and mental state in healthcare lectures or electives can be analyzed to determine whether undergraduates have hidden illnesses and how well they understand the healthcare content. In this study, at first, a mental state recognition method of undergraduates in medical college based on data mining technology is proposed. Then, the vision-based expression and posture are used for expanding the channels of emotion recognition, and a dual-channel emotion recognition model based on artificial intelligence (AI) during healthcare lectures or electives in a medical college is proposed. Finally, the simulation is driven by TensorFlow with respect to mental state recognition of undergraduates in medical college and emotion recognition. The simulation results show that the recognition accuracy of mental state recognition of undergraduates in a medical college is more than 92%, and the rejection rate and misrecognition rate are very low, and false match rate and false non-match rate of mental state recognition is significantly better than the other three benchmarks. The emotion recognition of the dual-channel emotion recognition method is over 96%, which effectively integrates the emotional information expressed by facial expressions and postures. Copyright © 2022 Wang and Zheng.","AI; emotion recognition; healthcare; medical college; mental state","article; artificial intelligence; body position; controlled study; data mining; emotion; facial expression; human; human experiment; mass medium; medical school; mental health; simulation; vision",Article,Scopus,2-s2.0-85125087109
"Andriani K.F., Felício-Sousa P., Morais F.O., Da Silva J.L.F.","55249920400;57212301588;57451795700;8317290700;","Role of quantum-size effects in the dehydrogenation of CH4on 3d TM: Nclusters: DFT calculations combined with data mining",2022,"Catalysis Science and Technology","12","3",,"916","926",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124594934&doi=10.1039%2fd1cy01785c&partnerID=40&md5=9c0d9239711595fc04be9c15458dc68e","In this work, we report a theoretical investigation of the role of quantum-size effects (QSEs) in the dehydrogenation of methane (CH4) on 3d transition-metal clusters, TMn, where TM = Fe, Co, Ni, and Cu, and n = 4-15. Our calculations were based on density functional theory combined with the unity bond index-quadratic exponential potential (UBI-QEP) approach and data mining (Spearman rank correlation, clustering). We found via clustering techniques that QSEs or the chemical species, TMs, do not affect the adsorption modes (geometric orientation of the molecules) of CH4, CH3, CH3 + H, and H on the TMn clusters. However, QSEs play a crucial role in modulating the magnitude of the adsorption energy, reaction energy, dissociation energy, and activation energy, in particular, for Cun clusters due to the unpaired electron for clusters with an odd number of electrons. Through the UBI-QEP approach, we found small activation energy barriers for small Fen clusters and larger ones for Nin clusters, i.e., QSEs can be explored to tune energy barriers. These findings are supported by Spearman analysis; however, we could not identify a general trend due to the quantum-size effects that correlate activation energy with the adsorption and dissociation energies for the studied systems. © The Royal Society of Chemistry.",,"Activation analysis; Adsorption; Binding energy; Data mining; Dehydrogenation; Density functional theory; Dissociation; Energy barriers; Size determination; Transition metals; 3d transition metals; Adsorption energies; CH 4; DFT calculation; Dissociation energies; Quantum size effects; Theoretical investigations; Transition-metal clusters; Unity bond index- quadratic exponential potentials; [Co/Cu]; Activation energy",Article,Scopus,2-s2.0-85124594934
"Ayanso A., Han M., Zihayat M.","9435371800;57321097100;54961023800;","An automated mobile app labeling framework based on primary motivations for smartphone use",2022,"International Journal of Web Information Systems","18","1",,"23","40",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118372290&doi=10.1108%2fIJWIS-08-2021-0085&partnerID=40&md5=d85dff80db63a1b0d400af7a7c0bc6d7","Purpose: This paper aims to propose an automated mobile app labeling framework based on a novel app classification scheme that is aligned with users’ primary motivations for using smartphones. The study addresses the gaps in incorporating the needs of users and other context information in app classification as well as recommendation systems. Design/methodology/approach: Based on a corpus of mobile app descriptions collected from Google Play store, this study applies extensive text analytics and topic modeling procedures to profile mobile apps within the categories of the classification scheme. Sufficient number of representative and labeled app descriptions are then used to train a classifier using machine learning algorithms, such as rule-based, decision tree and artificial neural network. Findings: Experimental results of the classifiers show high accuracy in automatically labeling new apps based on their descriptions. The accuracy of the classification results suggests a feasible direction in facilitating app searching and retrieval in different Web-based usage environments. Research limitations/implications: As a common challenge in textual data projects, the problem of data size and data quality issues exists throughout the multiple phases of experiments. Future research will extend the data collection scope in many aspects to address the issues that constrained the current experiments. Practical implications: These empirical experiments demonstrate the feasibility of textual data analysis in profiling apps and user context information. This study also benefits app developers by improving app descriptions through a better understanding of user needs and context information. Finally, the classification framework can also guide practitioners in customizing products and services beyond mobile apps where context information and user needs play an important role. Social implications: Given the widespread usage and applications of smartphones today, the proposed app classification framework will have broader implications to different Web-based application environments. Originality/value: While there have been other classification approaches in the literature, to the best of the authors’ knowledge, this framework is the first study on building an automated app labeling framework based on primary motivations of smartphone usage. © 2021, Emerald Publishing Limited.","App searching and retrieval; Applications of Web mining and searching; Mobile app classification; Text analytics; Topic modeling; Unsupervised and supervised learning; Web search and information extraction","Automation; Classification (of information); Data mining; Decision trees; E-learning; Information retrieval; Learning algorithms; Machine learning; Motivation; Neural networks; Smartphones; Text processing; App searching and retrieval; Application of web mining and searching; Mobile app; Mobile app classification; Text analytics; Topic Modeling; Web Mining; Web search and information extraction; Web searches; Web-searching; Websites",Article,Scopus,2-s2.0-85118372290
"Xu Y., Kong X., Zhu Z., Jiang C., Xiao S.","57465066500;51061071700;57465355400;57465210000;57466092000;","Recovery Algorithm of Power Metering Data Based on Collaborative Fitting",2022,"Energies","15","4","1570","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125167101&doi=10.3390%2fen15041570&partnerID=40&md5=9245b8e73e4e20e12f059bac4350f63e","Electric energy metering plays a crucial role in ensuring fair and equitable transactions between grid companies and power users. With the implementation of the State Grid Corporation’s energy Internet strategy, higher requirements have been put forward for power grid companies to reduce costs and increase efficiency and user service capabilities. Meanwhile, the accuracy and real-time requirements for electric energy measurements have also increased. Electricity information collection systems are mainly used to collect the user-side energy metering data for the power users. Attributed to communication errors, communication delays, equipment failures and other reasons, some of the collected data is missed or confused, which seriously affects the refined management and service quality of power grid companies. How to deal with such data has been one of the important issues in the fields of machine learning and data mining. This paper proposes a collaborative fitting algorithm to solve the problem of missing collected data based on latent semantics. Firstly, a tree structure of user history data is established, and then the user groups adjacent to the user with missing data are obtained from this. Finally, the missing data are recovered using the alternating least-squares matrix factorization algorithm. Through numerical verification, this method has high reliability and accuracy in recoverying the missing data. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Collaborative; Data recovery; Latent semantic; Matrix factorization; Nearest neighbor; Power metering","Electric power transmission networks; Matrix algebra; Matrix factorization; Numerical methods; Recovery; Semantics; Trees (mathematics); Collaborative; Data recovery; Latent semantics; Matrix factorizations; Missing data; Nearest-neighbour; Power grid company; Power metering; Power users; Recovery algorithms; Data mining",Article,Scopus,2-s2.0-85125167101
"Lin C.-F., Yang S.-C.","57443057400;7406948351;","Taiwan Stock Tape Reading Periodically Using Web Scraping Technology with GUI",2022,"Applied System Innovation","5","1","28","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125141751&doi=10.3390%2fasi5010028&partnerID=40&md5=296d83ddf4db6bf18990867ecbe4eb77","Stock tape reading involves surveilling stock prices once in a while and recording stock prices. The method of observing stock prices may be television or stock exchange. The time step for recoding stock prices is every stock user’s experience and their theory, perhaps 3 min or 2 h and so on. As an example, the Taiwan stock market starts at 9:00 a.m. to 13:30 p.m. It will have a 4 h operating time. Splitting the 4 h operating time for tape reading is the skill of stock users. The stock price sequence generated by tape reading can be predicted by stock users, but finally, it is the stock user’s experience. Therefore, the meaning of tape reading is to record the stock price, but its concept should have no prediction purpose. This study used thread technology and proposed a tape-reading method with web scraping. This method can periodically scrape stock prices and generate a stock price sequence to Excel file. This application can satisfy the demand of these stock users, who are called day trading users. Because these day trading users want to gain stock price sequences minute by minute, rather than the stock exchange format day by day, and also ones which are better than the those provided by the stock website service, because its stock sequence format is limited and not normalized, these day trading users think that minute-by-minute stock price sequences are very clear to forecast. This study implemented the prior scheme and designed the GUI to query a company’s stock price and its stock news, even per second, etc., and how long it took to update the stock price, and the GUI also included a time-up feature to stop scraping stock prices if users just wanted to scrape stock prices periodically. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Data mining; Stock scraping; Web scraping",,Article,Scopus,2-s2.0-85125141751
"Xu B., Yuan X.","57218158895;57222153050;","A Novel Method of BP Neural Network Based Green Building Design—The Case of Hotel Buildings in Hot Summer and Cold Winter Region of China",2022,"Sustainability (Switzerland)","14","4","2444","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125080818&doi=10.3390%2fsu14042444&partnerID=40&md5=8789293cf4de0d9dd3cf052835de0d4a","With the advent of the big data era, architectural design gradually tends to become more quantified and intelligent. This study proposes a novel green design method for energy-saving buildings based on a BP neural network. This study changed the traditional trial–error mode by evaluating energy consumption based on design performance parameters such as building shape, space, and interface. Instead, energy consumption quota values obtained from statistical data, as well as thermal parameters and energy system parameters in energy-saving standards, were taken as input parameters, and then the design scheme of building shape can be obtained through BP neural network technology. Based on data of 61 hotel buildings in a representative city among a hot summer and cold winter climate zone, the BP neural network model is established to control the building design variables, with 41 kgce/m2·a as its energy-saving design target. Through the energy consumption quota, the trained BP network is applied to predict the optimal architectural design parameters, including the building orientation angle, shape coefficient, window–wall ratio, etc., for twelve building typologies in an area range of 5000~60,000 m2 . With recommended control thresholds of quantifiable architectural design elements obtained, this research can provide effective design decision-making suggestions for architects. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","BP neural network; Data mining; Energy saving; Green building; Wisdom","architectural design; artificial neural network; building; seasonal variation; China",Article,Scopus,2-s2.0-85125080818
"Cornadó C., Vima-Grau S., Garcia-Almirall P., Uzqueda A., de la Asunción M.","55199859900;57370178300;53879830900;57196703042;57463442800;","Decision-Making Tool for the Selection of Priority Areas for Building Rehabilitation in Barcelona",2022,"Buildings","12","2","247","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125078631&doi=10.3390%2fbuildings12020247&partnerID=40&md5=218163a0ca8da9649fda379837dc9191","The promotion of rehabilitation is an urgent necessity in today’s consolidated cities, both due to the need to update their buildings to achieve habitability and safety standards that are required nowadays, as well as to stop the deterioration of buildings in vulnerable environments, where paradoxically the obtainment of economic resources to invest in building maintenance and upgrade is scarcer. Decision making on the delimitation of areas in which the need to invest is higher is extremely complex and often relies on large secondary data studies that are contrasted with local stakeholders’ intuition and knowledge on the ground. Usually, rehabilitation aids are directed to relatively large areas, where a certain need may be found. However, these areas are often excessively wide and specific needs that would require special focus can be diluted in the whole. The current trend of area-based and site-specific rehabilitation programs calls for precise and focused data studies and methodologies. The research presented here provides a methodology for the selection of priority areas to promote rehabilitation in the context of Barcelona’s vulnerable neighborhoods. The selection methodology combines primary and secondary data with a very high level of disaggregation that identifies where the needs are greatest, and it also provides a tool that is still based on primary disaggregated data for the delimitation of areas. The results obtained highlight specific priority areas such as parts of the Raval, Carmel and Besòs-Maresme neighborhoods within larger zones that had been previously defined as vulnerable. The proposed methodology seeks to provide tools to foster evidence-based decision making, thus improving both the understanding of reality and its spatial distribution through data mining techniques and data visualization. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Area-based rehabilitation; Data mining; Data visualization; Data-based decision making; Housing renewal; Public rehabilitation policies; Urban regeneration",,Article,Scopus,2-s2.0-85125078631
"Wen Y.-T., Yang H.-K., Peng W.-C.","56383304200;57209975145;8358716100;","Mining Willing-to-Pay Behavior Patterns from Payment Datasets",2022,"ACM Transactions on Intelligent Systems and Technology","13","1","14","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125012328&doi=10.1145%2f3485848&partnerID=40&md5=fcc9929f13e82516c94549c45393fd30","The customer base is the most valuable resource to E-commerce companies. A comprehensive understanding of customers' preferences and behavior is crucial to developing good marketing strategies, in order to achieve optimal customer lifetime values (CLVs). For example, by exploring customer behavior patterns, given a marketing plan with a limited budget, a set of potential customers is able to be identified to maximize profit. In other words, personalized campaigns at the right time and in the right place can be treated as the last stage of consumption. Moreover, effective future purchase estimation and recommendation help guide the customer to the up-selling stage. The proposed willing-to-pay prediction model (W2P) exploits the transaction data to predict customer payment behavior based on a probabilistic graphical model, which provides semantic explanation of the estimated results and deals with the sparsity of payment data from each customer. Existing work in this domain ranks the customers by their probabilities of purchase in different conditions. However, the customer with the highest purchase probability does not necessarily spend the most. Therefore, we propose a CLV maximization algorithm based on the prediction results. In addition, we improve the model by behavioral segmentation wherein we group the customers by payment behaviors to reduce the size of the offline models and enhance the accuracy for low-frequency customers. The experiment results show that our model outperforms the state-of-the-art methods in purchase behavior prediction. © 2022 Association for Computing Machinery.","customer lifetime value prediction; data sparsity; Financial technology","Budget control; Data mining; Forecasting; Semantics; Strategic planning; Behaviour patterns; Customer behavior; Customer lifetime value; Customer lifetime value prediction; Customer preferences; Customerbase; Data sparsity; Marketing plan; Marketing strategy; Value prediction; Sales",Article,Scopus,2-s2.0-85125012328
"Wang Y., Yin H., Chen T., Liu C., Wang B., Wo T., Xu J.","57192074247;55007318200;57196727070;57203398228;57217164565;15838274700;57050877700;","Passenger Mobility Prediction via Representation Learning for Dynamic Directed and Weighted Graphs",2022,"ACM Transactions on Intelligent Systems and Technology","13","1","2","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125011925&doi=10.1145%2f3446344&partnerID=40&md5=88a89bf9b15684e4e74e2d2b15bace4b","In recent years, ride-hailing services have been increasingly prevalent, as they provide huge convenience for passengers. As a fundamental problem, the timely prediction of passenger demands in different regions is vital for effective traffic flow control and route planning. As both spatial and temporal patterns are indispensable passenger demand prediction, relevant research has evolved from pure time series to graph-structured data for modeling historical passenger demand data, where a snapshot graph is constructed for each time slot by connecting region nodes via different relational edges (origin-destination relationship, geographical distance, etc.). Consequently, the spatiotemporal passenger demand records naturally carry dynamic patterns in the constructed graphs, where the edges also encode important information about the directions and volume (i.e., weights) of passenger demands between two connected regions. aspects in the graph-structure data. representation for DDW is the key to solve the prediction problem. However, existing graph-based solutions fail to simultaneously consider those three crucial aspects of dynamic, directed, and weighted graphs, leading to limited expressiveness when learning graph representations for passenger demand prediction. Therefore, we propose a novel spatiotemporal graph attention network, namely Gallat (Graph prediction with all attention) as a solution. In Gallat, by comprehensively incorporating those three intrinsic properties of dynamic directed and weighted graphs, we build three attention layers to fully capture the spatiotemporal dependencies among different regions across all historical time slots. Moreover, the model employs a subtask to conduct pretraining so that it can obtain accurate results more quickly. We evaluate the proposed model on real-world datasets, and our experimental results demonstrate that Gallat outperforms the state-of-the-art approaches. © 2021 Association for Computing Machinery.","Dynamic graph; passenger demand prediction; representation learning","Data mining; Directed graphs; Forecasting; Demand prediction; Dynamic graph; Flow routes; Mobility predictions; Passenger demand prediction; Passenger demands; Representation learning; Timeslots; Traffic flow control; Weighted graph; Graphic methods",Article,Scopus,2-s2.0-85125011925
"Rong P., Zhang F., Yang Q., Chen H., Shi Q., Zhong S., Chen Z., Wang H.","57212168615;8879653100;57459662700;57190217973;57191043449;55682669900;57060010900;8844189200;","Processing Laue Microdiffraction Raster Scanning Patterns with Machine Learning Algorithms: A Case Study with a Fatigued Polycrystalline Sample",2022,"Materials","15","4","1502","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124957625&doi=10.3390%2fma15041502&partnerID=40&md5=ad10609a79296bd3ea7e1cd061138934","The massive amount of diffraction images collected in a raster scan of Laue microdiffraction calls for a fast treatment with little if any human intervention. The conventional method that has to index diffraction patterns one-by-one is laborious and can hardly give real-time feedback. In this work, a data mining protocol based on unsupervised machine learning algorithm was proposed to have a fast segmentation of the scanning grid from the diffraction patterns without indexation. The sole parameter that had to be set was the so-called “distance threshold” that determined the number of segments. A statistics-oriented criterion was proposed to set the “distance threshold”. The protocol was applied to the scanning images of a fatigued polycrystalline sample and identified several regions that deserved further study with, for instance, differential aperture X-ray microscopy. The proposed data mining protocol is promising to help economize the limited beamtime. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Fatigued microstructure; Laue microdiffraction; Unsupervised machine learning","Data mining; Diffraction patterns; Fatigue of materials; Interferometry; Learning algorithms; Machine learning; Case-studies; Diffraction images; Fatigued microstructure; Human intervention; Laue microdiffraction; Machine learning algorithms; Polycrystalline samples; Raster scanning; Raster scans; Unsupervised machine learning; Scanning",Article,Scopus,2-s2.0-85124957625
"Wu X.","57223173229;","Identification of Co-Clusters with Coherent Trends in Geo-Referenced Time Series",2022,"ISPRS International Journal of Geo-Information","11","2","134","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124876440&doi=10.3390%2fijgi11020134&partnerID=40&md5=f1062e3cc98c1fcc7b29e513853582da","Several studies have worked on co-clustering analysis of spatio-temporal data. However, most of them search for co-clusters with similar values and are unable to identify co-clusters with coherent trends, defined as exhibiting similar tendencies in the attributes. In this study, we present the Bregman co-clustering algorithm with minimum sum-squared residue (BCC_MSSR), which uses the residue to quantify coherent trends and enables the identification of co-clusters with coherent trends in geo-referenced time series. Dutch monthly temperatures over 20 years at 28 stations were used as the case study dataset. Station-clusters, month-clusters, and co-clusters in the BCC_MSSR results were showed and compared with co-clusters of similar values. A total of 112 co-clusters with different temperature variations were identified in the Results, and 16 representative co-clusters were illustrated, and seven types of coherent temperature trends were summarized: (1) increasing; (2) decreasing; (3) first increasing and then decreasing; (4) first decreasing and then increasing; (5) first increasing, then decreasing, and finally increasing; (6) first decreasing, then increasing, and finally decreasing; and (7) first decreasing, then increasing, decreasing, and finally increasing. Comparisons with co-clusters of similar values show that BCC_MSSR explored coherent spatio-temporal patterns in regions and certain time periods. However, the selection of the suitable co-clustering methods depends on the objective of specific tasks. © 2022 by the author. Licensee MDPI, Basel, Switzerland.","Co-clustering; Coherent trends; Data mining; Temperature series; The Netherlands",,Article,Scopus,2-s2.0-85124876440
"Nolazco-Flores J.A., Faundez-Zanuy M., Velázquez-Flores O.A., Del-Valle-soto C., Cordasco G., Esposito A.","9732640700;57238059400;57222158005;56257456700;57193482076;35310158400;","Mood State Detection in Handwritten Tasks Using PCA–mFCBF and Automated Machine Learning",2022,"Sensors","22","4","1686","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124871842&doi=10.3390%2fs22041686&partnerID=40&md5=1fa7618ce3f0782af715da1f0cb9500e","In this research, we analyse data obtained from sensors when a user handwrites or draws on a tablet to detect whether the user is in a specific mood state. First, we calculated the features based on the temporal, kinematic, statistical, spectral and cepstral domains for the tablet pressure, the horizontal and vertical pen displacements and the azimuth of the pen’s position. Next, we selected features using a principal component analysis (PCA) pipeline, followed by modified fast correlation–based filtering (mFCBF). PCA was used to calculate the orthogonal transformation of the features, and mFCBF was used to select the best PCA features. The EMOTHAW database was used for depression, anxiety and stress scale (DASS) assessment. The process involved the augmentation of the training data by first augmenting the mood states such that all the data were the same size. Then, 80% of the training data was randomly selected, and a small random Gaussian noise was added to the extracted features. Automated machine learning was employed to train and test more than ten plain and ensembled classifiers. For all three moods, we obtained 100% accuracy results when detecting two possible grades of mood severities using this architecture. The results obtained were superior to the results obtained by using state-of-the-art methods, which enabled us to define the three mood states and provide precise information to the clinical psychologist. The accuracy results obtained when detecting these three possible mood states using this architecture were 82.5%, 72.8% and 74.56% for depression, anxiety and stress, respectively. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","AutoML; Data augmentation; Feature extraction; Negative mood states recognition; SVM","Character recognition; Data mining; Feature extraction; Gaussian noise (electronic); Machine learning; Automated machines; Automl; Data augmentation; Features extraction; Negative mood state recognition; Principal-component analysis; State Detection; State recognition; SVM; Training data; Principal component analysis",Article,Scopus,2-s2.0-85124871842
"Ciampi M., Sicuranza M., Silvestri S.","23396134400;55441642700;56450341200;","A Privacy-Preserving and Standard-Based Architecture for Secondary Use of Clinical Data",2022,"Information (Switzerland)","13","2","87","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124809239&doi=10.3390%2finfo13020087&partnerID=40&md5=88b5e25f7cbf6f136ac2343b023f8abd","The heterogeneity of the formats and standards of clinical data, which includes both struc-tured, semi-structured, and unstructured data, in addition to the sensitive information contained in them, require the definition of specific approaches that are able to implement methodologies that can permit the extraction of valuable information buried under such data. Although many challenges and issues that have not been fully addressed still exist when this information must be processed and used for further purposes, the most recent techniques based on machine learning and big data analytics can support the information extraction process for the secondary use of clinical data. In particular, these techniques can facilitate the transformation of heterogeneous data into a common standard format. Moreover, they can also be exploited to define anonymization or pseu-donymization approaches, respecting the privacy requirements stated in the General Data Protection Regulation, Health Insurance Portability and Accountability Act and other national and re-gional laws. In fact, compliance with these laws requires that only de-identified clinical and personal data can be processed for secondary analyses, in particular when data is shared or exchanged across different institutions. This work proposes a modular architecture capable of collecting clinical data from heterogeneous sources and transforming them into useful data for secondary uses, such as research, governance, and medical education purposes. The proposed architecture is able to exploit appropriate modules and algorithms, carry out transformations (pseudonymization and standard-ization) required to use data for the second purposes, as well as provide efficient tools to facilitate the retrieval and analysis processes. Preliminary experimental tests show good accuracy in terms of quantitative evaluations. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","ETL architecture; HL7 FHIR; Information retrieval; Privacy laws; Pseudonymization; Secondary use of clinical data","Clinical research; Data Analytics; Data mining; Health insurance; Information retrieval; Laws and legislation; Medical education; Metadata; Clinical data; ETL architecture; HL7 FHIR; Privacy law; Privacy preserving; Pseudonymization; Secondary use; Secondary use of clinical data; Semistructured data; Unstructured data; Data privacy",Article,Scopus,2-s2.0-85124809239
"Colombo-Mendoza L.O., Paredes-Valverde M.A., Salas-Zárate M.P., Valencia-García R.","55247278900;55888672200;56326000900;55887649000;","Internet of Things-Driven Data Mining for Smart Crop Production Prediction in the Peasant Farming Domain",2022,"Applied Sciences (Switzerland)","12","4","1940","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124746387&doi=10.3390%2fapp12041940&partnerID=40&md5=da4bb69cc48584d890ee6eccf8eedf28","Internet of Things (IoT) technologies can greatly benefit from machine-learning techniques and artificial neural networks for data mining and vice versa. In the agricultural field, this con-vergence could result in the development of smart farming systems suitable for use as decision support systems by peasant farmers. This work presents the design of a smart farming system for crop production, which is based on low-cost IoT sensors and popular data storage services and data analytics services on the cloud. Moreover, a new data-mining method exploiting climate data along with crop-production data is proposed for the prediction of production volume from heterogeneous data sources. This method was initially validated using traditional machine-learning techniques and open historical data of the northeast region of the state of Puebla, Mexico, which were collected from data sources from the National Water Commission and the Agri-food Information Service of the Mexican Government. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Crop production prediction; Data mining; Internet of Things; Peasant farming; Predictive analytics; Smart farming system",,Article,Scopus,2-s2.0-85124746387
"Șerban C., Maftei C., Dobrică G.","24491154500;36499980000;57217314054;","Surface Water Change Detection via Water Indices and Predictive Modeling Using Remote Sensing Imagery: A Case Study of Nuntasi-Tuzla Lake, Romania",2022,"Water (Switzerland)","14","4","556","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124692229&doi=10.3390%2fw14040556&partnerID=40&md5=d76f5585b275f4da93395f09bda59299","Water body feature extraction using a remote sensing technique represents an important tool in the investigation of water resources and hydrological drought assessment. Nuntasi-Tuzla Lake, a component of the Danube Delta Natural Reserve, is located on the Romanian Black Sea littoral. On account of an event in summer 2020, when the lake surface water decreased significantly, this study aims to identify the variation of the Nuntasi-Tuzla Lake surface water over a long-term period in correlation with human intervention and climate change. To this end, it provides an analysis in the period 1965–2021 via hydrological drought indices and data mining classification. The latter approach is based on several water indices derived from Landsat TM/ETM+/OLI and MODIS full-time series datasets: Normalized Difference Vegetation Index (NDVI), Normalized Difference Vegetation Index (NDVI), Modified NDWI (MNDWI), Weighted Normalized Difference Water Index (WNDWI), and Water Ratio Index (WRI). The experimental results indicate that the proposed classification methods can extract relevant features from waterbodies using remote sensing imagery with a high accuracy. Moreover, the study shows a similarity in the evolution of surface water cover identified with the data mining classification and the drought periods detected in the flow data series for the Nuntasi and Sacele Rivers that supply the Nuntasi-Tuzla Lake. Overall, the results of our investigation show that human intervention and hydrological drought had an extensive impact on the long-term changes in surface water of the Nuntasi-Tuzla Lake. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Hydrological drought; Nuntasi-Tuzla Lake; Remote sensing; Romania; Water indices","Classification (of information); Climate change; Data mining; Drought; Extraction; Remote sensing; Vegetation; Human intervention; Hydrological droughts; Mining classification; Normalized difference vegetation index; Nuntasi-tuzlum lake; Remote sensing imagery; Remote-sensing; Romania; Water index; Waterbodies; Lakes; detection method; modeling; remote sensing; surface water",Article,Scopus,2-s2.0-85124692229
"Ma C., Yang J., Xia W., Liu J., Zhang Y., Sui X.","55268105300;57192452052;57204127782;56055157000;57454969500;57454969600;","A Model for Expressing Industrial Information Based on Object-Oriented Industrial Heat Sources Detected Using Multi-Source Thermal Anomaly Data in China",2022,"Remote Sensing","14","4","835","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124691973&doi=10.3390%2frs14040835&partnerID=40&md5=4b60678e196d2c13574a5f11c969e6b9","Industrial heat sources have made a great contribution to Chinese economic development. However, it has also been found that emissions from industrial heat sources are the main contribution to regional air pollution. Therefore, the detection of industrial heat sources and the expression of related information is becoming important. In this paper, the detection of industrial heat sources was used to express industrial information, thus that the accuracy of the detection of industrial thermal anomalies could be improved and the problems of noise and missing parameters addressed. A model for expressing industrial information based on object-oriented industrial heat sources and using multi-source thermal anomaly data in China was, therefore, proposed. It was a new real-time, objective, and real way to describe the production operation status of industrial heat sources on a large-scale area. First, 4340 working industrial heat sources in mainland China were detected by applying an adaptive k-means algorithm to ACF (NPP VIIRS 375-m active fire/hotspot data) data from the period 19 January 2012 to 31 December 2020. Secondly, several features of working industrial heat sources were extracted from NPP VIIRS 375-m active fire/hotspot data (ACF), VIIRS Nightfire data (VNF), and the Fires product based on Landsat-8 AIRCAS (L8F) data. Areas containing working industrial heat sources were then identified based on these different types of fire data. Light, land-surface temperature, and CO2 and N2O emissions data related to the working industrial heat sources were also extracted. The results show that feature parameters extracted from the multi-source thermal anomaly data mostly have a good positive correlation with the other parameters. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","China region; Heat source information description; Industrial heat sources; Landsat-8 fire data; VIIRS active fire data; VIIRS night-time fire data","Data mining; Industrial emissions; K-means clustering; Object detection; Active fires; China region; Heat source information description; Heat sources; Industrial heat source; Information descriptions; LANDSAT; Landsat-8 fire data; Night time; VIIRS active fire data; VIIRS night-time fire data; Fires",Article,Scopus,2-s2.0-85124691973
"Hoxha I., Meklachi T.","41261532000;57189443312;","Data Fitting with Rational Functions: Scaled Null Space Method with Applications of Fitting Large Scale Shocks on Economic Variables and S-Parameters",2022,"Algorithms","15","2","57","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124672926&doi=10.3390%2fa15020057&partnerID=40&md5=064fc808440ee650a026af4a6630d466","Curve fitting discrete data (x, y) with a smooth function is a complex problem when faced with sharply oscillating data or when the data are very large in size. We propose a straightforward method, one that is often overlooked, to fit discrete data (S, ys ) with rational functions. This method serves as a solid data fitting choice that proves to be fast and highly accurate. Its novelty lies on scaling positive explanatory data to the interval [0, 1], before solving the associated linear problem AX = 0. A rescaling is performed once the fitting function is derived. Each solution in the null space of A provides a rational fitting function. Amongst them, the best is chosen based on a pointwise error check. This avoids solving an overdetermined nonhomogeneous linear system Ax = b with a badly conditioned and scaled matrix A. With large data, the latter can lack accuracy and be computationally expensive. Furthermore, any linear combination of at least one solution in the basis of the null space produces a new fitting function, which gives the flexibility to choose the best rational function that fits the constraints of specific problems. We tested our method with many economic variables that experienced sharp oscillations owing to the effects of COVID-19-related shocks to the economy. Such data are intrinsically difficult to fit with a smooth function. Deriving such continuous model functions over a desired period is important in the analysis and prediction algorithms of such economic variables. The method can be expanded to model behaviors of interest in other applied sciences, such as electrical engineering, where the method was successfully fitted into network scattering parameter measurements with high accuracy. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Curve fitting with rational functions; Data mining; Machine learning; Prediction function; Scaling procedure; Stock crash model function; Vector fitting","Curve fitting; Data handling; Linear systems; Machine learning; Rational functions; Scattering parameters; Vector spaces; Crash modelling; Curve fitting with rational function; Curves fittings; Economic variables; Modeling functions; Null space; Prediction function; Scaling procedures; Stock crash model function; Vector fitting; Data mining",Article,Scopus,2-s2.0-85124672926
"Dahr J.M., Hamoud A.K., Najm I.A., Ahmed M.I.","56800888300;57201637276;56366336800;57217377176;","IMPLEMENTING SALES DECISION SUPPORT SYSTEM USING DATA MART BASED ON OLAP, KPI, AND DATA MINING APPROACHES",2022,"Journal of Engineering Science and Technology","17","1",,"275","293",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124621512&partnerID=40&md5=31a86c556f137ed579f83bfd074a7958","Organizations and companies endeavour to expand their resources, margin of profit, and a robust management system. Managers seek tools that adapt the organizational data into information to support their strategic decisions to enhance the organization in terms of performance and profit. A challenging issue helps managers and analysts in making accurate departmental decisions on the basis of the analytical results. A large amount of historical data can be converted into information to support short and long-term strategic decisions. The decision support system (DSS) is one of the essential tools that can handle these data to provide analytical results that help in making the right decisions. This study presents a framework for designing and implementing sales. The framework module explains the DSS implementation. First, the framework establishes data pre-processing. Second, the framework extracts, transforms, and loads from the level of operational data from a data warehouse. Third, building sales cube and knowledge-driven sales is supported on the basis of three data mining approaches (decision tree, clustering, and neural network). The basic elements of the DSS (key performance indicator (KPIs), reports, and knowledge-driven sales supporter) are implemented in the next stage. Finally, the web interface is performed to provide access for sales managers. DSS used data mart on the basis of online analytical processing and KPI as fundamental tools. The design and implementation of a data mart utilize an SQL server database management system and SQL server data tools. The adopted source data were sales data of two years (2017 and 2018) of Basra Governorate. The proposed knowledge-driven sales DSS assists in decision-making. Such a system may assist managers in evolving new insights and strategies. The proposed technique, including the OLAP operation, presents the analytical results in addition to the KPIs that provide the indicators when a critical situation affects the overall profit or performance. © School of Engineering, Taylor’s University","Data mart; Data mining; DSS; KPI; OLAP; Sales",,Article,Scopus,2-s2.0-85124621512
"Xu W., Sun C., Zhang Y., Yue Z., Shabbir S., Zou L., Chen F., Wang L., Yu J.","57215547344;57210314301;57207968567;57210315491;57215544277;57216732708;57216730128;57451882200;24292367100;","Accurate determination of structural H2O in rocks using LIBS coupled with machine learning algorithms extensively exploring the characteristics of the Hαline",2022,"Journal of Analytical Atomic Spectrometry","37","2",,"317","329",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124591787&doi=10.1039%2fd1ja00366f&partnerID=40&md5=0c64eb9a8c671cf44693c715cba5cb70","The application of laser-induced breakdown spectroscopy (LIBS) in elemental analysis and property assessment of geological materials has been demonstrated to be of great importance and effectiveness. The importance of the application becomes paramount for nonmetal elements, such as hydrogen, since competitive analytical techniques, X-ray fluorescence spectrometry for example, become insensitive to light elements. In particular, a precise determination of structural hydrogen in rocks is highly desired for Mars exploration where LIBS provides a unique technique for quantitative determination of water on a submillimeter scale in geological materials. The task is however challenging because the only reliable Hα line exhibits high excitation energy and high sensitivity to Stark broadening and shift, thus suffering from a heavy matrix effect in a LIBS measurement. The purpose of this work is to substantially improve the determination accuracy of structural water in rocks with an original method to deal with the matrix effect, or more exactly the influences of the bulk chemistry of a rock as well as the form and the concentration of the chemical compounds in rock containing H2O as a constituent. The basic idea is to extensively explore the particular sensitivity of the Hα line to the plasma physical properties for an effective correction of the matrix effect. Research was conducted using both physical and statistical data mining approaches, where the first established the influence of water content on the properties of the induced plasma, while the second, with a multivariate regression based on machine learning algorithms, extracted and appropriately manipulated the suitable information for a significant optimization of H2O determination in rocks. The obtained result of a RMSE of about 0.11 wt% for prediction with independent test samples represents an improvement of an order of magnitude with respect to the previously reported studies. © The Royal Society of Chemistry.",,"Data mining; Fluorescence spectroscopy; Geology; Hydrogen; Laser induced breakdown spectroscopy; Learning algorithms; Machine learning; Reflection; Regression analysis; Rocks; Geological materials; Light elements; Machine learning algorithms; Mars exploration; Matrices effect; Precise determinations; Property assessment; Quantitative determinations; Submillimetre; X ray fluorescence spectrometry; Atomic emission spectroscopy",Article,Scopus,2-s2.0-85124591787
"Lyu Y., Chow J.C.-C., Hwang J.-J., Li Z., Ren C., Xie J.","57218168928;7401728920;57193849420;57222735278;57222735863;57447007900;","Psychological Well-Being of Left-Behind Children in China: Text Mining of the Social Media Website Zhihu",2022,"International Journal of Environmental Research and Public Health","19","4","2127","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124417813&doi=10.3390%2fijerph19042127&partnerID=40&md5=a1afae4facaed4debf7567af17d0f50e","China’s migrant population has significantly contributed to its economic growth; however, the impact on the well-being of left-behind children (LBC) has become a serious public health problem. Text mining is an effective tool for identifying people’s mental state, and is therefore beneficial in exploring the psychological mindset of LBC. Traditional data collection methods, which use questionnaires and standardized scales, are limited by their sample sizes. In this study, we created a computational application to quantitively collect personal narrative texts posted by LBC on Zhihu, which is a Chinese question-and-answer online community website; 1475 personal narrative texts posted by LBC were gathered. We used four types of words, i.e., first-person singular pronouns, negative words, past tense verbs, and death-related words, all of which have been associated with depression and suicidal ideations in the Chinese Linguistic Inquiry Word Count (CLIWC) dictionary. We conducted vocabulary statistics on the personal narrative texts of LBC, and bilateral t-tests, with a control group, to analyze the psychological well-being of LBC. The results showed that the proportion of words related to depression and suicidal ideations in the texts of LBC was significantly higher than in the control group. The differences, with respect to the four word types (i.e., first-person singular pronouns, negative words, past tense verbs, and death-related words), were 5.37, 2.99, 2.65, and 2.00 times, respectively, suggesting that LBC are at a higher risk of depression and suicide than their counterparts. By sorting the texts of LBC, this research also found that child neglect is a main contributing factor to psychological difficulties of LBC. Furthermore, mental health problems and the risk of suicide in vulnerable groups, such as LBC, is a global public health issue, as well as an important research topic in the era of digital public health. Through a linguistic analysis, the results of this study confirmed that the experiences of left-behind children negatively impact their mental health. The present findings suggest that it is vital for the public and nonprofit sectors to establish online suicide prevention and intervention systems to improve the well-being of LBC through digital technology. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Left-behind children; Linguistic analysis; Personal narrative text; Psychological well-being; Text mining; Textual analysis","child health; data mining; mental health; psychology; quality of life; social media; World Wide Web; Article; child behavior; child neglect; Chinese (language); controlled study; data mining; depression; digital technology; high risk population; left behind children; linguistics; mental health; mortality; outcome assessment; psychological aspect; psychological well-being; public health problem; publication; risk factor; social aspects and related phenomena; social media; suicidal ideation; China",Article,Scopus,2-s2.0-85124417813
"Xuan Y., Lyu H., An W., Liu J., Liu X.","57447652600;56131336900;57209663448;57202270167;55717391400;","A data-driven deep learning approach for predicting separation-induced transition of submarines",2022,"Physics of Fluids","34","2","024101","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124405836&doi=10.1063%2f5.0079648&partnerID=40&md5=28ba3bc23d0d74d00d69456c546a5ed7","Separation-induced transitions can affect the hydrodynamic performance of submarines significantly. The onset of this transition plays a critical role in the design of aircrafts and underwater vehicles. Since the transition is affected by various factors, its prediction is a challenging task. Convolutional neural networks (CNNs) in the field of machine learning can extract features from high-dimensional data automatically and have good generalization performance. In this paper, we propose an end-to-end CNN-based model to automatically extract the features of separation-induced transitions by learning from image-expressed flow field data. The proposed data-driven deep learning model employs a high-resolution network, which is widely used in key-point detection in the field of computer vision, to extract the underlying features in the separation-induced transition under only a few empirical assumptions. A novel representation of separation-induced transition onset in the form of a heatmap is especially proposed to indicate the probability of transition onset. We use implicit-large-eddy-simulation data generated by the second-order discontinuous Galerkin method over Lyu's-1 model line submarine to verify the proposed method, and the results demonstrate that the new method is able to predict separation-induced transition onsets with quantified uncertainty. The proposed model can be used as an auxiliary tool for aerodynamic and hydrodynamic designs. © 2022 Author(s).",,"Clustering algorithms; Convolutional neural networks; Data mining; Deep learning; Forecasting; Hydrodynamics; Large eddy simulation; Aircraft vehicles; Convolutional neural network; Data driven; End to end; Generalization performance; High dimensional data; Hydrodynamics performance; Induced transitions; Learning approach; Underwater vehicles; Submarines",Article,Scopus,2-s2.0-85124405836
"Chen X., Qiu Q., Feng P.","57218999134;7101979860;7201811340;","Association Analysis between Mechanical Symmetry and Requirements",2022,"Symmetry","14","2","338","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124372260&doi=10.3390%2fsym14020338&partnerID=40&md5=15bde7f0db6e3276adb1f8290f554d9a","This paper focuses on obtaining the association rules between the symmetry of different mechanical structure layers and requirements through data mining, then suggests the selection of symmetry schemes under different requirement conditions, based on the mined association rules and their strength. Firstly, a thousand symmetry structure cases, namely text data, are transformed into binary data. Then, the data analysis software RapidMiner is used to build an association rule mining model to obtain the association rules from symmetry to symmetry, from symmetry to re-quirements, from requirements to requirements and from requirements to symmetry. Among them, this paper focuses on the association rules with requirements as the premise and symmetry as the conclusion. Finally, according to the support and confidence of the association rules, the selection of symmetry under a single requirement, multiple requirements and decomposable requirements is discussed and summarized. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Association rules; Data mining; Mechanical symmetry; Requirements",,Article,Scopus,2-s2.0-85124372260
"Scano A., Mira R.M., Gabbrielli G., Molteni F., Terekhov V.","55388485900;57216709625;57446276600;7006204056;57446721300;","Whole‐Body Adaptive Functional Electrical Stimulation Kinesitherapy Can Promote the Restoring of Physiological Muscle Synergies for Neurological Patients",2022,"Sensors","22","4","1443","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124372050&doi=10.3390%2fs22041443&partnerID=40&md5=fa059a6710a107c597f39f18320b6223","Background: Neurological diseases and traumas are major factors that may reduce motor functionality. Functional electrical stimulation is a technique that helps regain motor function, assisting patients in daily life activities and in rehabilitation practices. In this study, we evaluated the efficacy of a treatment based on whole‐body Adaptive Functional Electrical Stimulation Kinesitherapy (AFESK™) with the use of muscle synergies, a well‐established method for evaluation of motor coordination. The evaluation is performed on retrospectively gathered data of neurological patients executing whole‐body movements before and after AFESK‐based treatments. Methods: Twenty‐four chronic neurologic patients and 9 healthy subjects were recruited in this study. The patient group was further subdivided in 3 subgroups: hemiplegic, tetraplegic and paraplegic. All patients underwent two acquisition sessions: before treatment and after a FES based rehabilitation treatment at the VIKTOR Physio Lab. Patients followed whole‐body exercise protocols tailored to their needs. The control group of healthy subjects performed all movements in a single session and provided reference data for evaluating patients’ performance. sEMG was recorded on relevant muscles and muscle synergies were extracted for each patient’s EMG data and then compared to the ones extracted from the healthy volunteers. To evaluate the effect of the treatment, the motricity index was measured and patients’ extracted synergies were compared to the control group before and after treatment. Results: After the treatment, patients’ motricity index increased for many of the screened body segments. Muscle synergies were more similar to those of healthy people. Globally, the normalized synergy similarity in respect to the control group was 0.50 before the treatment and 0.60 after (p < 0.001), with improvements for each subgroup of patients. Conclusions: AFESK treatment induced favorable changes in muscle activation patterns in chronic neurologic patients, partially restoring muscular patterns similar to healthy people. The evaluation of the synergic relationships of muscle activity when performing test exercises allows to assess the results of rehabilitation measures in patients with impaired locomotor functions. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Muscle synergies; Neurological patients; Whole body FES","Data mining; Function evaluation; Functional electric stimulation; Neurology; Patient treatment; Petroleum reservoir evaluation; Control groups; Functional electri-cal stimulations; Healthy people; Healthy subjects; Major factors; Muscle synergies; Neurological disease; Neurological patient; Whole body FES; Whole-body; Muscle",Article,Scopus,2-s2.0-85124372050
"Flores V., Heras S., Julian V.","57286666200;15050558500;6602206677;","Comparison of Predictive Models with Balanced Classes Using the SMOTE Method for the Forecast of Student Dropout in Higher Education",2022,"Electronics (Switzerland)","11","3","457","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124355045&doi=10.3390%2felectronics11030457&partnerID=40&md5=e122ed6bcd54d2718bb5c4ff3acc0d2b","Based on the premise that university student dropout is a social problem in the university ecosystem of any country, technological leverage is a way that allows us to build technological proposals to solve a poorly met need in university education systems. Under this scenario, the study presents and analyzes eight predictive models to forecast university dropout, based on data mining methods and techniques, using WEKA for its implementation, with a dataset of 4365 academic records of students from the National University of Moquegua (UNAM), Peru. The objective is to determine which model presents the best performance indicators to forecast and prevent student dropout. The study aims to propose and compare the accuracy of eight predictive models with balanced classes, using the SMOTE method for the generation of synthetic data. The results allow us to confirm that the predictive model based on Random Forest is the one that presents the highest accuracy and robustness. This study is of great interest to the educational community as it allows for predicting the possible dropout of a student from a university career and being able to take corrective actions both at a global and individual level. The results obtained are highly interesting for the university in which the study has been carried out, obtaining results that generally outperform the results obtained in related works. © 2022 by the author. Licensee MDPI, Basel, Switzerland.","Data mining; Predictive model; SMOTE; University student dropout",,Article,Scopus,2-s2.0-85124355045
"Randall M., Lewis A., Stewart-Koster B., Anh N.D., Burford M., Condon J., van Qui N., Hiep L.H., van Bay D., van Sang N., Sammut J.","7101974559;35199653200;18134494100;57445375700;7006131287;35833306300;57445809300;57194085405;57445375900;25957789800;57444942900;","A Bayesian belief data mining approach applied to rice and shrimp aquaculture",2022,"PLoS ONE","17","2 February","e0262402","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124296975&doi=10.1371%2fjournal.pone.0262402&partnerID=40&md5=23f87f7fb015eaf6aba3efb436e5ef76","In many parts of the world, conditions for small scale agriculture are worsening, creating challenges in achieving consistent yields. The use of automated decision support tools, such as Bayesian Belief Networks (BBNs), can assist producers to respond to these factors. This paper describes a decision support system developed to assist farmers on the Mekong Delta, Vietnam, who grow both rice and shrimp crops in the same pond, based on an existing BBN. The BBN was previously developed in collaboration with local farmers and extension officers to represent their collective perceptions and understanding of their farming system and the risks to production that they face. This BBN can be used to provide insight into the probable consequences of farming decisions, given prevailing environmental conditions, however, it does not provide direct guidance on the optimal decision given those decisions. In this paper, the BBN is analysed using a novel, temporally-inspired data mining approach to systematically determine the agricultural decisions that farmers perceive as optimal at distinct periods in the growing and harvesting cycle, given the prevailing agricultural conditions. Using a novel form of data mining that combines with visual analytics, the results of this analysis allow the farmer to input the environmental conditions in a given growing period. They then receive recommendations that represent the collective view of the expert knowledge encoded in the BBN allowing them to maximise the probability of successful crops. Encoding the results of the data mining/inspection approach into the mobile Decision Support System helps farmers access explicit recommendations from the collective local farming community as to the optimal farming decisions, given the prevailing environmental conditions. © 2022 Randall et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"agricultural worker; article; Bayesian network; crop; data mining; decision support system; farming system; human; nonhuman; perception; probability; rice; shrimp; shrimp farming; Viet Nam; Bayes theorem; Bayes Theorem",Article,Scopus,2-s2.0-85124296975
"Dos Santos B.D., de Pinho C.M.D., Oliveira G.E.T., Korting T.S., Escada M.I.S., Amaral S.","57203753950;57443502600;57443697200;25825294600;6506455630;7004745915;","Identifying Precarious Settlements and Urban Fabric Typologies Based on GEOBIA and Data Mining in Brazilian Amazon Cities",2022,"Remote Sensing","14","3","704","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124243124&doi=10.3390%2frs14030704&partnerID=40&md5=a2cc18633f10ecf1a1cb3dd25df8b486","Although 70% of the Amazon population lives in urban areas, studies on the urban Amazon are scarce. Much of the urban Amazon population lives in precarious settlements. The distinctiveness and diversity of Amazonian precarious settlements are vast and must be identified to be considered in the development of appropriate public policies. Aiming at investigating precarious settlements in Amazon, this study is guided by the following questions: For the Brazilian Amazon region, is it possible to identify areas of precarious settlements by combining geoprocessing and remote sensing techniques? Are there different typologies of precarious settlements distinguishable by their spatial arrangements? Thus, we developed a methodology for identifying precarious settlements and subsequently classifying them into urban fabric typologies (UFT), choosing the cities of Altamira, Cametá, and Marabá as study sites. Our classification model utilized geographic objects-based image analysis (GEOBIA) and data mining of spectral data from WPM sensor images from the CBERS-4A satellite, jointly with texture metrics, context metrics, biophysical index, voluntary geographical information, and neighborhood relationships. With the C5.0 decision tree algorithm we carried out variable selection and classification of these geographic objects. Our estimated models show accuracy above 90% when applied to the study sites. Additionally, we described Amazonian UFT in six types to be identified. We concluded that Amazonian precarious settlements are morphologically diverse, with an urban fabric different from those commonly found in Brazilian metropolitan areas. Identifying and characterizing distinct precarious areas is vital for the planning and development of sustainable and effective public policies for the urban Amazon. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Amazonian precarious settlements; Amazonian urbanization; Data mining; GEOBIA; Urban fabric typology","Classification (of information); Decision trees; Public policy; Remote sensing; Textures; Urban growth; Amazon region; Amazonian precarious settlement; Amazonian urbanization; Brazilian Amazon; Geographic object-based image analysis; Image data; Study sites; Urban areas; Urban fabric typology; Urban fabrics; Data mining",Article,Scopus,2-s2.0-85124243124
"Allam Z., Bibri S.E., Jones D.S., Chabaud D., Moreno C.","57205544315;57116528800;56606411400;26643761800;57377619500;","Unpacking the ‘15-Minute City’ via 6G, IoT, and Digital Twins: Towards a New Narrative for Increasing Urban Efficiency, Resilience, and Sustainability",2022,"Sensors","22","4","1369","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124241137&doi=10.3390%2fs22041369&partnerID=40&md5=4c8f17843ec5dce6fc410c7e5742b661","The ‘15-minute city’ concept is emerging as a potent urban regeneration model in post-pandemic cities, offering new vantage points on liveability and urban health. While the concept is primarily geared towards rethinking urban morphologies, it can be furthered via the adoption of Smart Cities network technologies to provide tailored pathways to respond to contextualised challenges through the advent of data mining and processing to better inform urban decision-making processes. We argue that the ‘15-minute city’ concept can value-add from Smart City network technologies in particular through Digital Twins, Internet of Things (IoT), and 6G. The data gathered by these technologies, and processed via Machine Learning techniques, can unveil new patterns to understand the characteristics of urban fabrics. Collectively, those dimensions, unpacked to support the ‘15-minute city’ concept, can provide new opportunities to redefine agendas to better respond to economic and societal needs as well as align more closely with environmental commitments, including the United Nations’ Sustainable Development Goal 11 and the New Urban Agenda. This perspective paper presents new sets of opportunities for cities arguing that these new connectivities should be explored now so that appropriate protocols can be devised and so that urban agendas can be recalibrated to prepare for upcoming technology advances, opening new pathways for urban regeneration and resilience crafting. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","6G; Climate change; Connectivity; Data; Internet of Things (IoT); Resilience; Sensors; Smart cities; Sustainability; Wireless communications","Data handling; Data mining; Decision making; Internet of things; Learning systems; Smart city; Sustainable development; 6g; Connectivity; Internet of thing; Network technologies; Regeneration model; Resilience; Urban health; Urban morphology; Urban regeneration; Wireless communications; Climate change",Article,Scopus,2-s2.0-85124241137
"Payyanadan R.P., Angell L.S.","57105496700;36907040600;","A Framework for Building Comprehensive Driver Profiles",2022,"Information (Switzerland)","13","2","61","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124085733&doi=10.3390%2finfo13020061&partnerID=40&md5=c61abc99db2fcf94ef27b66d6f1113dd","Conventional approaches to modelling driver risk have incorporated measures such as driver gender, age, place of residence, vehicle model, and annual miles driven. However, in the last decade, research has shown that assessing a driver’s crash risk based on these variables does not go far enough—especially as advanced technology changes today’s vehicles, as well as the role and behavior of the driver. There is growing recognition that actual driver usage patterns and driving behavior, when it can be properly captured in modelling risk, offers higher accuracy and more individually tailored projections. However, several challenges make this difficult. These challenges include accessing the right types of data, dealing with high-dimensional data, and identifying the underlying structure of the variance in driving behavior. There is also the challenge of how to identify key variables for detecting and predicting risk, and how to combine them in predictive algorithms. This paper proposes a systematic feature extraction and selection framework for building Comprehensive Driver Profiles that serves as a foundation for driver behavior analysis and building whole driver profiles. Features are extracted from raw data using statistical feature extraction techniques, and a hybrid feature selection algorithm is used to select the best driver profile feature set based on outcomes of interest such as crash risk. It can give rise to individualized detection and prediction of risk, and can also be used to identify types of drivers who exhibit similar patterns of driving and vehicle/technology usage. The developed framework is applied to a naturalistic driving dataset— NEST, derived from the larger SHRP2 naturalistic driving study to illustrate the types of information about driver behavior that can be harnessed—as well as some of the important applications that can be derived from it. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Crash risk; Driver behavior analysis; Driver profile framework; Driver support systems; Individual differences; Insurance telematics; Naturalistic driving; Profile stability","Accidents; Automobile drivers; Clustering algorithms; Data mining; Extraction; Feature extraction; Risk assessment; Vehicles; Conventional approach; Crash risk; Driver behaviour analysis; Driver profile framework; Driving behaviour; Individual Differences; Insurance telematic; Naturalistic driving; Profile stability; Telematics; Telematics",Article,Scopus,2-s2.0-85124085733
"Araujo G.F., Machado R., Pettersson M.I.","57204625182;7101977934;57204162884;","Non-Cooperative SAR Automatic Target Recognition Based on Scattering Centers Models",2022,"Sensors","22","3","1293","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124078825&doi=10.3390%2fs22031293&partnerID=40&md5=dc5c3f98166d0f25e18b51abed171516","This article proposes an Automatic Target Recognition (ATR) algorithm to classify non-cooperative targets in Synthetic Aperture Radar (SAR) images. The scarcity or nonexistence of measured SAR data demands that classification algorithms rely only on synthetic data for training purposes. Based on a model represented by the set of scattering centers extracted from purely synthetic data, the proposed algorithm generates hypotheses for the set of scattering centers extracted from the target under test belonging to each class. A Goodness of Fit test is considered to verify each hypothesis, where the Likelihood Ratio Test is modified by a scattering center-weighting function common to both the model and target. Some algorithm variations are assessed for scattering center extraction and hypothesis generation and verification. The proposed solution is the first model-based classification algorithm to address the recently released Synthetic and Measured Paired Labeled Experiment (SAMPLE) dataset on a 100% synthetic training data basis. As a result, an accuracy of 91.30% in a 10-target test within a class experiment under Standard Operating Conditions (SOCs) was obtained. The algorithm was also pioneered in testing the SAMPLE dataset in Extend Operating Conditions (EOCs), assuming noise contamination and different target configurations. The proposed algorithm was shown to be robust for SNRs greater than −5 dB. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Automatic target recognition; Classification; Scattering center; Synthetic aperture radar","Automatic target recognition; Data mining; Radar target recognition; Statistical tests; Synthetic aperture radar; Classification algorithm; Non-cooperative; Non-cooperative target; Radar data; Scattering center models; Scattering centers; Synthetic aperture radar automatic target recognition; Synthetic aperture radar images; Synthetic data; Target recognition algorithms; Classification (of information); algorithm; automated pattern recognition; telecommunication; Algorithms; Pattern Recognition, Automated; Radar; Recognition, Psychology",Article,Scopus,2-s2.0-85124078825
"da Silva L.P., da Fonseca M.N., de Moura E.N., de Souza F.T.","57189270149;57223234499;57212931380;36617082000;","Ecosystems Services and Green Infrastructure for Respiratory Health Protection: A Data Science Approach for Paraná, Brazil",2022,"Sustainability (Switzerland)","14","3","1835","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124073552&doi=10.3390%2fsu14031835&partnerID=40&md5=0fcb4bc42266dd3d36f3220d11ad4d65","Urban ecosystem services have become a main issue in contemporary urban sustainable development, whose efforts are challenged by the phenomena of world urbanization and climate change. This article presents a study about the ecosystem services of green infrastructure towards better respiratory health in a socioeconomic scenario typical of the Global South countries. The study involved a data science approach comprising basic and multivariate statistical analysis, as well as data mining, for the municipalities of the state of Paraná, in Brazil’s South region. It is a cross-sectional study in which multiple data sets are combined and analyzed to uncover relationships or patterns. Data were extracted from national public domain databases. We found that, on average, the municipalities with more area of biodiversity per inhabitant have lower rates of hospitalizations resulting from respiratory diseases (CID-10 X). The biodiversity index correlates inversely with the rates of hospitalizations. The data analysis also demonstrated the importance of socioeconomic issues in the environmental-respiratory health phenomena. The data mining analysis revealed interesting associative rules consistent with the learning from the basic statistics and multivariate analysis. Our findings suggest that green infrastructure provides ecosystem services towards better respiratory health, but these are entwined with socioeconomics issues. These results can support public policies towards environmental and health sustainable management. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Biodiversity; Climate labs; Data mining; SDG 1; SDG 11; SDG 13; SDG 2; SDG 3; Urban health; Urban planning","biodiversity; climate change; ecosystem service; greenspace; public health; respiration; respiratory disease; spatiotemporal analysis; suburbanization; urbanization; Brazil; Parana [Brazil]",Article,Scopus,2-s2.0-85124073552
"Zhang C., Ma X., Qin P.","56083549700;57285880300;57216565182;","LiDAR-IMU-UWB-Based Collaborative Localization",2022,"World Electric Vehicle Journal","13","2","32","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124034784&doi=10.3390%2fwevj13020032&partnerID=40&md5=d9b63f3b45fcb1ff8aed57686a9dfd5f","This article introduced a positioning system composed of different sensors, such as Li-DAR, IMU, and ultra-wideband (UWB), for the positioning method in autonomous driving technology under closed coal mine tunnels. First, we processed the LiDAR data, extracted its feature points and merged the extracted feature point clouds to generate a skewed combined feature point cloud. Then, we used the skew combined feature point clouds for feature matching, performed pre-integration processing on the IMU sensor data, and completed the LiDAR-IMU odometer with the Li-DAR. Finally, we added UWB data to IMU pose node as a one-dimensional over-edge constraint. By updating the sliding window, the positioning accuracy was further improved. Moreover, we have conducted experiments to verify the proposed positioning system in a simulated roadway. The experimental results showed that the method proposed in this paper is superior to the single LiDAR method and the single UWB method in terms of positioning accuracy. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Feature extraction; Feature matching; Localization; Pose constrain","Coal mines; Data mining; Optical radar; Collaborative localizations; Combined features; Features extraction; Features matching; Localisation; Point-clouds; Pose constrain; Positioning accuracy; Positioning system; Ultrawide band; Ultra-wideband (UWB)",Article,Scopus,2-s2.0-85124034784
"Brackenridge R.E., Demyanov V., Vashutin O., Nigmatullin R.","35791171100;6701452362;57439499500;57142025100;","Improving Subsurface Characterisation with ‘Big Data’ Mining and Machine Learning",2022,"Energies","15","3","1070","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124023265&doi=10.3390%2fen15031070&partnerID=40&md5=3f7860ad2efaf89502e4e584308e5107","Large databases of legacy hydrocarbon reservoir and well data provide an opportunity to use modern data mining techniques to improve our understanding of the subsurface in the presence of uncertainty and improve predictability of reservoir properties. A data mining approach provides a way to screen dependencies in reservoir and fluid data and enable subsurface specialists to estimate absent properties in partial or incomplete datasets. This allows for uncertainty to be managed and reduced. An improvement in reservoir characterisation using machine learning results from the capacity of machine learning methods to detect and model hidden dependencies in large multivariate datasets with noisy and missing data. This study presents a workflow applied to a large basin‐scale reservoir characterization database. The study aims to understand the dependencies between reservoir attributes in order to allow for predictions to be made to improve the data coverage. The machine learning workflow comprises the following steps: (i) exploratory data analysis; (ii) detection of outliers and data partitioning into groups showing similar trends using clustering; (iii) identification of dependencies within reservoir data in multivariate feature space with self‐organising maps; and (iv) feature selection using supervised learning to identify relevant properties to use for predictions where data are absent. This workflow provides an opportunity to reduce the cost and increase accuracy of hydrocarbon exploration and production in mature basins. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Big data; Hydrocarbon exploration; Machine learning; Multivariant analysis; Reservoir; Subsurface characterisation; Supervised learning; Unsupervised learning","Data mining; Hydrocarbons; Large dataset; Multivariant analysis; Petroleum prospecting; Hydrocarbon exploration; Hydrocarbon reservoir; Large database; Machine-learning; Property; Reservoir characterization; Reservoir data; Subsurface characterizations; Uncertainty; Work-flows; Supervised learning",Article,Scopus,2-s2.0-85124023265
"Liu B., Zhang H., Kong L., Niu D.","57054251300;57439202900;53984232100;36783939200;","Factorizing Historical User Actions for Next-Day Purchase Prediction",2022,"ACM Transactions on the Web","16","1","3468227","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124012699&doi=10.1145%2f3468227&partnerID=40&md5=25fa94669683cb7306838cb9471c119e","It is common practice for many large e-commerce operators to analyze daily logged transaction data to predict customer purchase behavior, which may potentially lead to more effective recommendations and increased sales. Traditional recommendation techniques based on collaborative filtering, although having gained success in video and music recommendation, are not sufficient to fully leverage the diverse information contained in the implicit user behavior on e-commerce platforms. In this article, we analyze user action records in the Alibaba Mobile Recommendation dataset from the Alibaba Tianchi Data Lab, as well as the Retailrocket recommender system dataset from the Retail Rocket website. To estimate the probability that a user will purchase a certain item tomorrow, we propose a new model called Time-decayed Multifaceted Factorizing Personalized Markov Chains (Time-decayed Multifaceted-FPMC), taking into account multiple types of user historical actions not only limited to past purchases but also including various behaviors such as clicks, collects and add-to-carts. Our model also considers the time-decay effect of the influence of past actions. To learn the parameters in the proposed model, we further propose a unified framework named Bayesian Sparse Factorization Machines. It generalizes the theory of traditional Factorization Machines to a more flexible learning structure and trains the Time-decayed Multifaceted-FPMC with the Markov Chain Monte Carlo method. Extensive evaluations based on multiple real-world datasets demonstrate that our proposed approaches significantly outperform various existing purchase recommendation algorithms. © 2021 Association for Computing Machinery.","factorization machine; factorizing personalized Markov chains; Markov chain Monte Carlo; matrix factorization; Online purchase prediction; recommendation systems","Behavioral research; Collaborative filtering; Data mining; Electronic commerce; Factorization; Markov processes; Monte Carlo methods; Online systems; Recommender systems; Sales; E- commerces; Factorization machines; Factorizing personalized markov chain; Markov chain Monte Carlo; Markov Chain Monte-Carlo; Matrix factorizations; Online purchase prediction; Recommendation techniques; Transaction data; User action; Forecasting",Article,Scopus,2-s2.0-85124012699
"Gupta P.K., Maiti S.","57224564410;8635505100;","Enhancing data-driven modeling of fluoride concentration using new data mining algorithms",2022,"Environmental Earth Sciences","81","3","89","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124000708&doi=10.1007%2fs12665-022-10216-z&partnerID=40&md5=ec8b9267ca8ef61bd0d752aa268d66dc","Groundwater is an essential constituent of drinking water in hard rock areas and hence it requires the analysis of contaminant resources. Fluoride contamination with large spatial variation in the part of Sindhudurg district is reported. The present study focuses on the development of data-driven modeling of fluoride concentration using on-site measurement of physicochemical parameters. In this configuration, six machine learning(ML) architectures, namely data mining algorithms were explored including novel algorithms Gaussian process (GP) and long short term memory (LSTM). The results were compared with support vector machine (SVM), random forest (RF), extreme learning machine (ELM), and multi-layer perceptron (MLP) as a benchmark to test the robustness of the modeling process. In total 225 water samples from different dug-wells/bore- wells were obtained from the area (latitude:15.37–16.40 degree, longitude:73.19–74.18 degree) in the period of 2009–2016. Two subsets of data were divided with 80% data in training and 20% in testing. Different 9 physicochemical parameters pH, EC, TDS, Ca2+, Mg2+, Na+, Cl−, HCO3−, SO42− were used in the modeling of fluoride (F−). In this context logarithmic transformation of raw data was employed to improve the correlation between input and target and therefore to enhance the modeling accuracy. Different quantitative and qualitative (visual) measures were taken to establish the prediction power of models. Results revealed that GP outperform all other models in fluoride prediction followed by LSTM, SVM, MLP, RF, and ELM, respectively. Results also revealed that the model’s performance depends on model structure and data accuracy. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Data mining algorithms; Data transformation; Fluoride; Gaussian process","Data mining; Decision trees; Fluorine compounds; Gaussian distribution; Gaussian noise (electronic); Groundwater; Groundwater pollution; Groundwater resources; Long short-term memory; Potable water; Support vector machines; Data mining algorithm; Data-driven model; Datum transformation; Fluoride; Fluoride concentrations; Gaussian Processes; Multilayers perceptrons; Physico - chemical parameters; Random forests; Support vectors machine; Metadata; algorithm; concentration (composition); data mining; fluoride; groundwater pollution; hard rock; numerical model; prediction; India; Maharashtra; Sindhudurg",Article,Scopus,2-s2.0-85124000708
"Sann R., Lai P.-C., Liaw S.-Y., Chen C.-T.","57215081028;9276310500;7103188398;57141435600;","Predicting Online Complaining Behavior in the Hospitality Industry: Application of Big Data Analytics to Online Reviews",2022,"Sustainability (Switzerland)","14","3","1800","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123990380&doi=10.3390%2fsu14031800&partnerID=40&md5=5b4b72390de643e1a8a6bae6b83a0d3e","Purpose: This study aims to enrich the published literature on hospitality and tourism by applying big data analytics and data mining algorithms to predict travelers’ online complaint attributions to significantly different hotel classes (i.e., higher star-rating and lower star-rating). Design/methodology/approach: First, 1992 valid online complaints were manually obtained from over 350 hotels located in the UK. The textual data were converted into structured data by utilizing content analysis. Ten complaint attributes and 52 items were identified. Second, a two-step analysis approach was applied via data-mining algorithms. For this study, sensitivity analysis was conducted to identify the most important online complaint attributes, then decision tree models (i.e., the CHAID algorithm) were implemented to discover potential relationships that might exist between complaint attributes in the online complaining behavior of guests from different hotel classes. Findings: Sensitivity analysis revealed that Hotel Size is the most important online complaint attribute, while Service Encounter and Room Space emerged as the second and third most important factors in each of the four decision tree models. The CHAID analysis findings also revealed that guests at higher-star-rating hotels are most likely to leave online complaints about (i) Service Encounter, when staying at large hotels; (ii) Value for Money and Service Encounter, when staying at medium-sized hotels; (iii) Room Space and Service Encounter, when staying at small hotels. Additionally, the guests of lower-star-rating hotels are most likely to write online complaints about Cleanliness, but not Value for Money, Room Space, or Service Encounter, and to stay at small hotels. Practical implications: By utilizing new data-mining algorithms, more profound findings can be discovered and utilized to reinforce the strengths of hotel operations to meet the expectations and needs of their target guests. Originality/value: The study’s main contribution lies in the utilization of data-mining algorithms to predict online complaining behavior between different classes of hotel guests. © 2022 by the authors. Li-censee MDPI, Basel, Switzerland.","Big data analytics; Data mining algorithms; Decision trees; Hotel class; Online complaining attributes; Online complaining behavior","algorithm; data mining; hospitality industry; prediction; tourism; United Kingdom",Article,Scopus,2-s2.0-85123990380
"El-Hasnony I.M., Elzeki O.M., Alshehri A., Salem H.","57216627885;57220855783;57437511100;57189002733;","Multi-Label Active Learning-Based Machine Learning Model for Heart Disease Prediction",2022,"Sensors","22","3","1184","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123934462&doi=10.3390%2fs22031184&partnerID=40&md5=03abbbe0636ea6723ba75b4f4b2ed226","The rapid growth and adaptation of medical information to identify significant health trends and help with timely preventive care have been recent hallmarks of the modern healthcare data system. Heart disease is the deadliest condition in the developed world. Cardiovascular disease and its complications, including dementia, can be averted with early detection. Further research in this area is needed to prevent strokes and heart attacks. An optimal machine learning model can help achieve this goal with a wealth of healthcare data on heart disease. Heart disease can be predicted and diagnosed using machine-learning-based systems. Active learning (AL) methods improve classification quality by incorporating user–expert feedback with sparsely labelled data. In this paper, five (MMC, Random, Adaptive, QUIRE, and AUDI) selection strategies for multi-label active learning were applied and used for reducing labelling costs by iteratively selecting the most relevant data to query their labels. The selection methods with a label ranking classifier have hyperparameters optimized by a grid search to implement predictive modelling in each scenario for the heart disease dataset. Experimental evaluation includes accuracy and F-score with/without hyperparameter optimization. Results show that the generalization of the learning model beyond the existing data for the optimized label ranking model uses the selection method versus others due to accuracy. However, the selection method was highlighted in regards to the F-score using optimized settings. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Active learning; Chronic diseases; Data mining; Heart disease; Machine learning; Multi-label classification","Cardiology; Data mining; Diseases; Health care; Heart; Iterative methods; Machine learning; Medical computing; Medical information systems; Active Learning; Chronic disease; F-score; Heart disease; Label rankings; Machine learning models; Multi-labels; Rapid adaptation; Rapid growth; Selection methods; Classification (of information); cardiovascular disease; health care delivery; heart disease; human; machine learning; supervised machine learning; Cardiovascular Diseases; Delivery of Health Care; Heart Diseases; Humans; Machine Learning; Supervised Machine Learning",Article,Scopus,2-s2.0-85123934462
"Nolde J.M., Mian A., Schlaich L., Chan J., Lugo-Gavidia L.M., Barrie N., Gopal V., Hillis G.S., Chow C.K., Schlaich M.P.","57205561339;57358209300;57219834723;57209011086;55575076200;56644816900;57435885500;7005385091;8871779800;7003349164;","Automatic data extraction from 24 hour blood pressure measurement reports of a large multicenter clinical trial",2022,"Computer Methods and Programs in Biomedicine","214",,"106588","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123878297&doi=10.1016%2fj.cmpb.2021.106588&partnerID=40&md5=9d2e31bfc14693154fe9e2acae75e074","Background and objectives: Ambulatory blood pressure monitoring (ABPM) is usually reported in descriptive values such as circadian averages and standard deviations. Making use of the original, individual blood pressure measurements may be advantageous, particularly for research purposes, as this increases the flexibility of the analytical process, enables alternative statistical analyses and provide novel insights. Here we describe the development of a new multistep, hierarchical data extraction algorithm to collect raw data from .pdf reports and text files as part of a large multi-center clinical study. Methods: Original reports were saved in a nested file system, from which they were automatically extracted, read and saved into databases with custom made programs written in Python 3. Data were further processed, cleaned and relevant descriptive statistics such as averages and standard deviations calculated according to a variety of definitions of day- and night-time. Additionally, data control mechanisms for manual review of the data and programmatic auto-detection of extraction errors was implemented as part of the project. Results: The developed algorithm extracted 97% of the data automatically, the missing data consisted mostly of reports that were saved incorrectly or not formatted in the specified way. Manual checks comparing samples of the extracted data to original reports indicated a high level of accuracy of the extracted data, no errors introduced due to flaws in the extraction software were detected in the extracted dataset. Conclusions: The developed multistep, hierarchical data extraction algorithm facilitated collection from different file formats and paired with database cleaning and data processing steps led to an effective and accurate assembly of raw ABPM data for further and adjustable analyses. Manual work was minimized while data quality was ensured with standardized, reproducible procedures. © 2021 Elsevier B.V.","Automation; Blood pressure; Cardiovascular risk; Data extraction; Database","Blood pressure; Computer software; Data handling; Data mining; Data visualization; Extraction; Pressure measurement; Python; Risk assessment; Statistics; Average deviation; Blood pressure measurement; Blood-pressure monitoring; Cardiovascular risk; Data extraction; Extraction algorithms; Hierarchical data; Measurement reports; Multisteps; Standard deviation; Database systems; algorithm; article; automation; blood pressure; blood pressure monitoring; cardiovascular risk; cleaning; controlled study; data extraction; data quality; human; manual labor; multicenter study; night; software; blood pressure; clinical trial; factual database; Algorithms; Blood Pressure; Blood Pressure Monitoring, Ambulatory; Databases, Factual; Software",Article,Scopus,2-s2.0-85123878297
"Mirhashemi S.H., jou P.H., Panahi M.","57211467075;55321316800;57193681013;","Extracting association rules in relation to precipitation and effective factors",2022,"Sustainable Water Resources Management","8","1","35","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123769972&doi=10.1007%2fs40899-022-00614-3&partnerID=40&md5=c82aedb1f62107eb2a01a1c6060a3e36","In recent years, knowledge production from the massive amount of data using data mining techniques has attracted attention. Meanwhile, prediction of precipitation in various hydrological issues such as runoff, flood, and drought as well as watershed management is of great importance. Accordingly, the purpose of this research is to extract association rules using data mining techniques to verify and predict the amount of precipitation. The monthly data of precipitation and effective factors related to it were used in this study. This research was carried out in Qazvin Plain for 30 years from 1988 to 2018. Different factors affecting the amount of precipitation with different intervals, including time without delay and delay of 1 to 3 months, were used. Four scenarios were defined based on the four timescales of the influential factors. For each scenario, rules on precipitation and its influential factors were extracted by the Apriori algorithm. The extracted rules were evaluated by the indicators of confidence, support, and lift. The accuracy of the rules was evaluated for all four scenarios according to the three indicators and the best scenario was chosen. According to the results of the evaluation indicators, it was determined that effective factors with the 2-month delay had the most substantial effect on predicting the amount of precipitation. In the last step, the independent relationship between precipitation and factors affecting the 2-month delay was examined. Finally, it was determined that the average pressure level factor of the station with a 2-month delay had the most significant relationship with precipitation in Qazvin Plain. © 2022, The Author(s), under exclusive licence to Springer Nature Switzerland AG.","Apriori algorithm; Hydrology; Mean station pressure; Qazvin plain","Association rules; Data mining; Forecasting; Soil conservation; Water conservation; Water management; Apriori algorithms; Data-mining techniques; Drought management; Influential factors; Knowledge production; Management IS; Mean station pressure; Qazvin plain; Station pressure; Watersheds management; Learning algorithms; algorithm; data mining; drought; flood; hydrological response; precipitation assessment; runoff; timescale; water management; watershed; Iran; Qazvin [Iran]; Qazvin [Qazvin (PRV)]",Article,Scopus,2-s2.0-85123769972
"Zhang Q., Yang L.T., Chen Z., Li P.","56017374400;57203323020;56020772800;57214069572;","PPHOPCM: Privacy-Preserving High-Order Possibilistic c-Means Algorithm for Big Data Clustering with Cloud Computing",2022,"IEEE Transactions on Big Data","8","1",,"25","34",,46,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123739288&doi=10.1109%2fTBDATA.2017.2701816&partnerID=40&md5=9fa6f36165faca6950573eed1a5e5b07","As one important technique of fuzzy clustering in data mining and pattern recognition, the possibilistic c-means algorithm (PCM) has been widely used in image analysis and knowledge discovery. However, it is difficult for PCM to produce a good result for clustering big data, especially for heterogenous data, since it is initially designed for only small structured dataset. To tackle this problem, the paper proposes a high-order PCM algorithm (HOPCM) for big data clustering by optimizing the objective function in the tensor space. Further, we design a distributed HOPCM method based on MapReduce for very large amounts of heterogeneous data. Finally, we devise a privacy-preserving HOPCM algorithm (PPHOPCM) to protect the private data on cloud by applying the BGV encryption scheme to HOPCM, In PPHOPCM, the functions for updating the membership matrix and clustering centers are approximated as polynomial functions to support the secure computing of the BGV scheme. Experimental results indicate that PPHOPCM can effectively cluster a large number of heterogeneous data using cloud computing without disclosure of private data. © 2015 IEEE.","Big data clustering; Cloud computing; Possibilistic c-means; Privacy preserving; Tensor space","Big data; Cluster analysis; Cluster computing; Clustering algorithms; Data mining; Fuzzy clustering; Pattern recognition; Privacy-preserving techniques; Tensors; Big data clustering; C-means algorithms; Cloud-computing; Heterogeneous data; High-order; Higher-order; Possibilistic C-means; Privacy preserving; Private data; Tensor spaces; Cloud computing",Article,Scopus,2-s2.0-85123739288
"Li S., Guo Q., Li A.","57222240831;56390923700;55388681100;","Pan-Sharpening Based on CNN+ Pyramid Transformer by Using No-Reference Loss",2022,"Remote Sensing","14","3","624","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123734279&doi=10.3390%2frs14030624&partnerID=40&md5=098c62f1ea6e6a90ef4bd181f5d68453","The majority of existing deep learning pan-sharpening methods often use simulated degraded reference data due to the missing of real fusion labels which affects the fusion performance. The normally used convolutional neural network (CNN) can only extract the local detail information well which may cause the loss of important global contextual characteristics with long-range depen-dencies in fusion. To address these issues and to fuse spatial and spectral information with high quality information from the original panchromatic (PAN) and multispectral (MS) images, this paper presents a novel pan-sharpening method by designing the CNN+ pyramid Transformer network with no-reference loss (CPT-noRef). Specifically, the Transformer is used as the main architecture for fusion to supply the global features, the local features in shallow CNN are combined, and the multi-scale features from the pyramid structure adding to the Transformer encoder are learned simultaneously. Our loss function directly learns the spatial information extracted from the PAN image and the spectral information from the MS image which is suitable for the theory of pan-sharpening and makes the network control the spatial and spectral loss simultaneously. Both training and test processes are based on real data, so the simulated degraded reference data is no longer needed, which is quite different from most existing deep learning fusion methods. The proposed CPT-noRef network can effectively solve the huge amount of data required by the Transformer network and extract abundant image features for fusion. In order to assess the effectiveness and universality of the fusion model, we have trained and evaluated the model on the experimental data of WorldView-2(WV-2) and Gaofen-1(GF-1) and compared it with other typical deep learning pan-sharpening methods from both the subjective visual effect and the objective index evaluation. The results show that the proposed CPT-noRef network offers superior performance in both qualitative and quantitative evaluations compared with existing state-of-the-art methods. In addition, our method has the strongest gener-alization capability by testing the Pleiades and WV-2 images on the network trained by GF-1 data. The no-reference loss function proposed in this paper can greatly enhance the spatial and spectral information of the fusion image with good performance and robustness. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; No-reference loss; Pan-sharpening; Pyramid structure; Remote sensing image fusion; Transformer","Convolutional neural networks; Data mining; Deep learning; Function evaluation; Image enhancement; Remote sensing; Testing; Convolutional neural network; Deep learning; No-reference; No-reference loss; Pan-sharpening; Pyramid structure; Remote sensing image fusion; Remote sensing images; Spatial informations; Transformer; Image fusion",Article,Scopus,2-s2.0-85123734279
"Karlapudi V., Paleti S.T., Kambhampati S.B.S., Vaishya R.","57432280700;57222086400;23004796100;6602902951;","Bibliometric analysis of orthopaedic related publications by Indian authors from the last decade",2022,"Journal of Clinical Orthopaedics and Trauma","25",,"101775","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123729104&doi=10.1016%2fj.jcot.2022.101775&partnerID=40&md5=3d362db7a6e4c3b5ed88883551b45a34","Purpose: Bibliometric studies have been established methods of analysing publications on a particular topic. These studies have been done on various orthopaedic topics and are increasing. The advantages of these studies have been highlighted in previous publications. Although some studies have been done on Indian publications from other specialties, those analysing Indian Orthopaedic Publications are lacking. Methods: We performed a search in Scopus to look for all publications related to orthopaedics from India. Our search strategy in Scopus included ((TITLE-ABS-KEY(Orthopaedics OR Orthopaedics) AND AFFIL(India)) AND PUBYEAR > 2009 AND PUBYEAR < 2020) which resulted in 3270 articles on 02/11/2021. We analyzed the most publishing universities, city, state, specialty, authors, and anatomic location of these publications. We also mined the data to draw word clouds based on data obtained from the titles of articles, keywords and the affiliations of each of the articles published. Results: Tamil Nadu and New Delhi and their institutes appear to be the epicenter of publication activities in Orthopaedics in India. There has been a healthy trend of growth of articles in the orthopaedic specialty. Since there is a significant overlap of technology and engineering, it is not surprising to see engineering and technology institutes among the top 10 published institutes and even journals for the publications on orthopaedics. Conclusion: There has been a steady increase in the number of publications in the last decade. New Delhi and its Universities and Institutes appear to contribute the majority of citations and publications related to orthopaedics. Journal of Clinical Orthopaedics and Trauma was the most publishing journal for Indian authors on Orthopaedic related articles. © 2022","Bibliometrics; Data mining; Indian orthopaedic publications; Orthopaedics; Publications","anatomical location; Article; bibliometrics; data mining; human; injury; orthopedics; publication; Tamil Nadu",Article,Scopus,2-s2.0-85123729104
"Sreekumar R., Khursheed F.","57432016200;24080535400;","Identifying cancer sub-types from genomic scale data sets using confidence based integration (CBI)",2022,"Journal of Biomedical Informatics","126",,"103997","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123699973&doi=10.1016%2fj.jbi.2022.103997&partnerID=40&md5=daad943ad44393c84c62c055e517761e","Precision medicine is a method involving refined diagnosis of patients and searching for causes that are unseen in their patient cohorts who otherwise have largely similar health conditions. As the technology evolved to extract features from a wide variety of sources including genetics, a large quantum of data is available to the researchers for conducting micro studies in the field of disease and cures. In cancer research, integrative methods using genomic data sets has become a major area of interest. The petabytes of data that is available at The Cancer Genome Atlas (TCGA), a program jointly under NCI and National Human Genome Research Institute, has made possible more nuanced research in cancer genomics. Our method, Confidence Based Integration (CBI) is an integration method to extract similar as well as complementing information from the genomic data sets. This information will provide insight into the status of patients and their prospects. We used the expression data sets of gene, miRNA and DNA methylation in our fusion experiments on five different cancer types. These data sets, after fusion, are clustered using 'Spectral Clustering' algorithm, which derives clusters that form the disease sub types. Survival properties of each sub type demonstrates the reasons to consider the samples inside them highly similar. The performance of CBI, we report, is better, in terms of P-value in log-rank test, than other methods like similarity network fusion or SNF in forming clusters of significance. Individual features clustered extremely poor compared to CBI in most of the experiments. © 2022 Elsevier Inc.",,"Alkylation; Clustering algorithms; Data integration; Data mining; Diagnosis; Genes; Integration; RNA; Area of interest; Cancer genome; Cancer research; Data set; Genome research; Genomic data sets; Genomics; Health condition; Human genomes; Petabytes; Diseases; microRNA; Article; cancer research; cancer survival; confidence based integration; controlled study; DNA methylation; gene expression; genetic algorithm; genomics; human; information processing; k nearest neighbor; log rank test; malignant neoplasm; personalized medicine; population research; sample size; Similarity Network Fusion",Article,Scopus,2-s2.0-85123699973
"Cheng Y.-B., Dickey H., Tran R., Yimam Y.T., Schmid B., Paxton B., Schreuder M.","57431857800;57431857900;57431058600;56373257600;56041437900;57430859400;57370100200;","Land Surface Parameterization at Exposed Playa and Desert Region to Support Dust Emissions Estimates in Southern California, United States",2022,"Remote Sensing","14","3","616","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123696941&doi=10.3390%2frs14030616&partnerID=40&md5=ea48a9457b209dfc6d81cfea946cf804","Remote sensing technologies provide a unique opportunity to identify ground surfaces that are more susceptible to dust emissions at a large scale. As part of the Salton Sea Air Quality Mitigation Program (SSAQMP) of the Imperial Irrigation District (IID), efforts have been made to improve our understanding of fugitive, wind-blown dust emissions around the Salton Sea region in Southern California, United States. Field campaigns were conducted for multiple years to evaluate surface conditions and measure the dust emissions potential in the area. Data collected during the field work were coupled with remote sensing imagery and data mining techniques to map surface characteristics that are important in identifying dust emissions potential. Around the playa domain, surface crust type, sand presence, and soil moisture were estimated. Geomorphic surface types were mapped in the desert domain. Overall accuracy ranged from 91.7% to 99.4% for the crust type mapping. Sand presence mapping showed consistent and slightly better accuracy, ranging from 96.2% to 99.7%. Soil moisture assessment agreed with precipitation records. Geomorphic mapping in the desert domain achieved accuracy above 93.5%, and the spatial pattern was consistent with previous studies. These land surface condition assessments provide important information to support dust emissions estimates in the region. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Desert landforms; Fugitive dust; Playa; Remote sensing; Surface moisture; Surface sand","Air quality; Arid regions; Data mining; Dust; Landforms; Mapping; Soil moisture; Surface measurement; Desert landform; Dust emission; Emission potential; Fugitive dust; Playa; Remote-sensing; Salton Sea; Southern California; Surface moistures; Surface sand; Remote sensing",Article,Scopus,2-s2.0-85123696941
"Fahy C., Yang S.","57191279549;57205311873;","Finding and Tracking Multi-Density Clusters in Online Dynamic Data Streams",2022,"IEEE Transactions on Big Data","8","1",,"178","192",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123684450&doi=10.1109%2fTBDATA.2019.2922969&partnerID=40&md5=55f31cfa704cd7c6f21eb593f4dad9f6","Change is one of the biggest challenges in dynamic stream mining. From a data-mining perspective, adapting and tracking change is desirable in order to understand how and why change has occurred. Clustering, a form of unsupervised learning, can be used to identify the underlying patterns in a stream. Density-based clustering identifies clusters as areas of high density separated by areas of low density. This paper proposes a Multi-Density Stream Clustering (MDSC) algorithm to address these two problems; the multi-density problem and the problem of discovering and tracking changes in a dynamic stream. MDSC consists of two on-line components; discovered, labelled clusters and an outlier buffer. Incoming points are assigned to a live cluster or passed to the outlier buffer. New clusters are discovered in the buffer using an ant-inspired swarm intelligence approach. The newly discovered cluster is uniquely labelled and added to the set of live clusters. Processed data is subject to an ageing function and will disappear when it is no longer relevant. MDSC is shown to perform favourably to state-of-the-art peer stream-clustering algorithms on a range of real and synthetic data-streams. Experimental results suggest that MDSC can discover qualitatively useful patterns while being scalable and robust to noise. © 2015 IEEE.","Change detection; Concept drift; Concept evolution; Data stream clustering; Multi-density clustering; Swarm intelligence","Clustering algorithms; Data mining; Statistics; Change detection; Concept drifts; Concept evolutions; Data stream; Data stream clustering; Density clustering; Density clusters; Dynamic data; Multi-density clustering; Stream clustering; Swarm intelligence",Article,Scopus,2-s2.0-85123684450
"Duan J., Guo L.","55023879700;56670571800;","Variable-Length Subsequence Clustering in Time Series",2022,"IEEE Transactions on Knowledge and Data Engineering","34","2",,"983","995",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123637435&doi=10.1109%2fTKDE.2020.2986965&partnerID=40&md5=5a4d6e3a5e6e32f2288c318fa9cd039a","Subsequence clustering is an important issue in time series data mining. Observing that most time series consist of various patterns with different unknown lengths, we propose an optimization framework to adaptively estimate the lengths and representations for different patterns. Our framework minimizes the inner subsequence cluster errors with respect to subsequence clusters and segmentation under time series cover constraint where the subsequence cluster lengths can be variable. To optimize our framework, we first generate abundant initial subsequence clusters with different lengths. Then, three cluster operations, i.e., cluster splitting, combination and removing, are used to iteratively refine the cluster lengths and representations by respectively splitting clusters consisting of different patterns, joining neighboring clusters belonging to the same pattern and removing clusters to the predefined cluster number. During each cluster refinement, we employ an efficient algorithm to alternatively optimize subsequence clusters and segmentation based on dynamic programming. Our method can automatically and efficiently extract the unknown variable-length subsequence clusters in the time series. Comparative results with the state-of-the-art are conducted on various synthetic and real time series, and quantitative and qualitative performances demonstrate the effectiveness of our method. © 1989-2012 IEEE.","subsequence clustering; Time series data mining; time series segmentation; variable-length patterns","Clustering algorithms; Data mining; Dynamic programming; Iterative methods; Cluster errors; Cluster numbers; Optimization framework; Splittings; Subsequence clustering; Time series data mining; Time-series segmentation; Times series; Variable length; Variable-length pattern; Time series",Article,Scopus,2-s2.0-85123637435
"Luo Z., Cui Y., Zhao S., Yin J.","55991445400;57429740200;55497128900;57226833053;","g-Inspector: Recurrent Attention Model on Graph",2022,"IEEE Transactions on Knowledge and Data Engineering","34","2",,"680","690",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123622555&doi=10.1109%2fTKDE.2020.2983689&partnerID=40&md5=67a145f44b09be6feaf9d4fde038f24d","Graph classification problem is becoming one of research hotspots in the realm of graph mining, which has been widely used in cheminformatics, bioinformatics and social network analytics. Existing approaches, such as graph kernel methods and graph Convolutional Neural Network, are facing the challenges of non-interpretability and high dimensionality. To address the problems, we propose a novel recurrent attention model, called g-Inspector, which applies the attention mechanism to investigate the significance of each region to make the results interpretable. It also takes a shift operation to guide the inspector agent to discover the next relevant region, so that the model sequentially loads small regions instead of the entire large graph, to solve the high dimensionality problem. The experiments conducted on standard graph datasets show the effectiveness of our g-Inspector in graph classification problems. © 1989-2012 IEEE.","Graph classification; graph mining; recurrent neural network; reinforcement learning","Classification (of information); Data mining; Graph neural networks; Reinforcement learning; AS graph; Attention model; Cheminformatics; Graph classification; Graph kernels; Graph mining; High dimensionality; Hotspots; Interpretability; Kernel-methods; Recurrent neural networks",Article,Scopus,2-s2.0-85123622555
"Chen J., Zhao Y., Wan Y., Zhu L., Li B., Wu J., Li L., Huang Y., Li Y., Long X., Deng S.","57212311882;57429543200;50263355300;57194028775;57257106300;57429723500;57428659700;57208240430;57428659800;57429723600;35112966400;","Electrochemiluminescent Ion-Channeling Framework for Membrane Binding and Transmembrane Activity Assays",2022,"Analytical Chemistry","94","4",,"2154","2162",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123620340&doi=10.1021%2facs.analchem.1c04593&partnerID=40&md5=94accc72651146bd79e52c1ca94e5ab7","Recent upgrades in the electrochemiluminescence (ECL) technique showcased its brilliant knack in probing microscopic biointerfacial events, many of which were actually underlain by the ionotropic membrane processes, yet not being ostensive. Here, by modeling an artificial lipoid-supported porin ensemble, we explore and establish the ECL potency in profiling ion-channel activities. A lipophilic hollowed construct dubbed ZnPC was made out of the dynamic covalent chemistry, and its unique geometry was characterized that configured stoichiometric ECL-emissive units in a cubic stance; while the aliphatic vertices of ZnPC helped it safely snorkel and steadily irradiate in a biofilm fusion. After expounding basic ECL properties, the brightness was traced out in response to halogen contents that was lit up by F–/Cl– but down by Br–/I–. The overall pattern fitted the Langmuir isotherm, from which the membrane-binding strengths of the four were analyzed, compared, and collaterally examined in impedimetrics. On the other hand, one could derive anionic transmembrane kinetics from the time-dependent ECL statistics that pinpointed the ECL signaling via the nanocage-directed mass-transfer pathway. More data mining unveiled an ECL-featured Hofmeister series and the thermodynamic governing force behind all scenes. Finally, combining with halide-selective fluorometry, the synthetic conduit was identified as an ECL symporter. In short, this work develops a novel ECL model for the evaluation of life-mimicking membrane permeation. It might intrigue the outreach of ECL applications in the measurement of diverse surface-confined transient scenarios, e.g., in vitro gated ion or molecule trafficking, which used to be handled by nanopore and electrofluorochromic assays. © 2022 American Chemical Society",,"Data mining; Isotherms; Mass transfer; Membranes; Proteins; Activity assays; Channel activity; Dynamic covalent chemistry; Electrochemiluminescence; Electrochemiluminescent; Ion channel; Ion channelling; Membrane binding; Membrane process; Transmembranes; Ions",Article,Scopus,2-s2.0-85123620340
"Dwivedi K.K., Lakhani P., Kumar S., Kumar N.","57428702900;57208209929;57218172988;57194127100;","A hyperelastic model to capture the mechanical behaviour and histological aspects of the soft tissues",2022,"Journal of the Mechanical Behavior of Biomedical Materials","126",,"105013","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123618466&doi=10.1016%2fj.jmbbm.2021.105013&partnerID=40&md5=9612a01082f17a45cfef4fec9dc470f0","It is well established that the soft connective tissues show a nonlinear elastic response that comes from their microstructural arrangement. Tissues' microstructure alters with various physiological conditions and may affect their mechanical responses. Therefore, the accurate prediction of tissue's mechanical response is crucial for clinical diagnosis and treatments. Thus, a physically motivated and mathematically simplified model is required for the accurate prediction of tissues' mechanical and structural responses. This study explored the ‘Exp-Ln’ hyperelastic model (Khajehsaeid et al., 2013) to capture soft tissues' mechanical and histological behaviour. In this work, uniaxial tensile test data for the belly and back pig skin were extracted from the experiments performed in our laboratory, whereas uniaxial test data for other soft tissues (human skin, tendon, ligament, and aorta) were extracted from the literature. The ‘Exp-Ln; and other hyperelastic models (e.g. Money Rivlin, Ogden, Yeoh, and Gent models) were fitted with these experimental data, and obtained results were compared between the models. These results show that the ‘Exp-Ln’ model could capture the mechanical behaviour of soft tissues more accurately than other hyperelastic models. This model was also found numerically stable for all modes and ranges of deformation. This study also investigated the link between ‘Exp-Ln’ material parameters and tissue's histological parameters. The histological parameters such as collagen content, fibre free length, crosslink density, and collagen arrangement were measured using staining and ATR-FTIR techniques. The material parameters were found statistically correlated with the histological parameters. Further, ‘Exp-Ln’ model was implemented in ABAQUS through the VUMAT subroutine, where the mechanical behaviour of various soft tissues was simulated for different modes of deformation. The finite element analysis results obtained using the ‘Exp-Ln’ model agreed with the experiments and were more accurate than other hyperelastic models. Overall, these results demonstrate the capability of ‘Exp-Ln’ model to predict the mechanical and structural responses of the soft tissues. © 2021","Collagen structure; FEM; Histology; Hyperelastic model; Soft tissue","Collagen; Data mining; Deformation; Diagnosis; Elasticity; Forecasting; Histology; Mammals; Physiological models; Tensile testing; Tissue; Accurate prediction; Collagen structure; Histological parameters; Hyperelastic models; Materials parameters; Mechanical behavior; Mechanical response; Soft tissue; Structural response; Test data; Finite element method; collagen; animal; biological model; elasticity; finite element analysis; mechanical stress; pig; skin; Animals; Collagen; Elasticity; Finite Element Analysis; Models, Biological; Skin; Stress, Mechanical; Swine",Article,Scopus,2-s2.0-85123618466
"Vandromme M., Jacques J., Taillard J., Jourdan L., Dhaenens C.","57192958360;34768472100;24470160000;6603180053;57204296870;","A Biclustering Method for Heterogeneous and Temporal Medical Data",2022,"IEEE Transactions on Knowledge and Data Engineering","34","2",,"506","518",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123609354&doi=10.1109%2fTKDE.2020.2983692&partnerID=40&md5=4bfdf9ba049d37c6d3d830e8ecc5f300","We address the problem of biclustering on heterogeneous data, that is, data of various types (binary, numeric, symbolic, temporal). We propose a new method, HBC-t (Heterogeneous BiClustering for temporal data), designed to extract biclusters from heterogeneous, temporal, large-scale, sparse data matrices. HBC-t is based on HBC, using similar mechanisms but adding support for temporal data. The goal of this method is to handle Electronic Health Records (EHR) data gathered by hospitals on patients, stays, acts, diagnoses, prescriptions, etc.; and to provide valuable insights on this data. Temporal data accounts for a majority of the data available for this study, and in EHR in general where medical events are timestamped. Therefore, it is crucial to have an algorithm that supports this type of data. The proposed algorithm takes advantage of the data sparsity and uses a constructive greedy heuristic to build a large number of possibly overlapping biclusters. HBC-t is successfully compared with several other biclustering algorithms on numeric and temporal data. Experiments on full-scale real-life data sets further assert its scalability and efficiency. © 1989-2012 IEEE.","Clustering algorithms; data mining; electronic medical records; knowledge discovery","Data mining; Diagnosis; Medical computing; Bi clusters; Biclustering; Data matrix; Data sparsity; Greedy heuristics; Heterogeneous data; Large-scales; Medical data; Sparse data; Temporal Data; Clustering algorithms",Article,Scopus,2-s2.0-85123609354
"Wang R., Bian J., Nie F., Li X.","56900069000;57222149916;14008099400;55936260100;","Unsupervised Discriminative Projection for Feature Selection",2022,"IEEE Transactions on Knowledge and Data Engineering","34","2",,"942","953",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123599625&doi=10.1109%2fTKDE.2020.2983396&partnerID=40&md5=e3d569f2ecc8b477b2ec8ba521cedfd7","Feature selection is one of the most important techniques to deal with the high-dimensional data for a variety of machine learning and data mining tasks, such clustering, classification, and retrieval, etc. Fuzziness is a widespread nature of data in nature human society. However, most existing feature selection methods ignore the existence of fuzziness in the data, resulting in sub-optimal feature subsets. To address the problem, we propose a novel unsupervised feature selection method, called Unsupervised Discriminative Projection for Feature Selection (UDPFS) to select discriminative features by conducting fuzziness learning and sparse learning, simultaneously. Specifically, we use projection matrix transform data as its low-dimensional representation, which are partitioned into clusters by using membership matrix with sparse constraint. In addition, ell{2, 1}ℓ2,1-norm regularization is applied to the projection matrix. Then, a discriminative projection matrix with row sparse is obtained by perform fuzziness learning and sparse learning, simultaneously. An effective alternative optimization algorithm is proposed to solve the objective function. Evaluate experimental results on several real-world datasets show the effectiveness and superiority of the proposed unsupervised feature selection method. © 1989-2012 IEEE.","Dimension reduction; fuzziness learning; sparse learning; unsupervised feature selection","Clustering algorithms; Data mining; Extraction; Feature extraction; Fuzzy set theory; Matrix algebra; Unsupervised learning; Clustering classifications; Data mining tasks; Dimension reduction; Feature selection methods; Features selection; Fuzziness learning; High dimensional data; Projection matrix; Sparse learning; Unsupervised feature selection; Fuzzy systems",Article,Scopus,2-s2.0-85123599625
"Zhao K., Cong G., Li X.","56531246300;8987190900;57204777955;","PGeoTopic: A Distributed Solution for Mining Geographical Topic Models",2022,"IEEE Transactions on Knowledge and Data Engineering","34","2",,"881","893",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123586100&doi=10.1109%2fTKDE.2020.2989142&partnerID=40&md5=40b8e5402effcc175b74a04bd2b7a687","Geographical topic models have been used to mine geo-tagged documents for topical region and geographical topics, and also have applications in recommendations, user mobility modeling, event detection, etc. Existing studies focus on learning effective geographical topic models while ignoring the efficiency issue. However, it is very expensive to train geographical topic models - it may take days to train a geographical topic model of a small scale on a collection of documents with millions of word tokens. In this paper, we propose the first distributed solution, called {sf PGeoTopic}PGeoTopic, for training geographical topic models. The proposed solution comprises several novel technical components to increase parallelism, reduce memory requirement, and reduce communication cost. Experiments show that our approach for mining geographical topic models is scalable with both model size and data size on distributed systems. © 1989-2012 IEEE.","distributed machine learning; Geographical topic model","Machine learning; Collection of documents; Distributed machine learning; Distributed solutions; Events detection; Geographical topic model; Mobility modeling; Model events; Small scale; Topic Modeling; Users' mobility; Data mining",Article,Scopus,2-s2.0-85123586100
"Alajlani M.M.","57191247693;","The Chemical Property Position of Bedaquiline Construed by a Chemical Global Positioning System-Natural Product",2022,"Molecules","27","3","753","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123575488&doi=10.3390%2fmolecules27030753&partnerID=40&md5=018b84971e31cb2f964e1bf4cad4bca7","Bedaquiline is a novel adenosine triphosphate synthase inhibitor anti-tuberculosis drug. Bedaquiline belongs to the class of diarylquinolines, which are antituberculosis drugs that are quite different mechanistically from quinolines and flouroquinolines. The fact that relatively similar chemical drugs produce different mechanisms of action is still not widely understood. To enhance discrimination in favor of bedaquiline, a new approach using eight-score principal component analysis (PCA), provided by a ChemGPS-NP model, is proposed. PCA scores were calculated based on 35 + 1 different physicochemical properties and demonstrated clear differences when compared with other quinolines. The ChemGPS-NP model provided an exceptional 100 compounds nearest to bedaquiline from antituberculosis screening sets (with a cumulative Euclidian distance of 196.83), compared with the different 2Dsimilarity provided by Tanimoto methods (extended connective fingerprints and the Molecular ACCess System, showing 30% and 182% increases in cumulative Euclidian distance, respectively). Potentially similar compounds from publicly available antituberculosis compounds and Maybridge sets, based on bedaquiline’s eight-dimensional similarity and different filtrations, were identified too. © 2022 by the author. Licensee MDPI, Basel, Switzerland.","Antituberculosis agents; Bedaquiline; ChemGPS-NP; Chemical property space; Data mining; Screening; Structural similarity",,Article,Scopus,2-s2.0-85123575488
"Satinet C., Fouss F.","57425729600;8685481800;","A Supervised Machine Learning Classification Framework for Clothing Products’ Sustainability",2022,"Sustainability (Switzerland)","14","3","1334","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123445215&doi=10.3390%2fsu14031334&partnerID=40&md5=f26c35301660cb611364dccb1be2ce47","These days, many sustainability-minded consumers face a major problem when trying to identify environmentally sustainable products. Indeed, there are a variety of confusing sustainability certifications and few labels capturing the overall environmental impact of products, as the existing procedures for assessing the environmental impact of products throughout their life cycle are time consuming, costly, and require a lot of data and input from domain experts. This paper explores the use of supervised machine learning tools to extrapolate the results of existing life cycle assessment studies (LCAs) and to develop a model—applied to the clothing product category—that could easily and quickly assess the products’ environmental sustainability throughout their life cycle. More precisely, we assemble a dataset of clothing products with their life cycle characteristics and corresponding known total environmental impact and test, on a 5-fold cross-validation basis, nine state-of-the-art supervised machine learning algorithms. Among them, the random forest algorithm has the best performance with an average accuracy of 91% over the five folds. The resulting model provides rapid environmental feedback on a variety of clothing products with the limited data available to online retailers. It could be used to quickly provide interested consumers with product-level sustainability information, or even to develop a unique and all-inclusive environmental label. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Classification; Data mining; Environmental impact assessment; Supervised machine learning; Sustainability","algorithm; environmental impact; institutional framework; machine learning; sustainability",Article,Scopus,2-s2.0-85123445215
"Zwilling M.","55605630500;","Trends and Challenges Regarding Cyber Risk Mitigation by CISOs—A Systematic Literature and Experts’ Opinion Review Based on Text Analytics",2022,"Sustainability (Switzerland)","14","3","1311","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123430288&doi=10.3390%2fsu14031311&partnerID=40&md5=f987d6f88cec71b968bb46b2d4378cdf","Background: Cyber security has turned out to be one of the main challenges of recent years. As the variety of system and application vulnerabilities has increased dramatically in recent years, cyber attackers have managed to penetrate the networks and infrastructures of larger numbers of companies, thus increasing the latter’s exposure to cyber threats. To mitigate this exposure, it is crucial for CISOs to have sufficient training and skills to help them identify how well security controls are managed and whether these controls offer the company sufficient protection against cyber threats, as expected. However, recent literature shows a lack of clarity regarding the manner in which the CISOs’ role and the companies’ investment in their skills should change in view of these developments. Therefore, the aim of this study is to investigate the relationship between the CISOs’ level of cyber security-related preparation to mitigate cyber threats (and specifically, the companies’ attitudes toward investing in such preparation) and the recent evolution of cyber threats. Methods: The study data are based on the following public resources: (1) recent scientific literature; (2) cyber threat-related opinion news articles; and (3) OWASP’s reported list of vulnerabilities. Data analysis was performed using various text mining methods and tools. Results: The study’s findings show that although the implementation of cyber defense tools has gained more serious attention in recent years, CISOs still lack sufficient support from management and sufficient knowledge and skills to mitigate current and new cyber threats. Conclusions: The research outcomes may allow practitioners to examine whether the companies’ level of cyber security controls matches the CISOs’ skills, and whether a comprehensive security education program is required. The present article discusses these findings and their implications. © 2022 by the author. Licensee MDPI, Basel, Switzerland.","CISO’s role; Cyber education; Cyber management; Cyber security; Cyber security frameworks; Cyber security vulnerabilities","data management; data mining; education; Internet; literature review; social security; training",Article,Scopus,2-s2.0-85123430288
"Chun S.-H., Jang J.-W.","7202148435;57425148000;","A New Trend Pattern-Matching Method of Interactive Case-Based Reasoning for Stock Price Predictions",2022,"Sustainability (Switzerland)","14","3","1366","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123398832&doi=10.3390%2fsu14031366&partnerID=40&md5=476501a0ad1cf9307fcb6ba7d600c98a","In this paper, we suggest a new case-based reasoning method for stock price predictions using the knowledge of traders to select similar past patterns among nearest neighbors obtained from a traditional case-based reasoning machine. Thus, this method overcomes the limitation of conventional case-based reasoning, which does not consider how to retrieve similar neighbors from previous patterns in terms of a graphical pattern. In this paper, we show how the proposed method can be used when traders find similar time series patterns among nearest cases. For this, we suggest an interactive prediction system where traders can select similar patterns with individual knowledge among automatically recommended neighbors by case-based reasoning. In this paper, we demonstrate how traders can use their knowledge to select similar patterns using a graphical interface, serving as an exemplar for the target. These concepts are investigated against the backdrop of a practical application involving the prediction of three individual stock prices, i.e., Zoom, Airbnb, and Twitter, as well as the prediction of the Dow Jones Industrial Average (DJIA). The verification of the prediction results is compared with a random walk model based on the RMSE and Hit ratio. The results show that the proposed technique is more effective than the random walk model but it does not statistically surpass the random walk model. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Artificial intelligence; Case-based reasoning; Data mining; Financial prediction; Knowledge discovery; Learning techniques","methodology; prediction; price dynamics; stock market; trend analysis",Article,Scopus,2-s2.0-85123398832
"Saho K., Fujimoto M., Kobayashi Y., Matsumoto M.","54933834900;36607880500;24069314400;7404940804;","Experimental Verification of Micro-Doppler Radar Measurements of Fall-Risk-Related Gait Differences for Community-Dwelling Elderly Adults",2022,"Sensors","22","3","930","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123291629&doi=10.3390%2fs22030930&partnerID=40&md5=b476efce2822e228391eb1130541ae5c","In a previous study, we developed a classification model to detect fall risk for elderly adults with a history of falls (fallers) using micro-Doppler radar (MDR) gait measurements via simulation. The objective was to create daily monitoring systems that can identify elderly people with a high risk of falls. This study aimed to verify the effectiveness of our model by collecting actual MDR data from community-dwelling elderly people. First, MDR gait measurements were performed in a community setting, and the efficient gait parameters for the classification of fallers were extracted. Then, a support vector machine model that was trained and validated using the simulated MDR data was tested for the gait parameters extracted from the actual MDR data. A classification accuracy of 78.8% was achieved for the actual MDR data. The validity of the experimental results was confirmed based on a comparison with the results of our previous simulation study. Thus, the practicality of the faller classification model constructed using the simulated MDR data was verified for the actual MDR data. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Elderly people; Fall risk; Faller classification; Gait measurement; Micro-Doppler radar; Support vector machine","Data mining; Doppler radar; Housing; Radar measurement; Risk assessment; Classification models; Elderly adults; Elderly people; Fall risk; Faller classification; Gait measurements; Micro-dopple radar; Micro-Doppler; Radar data; Support vectors machine; Support vector machines; adult; aged; falling; gait; human; independent living; support vector machine; telecommunication; Accidental Falls; Adult; Aged; Gait; Humans; Independent Living; Radar; Support Vector Machine",Article,Scopus,2-s2.0-85123291629
"Zhao L., Lee S.H., Li M., Sun P.","57197822894;56583813800;57213174397;57222597553;","The Use of Social Media to Promote Sustainable Fashion and Benefit Communications: A Data‐Mining Approach",2022,"Sustainability (Switzerland)","14","3","1178","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123107357&doi=10.3390%2fsu14031178&partnerID=40&md5=c04b2c9a56cdf14d4987ce47d0f1fa6d","Numerous brands utilize social media to capture consumers’ interests while promoting their sustainability goals. To understand how sustainable fashion brands communicate with their consumers, this study explored the visual and textual information sustainable fashion brands post on social media. Data were collected from sustainable fashion brands’ social media pages, and a total of 1525 images and captions and 140,735 comments were analyzed. By employing color theory and the theory of speech acts, HSV color analysis and the SVM classification model were used to extract information. The results showed that the images and captions posted by all three brands were consistent with their brand identities and sustainability goals. We also found that there were significant differences among the three brands when comparing posts employing expressive and assertive acts with posts using directive and assertive acts. These results indicate that social media users are more likely to leave comments when they read posts containing expressive and directive acts. These findings will allow fashion social media marketers to select appealing images and colors to engage consumers as well as to choose appropriate speech acts to deliver information to achieve their sustainability goals. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Color theory; Data mining; Social media; Speech act theory; Sustainable fashion brand","communication; data mining; image analysis; social media; sustainability",Article,Scopus,2-s2.0-85123107357
"Kang W., Lee K., Jang E.-K.","57211857133;57417512300;56818586100;","Evaluation and Validation of Estimated Sediment Yield and Transport Model Developed with Model Tree Technique",2022,"Applied Sciences (Switzerland)","12","3","1119","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123091539&doi=10.3390%2fapp12031119&partnerID=40&md5=4945b5b7edd400fb2fe108bdd8fb8e02","This study evaluated the applicability of existing sediment yield and transport estimation models developed using data mining classification and prediction techniques and validated them. Field surveys were conducted by using an acoustic Doppler current profiler and laser in situ scattering and transmission at measuring points in the main stream of the Nakdong River located where the tributaries of the Geumho, Hwang, and Nam Rivers join. Surveys yielded estimations of water velocity, discharge, and suspended sediment concentrations were measured. In contrast with models based on the general watershed characteristics factors, some models based on hydraulic explanatory flow variables demonstrated an excellent predictability. This is because the selected submodels for validation, which provided excellent prediction results, were based on a large number of calibration data. It indicates that a sufficient number of reliable data is required in developing a sediment yield estimation model using data mining. For practical applications of data mining to extant sediment yield estimation models, comprehensive considerations are required, including the purpose and background of model development, and data range. Furthermore, the existing models should be periodically updated with the consideration of temporal and spatial lumping problems. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Data mining; Model tree; Sediment transport; Sediment yield; Specific degradation",,Article,Scopus,2-s2.0-85123091539
"Murakami R., Chakraborty B.","57222172472;22333246700;","Investigating the Efficient Use of Word Embedding with Neural-Topic Models for Interpretable Topics from Short Texts",2022,"Sensors","22","3","852","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123052608&doi=10.3390%2fs22030852&partnerID=40&md5=8d5d7d7724fa16b31d596c96d283a961","With the rapid proliferation of social networking sites (SNS), automatic topic extraction from various text messages posted on SNS are becoming an important source of information for understanding current social trends or needs. Latent Dirichlet Allocation (LDA), a probabilistic generative model, is one of the popular topic models in the area of Natural Language Processing (NLP) and has been widely used in information retrieval, topic extraction, and document analysis. Unlike long texts from formal documents, messages on SNS are generally short. Traditional topic models such as LDA or pLSA (probabilistic latent semantic analysis) suffer performance degradation for short-text analysis due to a lack of word co-occurrence information in each short text. To cope with this problem, various techniques are evolving for interpretable topic modeling for short texts, pretrained word embedding with an external corpus combined with topic models is one of them. Due to recent developments of deep neural networks (DNN) and deep generative models, neuraltopic models (NTM) are emerging to achieve flexibility and high performance in topic modeling. However, there are very few research works on neural-topic models with pretrained word embedding for generating high-quality topics from short texts. In this work, in addition to pretrained word embedding, a fine-tuning stage with an original corpus is proposed for training neural-topic models in order to generate semantically coherent, corpus-specific topics. An extensive study with eight neural-topic models has been completed to check the effectiveness of additional fine-tuning and pretrained word embedding in generating interpretable topics by simulation experiments with several benchmark datasets. The extracted topics are evaluated by different metrics of topic coherence and topic diversity. We have also studied the performance of the models in classification and clustering tasks. Our study concludes that though auxiliary word embedding with a large external corpus improves the topic coherency of short texts, an additional fine-tuning stage is needed for generating more corpus-specific topics from short-text data. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Coherent topic; Fine-tuning short-text data; Neural-topic model; Pretrained word embedding","Data mining; Deep neural networks; Embeddings; Natural language processing systems; Semantics; Social networking (online); Coherent topic; Embeddings; Fine tuning; Fine-tuning short-text data; Latent Dirichlet allocation; Neural-topic model; Pretrained word embedding; Short texts; Text data; Topic Modeling; Statistics; cluster analysis; information retrieval; natural language processing; text messaging; Cluster Analysis; Information Storage and Retrieval; Natural Language Processing; Neural Networks, Computer; Text Messaging",Article,Scopus,2-s2.0-85123052608
"Thrun M.C.","56450887700;","Exploiting Distance-Based Structures in Data Using an Explainable AI for Stock Picking",2022,"Information (Switzerland)","13","2","51","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123046635&doi=10.3390%2finfo13020051&partnerID=40&md5=383e035c1d0575dfd24a479bbae921f1","In principle, the fundamental data of companies may be used to select stocks with a high probability of either increasing or decreasing price. Many of the commonly known rules or used explanations for such a stock-picking process are too vague to be applied in concrete cases, and at the same time, it is challenging to analyze high-dimensional data with a low number of cases in order to derive data-driven and usable explanations. This work proposes an explainable AI (XAI) approach on the quarterly available fundamental data of companies traded on the German stock market. In the XAI, distance-based structures in data (DSD) that guide decision tree induction are identified. The leaves of the appropriately selected decision tree contain subsets of stocks and provide viable explanations that can be rated by a human. The prediction of the future price trends of specific stocks is made possible using the explanations and a rating. In each quarter, stock picking by DSD-XAI is based on understanding the explanations and has a higher success rate than arbitrary stock picking, a hybrid AI system, and a recent unsupervised decision tree called eUD3.5. © 2022 by the author. Licensee MDPI, Basel, Switzerland.","Decision trees; Explainable AI; Fundamental analysis; Information visualization","Clustering algorithms; Commerce; Data mining; Data visualization; AI systems; Data driven; Decision tree induction; Distance-based; Explainable AI; Fundamental analysis; High dimensional data; High probability; Information visualization; Price trends; Decision trees",Article,Scopus,2-s2.0-85123046635
"Mansour R., Romaguera L.V., Huet C., Bentridi A., Vu K.-N., Billiard J.-S., Gilbert G., Tang A., Kadoury S.","57216753610;57201665413;57204501368;57242831400;56184209000;6701427529;57415737100;57225718426;16230455400;","Abdominal motion tracking with free-breathing XD-GRASP acquisitions using spatio-temporal geodesic trajectories",2022,"Medical and Biological Engineering and Computing","60","2",,"583","598",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123003252&doi=10.1007%2fs11517-021-02477-w&partnerID=40&md5=fc6dd9c4e93b2db46881c4ec1b784ca4","Free-breathing external beam radiotherapy remains challenging due to the complex elastic or irregular motion of abdominal organs, as imaging moving organs leads to the creation of motion blurring artifacts. In this paper, we propose a radial-based MRI reconstruction method from 3D free-breathing abdominal data using spatio-temporal geodesic trajectories, to quantify motion during radiotherapy. The prospective study was approved by the institutional review board and consent was obtained from all participants. A total of 25 healthy volunteers, 12 women and 13 men (38 years ± 12 [standard deviation]), and 11 liver cancer patients underwent imaging using a 3.0 T clinical MRI system. The radial acquisition based on golden-angle sparse sampling was performed using a 3D stack-of-stars gradient-echo sequence and reconstructed using a discretized piecewise spatio-temporal trajectory defined in a low-dimensional embedding, which tracks the inhale and exhale phases, allowing the separation between distinct motion phases. Liver displacement between phases as measured with the proposed radial approach based on the deformation vector fields was compared to a navigator-based approach. Images reconstructed with the proposed technique with 20 motion states and registered with the multiscale B-spline approach received on average the highest Likert scores for the overall image quality and visual SNR score 3.2 ± 0.3 (mean ± standard deviation), with liver displacement errors varying between 0.1 and 2.0 mm (mean 0.8 ± 0.6 mm). When compared to navigator-based approaches, the proposed method yields similar deformation vector field magnitudes and angle distributions, and with improved reconstruction accuracy based on mean squared errors. Graphic Abstract: Schematic illustration of the proposed 4D-MRI reconstruction method based on radial golden-angle acquisitions and a respiration motion model from a manifold embedding used for motion tracking. First, data is extracted from the center of k-space using golden-angle sampling, which is then mapped onto a low-dimensional embedding, describing the relationship between neighboring samples in the breathing cycle. The trained model is then used to extract the respiratory motion signal for slice re-ordering. The process then improves the image quality through deformable image registration. Using a reference volume, the deformation vector field (DVF) of sequential motion states are extracted, followed by deformable registrations. The output is a 4DMRI which allows to visualize and quantify motion during free-breathing. [Figure not available: see fulltext.]. © 2021, International Federation for Medical and Biological Engineering.","4D-MRI; Abdominal motion tracking; Deformable registration; Geodesic manifold trajectory; XD-GRASP","Air navigation; Data mining; Deformation; Embeddings; Geodesy; Image enhancement; Image quality; Image reconstruction; Mean square error; Motion analysis; Radiotherapy; Statistics; Tensors; Trajectories; 4d-MRI; Abdominal motion tracking; Deformable registration; Deformation vectors; Free breathing; Geodesic manifold trajectory; MRI reconstruction; Spatio-temporal; Vector fields; XD-GRASP; Magnetic resonance imaging",Article,Scopus,2-s2.0-85123003252
"Lan H., Stewart K., Sha Z., Xie Y., Chang S.","57251995100;57203043731;7006705338;7403959475;57200142485;","Data Gap Filling Using Cloud-Based Distributed Markov Chain Cellular Automata Framework for Land Use and Land Cover Change Analysis: Inner Mongolia as a Case Study",2022,"Remote Sensing","14","3","445","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122921386&doi=10.3390%2frs14030445&partnerID=40&md5=c3d2882ba63b4ccbb1625669448e38bd","With advances in remote sensing, massive amounts of remotely sensed data can be har-nessed to support land use/land cover (LULC) change studies over larger scales and longer terms. However, a big challenge is missing data as a result of poor weather conditions and possible sensor malfunctions during image data collection. In this study, cloud-based and open source distributed frameworks that used Apache Spark and Apache Giraph were used to build an integrated infrastructure to fill data gaps within a large-area LULC dataset. Data mining techniques (k-medoids clustering and quadratic discriminant analysis) were applied to facilitate sub-space analyses. Ancil-lary environmental and socioeconomic conditions were integrated to support localized model train-ing. Multi-temporal transition probability matrices were deployed in a graph-based Markov–cellu-lar automata simulator to fill in missing data. A comprehensive dataset for Inner Mongolia, China, from 2000 to 2016 was used to assess the feasibility, accuracy, and performance of this gap-filling approach. The result is a cloud-based distributed Markov–cellular automata framework that ex-ploits the scalability and high performance of cloud computing while also achieving high accuracy when filling data gaps common in longer-term LULC studies. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Distributed cloud computing; Gap filling; LULC; Markov–CA","Cellular automata; Data mining; Discriminant analysis; Filling; Graphic methods; Land use; Large dataset; Markov processes; Remote sensing; Cellular automatons; Cloud-based; Cloud-computing; Data gap; Distributed cloud computing; Distributed clouds; Gap filling; Inner Mongolia; Land use/land cover; Markov–CA; Distributed cloud",Article,Scopus,2-s2.0-85122921386
"Wang L., Meng Z.","57412858600;7201894858;","Multichannel Two-Dimensional Convolutional Neural Network Based on Interactive Features and Group Strategy for Chinese Sentiment Analysis",2022,"Sensors","22","3","714","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122880976&doi=10.3390%2fs22030714&partnerID=40&md5=93d1ecb5bf06f4ecd2ba9ba31ea0b23e","In Chinese sentiment analysis tasks, many existing methods tend to use recurrent neural networks (e.g., long short-term memory networks and gated recurrent units) and standard one-dimensional convolutional neural networks (1D-CNN) to extract features. This is because a recurrent neural network can deal with the order dependence of the data to a certain extent and the one-dimensional convolution can extract local features. Although these methods have good performance in sentiment analysis tasks, recurrent neural networks (RNNs) cannot be parallelized, resulting in time-inefficiency, and the standard 1D-CNN can only extract a single sample feature, with the result that the feature information cannot be fully utilized. To this end, in this paper, we propose a multichannel two-dimensional convolutional neural network based on interactive features and group strategy (MCNN-IFGS) for Chinese sentiment analysis. Firstly, we no longer use word encoding technology but use character-based integer encoding to retain more fine-grained information. Besides, in character-level vectors, the interactive features of different elements are introduced to improve the dimensionality of feature vectors and supplement semantic information so that the input matches the model network. In order to ensure that more sentiment features are learned, group strategies are used to form several feature mapping groups, so the learning object is converted from the traditional single sample to the learning of the feature mapping group, so as to achieve the purpose of learning more features. Finally, multichannel two-dimensional convolutional neural networks with different sizes of convolution kernels are used to extract sentiment features of different scales. The experimental results on the Chinese dataset show that our proposed method outperforms other baseline and state-of-the-art methods. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Feature mapping group; Group strategy; Interactive features; Multichannel; Two-dimensional convolutional neural network","Convolution; Convolutional neural networks; Data mining; Encoding (symbols); Mapping; Recurrent neural networks; Semantics; Signal encoding; Convolutional neural network; Feature mapping; Feature mapping group; Group strategy; Interactive features; Multi channel; Network-based; Sentiment analysis; Two-dimensional; Two-dimensional convolutional neural network; Sentiment analysis; algorithm; China; semantics; Algorithms; China; Neural Networks, Computer; Semantics; Sentiment Analysis",Article,Scopus,2-s2.0-85122880976
"Prabakaran S., Ramar R., Hussain I., Kavin B.P., Alshamrani S.S., Alghamdi A.S., Alshehri A.","57220037160;57188971024;57214106024;57204980227;56825932900;57220046135;57210352986;","Predicting Attack Pattern via Machine Learning by Exploiting Stateful Firewall as Virtual Network Function in an SDN Network",2022,"Sensors","22","3","709","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122866234&doi=10.3390%2fs22030709&partnerID=40&md5=e9e193946dbd48c4c9d1b0b0f38b6f27","Decoupled data and control planes in Software Defined Networks (SDN) allow them to handle an increasing number of threats by limiting harmful network links at the switching stage. As storage, high-end servers, and network devices, Network Function Virtualization (NFV) is designed to replace purpose-built network elements with VNFs (Virtualized Network Functions). A Software Defined Network Function Virtualization (SDNFV) network is designed in this paper to boost network performance. Stateful firewall services are deployed as VNFs in the SDN network in this article to offer security and boost network scalability. The SDN controller’s role is to develop a set of guidelines and rules to avoid hazardous network connectivity. Intruder assaults that employ numerous socket addresses cannot be adequately protected by these strategies. Machine learning algorithms are trained using traditional network threat intelligence data to identify potentially malicious linkages and probable attack targets. Based on conventional network data (DT), Bayesian Network (BayesNet), Naive-Bayes, C4.5, and Decision Table (DT) algorithms are used to predict the target host that will be attacked. The experimental results shows that the Bayesian Network algorithm achieved an average prediction accuracy of 92.87%, Native–Bayes Algorithm achieved an average prediction accuracy of 87.81%, C4.5 Algorithm achieved an average prediction accuracy of 84.92%, and the Decision Tree algorithm achieved an average prediction accuracy of 83.18%. There were 451 k login attempts from 178 different countries, with over 70 k source IP addresses and 40 k source port addresses recorded in a large dataset from nine honeypot servers. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Attack prediction; Bayesian network; Decision table; Firewall; Machine learning; Network function virtualization; SDNFV; Software defined network","Bayesian networks; Computer system firewalls; Data mining; Decision trees; Digital storage; Forecasting; Large dataset; Machine learning; Software defined networking; Transfer functions; Virtual reality; Attack prediction; Bayesia n networks; Firewall; Network functions; Predicting attacks; Prediction accuracy; Software defined network function virtualization; Software-defined networks; Stateful firewalls; Network function virtualization; algorithm; article; assault; Bayesian learning; Bayesian network; decision tree; intelligence; machine learning; practice guideline; prediction; security; software",Article,Scopus,2-s2.0-85122866234
"Hu T., Wei D., Su Y., Wang X., Zhang J., Sun X., Liu Y., Guo Q.","55490127200;57221315573;56329651900;57411923900;57216516663;57215413782;55742311000;57225155352;","Quantifying the shape of urban street trees and evaluating its influence on their aesthetic functions based mobile lidar data",2022,"ISPRS Journal of Photogrammetry and Remote Sensing","184",,,"203","214",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122812720&doi=10.1016%2fj.isprsjprs.2022.01.002&partnerID=40&md5=d4048553d17eadd6b02fb1b5c13f58d1","Street trees are important components of an urban green space and understanding and measuring their ecological and cultural services is crucial for assessing the quality of streets and managing urban environments. Currently, most studies mainly focus on evaluating the ecological services of street trees by measuring the amount of greenness, but how to evaluate their aesthetic functions through quantitative measurements of street trees remain unclear. To address this problem, we propose a method to assess the aesthetic functions of street trees by quantifying the shape of greenness inspired by assessments of skyline aesthetics. Using a state-of-the-art mobile mapping system, we collected downtown-wide lidar data and panoramic images in Jinzhou City, Hebei Province, China. We developed a method for extracting the canopy line from the mobile lidar data, and then identified two basic elements, peaks and gaps, from street canopy lines and extracted six indexes (i.e., richness of peaks, evenness of peaks, frequency of peaks, total length of gaps, evenness of gaps and frequency of gaps) to describe the fluctuations and continuities of street canopy lines. We analyzed the abundance and spatial distribution of these indexes together with survey responses on the streets’ aesthetics and found that most of them were significantly correlated with human perception of streets. Compared to indexes of amount of greenness (e.g., green volume and green view index), these shape indexes have stronger influences on the physical aesthetic beauty of street trees. These findings suggest that a comprehensive assessment of the aesthetic function of street trees should consider both shape and amount of greenness. This study provides a new perspective for the assessment of urban green spaces and can assist future urban greening planning and urban landscape management. © 2022 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Aesthetical value; Greenness; Mobile mapping system; Shape; Street tree","Data mining; Ecology; Forestry; Function evaluation; Optical radar; Trees (mathematics); Aesthetic functions; Aesthetical values; Greenness; LIDAR data; Mobile lidar; Mobile mapping systems; Shape; Street trees; Urban green spaces; Urban streets; Mapping; canopy; esthetics; lidar; mapping; perception; spatial distribution; urban planning; China; Hebei; Jinzhou; Liaoning",Article,Scopus,2-s2.0-85122812720
"Li Y., Wen W., Miao T., Wu S., Yu Z., Wang X., Guo X., Zhao C.","57221627556;35115675300;36017292200;55552429400;57202425599;57213151389;7404331653;55724676300;","Automatic organ-level point cloud segmentation of maize shoots by integrating high-throughput data acquisition and deep learning",2022,"Computers and Electronics in Agriculture","193",,"106702","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122684934&doi=10.1016%2fj.compag.2022.106702&partnerID=40&md5=7688c02eef3340799a43d63c39e1a3cd","Point cloud segmentation is essential for studying the 3D spatial characteristics of plants. Notably, the segmentation accuracy greatly impacts subsequent 3D plant phenotypes extraction and 3D plant reconstruction. Automated segmentation approaches for plant point clouds are a bottleneck in achieving big data processing of 3D plant phenotypes. Using maize as a representative crop, this study developed DeepSeg3DMaize, a technique for plant point cloud segmentation that integrates high-throughput data acquisition and deep learning. A high-throughput data acquisition platform for individual plants and an association mapping panel containing 515 inbred lines were used to construct the training dataset. Specifically, the MVS-Pheno platform was used to acquire high-throughput data, and Label3DMaize was used for point cloud data labeling. Based on the dataset, PointNet was introduced to implement stem-leaf and organ instance segmentation, and six phenotypes were extracted. According to the results, the mean precision and F1-Score of stem-leaf segmentation were 0.91 and 0.85, respectively. Meanwhile, the mean precision and F1-Score for organ instance segmentation were 0.94 and 0.93, respectively. The correlations of the six parameters (leaf length, leaf width, leaf inclination, leaf growth height, plant height, and stem height) extracted from the segmentation results with the measured values were 0.90, 0.82, 0.94, 0.95, 0.99, and 0.94, respectively. High-throughput data acquisition, automatic organ segmentation, and phenotypic data extraction form an automatic phenotypic data processing pipeline, which is practical for dealing with large amounts of initial data. Besides, it provides a systematic reference for the automated analysis of 3D phenotypic features at the individual plant level. © 2022 Elsevier B.V.","Deep learning; High throughput; Maize; Phenotype; Pipeline; Point cloud segmentation","Data handling; Data mining; Deep learning; Extraction; Instance Segmentation; Pipeline processing systems; Pipelines; Deep learning; F1 scores; High-throughput; High-throughput data; Maize; Mean precision; Phenotype; Phenotypic data; Point cloud segmentation; Spatial characteristics; Data acquisition; data acquisition; learning; maize; mapping; phenotype; pipeline; segmentation",Article,Scopus,2-s2.0-85122684934
"Al Haddad C., Antoniou C.","57212564883;7004031914;","A data–information–knowledge cycle for modeling driving behavior",2022,"Transportation Research Part F: Traffic Psychology and Behaviour","85",,,"83","102",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122629405&doi=10.1016%2fj.trf.2021.12.017&partnerID=40&md5=7790219484f95cc441003911e456efad","When talking about automation, “autonomous vehicles”, often abbreviated as AVs, come to mind. In transitioning from the “driver” mode to the different automation levels, there is an inevitable need for modeling driving behavior. This often happens through data collection from experiments and studies, but also information extraction, a key step in behavioral modeling. Particularly, naturalistic driving studies and field operational trials are used to collect meaningful data on drivers’ interactions in real–world conditions. On the other hand, information extraction methods allow to predict or mimic driving behavior, by using a set of statistical learning methods. In simple words, the way to understand drivers’ needs and wants in the era of automation can be represented in a data–information cycle, starting from data collection, and ending with information extraction. To develop this cycle, this research reviews studies with keywords “data collection”, “information extraction”, “AVs”, while keeping the focus on driving behavior. The resulting review led to a screening of about 161 papers, out of which about 30 were selected for a detailed analysis. The analysis included an investigation of the methods and equipment used for data collection, the features collected, the size and frequency of the data along with the main problems associated with the different sensory equipment; the studies also looked at the models used to extract information, including various statistical techniques used in AV studies. This paved the way to the development of a framework for data analytics and fusion, allowing the use of highly heterogeneous data to reach the defined objectives; for this paper, the example of impacts of AVs on a network level and AV acceptance is given. The authors suggest that such a framework could be extended and transferred across the various transportation sectors. © 2022 Elsevier Ltd","Behavioral modeling; Data analytics; Data collection; Data fusion; Impacts of AVs; Information extraction","Artificial intelligence; Automation; Data acquisition; Data Analytics; Data fusion; Data mining; Information retrieval; Learning systems; Sensory analysis; Automation levels; Autonomous Vehicles; Behavioral model; Data analytics; Data collection; Data informations; Driving behaviour; Impact of AV; Information extraction; Model drivings; Behavioral research",Article,Scopus,2-s2.0-85122629405
"Chen J., Ding R., Jin Y., Zhu C., Jiang X., Wang S., Li Z., Li W., Wu C.","57202120819;57407206100;57268303000;57207876842;57407427500;57217160228;57407206200;57216171739;25122022200;","HRPIF data mining based on data-dependent/independent acquisition for Rhei Radix et Rhizoma metabolite screening in rats",2022,"Journal of Chromatography B: Analytical Technologies in the Biomedical and Life Sciences","1190",,"123095","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122612644&doi=10.1016%2fj.jchromb.2021.123095&partnerID=40&md5=50d625bdd7cd0c8818c493312878d4aa","In traditional Chinese medicine (TCM), components with identical nuclei often share structural similarity, indicating the possibility of similar second-level mass spectrometry (MS/MS) fragments. High-resolution product-ion filter (HRPIF) technique can be utilized to identify metabolites, with similar fragments, in vivo. In principle, this technique applies to TCM; however, its application has been restricted due to the limitations of traditional MS/MS data acquisition. Therefore, a novel analysis strategy, based on data-dependent acquisition (DDA) and data-independent acquisition (DIA) datasets, has been developed for the determination of template product ions and efficient non-targeted identification of TCM-related components in vivo by HRPIF and background subtraction (BS). This DDA-DIA combination strategy, taking Rhei Radix et Rhizoma as a test case, identified 71 anthraquinone prototype components in vitro (36 of which were discovered for the first time), and 45 related components in vivo, confirming glucuronidation and sulfation as the main reactions. The developed strategy could rapidly identify TCM-related components in vivo with high sensitivity, indicating the immense importance of this novel HRPIF data mining technology in TCM analysis. © 2021","Background subtraction; Data dependent acquisition; Data independent acquisition; High resolution product-ion filter; Metabolism; Rhei Radix et Rhizoma","Data mining; Diagnosis; Filtration; Ions; Ketones; Mass spectrometry; Metabolites; Background subtraction; Data dependent; Data dependent acquisition; Data independent acquisition; High resolution; High resolution product-ion filter; In-vivo; Product ions; Rhei radix et rhizoma; Traditional Chinese Medicine; Data acquisition",Article,Scopus,2-s2.0-85122612644
"Beulah D., Raj P.V.K.","57406467100;57213665546;","The Ensemble of Unsupervised Incremental Learning Algorithm for Time Series Data",2022,"International Journal of Engineering, Transactions B: Applications","35","2",,"319","326",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122564387&doi=10.5829%2fije.2022.35.02b.07&partnerID=40&md5=fffe80d38e4de4a7a5cde84c403c1fc8","Data mining is one of the key concepts to discover hidden knowledge from available data. Along with the data mining, data analytics is a field to analyze and process data in a scientific and cognitive angle. It is more helpful to convert knowledge to actionable knowledge for accurate decision making. Data Stream Mining is another challenging area than normal Data Mining due to its dynamics. Dynamics of data in a stream includes changes in data frequency, volume and nature. T his pap er focuses on the behavior of data mining of machines in process/manufacturing industries. In general, such data is continuous numerical and time series data captured by various industrial sensors. By nature, equipmentor machinery behaviour can change over time. It requires calibration/replacement before failure of machinery. By analyzing data, one can find the behavior or state change. To identify that, dynamic models are required to be builtusing data mining and data stream mining. Thus, we are followin g a semi-novel approach for building such models using “Ensemble of Unsupervised Incremental Learning"" method. Results show how the existing methods are different from the proposed method. This method can be applied for any other domain like image/audio/video or text mining. © 2022 Materials and Energy Research Center. All rights reserved.","Data mining; Data stream mining; Incremental learning; Unsupervised learning","Data Analytics; Decision making; Learning algorithms; Learning systems; Machinery; Time series; Data analytics; Data streams mining; Decisions makings; Hidden knowledge; In-process; Incremental learning; Numerical series; Process data; Process manufacturing industries; Time-series data; Data mining",Article,Scopus,2-s2.0-85122564387
"Kim S., Choi Y., Won J.-H., Mi Oh J., Lee H.","57405315000;57218206734;57221134512;55375582800;55811592700;","An annotated corpus from biomedical articles to construct a drug-food interaction database",2022,"Journal of Biomedical Informatics","126",,"103985","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122495606&doi=10.1016%2fj.jbi.2022.103985&partnerID=40&md5=a7de98376cbd4c21f64c0bc6e572dad2","Motivation: While drug-food interaction (DFI) may undermine the efficacy and safety of drugs, DFI detection has been difficult because a well-organized database for DFI did not exist. To construct a DFI database and build a natural language processing system extracting DFI from biomedical articles, we formulated the DFI extraction tasks and manually annotated texts that could have contained DFI information. In this article, we introduced a new annotated corpus for extracting DFI, the DFI corpus. Results: The DFI corpus contains 2270 abstracts of biomedical articles accessible through PubMed and 2498 sentences that contain DFI and/or drug-drug information (DDI), a substantial amount of information about drug/food entities, evidence-levels of abstracts and relations between named entities. BERT models pre-trained on the biomedical domain achieved a F1 score 55.0% in extracting DFI key-sentences. To the best of our knowledge, the DFI corpus is the largest public corpus for drug-food interaction. Availability and implementation: Our corpus is available at https://github.com/ccadd-snu/corpus-for-DFI-extraction. © 2022 Elsevier Inc.","Biomedical corpora; Drug interaction; Drug-food interaction; Information extraction; Natural language processing","Abstracting; Data mining; Database systems; Drug interactions; Amount of information; Biomedical corpus; Biomedical domain; F1 scores; Information extraction; Interaction database; Interaction detection; Interaction extraction; Interaction information; Named entities; Natural language processing systems; cannabidiol; carboplatin; cardiovascular agent; cinnamic acid; cisplatin; cyclophosphamide; doxorubicin; iron; levothyroxine; nadolol; simvastatin; temozolomide; warfarin; Article; data extraction; drug database; food drug interaction; human; information retrieval; natural language processing",Article,Scopus,2-s2.0-85122495606
"Kuusimäki T., Sainio J., Kurki S., Vahlberg T., Kaasinen V.","57205494303;57396999400;56017774800;6602710478;6701462469;","Prediagnostic expressions in health records predict mortality in Parkinson's disease: A proof-of-concept study",2022,"Parkinsonism and Related Disorders","95",,,"35","39",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122332959&doi=10.1016%2fj.parkreldis.2021.12.015&partnerID=40&md5=11d0d736005998f062c5176cd9221ed3","Introduction: The relationship of prodromal markers of PD with PD mortality is unclear. Electronic health records (EHRs) provide a large source of raw data that could be useful in the identification of novel relevant prognostic factors in PD. We aimed to provide a proof of concept for automated data mining and pattern recognition of EHRs of PD patients and to study associations between prodromal markers and PD mortality. Methods: Data from EHRs of PD patients (n = 2522) were collected from the Turku University Hospital database between 2006 and 2016. The data contained >27 million words/numbers and >750000 unique expressions. The 5000 most common words were identified in three-year time period before PD diagnosis. Cox regression was used to investigate the association of expressions with the 5-year survival of PD patients. Results: During the five-year period after PD diagnosis, 839 patients died (33.3%). If expressions associated with psychosis/hallucinations were identified within 3 years before the diagnosis, worse survival was observed (hazard ratio = 1.71, 95%CI = 1.46–1.99, p < 0.001). Similar effects were observed for words associated with cognition (1.23, 1.05–1.43, p = 0.009), constipation (1.34, 1.15–1.56, p = 0.0002) and pain (1.34, 1.12–1.60, p = 0.001). Conclusions: Automated mining of EHRs can predict relevant clinical outcomes in PD. The approach can identify factors that have previously been associated with survival and detect novel associations, as observed in the link between poor survival and prediagnostic pain. The significance of early pain in PD prognosis should be the focus of future studies with alternate methods. © 2022 The Authors","Data mining; Electronical health records; Mortality; Parkinson's disease; Prodromal; Prognosis","aged; Article; cognition; constipation; controlled study; data mining; death; disease association; disease course; disease duration; electronic health record; female; Finland; hallucination; hazard ratio; human; major clinical study; male; methodology; mortality; pain; Parkinson disease; pattern recognition; population research; prodromal symptom; proof of concept; proportional hazards model; psychosis; sex difference; survival analysis; university hospital",Article,Scopus,2-s2.0-85122332959
"Rengo M., Landolfi F., Picchia S., Bellini D., Losquadro C., Badia S., Caruso D., Iannicelli E., Osti M.F., Tombolini V., Carbone I., Giunta G., Laghi A.","55387537100;57219247579;56176696800;6602835722;57190986941;57211130073;57396806500;7004615638;7004038181;7004832297;7004207292;57396600800;18340271400;","Rectal cancer response to neoadjuvant chemoradiotherapy evaluated with MRI: Development and validation of a classification algorithm",2022,"European Journal of Radiology","147",,"110146","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122232339&doi=10.1016%2fj.ejrad.2021.110146&partnerID=40&md5=37f13c2800249b111c1046fd4a7a30d0","Objective: The aim of this study was to develop and validate a decision support model using data mining algorithms, based on morphologic features derived from MRI images, to discriminate between complete responders (CR) and non-complete responders (NCR) patients after neoadjuvant chemoradiotherapy (CRT), in a population of patients with locally advanced rectal cancer (LARC). Methods: Two populations were retrospectively enrolled: group A (65 patients) was used to train a data mining decision tree algorithm whereas group B (30 patients) was used to validate it. All patients underwent surgery; according to the histology evaluation, patients were divided in CR and NCR. Staging and restaging MRI examinations were retrospectively analysed and seven parameters were considered for data mining classification. Five different classification methods were tested and evaluated in terms of sensitivity, specificity, accuracy and AUC in order to identify the classification model able to achieve the best performance. The best classification algorithm was subsequently applied to group B for validation: sensitivity, specificity, positive and negative predictive value, accuracy and ROC curve were calculated. Inter and intra-reader agreement were calculated. Results: Four features were selected for the development of the classification algorithm: MRI tumor regression grade (MR-TRG), staging volume (SV), tumor volume reduction rate (TVRR) and signal intensity reduction rate (SIRR). The decision tree J48 showed the highest efficiency: when applied to group B, all the CR and 18/21 NCR were correctly classified (sensitivity 85.71%, specificity 100%, PPV 100%, NPV 94.2%, accuracy 95.7%, AUC 0.833). Both inter- and intra-reader evaluation showed good agreement (κ > 0.6). Conclusions: The proposed decision support model may help in distinguishing between CR and NCR patients with LARC after CRT. © 2022 Elsevier B.V.","Data mining; Magnetic Resonance Imaging; Neoadjuvant Therapy; Rectal cancer","dexamethasone; fluorouracil; ondansetron; oxaliplatin; adult; advanced cancer; Article; calculation; cancer classification; cancer grading; cancer morphology; cancer radiotherapy; cancer staging; classification algorithm; continuous infusion; controlled study; decision support system; decision tree; diagnostic accuracy; diagnostic test accuracy study; diagnostic value; diffusion weighted imaging; female; histopathology; human; human tissue; interrater reliability; intrarater reliability; locally advanced rectal cancer; major clinical study; male; middle aged; multiple cycle treatment; neoadjuvant chemotherapy; nuclear magnetic resonance imaging; predictive value; radiation dose; receiver operating characteristic; rectum cancer; reproducibility; retrospective study; sensitivity and specificity; signal processing; treatment response; tumor regression; tumor volume; validation study; algorithm; chemoradiotherapy; neoadjuvant therapy; nuclear magnetic resonance imaging; rectum tumor; treatment outcome; Algorithms; Chemoradiotherapy; Humans; Magnetic Resonance Imaging; Neoadjuvant Therapy; Rectal Neoplasms; Retrospective Studies; Treatment Outcome",Article,Scopus,2-s2.0-85122232339
"Ouyang P., Wang J.-J., Jasmine Chang A.-C.","57222321518;55917088400;57390707700;","Patients need emotional support: Managing physician disclosure information to attract more patients",2022,"International Journal of Medical Informatics","158",,"104674","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121968726&doi=10.1016%2fj.ijmedinf.2021.104674&partnerID=40&md5=cb42a9615a8f3e05d29528630461cfc7","Background: Information asymmetry causes barriers for the patient's decision-making in the online health community. Patients can rely on the physician's self-disclosed information to alleviate it. However, the impact of physician's self-disclosed information on the patient's decision has rarely been discussed. Objectives: To investigate the impact of the physician's self-disclosed information on the patient's decision in the online health community and to examine the moderating effect of the physician's online reputation. Methods: Drawing on the limited-capacity model of attention, we develop a theoretical model to estimate the impact of physician's self-disclosure information on patient's decision and the contingent roles of physician's online reputation in online healthcare community by econometric methods. We designed a web crawler based on R language program to collect more than 20,000 physicians’ data from their homepage in Haodf—a leading online healthcare community platform in China. The attributes of the physician's information disclosure are measured by the following variables: emotion orientation, the quantity of information and the semantic topics diversity. Results: The empirical analysis derives the following findings: (1) The emotion orientation in physician's self-disclosure information is positively associated with patient's decision; (2) Both excessive quantity of information and semantic topics diversity can raise barriers for patient's decision; (3) When the level of physician's online reputation is high, the negative effect of the quantity of information and semantic topics diversity are all strengthened while the positive effect of the emotion orientation is not strengthened. Conclusions: This study has a profound importance for a deep understanding of the impact of physician's self-disclosure information and contributes to the literature on information disclosure, the limited capacity model of attention, patient's decision. Also, this study provides implications for practice. © 2021","Attention; Online health community; Self-disclosed information; Text mining","Data mining; Decision making; Health care; Web crawler; Attention; Capacity modeling; Information disclosure; Limited capacity; Models of attention; Online health communities; Online healthcare communities; Self-disclosed information; Self-disclosure; Topic diversity; Semantics; adult; article; attention; China; drawing; emotion; human; language; mining; self disclosure; theoretical study",Article,Scopus,2-s2.0-85121968726
"Robson B., Boray S., Weisman J.","7004890572;56856847200;57390570800;","Mining real-world high dimensional structured data in medicine and its use in decision support. Some different perspectives on unknowns, interdependency, and distinguishability",2022,"Computers in Biology and Medicine","141",,"105118","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121965398&doi=10.1016%2fj.compbiomed.2021.105118&partnerID=40&md5=e5108c02dc692b657311fed3043c892f","There are many difficulties in extracting and using knowledge for medical analytic and predictive purposes from Real-World Data, even when the data is already well structured in the manner of a large spreadsheet. Preparative curation and standardization or “normalization” of such data involves a variety of chores but underlying them is an interrelated set of fundamental problems that can in part be dealt with automatically during the datamining and inference processes. These fundamental problems are reviewed here and illustrated and investigated with examples. They concern the treatment of unknowns, the need to avoid independency assumptions, and the appearance of entries that may not be fully distinguished from each other. Unknowns include errors detected as implausible (e.g., out of range) values that are subsequently converted to unknowns. These problems are further impacted by high dimensionality and problems of sparse data that inevitably arise from high-dimensional datamining even if the data is extensive. All these considerations are different aspects of incomplete information, though they also relate to problems that arise if care is not taken to avoid or ameliorate consequences of including the same information twice or more, or if misleading or inconsistent information is combined. This paper addresses these aspects from a slightly different perspective using the Q-UEL language and inference methods based on it by borrowing some ideas from the mathematics of quantum mechanics and information theory. It takes the view that detection and correction of probabilistic elements of knowledge subsequently used in inference need only involve testing and correction so that they satisfy certain extended notions of coherence between probabilities. This is by no means the only possible view, and it is explored here and later compared with a related notion of consistency. © 2021 Elsevier Ltd","Approximations; Assumptions; Bayes net; Bayes' rule; Clinical decision support; Coherence; Distinguishability; Hyperbolic Dirac net; Inference net; Interdependency; Real world data; Unknowns","Data mining; Information theory; Medicine; Quantum theory; Approximation; Assumption; Bayes net; Bayes' rule; Clinical decision support; Distinguishability; Hyperbolic dirac net; Inference net; Interdependency; Real world data; Real-world; Unknown; Decision support systems; article; data mining; decision support system; human; human experiment; information science; language; probability; quantum mechanics",Article,Scopus,2-s2.0-85121965398
"Wu B., Jiang Z., Zhu S., Zhang H., Wang Y., Zhang Y.","57390573500;56020606800;55939864600;55881706100;56274770600;57390735000;","Data-Driven Decision-Making method for Functional Upgrade Remanufacturing of used products based on Multi-Life Customization Scenarios",2022,"Journal of Cleaner Production","334",,"130238","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121964935&doi=10.1016%2fj.jclepro.2021.130238&partnerID=40&md5=f0ac2baf0aa4845a5b5abf50c5743685","With rapid changes in technology and customer preferences, functional obsolescence of used products poses serious challenges to resumed remanufacturing. Upgrade remanufacturing is a potential solution for dealing with the problems of functional obsolescence. The multi-attribute remaining life and customized life of the functional unit are critical elements of decision-making for upgrade remanufacturing solution, yet there are many possible scenarios for the multi-attribute remaining life and the customized life. The optimal solution varies with the various scenarios, which makes the decision-making for choosing the optimal solution of Functional Upgrade Remanufacturing (FUR) very individual and complicated. To this end, the paper proposes a Data-Driven Decision-Making (DDDM) method for FUR of used products based on Multi-Life Customization Scenarios (MLCS). MLCS describes the relationship between the remaining physical, technical, economic life and customized life. Firstly, the used product is decomposed into several functional units that are taken as the objects for upgrade remanufacturing, and the mapping between MLCS and decision-making for FUR is established through data mining. Then the DDDM method of Bayesian network is employed to inference, which is constructed based on historical data, and the solution with the largest posteriori probability is taken as the optimal solution. Finally, a case study on decision-making for FUR of a used mechanical hydraulic power steering system is demonstrated to validate the proposed method. © 2021 Elsevier Ltd","Bayesian network; Data-driven decision-making; Functional unit; Multi-life customization scenarios; Upgrade remanufacturing","Data mining; Decision making; Obsolescence; Optimal systems; Bayesia n networks; Customisation; Data driven decision; Data-driven decision-making; Decisions makings; Functional units; Functional upgrades; Multi-life customization scenario; Upgrade remanufacturing; Used product; Bayesian networks",Article,Scopus,2-s2.0-85121964935
"Shao W., Prabowo A., Zhao S., Koniusz P., Salim F.D.","57208885593;57212536152;57212536155;36622160000;7801420000;","Predicting flight delay with spatio-temporal trajectory convolutional network and airport situational awareness map",2022,"Neurocomputing","472",,,"280","293",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121961967&doi=10.1016%2fj.neucom.2021.04.136&partnerID=40&md5=3a3929bad3de84c45fcf7bcb28373634","To model and forecast flight delays accurately, it is crucial to harness various vehicle trajectory and contextual sensor data on airport tarmac areas. These heterogeneous sensor data, if modelled correctly, can be used to generate a situational awareness map. Existing techniques apply traditional supervised learning methods onto historical data, contextual features and route information among different airports to predict flight delay are inaccurate and only predict arrival delay but not departure delay, which is essential to airlines. In this paper, we propose a vision-based solution to achieve a high forecasting accuracy, applicable to the airport. Our solution leverages a snapshot of the airport situational awareness map, which contains various trajectories of aircraft and contextual features such as weather and airline schedules. We propose an end-to-end deep learning architecture, TrajCNN, which captures both the spatial and temporal information from the situational awareness map. Additionally, we reveal that the situational awareness map of the airport has a vital impact on estimating flight departure delay. Our proposed framework obtained a good result (around 18 min error) for predicting flight departure delay at Los Angeles International Airport. © 2021 Elsevier B.V.","Feature engineering; Flight delay prediction; Spatio-temporal data mining","Air traffic control; Air transportation; Convolutional neural networks; Data mining; Deep learning; Forecasting; Contextual feature; Convolutional networks; Feature engineerings; Flight delays; Flight delays predictions; Modeling and forecast; Sensors data; Situational awareness; Spatio-temporal data mining; Spatio-temporal trajectories; Airports; aircraft; airport; article; awareness; California; data mining; deep learning; forecasting; prediction; vision; weather",Article,Scopus,2-s2.0-85121961967
"Hosseinpour H., Samadzadegan F., Javan F.D.","57388603800;55898340800;57388841400;","CMGFNet: A deep cross-modal gated fusion network for building extraction from very high-resolution remote sensing images",2022,"ISPRS Journal of Photogrammetry and Remote Sensing","184",,,"96","115",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121925899&doi=10.1016%2fj.isprsjprs.2021.12.007&partnerID=40&md5=1d55bb0eb6593f530baf33a3c8049102","The extraction of urban structures such as buildings from very high-resolution (VHR) remote sensing imagery has improved dramatically, thanks to recent developments in deep multimodal fusion models. However, Due to the variety of colour intensities with complex textures of building objects in VHR images and the low quality of the digital surface model (DSM), it is challenging to develop the optimal cross-modal fusion network that takes advantage of these two modalities. This research presents an end-to-end cross-modal gated fusion network (CMGFNet) for extracting building footprints from VHR remote sensing images and DSMs data. The CMGFNet extracts multi-level features from RGB and DSM data by using two separate encoders. We offer two methods for fusing features in two modalities: Cross-modal and multi-level feature fusion. For cross-modal feature fusion, a gated fusion module (GFM) is proposed to combine two modalities efficiently. The multi-level feature fusion fuses the high-level features from deep layers with shallower low-level features through a top-down strategy. Furthermore, a residual-like depth-wise separable convolution (R-DSC) is introduced to enhance the performance of the up-sampling process and decrease the parameters and time complexity in the decoder section. Experimental results from challenging datasets show that the CMGFNet outperforms other state-of-the-art models. The efficacy of all significant elements is also confirmed by the extensive ablation study. © 2021 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Building extraction; Cross-modal; Digital surface model; Gated fusion module; VHR remote sensing image","Buildings; Complex networks; Data mining; Extraction; Image fusion; Remote sensing; Textures; Building extraction; Cross-modal; Digital surface models; Features fusions; Fusion modules; Gated fusion module; High-resolution remote sensing images; Multilevels; Very high resolution; Very high-resolution remote sensing image; Image enhancement; complexity; numerical model; remote sensing; satellite imagery",Article,Scopus,2-s2.0-85121925899
"Panda K.C., Singh R.M., Thakural L.N., Sahoo D.P.","57218251871;57221126972;23487258800;57219264780;","Representative grid location-multivariate adaptive regression spline (RGL-MARS) algorithm for downscaling dry and wet season rainfall",2022,"Journal of Hydrology","605",,"127381","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121924863&doi=10.1016%2fj.jhydrol.2021.127381&partnerID=40&md5=07ec374347b6a87ad36ebc8af323436b","The high dimensionality of predictor variables reduces the predictive accuracy of statistical downscaling models. Principal component analysis (PCA) is one of the extensively used approaches for reducing the dimensionality of the predictors. However, PCA reduces the efficiency of downscaling models when a nonlinear predictor-predictand relationship exists. To solve this issue, the representative grid location (RGL) approach was used to minimise the dimension of the predictor variables. Thus, a novel RGL-MARS (Multivariate Adaptive Regression Spline) based downscaling model was proposed in the current study. The proposed model was compared with PCA-MLR (Principal Component Analysis-Multiple Linear Regression), MARS, and PCA-MARS downscaling methods. Eight general circulation models (GCMs) were considered, out of which only CAN-ESM2 (second-generation Canadian Earth System Model) GCM was found suitable for the study area. Three criteria, i.e., correlation coefficient (CC), mutual information (MI), and decision tree (DT), were used for the selection of dominant predictors. It was observed that CMD (CC + MI + DT) predictors, when used with the RGL-MARS downscaling method, showed the best performance (NSE = 74–89%; root mean square error (RMSE) = 10–40 mm). The results revealed that rainfall simulated by the RGL-MARS model captured the standard deviation and coefficient of variation in the observed rainfall, whereas the rest failed to do so. The RGL-MARS model solved the issue of underprediction of wet season rainfall and overprediction of dry season rainfall. The RGL-MARS addressed the issue with an improved NSE of 62–89% and RMSE of 18–45 mm; thus, the result exhibited that integration of MARS with the RGL perform better compared to the PCA. This study also demonstrated that downscaling model outcomes were more reliable for the wet than the dry season. The future rainfall projection indicated that the rainfall fluctuation might be more in the dry season than the wet season. The proposed downscaling method can improve the accuracy of rainfall projections under various climatic conditions, subsequently, coupled with a range of hydrological and land surface models to better understand the catchment characteristics and the water balance dynamics in future climate studies. © 2021 Elsevier B.V.","Decision tree; General circulation models; Multiple linear regression; Multivariate adaptive regression spline; Representative grid location; Statistical downscaling","Catchments; Data mining; Decision trees; Drought; Location; Mean square error; Principal component analysis; Rain; Down-scaling; Downscaling methods; Dry seasons; General circulation model; Grid location; Multivariate adaptive regression splines; Predictor variables; Principal-component analysis; Representative grid location; Statistical downscaling; Multiple linear regression; algorithm; circulation modeling; decision analysis; downscaling; dry season; general circulation model; land surface; regression analysis; water budget; wet season",Article,Scopus,2-s2.0-85121924863
"Rath R., Dutta D., Kamesh R., Sharqawy M.H., Moulik S., Roy A.","57388543800;57388780400;56537094700;16480839900;55054672700;57220714384;","Rational design of high power density “Blue Energy Harvester” pressure retarded osmosis (PRO) membranes using artificial intelligence-based modeling and optimization",2022,"Energy Conversion and Management","253",,"115160","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121900357&doi=10.1016%2fj.enconman.2021.115160&partnerID=40&md5=3210b256407c9cd30601b680af904ebd","The challenge in harvesting Salinity Gradient Power (SGP) through pressure retarded osmosis (PRO) requires design of high power density (PD) membranes and optimized process for operation. Recent studies show that for a feasible PRO operation the minimum net PD should be around 50 W/m2. In this study, a data-driven approach has been adopted for designing optimum membranes as well as operating conditions. 200 papers, from last decade, were extensively reviewed and 34 experimental research articles were shortlisted for possible data mining, to predict water flux (WF) and PD. Comprehensive screened/pre-processed data related to both membrane and process (16 inputs) was obtained from 18 articles amounting to 339 data points. Two artificial neural network (ANN) models were explored (i) Levenberg-Marquardt (ANN-LM), and (ii) Bayesian Regularization (ANN-BR) along with a combination of three different activation functions i.e., hyperbolic tangent sigmoid transfer function (Tan-Sigmoid), logarithmic sigmoid transfer function (Log-Sigmoid) in the input and output layers. Out of the six resulting combinations, the best performing combination was found to be Tan-Sigmoid activation function in both layers with ANN-BR model having an R2 value of 0.97 for WF and 0.98 for PD. Membrane properties like the type of membrane, thickness, and water permeability coefficient were found to be the major contributing factors for the prediction of WF while for PD, operating conditions such as applied pressure were found to play the major contributing factor (10–16 %). Optimization results yield a maximum WF of 147 LMH and PD of 87 W/m2. These results were compared with the solution diffusion (S-D) model. © 2021 Elsevier Ltd","Artificial intelligence; High power density membrane; Inverse design; Machine learning; Pressure retarded osmosis; Salinity gradient power","Chemical activation; Data mining; Hyperbolic functions; Machine learning; Neural networks; Transfer functions; Activation functions; High power density membrane; High-power-density; Inverse designs; Operating condition; Power densities; Pressure retarded osmose; Salinity gradient power; Sigmoid transfer function; Water flux; Osmosis",Article,Scopus,2-s2.0-85121900357
"Kim E.H.J., Jeong Y.K., Kim Y., Song M.","57193928088;55971717700;57214047074;35243608100;","Exploring scientific trajectories of a large-scale dataset using topic-integrated path extraction",2022,"Journal of Informetrics","16","1","101242","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121685554&doi=10.1016%2fj.joi.2021.101242&partnerID=40&md5=bf4a5b7d1dd807f8195e61dd6923179c","Main path analysis (MPA) is the most widely accepted approach to tracing knowledge transfer in a research field. In this study, we extracted multiple longest paths from the multidisciplinary academic field's citation network and integrating topic modeling to the extracted paths. We consider three main aspects of trajectory analysis when analyzing the represented documents through the extracted paths: emergence, authority, and topic dynamics. For path extraction, we adopt the longest path algorithm that consists of the following three steps: 1) topological sort, 2) edge relaxation, and 3) multiple path extraction. For topic integration into multiple paths, we employ latent Dirichlet allocation (LDA) by utilizing the topic-document matrix that LDA derives to select an article's topic from the citation network, where each article is labeled with the topic that is assigned with the highest topical probability for that article. We conduct a series of experiments to examine the results on a dataset from the field of healthcare informatics that PubMed provides. © 2021 Elsevier Ltd","Citation analysis; Healthcare informatics; Longest path; Main path analysis; Topic modeling","Data mining; Health care; Knowledge management; Large dataset; Regression analysis; Statistics; Citation analysis; Citation networks; Health care informatics; Knowledge transfer; Large-scale datasets; Latent Dirichlet allocation; Longest path; Main path analysis; Multiple-path; Topic Modeling; Extraction",Article,Scopus,2-s2.0-85121685554
"Wang M., Wang Z., Geng Y., Lin B.","57198454333;56176432100;24075782800;7403508277;","Interpreting the neural network model for HVAC system energy data mining",2022,"Building and Environment","209",,"108449","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121684149&doi=10.1016%2fj.buildenv.2021.108449&partnerID=40&md5=7e9890bbab3a5209c2276844e3e5f63a","To enhance the energy efficiency of heating, ventilation and air conditioning (HVAC) systems, which is a non-linear and complicated system, machine learning has been used intensively. However, traditional white-box machine-learning models with good interpretability often do not have satisfactory accuracy, while black-box machine-learning models that are more accurate could not be interpreted easily, which impedes its application. In this study, we propose a method to interpret a neural network (NN) model using gradients of the model, which quantifies the marginal influence of inputs to the output, based on the chain rule. Then the NN model is compared with other machine-learning models (the linear regression model and the XGBoost model) in accuracy, interpretability, and robustness. We then compared our result with the correlation analysis, a widely used method to extract the relation between the outputs (in this case, energy consumption) and inputs. Further, we perform feature selection based on gradients of the NN model, reducing 40% calculation time without sacrificing model accuracy. The feature importance given by the NN model is proved to be reasonable and informative compared with the other two models. The scope of this study is neither to verify the superiority of the NN model, nor to predict the energy consumption accurately. Instead, the goal of this study is to provide a method to interpret the results of NN models. © 2021 Elsevier Ltd","Data mining; Energy efficiency; Gradient; HVAC; Interpretability; Neural network","Data mining; Energy utilization; HVAC; Machine learning; Regression analysis; Conditioning systems; Energy data; Energy-consumption; Gradient; Heating ventilation and air conditioning; Interpretability; Machine learning models; Neural network model; Neural-networks; System energy; Energy efficiency; air conditioning; artificial neural network; correlation; data mining; energy efficiency; heating; machine learning; numerical model; ventilation",Article,Scopus,2-s2.0-85121684149
"Ho K.-H., Shih C.-M., Liu A.-J., Chen K.-C.","56305833600;10043888000;55905115600;7410238862;","Hypoxia-inducible lncRNA MIR210HG interacting with OCT1 is involved in glioblastoma multiforme malignancy",2022,"Cancer Science","113","2",,"540","552",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121614753&doi=10.1111%2fcas.15240&partnerID=40&md5=7ea60e35a3d761b7bdebaadcfaf37a56","An insufficient oxygen supply within the intratumoral environment, also known as hypoxia, induces glioblastoma multiforme (GBM) invasion, stemness, and temozolomide (TMZ) drug resistance. Long noncoding (lnc)RNAs have been reported to be involved in hypoxia and GBM progression. However, their roles in hypoxic GBM malignancy are still unclear. We investigated the mechanisms of hypoxia-mediated lncRNAs in regulating GBM processes. Using The Cancer Genome Atlas (TCGA) and data mining, hypoxia-correlated lncRNAs were identified. A hypoxia-upregulated lncRNA, MIR210HG, locating in nuclear regions, predicted poor prognoses of patients and modulated hypoxia-promoted glioma stemness, TMZ resistance, and invasion. Depletion of hypoxic MIR210HG suppressed GBM and patient-derived cell growth and increased TMZ sensitivity in vitro and vivo. Using RNA sequencing and gene set enrichment analysis (GSEA), MIR210HG-upregulated genes significantly belonged to the targets of octamer transcription factor 1 (OCT1) transcription factor. The direct interaction between OCT1 and MIR210HG was also validated. Two well-established worse prognostic factors of GBM, insulin-like growth factor–binding protein 2 (IGFBP2) and fibroblast growth factor receptor 1 (FGFR1), were identified as downstream targets of OCT1 through MIR210HG mediation in hypoxia. Consequently, the lncRNA MIR210HG is upregulated by hypoxia and interacts with OCT1 for modulating hypoxic GBM, leading to poor prognoses. These findings might provide a better understanding in functions of hypoxia/MIR210HG signaling for regulating GBM malignancy. © 2021 The Authors. Cancer Science published by John Wiley & Sons Australia, Ltd on behalf of Japanese Cancer Association.","FGFR1; hypoxic glioblastoma; IGFBP2; MIR210HG; OCT1","fibroblast growth factor receptor 1; hypoxia inducible factor 1alpha; Ki 67 antigen; long untranslated RNA; microRNA 210; octamer transcription factor 1; phosphatidylinositol 3 kinase; protein kinase B; somatomedin binding protein 2; temozolomide; alkylating agent; FGFR1 protein, human; fibroblast growth factor receptor 1; IGFBP2 protein, human; long untranslated RNA; octamer transcription factor 1; POU2F1 protein, human; somatomedin binding protein 2; temozolomide; Akt signaling; Article; binding site; cancer inhibition; cancer prognosis; cancer registry; cancer resistance; cancer survival; carcinogenesis; cell growth; cell invasion; cell nuclear structure; cellular distribution; chemosensitivity; cohort analysis; colony formation; controlled study; cytotoxicity; data mining; differential gene expression; down regulation; epithelial mesenchymal transition; expressed sequence tag; false discovery rate; focal adhesion; gene expression level; gene function; gene knockdown; gene location; gene overexpression; gene repression; gene set enrichment analysis; genetic association; glioblastoma; glioma cell; growth inhibition; human; human cell; hypoxia; in vitro study; in vivo study; molecular pathology; precipitation; predictive value; promoter region; protein expression level; protein induction; protein RNA binding; protein targeting; RNA pull-down; RNA sequencing; treatment response; tumor invasion; tumor volume; tumor xenograft; upregulation; animal; cell transformation; drug effect; drug resistance; gene expression regulation; genetics; glioblastoma; metabolism; mouse; pathology; prognosis; signal transduction; tumor cell line; tumor hypoxia; Animals; Antineoplastic Agents, Alkylating; Cell Line, Tumor; Cell Transformation, Neoplastic; Drug Resistance, Neoplasm; Gene Expression Regulation, Neoplastic; Glioblastoma; Humans; Insulin-Like Growth Factor Binding Protein 2; Mice; Octamer Transcription Factor-1; Prognosis; Receptor, Fibroblast Growth Factor, Type 1; RNA, Long Noncoding; Signal Transduction; Temozolomide; Tumor Hypoxia",Article,Scopus,2-s2.0-85121614753
"Yadav P.","57141837900;","Hybridized optimization oriented fast negative sequential patterns mining",2022,"Multimedia Tools and Applications","81","4",,"5279","5303",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121386567&doi=10.1007%2fs11042-021-11773-4&partnerID=40&md5=c04376f3af3b1e13ad4551ecf1a200a3","Recently, negative sequential patterns (NSP) (like missing medical treatments) mining is important in data mining research since it includes negative correlations between item sets, which are overlooked by positive sequential pattern mining (PSP) (for instance, utilization of medical service). Yet, discovering the NSP is very complex than finding PSP because of the important problem complexity occurred by high computational cost, non-occurring elements, as well as huge search space in evaluating NSC, and most of the NSP based existing works are inefficient. Therefore, this paper intends to propose a fast NSP mining algorithm for the disease prediction model. This model includes Data normalization, Data separation based on labels, and Pattern recognition phases. In the midst of data separation, the maximum occurring data is optimally selected using a new algorithm that hybridizes the FireFly (FF) algorithm and Grey Wolf Optimization (GWO). This proposed Firefly induced Grey Wolf optimization (F-GWO) algorithm automatically selects the maximum occurring information as per the PSP support. The proposed model is compared over other conventional methods with varied measures. Especially, the computation cost of our model is 46.87%, 6.27%, 9.37%, 2.76%, and 66.62% better than the existing GA, ABC, PSO, FF, and GWO models respectively. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","FF; GWO; Negative sequential pattern; Optimization algorithm; Positive sequential pattern","Data mining; Particle swarm optimization (PSO); Pattern recognition; Data separation; Gray wolf optimization; Gray wolves; Negative sequential pattern; Optimisations; Optimization algorithms; Positive sequential pattern; Sequential patterns; Sequential-pattern mining; Bioluminescence",Article,Scopus,2-s2.0-85121386567
"Liu Z., Liu D., Xiong J., Yuan X.","57376320000;57211237812;57376167300;57376463100;","A Parallel AdaBoost Method for Device-Free Indoor Localization",2022,"IEEE Sensors Journal","22","3",,"2409","2418",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121367055&doi=10.1109%2fJSEN.2021.3133904&partnerID=40&md5=6f2e25ea880a819e33832ac5ab292489","Device-free indoor localization methods based on Channel State Information (CSI) have become an increasingly important technique. The Naive Bayes (NB) classifier has been applied to indoor localization schemes for its simplicity and effectiveness. However, the NB classifier is a weak classifier, leading to unsatisfactory classification accuracy. In order to address this problem, in this paper, we propose the parallel AdaBoost localization method that combines multiple NB classifiers into two strong classifiers based on amplitude and phase. Then, we obtain the weighting coefficient of the classifiers by using the learning strategy, and effectively fuse the results of the classifiers for improving the indoor localization accuracy. Experimental results show that compared with the classical FIFS, CSI-MIMO, PCNB, BLS, ABPS, LSTM and FapFi algorithms, the proposed algorithm has higher localization accuracy. © 2001-2012 IEEE.","AdaBoost; Channel state information; Feature extraction; Indoor localization; Naive Bayes","Adaptive boosting; Barium compounds; Channel state information; Extraction; Feature extraction; Indoor positioning systems; Long short-term memory; Channel-state information; Device-free; Features extraction; Indoor localization; Localization accuracy; Localization method; Localization schemes; Location awareness; Naive bayes; Data mining",Article,Scopus,2-s2.0-85121367055
"Shen X., Li C., Chen W., Wang Y.","7402721417;57218347308;57203038216;35346204100;","MapICT: Unsupervised Radio-Map Learning from Imbalanced Crowd-Sourced Trajectories",2022,"IEEE Sensors Journal","22","3",,"2399","2408",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121365540&doi=10.1109%2fJSEN.2021.3133865&partnerID=40&md5=68bacd5071b5abb9c57a213007ffd4ee","The indoor localization based on WiFi fingerprints has attracted great attention, but the most significant and challenging work is the construction of the radio map without the need of site survey. With the rapid popularization of mobile and wearable devices, the method of collecting massive trajectory data by crowd-sourcing is becoming increasingly convenient and cost-effective, making unsupervised radio-map learning from unlabeled trajectories possible and promising. However, these crowd-sourced trajectories have low qualities and tend to present an imbalanced distribution due to the presence of hot spots, restricted areas in the building, as well as the random movement of participants. To address above challenges, MapICT scheme is proposed in this paper, using low-quality trajectory data to build radio map without site survey. First, Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) algorithm is suggested to extract target fingerprint vertexes, so as to balance the distribution of trajectory data. Second, target edges that represent relationship between vertexes are extracted by inertial data. Finally, a two-dimensional radio map can be constructed by vertexes and edges. Through simulation experiments and experiments in actual environment, the MapICT scheme proves to be feasible and effective, with the locating accuracy in teaching building and in mall being 11.98% and 7.38% respectively higher than that achieved by existing methods. © 2001-2012 IEEE.","Crowd-sourcing; Imbalanced distribution; Radio map; Unsupervised radio-map learning; WiFi","Clustering algorithms; Cost effectiveness; Data mining; Surveys; Wireless local area networks (WLAN); Crowd sourcing; Imbalanced distribution; Map learning; Radio maps; Site surveys; Trajectories datum; Unsupervised radio-map learning; Wifi; Wireless fidelities; Trajectories",Article,Scopus,2-s2.0-85121365540
"Macêdo J.B., das Chagas Moura M., Aichele D., Lins I.D.","57204951658;57223261074;57224212461;27467582900;","Identification of risk features using text mining and BERT-based models: Application to an oil refinery",2022,"Process Safety and Environmental Protection","158",,,"382","399",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121322209&doi=10.1016%2fj.psep.2021.12.025&partnerID=40&md5=ebde6a03677cdcc8e9bb6cae7027a714","The uncontrollable release of hazardous substances may lead to catastrophic accidents. In this context, risk studies are aimed at recommending either preventive measures or designing safeguards to mitigate the consequences. To that end, risk experts postulate possible leakages, then identify their causes and consequences, and finally evaluate and classify the risks into categories. These analyses rely on examination different engineering textual documents and attendance numerous meetings, which is very time consuming. Moreover, this qualitative process of hazard identification and assessment are usually the first steps of quantitative risk analysis (QRA) and is paramount to ensure its quality. Therefore, we here propose to use text mining and fine-tuned trained bidirectional encoder representations from transformers (BERT) models to support and reduce the efforts required for completing the early stages of QRA. Our idea is to apply these techniques to identify the potential consequences of accidents related to the operation of an oil refinery and classify each scenario in terms of severity of the consequence and likelihood of occurrence. The proposed method was applied to an actual oil refinery and presented very promising results. The potential consequences, the severity and likelihood categories were predicted with a mean accuracy of 97.42%, 86.44%, and 94.34% respectively. The models resulting from this research were embedded into a web-based app that is called HALO (hazard analysis based on language processing for oil refineries). © 2021","Hazard assessment; Hazard identification; Natural language processing; Oil refineries; Text mining","Accidents; Data mining; Natural language processing systems; Petroleum refining; Quality control; Risk analysis; Risk assessment; Catastrophic accidents; Hazard Assessment; Hazard identification; Hazardous substances; Identification of risks; Model application; Oil refineries; Preventive measures; Quantitative risk analysis; Text-mining; Hazards",Article,Scopus,2-s2.0-85121322209
"Mrosewski I., Dähn T., Hehde J., Kalinowski E., Lindner I., Meyer T.M., Olschinsky-Szermer M., Pahl J., Puls M., Sachse K., Switkowski R.","56182133500;57374539500;57374644100;57374539600;57374539700;57374854800;57375073100;57374854900;57374751800;57374960900;57205681698;","Indirectly determined hematology reference intervals for pediatric patients in Berlin and Brandenburg",2022,"Clinical Chemistry and Laboratory Medicine","60","3",,"408","432",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121315727&doi=10.1515%2fcclm-2021-0853&partnerID=40&md5=4ae447bf4a2b538bb5db86419e5dd1aa","Objectives: Establishing direct reference intervals (RIs) for pediatric patients is a very challenging endeavor. Indirectly determined RIs can address this problem by utilization of existing clinical laboratory databases. In order to provide better laboratory services to the local pediatric population, we established population-specific hematology RIs via data mining. Methods: Our laboratory information system (LIS) was searched for pediatric blood counts of patients aged from 0 days to 18 years, performed from 1st of January 2018 until 31st of March 2021. In total, 27,554 blood counts on our SYSMEX XN-9000 were initially identified. After application of pre-defined exclusion criteria, 18,531 sample sets remained. Age-and sex-specific RIs were established in accordance with International Federation of Clinical Chemistry and Laboratory Medicine (IFCC) and Clinical Laboratory Standards Institute (CLSI) recommendations. Results: When compared to pediatric RIs supplied by other authors, the RIs determined specifically for pediatric patients from Berlin and Brandenburg showed several relevant differences, especially with regard to white blood cell counts (WBCs), red blood cell counts (RBCs), red cell distribution widths (RDW) and platelet counts (PLTs) within the distinct age groups. Additionally, alterations to several published age-specific partitions had to be made, while new sex-specific partitions were introduced for WBCs and PLTs. Conclusions: Generic RIs from textbooks, manufacturer information and medical publications-even from nationwide or multicenter studies-commonly used in many laboratories might not reflect the specifics of local patient populations properly. RIs should be tailored to the serviced patient population whenever possible. Careful data mining appears to be suitable for this task. © 2021 Walter de Gruyter GmbH, Berlin/Boston.","data mining; hematology; indirect reference range; pediatric; reference interval (RI); reference range",,Article,Scopus,2-s2.0-85121315727
"Wu L., Hu Y., Zhang X., Zhang J., Liu M.","57191222276;55682757000;35723401000;57221111745;56337948100;","Development of a knowledge mining approach to uncover heterogeneous risk predictors of acute kidney injury across age groups",2022,"International Journal of Medical Informatics","158",,"104661","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121112387&doi=10.1016%2fj.ijmedinf.2021.104661&partnerID=40&md5=8d8410dee2c7e6b01978204791d13667","Objectives: Acute kidney injury (AKI) risk increases with age and the underlying clinical predictors may be heterogeneous across age strata. This study aims to uncover the AKI risk factor heterogeneity among general inpatients across age groups using electronic medical records (EMR). Methods: Patient data (n = 179,370 encounters) were collected from an academic hospital between 2007 and 2016, and were stratified into four age groups: 18–35, 36–55, 56–65, and > 65. Potential risk factors extracted for the cohort included demographics, vital signs, laboratory values, past medical diagnoses, medications and admission diagnoses. We developed a data driven knowledge mining approach consisting of a machine learning algorithm to identify AKI predictors across age strata and a statistical method to quantify the impact of those factors on AKI risk. Identified predictors were evaluated for their predictability of AKI in terms of area-under-the-receiver-operating-characteristic-curve (AUC) and validated against expert knowledge. Results: Among the final analysis cohort of 76,957 hospital admissions, AKI prediction across age groups 18–35 (16.73%), 36–55 (32.74%), 56–65 (23.52%), and > 65 years (27.01%) achieved AUC of 0.85 (95% CI, 0.80–0.88), 0.86 (95% CI, 0.83–0.89), 0.87 (95% CI, 0.86–0.90), and 0.87 (95% CI, 0.86–0.90), respectively. Compared to expert knowledge, absolute consistency rates of the top-150 identified risk factors for each group were 78.4%, 77.2%, 81.3%, and 79.5%, respectively. Impact of many predictors on AKI varied across age groups; for example, high body mass index (BMI) was found to be associated with higher AKI risk in elderly patients, but low BMI was found to be associated with higher AKI risk in younger patients. Conclusions: We verified the effectiveness of the knowledge mining method from the perspectives of accuracy, stability and credibility, and used this approach to clarify the heterogeneity of AKI risk factors between age groups. Future decision support systems need to consider such heterogeneity to enhance personalized patient care. © 2021 Elsevier B.V.","Acute kidney injury; Knowledge mining approach; Machine learning; Risk heterogeneity, electronic medical records","Decision support systems; Diagnosis; E-learning; Hospital data processing; Hospitals; Learning algorithms; Machine learning; Medical computing; Records management; Acute kidney injury; Age groups; Body mass; Expert knowledge; Injury risk; Knowledge mining; Knowledge mining approach; Mass index; Risk factors; Risk heterogeneity, electronic medical record; Data mining",Article,Scopus,2-s2.0-85121112387
"Lysdahl A.K., Christensen C.W., Pfaffhuber A.A., Vöge M., Andresen L., Skurdal G.H., Panzner M.","56786511100;56398995500;35079266600;7004675215;7003391149;57195310650;25650043500;","Integrated bedrock model combining airborne geophysics and sparse drillings based on an artificial neural network",2022,"Engineering Geology","297",,"106484","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120992778&doi=10.1016%2fj.enggeo.2021.106484&partnerID=40&md5=6fec3cf32e5100ee0fec5c63219c0a36","Cost overruns caused by unforeseen geological challenges are commonplace for large infrastructure projects. Thorough ground investigations can reduce this risk, but geotechnical drillings and laboratory test are expensive and time consuming. Airborne electromagnetics (AEM) is a low-cost geophysical method being increasingly used for geotechnical ground investigations. However, extracting engineering parameters from these complex data is challenging. We present a novel approach of extracting depth to bedrock from AEM data using artificial neural networks (ANN) and sparse drillings. Using synthetic models, we test its theoretical performance and analyse sources of error. We find that geological complexity is the main limitation on performance. We also test the algorithm on real field data from a complex geological setting. Results show that ANNs produce bedrock models that rival the accuracy of manual interpretations by experts and that are markedly more accurate than existing automated resistivity model interpretation methods. Using ANN based bedrock interpretation, one needs 2 to 3.5 times fewer geotechnical drillings (i.e., a reduction of 50–70%) in the early phases of a project compared to ground investigations using only borehole data. Further improvements may be possible with strategic planning of drilling campaigns and careful data pre-processing. © 2021 The Authors","Airborne geophysics; Artificial neural network; Depth to bedrock; Geological risk; Ground investigations; Linear infrastructure","Costs; Data handling; Data mining; Geology; Geophysics; Infill drilling; Neural networks; Airborne electromagnetic; Airborne geophysics; Cost-overruns; Depth to bedrock; Drilling tests; Geological risks; Geotechnical drilling; Ground investigation; Infrastructure project; Linear infrastructure; Complex networks; artificial neural network; data processing; electromagnetic field; laboratory method; strategic approach",Article,Scopus,2-s2.0-85120992778
"Hong X., Zhu C., An S., Zhao Y.","57370334500;57369782000;57369595400;57370148600;","Simulation analysis of transient output characteristics of inverter with asynchronous motor load based on second-order filtering link",2022,"Sustainable Energy Technologies and Assessments","49",,"101725","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120977337&doi=10.1016%2fj.seta.2021.101725&partnerID=40&md5=5c2886df4996c1d01a1e8f1ef74c573d","Based on the voltage and current oscillation characteristics of the second-order filtering link, an LC boundary condition analysis method for the steady-state and transient minimum output power of the inverter power supply for asynchronous motor drive based on the concept of dynamic capacitive and inductive coefficients is proposed. For three typical motor topologies below 100 kW class, a dynamic equivalent resistive model based on turndown rate - input frequency is established and its starting transient and speed steady-state resistive characteristics are analyzed. On this basis, the dynamic capacitance coefficient-current amplification curve and dynamic inductance coefficient-voltage amplification curve are introduced, and the optimal (C,L) combination solution is extracted by data screening and 3D fitting to control the inverter output current inrush while improving the voltage utilization efficiency. The optimal (C,L) combination solution is extracted by data filtering and 3D fitting to control the inverter output current inrush while improving the voltage utilization efficiency. The image2data tool is used to compare and analyze the characteristics of second-order filtering parameters selection under two common strategies of specific harmonic cancellation pulse-width modulation (SHEPWM) and triangular carrier pulse-width modulation (TCPWM) for motor drives, and introduce the device voltage distortion model to improve the system accuracy. It is found that the output power of SHEPWM is about 0.03% higher than that of TCPWM under the same loading condition, and the obtained boundary LC parameters can simultaneously meet the transient starting power requirement of the motor and optimize the output efficiency of the inverter at steady state speed. © 2021 Elsevier Ltd","Asynchronous motor; Inverter; LC filter; Output characteristics; Steady-state model","Capacitance; Curve fitting; Data mining; Digital storage; Efficiency; Electric drives; Electric inverters; Equivalence classes; Equivalent circuits; Extraction; Induction motors; Passive filters; Power quality; Topology; Transient analysis; Variable speed drives; Voltage control; Amplification curves; Asynchronoi motor; Inverte; LC filter; Motor drive; Output characteristics; Output current; Output power; Pulsewidth modulations (PWM); Steady-state models; Pulse width modulation; accuracy assessment; boundary condition; comparative study; computer simulation; efficiency measurement; engine; numerical model; power generation; three-dimensional modeling",Article,Scopus,2-s2.0-85120977337
"Assad A., Bouferguene A.","57210584718;8723750700;","Data Mining Algorithms for Water Main Condition Prediction - Comparative Analysis",2022,"Journal of Water Resources Planning and Management","148","2","04021101","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120976658&doi=10.1061%2f%28ASCE%29WR.1943-5452.0001512&partnerID=40&md5=c8fef9eb24935d8315f74df09f324e80","Accurate prediction of water mains condition is critical for effective rehabilitation planning. Advances in machine learning techniques can improve condition predictions. This paper compares the capabilities of various data mining techniques in predicting the condition of water mains. Predictive models investigated include generalized linear model, deep learning, decision tree, random forest, XGBoost, AdaBoost, and support vector machines. Models are first constructed leveraging a portion of the City of Waterloo, Canada, database. Genetic algorithm and cross-validation are then employed to optimize the hyperparameter tuning process. Several performance metrics and statistical tests are employed to compare the performance of the developed models utilizing a new set of data not previously used. The XGBoost model yielded the most promising results, with a mean relative error of 1.29%. Water main conditions are numerically represented on a scale from 0 to 10, with 10 indicating the highest condition. Extensive sensitivity analysis is conducted to obtain deeper insights into the most critical attributes for condition prediction. The developed model may help city managers develop optimal rehabilitation and renewal plans, considering the current and expected condition of their pipe inventory. © 2021 American Society of Civil Engineers.","Artificial intelligence; Condition prediction; Data mining; Machine learning; Rehabilitation planning; Water mains","Adaptive boosting; Decision trees; Deep learning; Forecasting; Genetic algorithms; Sensitivity analysis; Statistical tests; Support vector machines; Accurate prediction; Comparative analyzes; Condition; Condition prediction; Data mining algorithm; Data-mining techniques; Developed model; Machine learning techniques; Rehabilitation planning; Water mains; Data mining; algorithm; artificial intelligence; comparative study; data mining; machine learning; numerical model; prediction; sensitivity analysis; water planning",Article,Scopus,2-s2.0-85120976658
"Dehbozorgi M.R., Rastegar M., Sami A.","56495223200;24802318500;7004124604;","Data mining-based cause identification of momentary outages in power distribution systems",2022,"Sustainable Cities and Society","77",,"103587","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120951276&doi=10.1016%2fj.scs.2021.103587&partnerID=40&md5=7581fdbadbd6cf09a011dbfaa65f374f","Electric power distribution systems face outages that prevent them from serving customers. Short-term outages are known as momentary outages, and their causes are not usually recorded in the outage dataset. While, frequent occurrences of momentary outages may lead to a long-term permanent outage, which can significantly reduce system reliability. Unlike previous works which focused on permanent outage diagnosing and prediction, this paper proposes data-mining based approaches to identify the most probable momentary outages’ causes. To achieve this goal, the outage dataset, sub-transmission substation load, and weather historical data are processed and integrated. Then, association rules that describe the antecedents leading to different permanent outages’ and momentary outages’ causes are derived by using the Apriori algorithm. The frequent itemsets of momentary outages are also obtained. Based on momentary outage rules and frequent itemsets, two procedures are proposed to find similarities between permanent and momentary outages to identify the most probable causes of momentary outages. Finding the cause of momentary outages, the operator can reduce the probability of permanent outage occurrences. Results of applying the proposed approaches on real data of a test distribution system show that expected energy not supplied of the distribution system can be decreased by more than 18%. © 2021 Elsevier Ltd","Association rule mining; Distribution system reliability; Momentary outages; Outage dataset","Data mining; Outages; Reliability; Thermoelectric equipment; Apriori algorithms; Distribution system reliability; Distribution systems; Electric power distribution systems; Frequent itemset; Historical data; Momentary outage; Outage dataset; Power-distribution system; System reliability; Association rules",Article,Scopus,2-s2.0-85120951276
"Shan X., Deng Q., Tang Z., Wu Z., Wang W.","51664063200;53983925100;57366597000;57366454200;56948437900;","An integrated data mining-based approach to identify key building and urban features of different energy usage levels",2022,"Sustainable Cities and Society","77",,"103576","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120811435&doi=10.1016%2fj.scs.2021.103576&partnerID=40&md5=006246f920120d4f011725240b35159b","Building and urban geometries, prerequisite at the design phase, are the key determinants of building energy consumptions. However, the key building and urban features of different energy consumption levels is rarely studied. This study proposed a data mining-based method to explore the significant building features of different building groups. In this approach, clustering classifies buildings into three clusters according to energy consumption, and the clustering results contribute a base for principal component analysis (PCA) and random forest (RF) to discover key building features affecting different energy consumption levels. To demonstrate the availability of the framework, it is applied to on a city dataset in China. The results indicate that the key geometric features for low and medium residential energy consumption clusters are Orientation, HW-South, HW-West, HW-North and HW-East, while the key determinants for high residential energy consumption cluster are Orientation and HW-South. The key features for public buildings are similar to those for residential buildings with exception of HW-East. The findings provide insights into the key influence geometric features of different building energy usage levels, which can guide the passive design of urban buildings to efficiently reduce energy consumptions at design stage. © 2021 Elsevier Ltd","Building energy performance; Geometrical design features; K-means clustering; Principal component analysis; Random forest algorithm","Architectural design; Availability; Data mining; Decision trees; Energy efficiency; Energy utilization; Geometry; Housing; K-means clustering; Building energy performance; Design features; Energy usage; Geometrical design feature; Geometrical designs; K-means++ clustering; Principal-component analysis; Random forest algorithm; Urban features; Usage level; Principal component analysis",Article,Scopus,2-s2.0-85120811435
"Tamakloe R., Das S., Nimako Aidoo E., Park D.","57206723737;55531140000;55596808500;7403245364;","Factors affecting motorcycle crash casualty severity at signalized and non-signalized intersections in Ghana: Insights from a data mining and binary logit regression approach",2022,"Accident Analysis and Prevention","165",,"106517","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120746520&doi=10.1016%2fj.aap.2021.106517&partnerID=40&md5=733c313a9877f2905c1b27f124e84988","Despite the countless benefits derived from motorcycle usage, it has become a significant public health concern, particularly in developing countries, due to the plateauing number of fatal/serious injuries associated with them. Although it has been well documented that the frequency and fatality rates of intersection-related motorcycle crashes are high, little research efforts have been made to explore the contributory factors influencing motorcycle-involved crashes at these locations. Interestingly, no study has investigated the latent patterns and chains of factors that simultaneously contribute to the injury severity sustained by motorcycle crash casualties at intersections under different traffic control conditions in developing countries. Since motorcycles are mostly used as taxis in developing countries, it is imperative to consider the injury severity sustained by all crash casualties in the motorcycle safety analysis. This study bridges the research gap by employing a plausible data mining tool to explore hidden rules associated with motorcycle crash casualty injury severity outcomes at both signalized and non-signalized intersections in Ghana's most densely populated region, Accra, using three-year crash data spanning 2016–2018. Besides, a binary logit regression model was also employed to explore the impact of crash factors on casualty severity outcomes using the same dataset. The results from both analysis techniques were consistent; however, the data mining technique provided chains of factors which provided additional insights into the groups of factors that collectively influence the casualty injury severity outcomes. From the rule discovery results, while full license status, daytime/daylight, and shoulder presence increased the risk of fatal injuries at signalized intersections, factors such as inattentiveness, good road surface, nighttime, shoulder absence, and young rider were highly likely to increase casualty fatalities at non-signalized intersections. By controlling all or some of these risk factors, the level of injury severity on the roadways could be reduced. Based on the findings, we provide enforcement, education, and engineering-based recommendations to help improve motorcycle safety. © 2021 Elsevier Ltd","Data mining; Developing countries; Injury severity; Intersection; Motorcycle; Safety","data mining; Ghana; human; injury; motorcycle; statistical model; traffic accident; Accidents, Traffic; Data Mining; Ghana; Humans; Logistic Models; Motorcycles; Wounds and Injuries",Article,Scopus,2-s2.0-85120746520
"Siddique C., Afifah F., Guo Z., Zhou Y.","57205602655;57219749564;57194654610;49962815300;","Data mining of plug-in electric vehicles charging behavior using supply-side data",2022,"Energy Policy","161",,"112710","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120683510&doi=10.1016%2fj.enpol.2021.112710&partnerID=40&md5=a9fa77c4575e861ca27b2a0dadf46964","This paper aims to better understand the charging patterns of plug-in electric vehicles (PEVs) and identify factors that may significantly impact PEVs’ charging behavior. We collected 189,864 supply-side charging session data over 13 months from 821 charging stations in Illinois from ChargePoint. Through descriptive and regression analyses, we characterize the distributions of key charging behavior indicators, including charging location, dwell time, and battery start state of charge (SOC), and quantify the impacts of closely related factors on these charging behaviors. We find that: (1) PEVs are more likely to charge in the morning at multifamily commercial locations with a lower start SOC compared with single family residential locations; (2) Weekday and morning sessions are more likely to utilize workplace charging and have shorter dwell time compared with weekend and afternoon sessions; (3) Single family residential area and locations with Levels 1/2 chargers have a higher start SOC and longer dwell time compared with other locations and DC fast chargers (DCFCs). These findings provide policy insights to identify potential time and locations to incentivize PEVs for grid services, as well as identify critical location categories for further charging infrastructure investment to better reduce range anxiety and promote PEV adoption. © 2021 Elsevier Ltd","Charging behaviors; Electric vehicle; Regression analysis; Supply-side data","Charging (batteries); Data mining; Housing; Investments; Location; Plug-in electric vehicles; Charging behavior; Charging patterns; Charging station; Dwell time; Illinois; Plug-in electric vehicle charging; Residential locations; States of charges; Supply sides; Supply-side data; Regression analysis; data mining; electric vehicle; regression analysis; residential location; workplace; Illinois; United States",Article,Scopus,2-s2.0-85120683510
"Yu M.G., Pavlak G.S.","57145526300;55924626100;","Extracting interpretable building control rules from multi-objective model predictive control data sets",2022,"Energy","240",,"122691","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120654105&doi=10.1016%2fj.energy.2021.122691&partnerID=40&md5=712f447a4e62043b32a3c9967d4380e1","Developing intelligent building control strategies is increasingly becoming a multi-objective problem as owners, occupants, and operators seek to balance performance across energy, operating expense, environmental concerns, indoor environmental quality, and electric grid incentives. Implementing multi-objective optimal controls in buildings is challenging and often not tractable due to the complexity of the problem and the computational burden that frequently accompanies such optimization problems. In this work, we extract near-optimal rule sets from a database of non-dominated solutions, created by applying multi-objective model predictive control to detailed EnergyPlus models. We first apply multi-criteria decision analysis to rank the non-dominated solutions and select a subset of consistent and plausible operating strategies that can satisfy operator or occupant preferences. Next, unsupervised clustering is applied to highlight recurring control patterns. In the final step, we build a supervised classification model to identify the right optimal temperature control patterns for a particular day. The performance of the simplified rule sets is then quantified through simulation. Despite the dramatically simpler form, the best rule sets were able to achieve 95–97% of the energy savings and 89–92% of the cost objective savings of the fully detailed model predictive controller, while achieving similar thermal comfort and peak electrical demand. © 2021 Elsevier Ltd","Classification; Clustering; Model predictive control of buildings; Multi-objective optimization; Rule extraction","Data mining; Energy conservation; Intelligent buildings; Model predictive control; Multiobjective optimization; Building controls; Clusterings; Control patterns; Model predictive control of building; Model-predictive control; Multi-objectives optimization; Multiobjective modeling; Nondominated solutions; Rule set; Rules extraction; Classification (of information); database; decision analysis; optimization; savings; supervised classification",Article,Scopus,2-s2.0-85120654105
"Su X., Shan Y., Li C., Mi Y., Fu Y., Dong Z.","55543209500;57361063900;57221706305;57226776518;35109059900;57226245510;","Spatial-temporal attention and GRU based interpretable condition monitoring of offshore wind turbine gearboxes",2022,"IET Renewable Power Generation","16","2",,"402","415",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120481278&doi=10.1049%2frpg2.12336&partnerID=40&md5=bb9078e622913b69a7e97ceeb9c45101","Effective monitoring and early warning of gearbox operating status are of great significance to the operation and maintenance (O&M) of offshore wind turbines (WTs). This study proposes a normal behaviour modelling (NBM) method based on the spatial-temporal attention module and the gated recurrent unit (GRU), for the condition monitoring of offshore WT gearboxes. The proposed method has a superior performance by extracting the spatial and temporal features from the supervisory control and data acquisition (SCADA) system, and also has the unique advantage of model interpretability. Specifically, in the NBM training stage, the spatial features of offshore wind farm SCADA data are extracted by the spatial attention module firstly. Then, the temporal features of the spatial feature sequences above are extracted and fused by the GRU network. Afterwards, the temporal attention module is applied to strengthen the expression of key time points. In the NBM testing stage, the output residual between the predicted and the measured values is calculated and monitored by the exponential weighted moving average (EWMA) control chart. Finally, the effectiveness and superiority of the proposed NBM method are verified by detailed simulations on the Donghai Bridge offshore wind farm. © 2021 The Authors. IET Renewable Power Generation published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology",,"Data acquisition; Data mining; Electric utilities; Offshore oil well production; Offshore wind farms; Offshore wind turbines; Behaviour models; Early warning; Model method; Normal behavior; Renewable power generation; Spatial features; Spatial temporals; Temporal features; Unit-based; Wind turbine gearboxes; Condition monitoring",Article,Scopus,2-s2.0-85120481278
"Reis-Filho J.A., Loiola M.","37013929100;55916539100;","Dimensions of fishing with explosives in the Brazilian central coast from data mining and fishers’ perception: Concentrated and problematic, yet avoidable",2022,"Ocean and Coastal Management","216",,"105985","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120456949&doi=10.1016%2fj.ocecoaman.2021.105985&partnerID=40&md5=17609b8e54a349bdbdc33d92ce06bfd6","Small-scale and subsistence fisheries play an important role in the livelihoods and food security of several local communities worldwide. However, the status and dynamics of many fishing practices are unknown, particularly regarding destructive fishing practices such as blast fishing. The main goals of this research were to provide an analysis of trends of the blast fishing in the Brazilian central coast, specifically in the Todos os Santos Bay (TSB), as well as to offer alternative sources of information that could be used to eradicate this harmful and illegal activity. Several sources of data were explored, including time-series analysis of complaints and surveillance reports from environmental and police agencies, web-based available data, and fisher interviews, which were used together to understand the status of blast fishing. Our data indicated sharp explosions occurring near urban centers and productive fishing villages chiefly targeting valuable pelagic schooling fish, such as mullets and pilchards. A significant number of species have been identified as by-catch resulting from blasts, indicating harmful effects on aquatic biota, which remains underemphasized since the onset of blasting (i.e., since the 1930s). The results of the spatial model (kernel density estimates) demonstrated that the sum of all current destructive blasting fishing occupies an extensive area in the TSB (approximately 32% of territory) and can be sufficient to continue causing loss of diversity and fishery resources coupled with other more recent damages (i.e., habitat loss, coastal development, overfishing, industrial and urban pollution). This study can be used to improve blast fishing maps frequently, and its diagnostic nature is likely to assist policy and surveillance makers in designing better interventions for the eradication this destructive practice. © 2021 Elsevier Ltd","Blast fishing; Brazilian fisheries; Environmental impacts; Illegal fisheries; Management actions","Blasting; Crime; Data mining; Environmental impact; Explosives; Food supply; Time series analysis; Alternative source; Blast fishing; Brazilian fishery; Food security; Illegal fishery; Local community; Management action; Santos bays; Small-scale fisheries; Subsistence fisheries; Fisheries; bycatch; data mining; fishery policy; fishery production; fishing; food security; livelihood; pelagic fish; perception; subsidence; Bahia; Brazil; Todos os Santos Bay; Clupeidae; Martes; Mugilidae",Article,Scopus,2-s2.0-85120456949
"Yin X., Liu Q., Huang X., Pan Y.","57219119358;55553889600;55255485200;56498216400;","Perception model of surrounding rock geological conditions based on TBM operational big data and combined unsupervised-supervised learning",2022,"Tunnelling and Underground Space Technology","120",,"104285","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120455089&doi=10.1016%2fj.tust.2021.104285&partnerID=40&md5=7837e2934e9a484fa4d11930309f2940","The perception of surrounding rock geological conditions ahead the tunnel face is essential for TBM safe and efficient tunnelling. This paper developed a perception approach of surrounding rock class based on TBM operational big data and combined unsupervised-supervised learning. In data preprocessing, four data mining techniques (i.e., Z-score, K-NN, Kalman filtering, and wavelet packet decomposition) were used to detect outliers, substitute outliers, suppress noise, and extract features, respectively. Then, GMM was used to revise the original surrounding rock class through clustering TBM load parameters and performance parameters in view of the shortcomings of the HC method in the TBM-excavated tunnel. After that, five various ensemble learning classification models were constructed to identify the surrounding rock class, in which model hyper-parameters were automatically tuned by Bayes optimization. In order to evaluate model performance, balanced accuracy, Kappa, F1-score, and training time were taken into account, and a novel multi-metric comprehensive ranking system was designed. Engineering application results indicated that LightGBM achieved the most superior performance with the highest comprehensive score of 6.9066, followed by GBDT (5.9228), XGBoost (5.4964), RF (3.7581), and AdaBoost (0.9946). Through the weighted purity reduction algorithm, the contributions of input features on the five models were quantitatively analyzed. Finally, the impact of class imbalance on model performance was discussed using the ADASYN algorithm, showing that eliminating class imbalance can further improve the model's perception ability. © 2021 Elsevier Ltd","Perception model; Supervised learning; Surrounding rock class; TBM; Unsupervised learning","Adaptive boosting; Big data; Data mining; Geology; Nearest neighbor search; Rocks; Statistics; Wavelet decomposition; Class imbalance; Class-based; Data preprocessing; Geological conditions; Modeling performance; Perception model; Surrounding rock; Surrounding rock class; TBM; Tunnel face; Supervised learning; algorithm; excavation; perception; rock mechanics; supervised learning; TBM",Article,Scopus,2-s2.0-85120455089
"Zhou Y., Medina J.C., Taylor J., Cathy Liu X.","57221414518;35339556500;55851948016;57241517900;","Empirical Verification of Car-Following Parameters Using Naturalistic Driving Data on Freeway Segments",2022,"Journal of Transportation Engineering Part A: Systems","148","2",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120302626&doi=10.1061%2fJTEPBS.0000629&partnerID=40&md5=f6d992f9d9ec7ca731032314f31de64e","Microscopic traffic simulation is a well-established tool for the analysis of transportation systems, with a wide variety of applications in operations, safety, and planning. An essential component of traffic simulation is the car-following model, which defines how vehicles interact with each other and controls acceleration/deceleration to maintain a desired set of speeds and distances when constrained by a leading vehicle. Car-following models are governed by a set of parameters that define the car's following behavior and can accommodate a range of values to reproduce desired conditions. Typically, calibration of a simulation scenario is conducted to approach a set of target macroscopic traffic condition indicators, such as speed, travel time, or queue, yet it rarely considers the accuracy of individual vehicle behavior, in part due to lack of detailed field data. In this paper, Naturalistic driving study (NDS) data sets were used to extract driving behavior on freeway segments at the microscopic level and directly characterize parameters in car-following models. The data extraction process is described, and the parameter values are illustrated for the Wiedemann 99 model implemented in commerically available software. Results highlight similarities and differences of these parameter values observed in the field and those by default in the software, and simulation outcomes upon NDS guided adjustment were analyzed. The process introduced can be expanded to similar data sets and other complex traffic conditions and therefore produce more accurate simulation results not only for metrics at a macroscopic level, but also for individual vehicle trajectories that closely mimic real-world driving. © 2021 American Society of Civil Engineers.","Calibration; Car-following model; Microscopic simulation; Naturalistic driving study (NDS); Volume estimation","Computer software; Data mining; Parameter estimation; Traffic control; Travel time; Vehicles; Car following; Car-following modeling; Data set; Freeway segments; Microscopic simulation; Microscopic traffic simulation; Naturalistic driving studies; Naturalistic driving study; Traffic conditions; Volume estimations; Calibration; calibration; car use; data set; model; motorway; road traffic",Article,Scopus,2-s2.0-85120302626
"Papageorgiou L., Andreou A., Christoforides E., Bethanis K., Vlachakis D., Thireou T., Eliopoulos E.","56421847900;50561031300;55143922000;6505866663;23096470700;6508158215;6701692141;","Hippo(crates): An integrated atlas for natural product exploration through a state-of-the art pipeline in chemoinformatics",2022,"World Academy of Sciences Journal","4","1","1","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120086854&doi=10.3892%2fWASJ.2021.136&partnerID=40&md5=4ee2ca290be9c4599d4531b32f011233","Modern drug discovery and pharmaceutics benefit from nature. Natural products (NPs) are used as a source of therapeutic agents with beneficial uses. Currently, there is considerable interest in the exploration of NPs for drug discovery and continuous investigations on the therapeutic claims and mechanisms of herbal medicines. To date, approximately one million NPs have been isolated and subjected to experimental assays to evaluate quantitative biological activities. This renders the use of an integrated database to assemble and correlate this valuable information from the literature, experimental studies and databases necessary. Although databases contain a large volume of information, it is frequently difficult and complex, even in well.organized databases, to extract the required information. Novel databases must be accompanied by efficient algorithms and techniques in order to extract beneficial knowledge by a simple query. The Hippo(crates) database aims to fill this gap in the field of chemoinformatics and natural products by providing retrieval not only linked to the Hippo(crates) database, but also to other worldwide chemical and biological databases. Part of the OPENSCREEN.GR project, the Hippo(crates) Database Graphical User Interface (HDGUI) web server was developed to provide a user.friendly access interface, integrating annotated information of NP origin (sources and species), biological activities, physicochemical properties, linear and 3D chemical structure, as well as relative terms that correlate chemical compounds and their use. In its current version (V1.0), the Hippo database provides 45,300 NPs, NP derivatives and synthetic compounds, which are separated into 32 major categories, including biological or medicinal properties. In the database, 22,830 NP source organisms are correlated, with >100,000 terms, including biological pathways, target organisms, target diseases, target types, target proteins and pathogens, and 6,070 three.dimensional structures of NP target proteins. For each entry, a cluster with similar compounds and a ligand.based or structure.based pharmacophore model is provided. The portal is designed as an easy.to.use web tool where the user can easily search, extract and correlate information and data for natural product chemical compounds through various fields, such as categories, keywords, targets, species, or two.dimensional or three.dimensional similarity structure in the Hippo(crates) atlas of the NP database. © 2021 Spandidos Publications. All Rights Reserved.","Chemoinformatics; Data mining; Database; Natural products; Pharmacophores; Semantic",,Article,Scopus,2-s2.0-85120086854
"Jiang X., Gao J., Sharif M.Z., Zhang X., Liu F.","57214916664;57190212230;57221620206;7410271223;7406213523;","Synergistic and dichotomous effects of nectar phenolics on honey bee colonies",2022,"Pakistan Journal of Zoology","54","1",,"107","113",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119968191&doi=10.17582%2fjournal.pjz%2f20200506050516&partnerID=40&md5=66744772daa7e634e20efb1f663eb1c2","Nectar phenolics have a widespread effect on honey bees and their colonies. Because of their complex, non-linear interactions, it is difficult to assess honey bee health risks from exposure to real-world floral nectar with complex phenolic mixture. In the study, we investigate the bee losses of Apis mellifera in the flowering period of the Mexican sunflower Tithonia diversifolia in southwestern China, and use data mining approach to model the relationships between nectar phenolics and bee losses. The results show that bee losses are closely related to the phenolics of isochlorogenic acid, p-coumaric acid, chlorogenic acid and galangin, identified from the sunflower nectar. The nectar phenolics do not cause bee-poisoning to death, but can trigger bee colonies to explore food sources at risk. Also, each of these phenolics acts in a dichotomous mode, with above a certain value destructing colonies and below such value affecting little. This study provides new insight into the mechanism underlying the catastrophic events of bee losses or honey harvests, which have been reported worldwide. © 2022 Zoological Society of Pakistan.","Apis mellifera; Catastrophic events; Data mining; Dichotomous effects; Nectar phenolics; Tithonia diversifolia","colony; honeybee; nectar; phenolic compound; synergism; Apis mellifera; Helianthus; Tithonia diversifolia; Tithonia rotundifolia",Article,Scopus,2-s2.0-85119968191
"Kumar V., Singh R.S., Dua Y.","57220974689;56125721800;57219409728;","Morphologically dilated convolutional neural network for hyperspectral image classification",2022,"Signal Processing: Image Communication","101",,"116549","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119934150&doi=10.1016%2fj.image.2021.116549&partnerID=40&md5=9b0bb6dcd998c0843caf77694d361485","The use of hyperspectral images is expanding rapidly with the advancement of remote sensing technologies. The precise classification of features for mapping land cover through hyperspectral images is a major research topic and a major focus. In the classification of hyperspectral images, several methods provided good classification results. Among all, convolutional neural network (CNN) is a widely used deep neural network due to its robust feature extraction capabilities. It can enhance the hyperspectral image classification accuracy. Mathematical morphology (MM) is a robust and straightforward spatial feature descriptor, which can reduce the computational workload. We proposed a novel model morphologically dilated Convolutional Neural Network (MDCNN), which can extract more robust spectral–spatial features. MDCNN adopt a process to concatenate the morphological feature maps with original hyperspectral data. CNN structure uses both traditional and dilated convolution. Replacing the dilated convolution in traditional convolution layers expands the receptive field without boosting parameters and thus improves network performance without increasing network complexity. The dilation layer does not reduce the number of parameters but reduces the size of the output feature map, which leads to the overall reduction in the number of parameters. 3D convolution extracts spectral–spatial features and maintains the correlation of spectral data. 2D CNN extracts spatial features and reduces the model's complexity, which can occur if only 3D convolution is used. Experimental findings show that the proposed approach can provide better classification results than traditional deep learning models and other state-of-the-art models on the Indian Pines, University of Pavia, and Salinas Scene data. © 2021 Elsevier B.V.","Binarization; Convolutional neural network (CNN); Dilated convolution; Hyperspectral image (HSI); Mathematical morphology","Classification (of information); Complex networks; Convolutional neural networks; Data mining; Deep neural networks; Hyperspectral imaging; Image classification; Image enhancement; Mathematical morphology; Morphology; Remote sensing; Spectroscopy; Binarizations; Classification results; Convolutional neural network; Dilated convolution; Feature map; Hyperspectral image; Hyperspectral image classification; Remote sensing technology; Spatial features; Convolution",Article,Scopus,2-s2.0-85119934150
"Liu J., Stewart H., Wiens C., Mcnitt-Gray J., Liu B.","57189988093;57197539290;57190780736;6602939414;56912491300;","Development of an integrated biomechanics informatics system with knowledge discovery and decision support tools for research of injury prevention and performance enhancement",2022,"Computers in Biology and Medicine","141",,"105062","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119930363&doi=10.1016%2fj.compbiomed.2021.105062&partnerID=40&md5=76339ede3621c5ce3cfb312ae47a3789","The field of biomechanics involves integrating a variety of data types such as waveform, video, discrete, and performance. These different sources of data must be efficiently and accurately associated to provide meaningful feedback to athletes, coaches, and healthcare professionals to prevent injury and improve rehabilitation/performance. There are many challenges in biomechanics research such as data storage, standardization, review, sharing, and accessibility. Data is stored in different formats, structures, and locations such as physical hard drives or Dropbox/Google Drive, leading to issues during sharing and collaboration. Data is reviewed and analyzed through different software applications that need to be downloaded and installed locally before they are available for use. An integrated biomechanics informatics system (IBIS) built based on the core principles in medical imaging informatics provides a solution to many of these challenges. The system provides a secure web-based platform that will be accessible remotely for authenticated users to upload, share, and download data. The web-based application includes built-in data viewers that are streamlined for reviewing multimedia data and decision support/knowledge discovery tools. These tools include automatic foot contact detection for pre-processing, built-in statistical analysis applications for longitudinal and cross-study analysis, and a multi-institutional collaboration module. The IBIS system creates a centralized hub to support multi-institutional collaborative biomechanics research and analysis that is remotely accessible to all users including athletes, coaches, researchers, and clinicians generating a novel streamlined research workflow, data analysis, and knowledge discovery process. © 2021","Biomechanics; Biomedical imaging informatics; Decision support; Integrated informatics system; Knowledge discovery; Rehabilitation; Sports medicine; Statistical analysis tool; Web-based application","Application programs; Authentication; Biomechanics; Data mining; Decision support systems; Digital storage; E-learning; Medical computing; Medical imaging; Sports; Sports medicine; Statistical methods; Web services; Biomedical imaging; Biomedical imaging informatic; Decision supports; Imaging informatics; Informatics systems; Injury prevention; Integrated informatic system; Performance; Statistical analysis tools; Web-based applications; Websites",Article,Scopus,2-s2.0-85119930363
"Zhou H., Tian X., Yu J., Zhao Y., Lin B., Chang C.","56415320700;57215621933;57352795300;57260566800;7403508277;57260344500;","Identifying buildings with rising electricity-consumption and those with high energy-saving potential for government's management by data mining approaches",2022,"Energy for Sustainable Development","66",,,"54","68",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119921053&doi=10.1016%2fj.esd.2021.11.001&partnerID=40&md5=bd74721b5071297b62d5d318f856a5f5","The government in Beijing has established a platform that covered the monthly electricity-use data of 11,370 public buildings, carried out a series of measures to control the quantity of buildings' electricity-consumption and its growth since 2013. Presently, it is trying to find out which buildings that will more probably have a rising trend or higher energy-saving potential than the others, so as to promote the energy-saving momentum and make its management more refine and targeted. Common methods used by the government for these tasks are traditional statistical charts and plots, the analysis results of which always highly rely on the experts' subjective judgment. Moreover, identifying the right objects from such a large database by manpower is also a big challenge. Therefore, this study proposed a series of data-mining methods, including k-means clustering, C4.5 decision tree, 2-dimension scatter diagram and outlier detection, to investigate and model the change patterns as well as compare the levels of the buildings' electricity-consumption. Results indicated that the information wanted by the government was successfully explored from the massive data. Finally, the paper gave some policy proposals on energy-use supervision and data management to help achieve the goal of further energy conservation in public buildings. © 2021 International Energy Initiative","2-D (dimension) coordinate; C4.5 decision tree; k-Means clustering; Policy; Public building","Data mining; Decision trees; Electric power utilization; Energy conservation; Historic preservation; Information management; K-means clustering; 2-D (dimension) coordinate; C4.5 decision trees; D dimensions; Electricity use; Electricity-consumption; Energy savings potential; Energy-savings; Government management; K-means++ clustering; Public buildings; Buildings; data mining; decision analysis; detection method; electricity supply; energy conservation; energy policy; energy use; governance approach; public space; trend analysis; Beijing [China]; China",Article,Scopus,2-s2.0-85119921053
"Dehler-Holland J., Okoh M., Keles D.","57226594125;57352710300;24512007000;","Assessing technology legitimacy with topic models and sentiment analysis – The case of wind power in Germany",2022,"Technological Forecasting and Social Change","175",,"121354","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119913599&doi=10.1016%2fj.techfore.2021.121354&partnerID=40&md5=cca795232599735218e45549af61a825","Legitimacy is a crucial factor determining the success of technologies in the early stages of development and for maintaining resource flows as well as public and political support across the technology life cycle. In sustainability transitions that unfold over long periods of time, the maintenance of legitimacy of technologies identified as vital for sustainability becomes a key challenge. In the energy sector, wind power contributes to the transition to an energy system with low greenhouse gas emissions. In Germany, wind power recently faced a series of lawsuits and decreasing investment activity. Therefore, we assess the legitimacy of wind power in Germany by analyzing newspaper articles from four national newspapers from 2009 to 2018. A large amount of articles motivates the use of topic models, sentiment analysis, and statistics to shed light on the changing alignment of wind power with its context. The results show that various issues temporarily gain prominence on the agenda. Lately, the legitimacy of wind power in Germany has been increasingly challenged by adverse effects on humans, animals, and landscapes. Policymakers and project developers may address aspects of pragmatic legitimacy, such as civic participation and the local distribution of profits. © 2021","Energy transitions; Natural language processing; Structural topic model; Technology legitimacy; text mining; Wind power Germany","Data mining; Energy policy; Gas emissions; Greenhouse gases; Investments; Life cycle; Newsprint; Sentiment analysis; Sustainable development; Energy transitions; Modeling analyzes; Public support; Resource flows; Sentiment analysis; Structural topic model; Technology legitimacy; Text-mining; Topic Modeling; Wind power germany; Wind power",Article,Scopus,2-s2.0-85119913599
"Amer F., Hockenmaier J., Golparvar-Fard M.","57202510239;15725022300;34868104300;","Learning and critiquing pairwise activity relationships for schedule quality control via deep learning-based natural language processing",2022,"Automation in Construction","134",,"104036","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119901718&doi=10.1016%2fj.autcon.2021.104036&partnerID=40&md5=bfefeabf3b0d414af1add0dcedb33e3c","In construction, schedule mistakes causing delays beyond substantial completion dates cost contractors expensive liquidated damages. Hence, several industry guidelines, such as the DCMA's 14 point assessment, define schedule quality and offer systematic methods for ensuring it. These guidelines list “logic” as an essential control metric, and they require planners to ensure their schedules are free of missing or wrong logical dependencies. Checking the logic requires extensive construction domain knowledge, and planners perform it entirely manually as there are no available software solutions that support it. This paper offers a novel machine learning-based solution that learns construction scheduling domain knowledge from existing records completely automatically and applies it to validate the logic in input schedules achieving an F1 score of 88.3%. Furthermore, we tailor our method to use the learned knowledge to schedule a list of unordered activities. The details of the method, experimental results, benefits, and limitations are discussed. © 2021","Construction planning; Data mining; Machine learning; Natural language processing; Neural networks; Quality control","Computer circuits; Deep learning; Domain Knowledge; Learning algorithms; Natural language processing systems; Planning; Quality control; Construction planning; Construction schedules; Domain knowledge; Industry guidelines; Learn+; Liquidated damages; Logical dependencies; Neural-networks; Software solution; Systematic method; Data mining",Article,Scopus,2-s2.0-85119901718
"Datta S., Roberts K.","57211502829;36551508300;","Fine-grained spatial information extraction in radiology as two-turn question answering",2022,"International Journal of Medical Informatics","158",,"104628","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119901597&doi=10.1016%2fj.ijmedinf.2021.104628&partnerID=40&md5=5726a56567d92c4a3d9b20eda22dbfe2","Objectives: Radiology reports contain important clinical information that can be used to automatically construct fine-grained labels for applications requiring deep phenotyping. We propose a two-turn question answering (QA) method based on a transformer language model, BERT, for extracting detailed spatial information from radiology reports. We aim to demonstrate the advantage that a multi-turn QA framework provides over sequence-based methods for extracting fine-grained information. Methods: Our proposed method identifies spatial and descriptor information by answering queries given a radiology report text. We frame the extraction problem such that all the main radiology entities (e.g., finding, device, anatomy) and the spatial trigger terms (denoting the presence of a spatial relation between finding/device and anatomical location) are identified in the first turn. In the subsequent turn, various other contextual information that acts as important spatial roles with respect to a spatial trigger term are extracted along with identifying the spatial and other descriptor terms qualifying a radiological entity. The queries are constructed using separate templates for the two turns and we employ two query variations in the second turn. Results: When compared to the best-reported work on this task using a traditional sequence tagging method, the two-turn QA model exceeds its performance on every component. This includes promising improvements of 12, 13, and 12 points in the average F1 scores for identifying the spatial triggers, Figure, and Ground frame elements, respectively. Discussion: Our experiments suggest that incorporating domain knowledge in the query (a general description about a frame element) helps in obtaining better results for some of the spatial and descriptive frame elements, especially in the case of the clinical pre-trained BERT model. We further highlight that the two-turn QA approach fits well for extracting information for complex schema where the objective is to identify all the frame elements linked to each spatial trigger and finding/device/anatomy entity, thereby enabling the extraction of more comprehensive information in the radiology domain. Conclusion: Extracting fine-grained spatial information from text in the form of answering natural language queries holds potential in achieving better results when compared to more standard sequence labeling-based approaches. © 2021 Elsevier B.V.","Deep learning; Information extraction; Natural language processing; Question answering; Radiology report; Spatial information","Data mining; Deep learning; Domain Knowledge; Information retrieval; Query processing; Radiation; Radiology; Clinical information; Deep learning; Descriptors; Fine grained; Frame elements; Information extraction; Question Answering; Radiology reports; Spatial information extraction; Spatial informations; Natural language processing systems; anatomical location; article; deep learning; extraction; human; human experiment; natural language processing; radiology",Article,Scopus,2-s2.0-85119901597
"Xing Y., He W., Cao G., Li Y.","57219335049;36985723200;23388553500;12785294900;","Using data mining to track the information spreading on social media about the COVID-19 outbreak",2022,"Electronic Library","40","1-2",,"63","82",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119694997&doi=10.1108%2fEL-04-2021-0086&partnerID=40&md5=421c4d3989f3f53be3ddf220c63162f9","Purpose: COVID-19, a causative agent of the potentially fatal disease, has raised great global public health concern. Information spreading on the COVID-19 outbreak can strongly influence people behaviour in social media. This paper aims to question of information spreading on COVID-19 outbreak are addressed with a massive data analysis on Twitter from a multidimensional perspective. Design/methodology/approach: The evolutionary trend of user interaction and the network structure is analysed by social network analysis. A differential assessment on the topics evolving is provided by the method of text clustering. Visualization is further used to show different characteristics of user interaction networks and public opinion in different periods. Findings: Information spreading in social media emerges from different characteristics during various periods. User interaction demonstrates multidimensional cross relations. The results interpret how people express their thoughts and detect topics people are most discussing in social media. Research limitations/implications: This study is mainly limited by the size of the data sets and the unicity of the social media. It is challenging to expand the data sets and choose multiple social media to cross-validate the findings of this study. Originality/value: This paper aims to find the evolutionary trend of information spreading on the COVID-19 outbreak in social media, including user interaction and topical issues. The findings are of great importance to help government and related regulatory units to manage the dissemination of information on emergencies, in terms of early detection and prevention. © 2021, Emerald Publishing Limited.","COVID-19 outbreak; Data mining; Information spreading; Social media; Twitter",,Article,Scopus,2-s2.0-85119694997
"Teixeira C., Fragoso L., Mattoso M., Carvalho D., Bezerra E., Soares J., Amorim G., Ogasawara E.","57222420359;57209889866;6602555822;56548303600;6506097045;56121141300;55953969800;57210600647;","A horizontal partitioning-based method for frequent pattern mining in transport timetable",2022,"Expert Systems","39","2","e12881","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119340553&doi=10.1111%2fexsy.12881&partnerID=40&md5=467dd46c11402dd2f634f0730992f1f7","Analysing transport timetables is an important task, as it brings the opportunity to discover which routes commonly lead to delays. Frequent pattern mining is a technique used to support such type of discovery. However, functional dependencies are intrinsic properties present in timetables, particularly related to attributes derived from the origin–destination matrix. Such functional dependencies compromise the search for patterns in timetables in both the number of association rules (ARs) generated and the computational cost. Several of these ARs refer to the same information. Redundancy removal techniques can reduce the number of ARs. However, these techniques are designed to be used after mining finishes, which increases the computational cost of finding useful ARs. This work presents timetable pattern mining (T-mine), a novel method for frequent pattern mining that improves knowledge discovery in timetables. We evaluated T-mine using Brazilian Flight Data and compared T-mine with the direct application of frequent pattern mining approaches with and without functional dependencies. Our experiments indicate that T-mine is about one order magnitude faster than other methods with functional dependencies. © 2021 John Wiley & Sons Ltd.",,"Association rules; Data mining; Scheduling; Transportation routes; Computational costs; Flight data; Frequent patterns minings; Functional dependency; Horizontal partitioning; Intrinsic property; Novel methods; Origin destination matrices; Pattern mining; Redundancy removal; Redundancy",Article,Scopus,2-s2.0-85119340553
"Liao M.-C., Hsieh T.-Y.","57340559400;7201941141;","Violations of Professional Technical Standards: Causes and Lessons Learned",2022,"Journal of Legal Affairs and Dispute Resolution in Engineering and Construction","14","1","04521043","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119195538&doi=10.1061%2f%28ASCE%29LA.1943-4170.0000482&partnerID=40&md5=d8e76e903012a0e81d6e4c845f2a9e14","This study was based on a database of 195 regulatory violations (N=592) of established rules of construction from 1996 to 2019 in Taiwan, and it sought to explore the potential causality of regulatory violations of professional technical standards and lessons learned. It also used data mining analysis to explore the reasons and distribution of violations of established rules of construction in the construction industry in Taiwan. First, the investigation on the occurrence rules for violating an established rule of construction in erecting (VERC) in the construction industry shows that in cases of demolition, construction, or completion of construction, the occurrence of natural disasters and earthquakes is a key factor in violations of established rules of construction. The study found that the main reasons for personnel being held responsible for VERC liabilities in accordance with laws included (1) failure of professional technical personnel to provide services with licenses, which lead to failure to implement safety evaluation for the overall structure in the construction process and failure to take any preventive measures or enhancements in the process; (2) risks to legal rights of professional technical personnel for VERC if the court adopts simplified judgment procedures; and (3) construction of illegal building structures could easily result in VERC liabilities. This study recommends the clarification of the VERC provision in Article 193 of the Taiwanese Criminal Code, namely, the construction contractor or overseer, proprietor, and design supervisor should be included to meet the current state of building construction practices, meet the principle of a legally prescribed punishment for a specified crime, and prevent disputes in the application of interpretation. Thus, regardless of whether the actor is an entity or an individual, the establishment of the VERC offense would remain unaffected in the process from development and design to construction management. Therefore, the study also provides an explanatory framework for the amendment of laws and supervision for the execution of building management by competent authorities for creating an engineering environment conducive to sustainable development. © 2021 This work is made available under the terms of the Creative Commons Attribution 4.0 International license,.","Data mining; Principle of a legally prescribed punishment for a specified crime; Professional technical personnel; Violating an established rule of construction in erecting (VERC)","Buildings; Construction; Construction industry; Crime; Disasters; Environmental regulations; Human resource management; Professional aspects; Project management; Risk assessment; Sustainable development; Key factors; Natural disasters; Natural earthquake; Principle of a legally prescribed punishment for a specified crime; Professional technical personnel; Regulatory violations; Safety evaluations; Technical personnel; Technical standards; Violating an established rule of construction in erecting (VERC); Data mining",Article,Scopus,2-s2.0-85119195538
"Ayman A., Sivagnanam A., Wilbur M., Pugliese P., Dubey A., Laszka A.","57203129866;57219692506;57210552279;57219692936;57340112800;57294198700;","Data-Driven Prediction and Optimization of Energy Use for Transit Fleets of Electric and ICE Vehicles",2022,"ACM Transactions on Internet Technology","22","1","3433992","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119175281&doi=10.1145%2f3433992&partnerID=40&md5=d4757c658450232f59b10ad542d86f7d","Due to the high upfront cost of electric vehicles, many public transit agencies can afford only mixed fleets of internal combustion and electric vehicles. Optimizing the operation of such mixed fleets is challenging because it requires accurate trip-level predictions of electricity and fuel use as well as efficient algorithms for assigning vehicles to transit routes. We present a novel framework for the data-driven prediction of trip-level energy use for mixed-vehicle transit fleets and for the optimization of vehicle assignments, which we evaluate using data collected from the bus fleet of CARTA, the public transit agency of Chattanooga, TN. We first introduce a data collection, storage, and processing framework for system-level and high-frequency vehicle-level transit data, including domain-specific data cleansing methods. We train and evaluate machine learning models for energy prediction, demonstrating that deep neural networks attain the highest accuracy. Based on these predictions, we formulate the problem of minimizing energy use through assigning vehicles to fixed-route transit trips. We propose an optimal integer program as well as efficient heuristic and meta-heuristic algorithms, demonstrating the scalability and performance of these algorithms numerically using the transit network of CARTA. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.","combinatorial optimization; deep learning; electric vehicle; energy use; environmental impact; genetic algorithm; integer program; machine learning; public transportation","Combinatorial optimization; Data mining; Deep neural networks; Digital storage; Fleet operations; Forecasting; Genetic algorithms; Heuristic algorithms; Integer programming; Urban transportation; Vehicles; Data driven; Deep learning; Energy use; Integer program; Internal combustion; Optimisations; Public transit; Public transportation; Transit agencies; Transit fleets; Environmental impact",Article,Scopus,2-s2.0-85119175281
"Garousi V., Cutting D., Felderer M.","13408954200;57210234641;24832720900;","Mining user reviews of COVID contact-tracing apps: An exploratory analysis of nine European apps",2022,"Journal of Systems and Software","184",,"111136","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118981042&doi=10.1016%2fj.jss.2021.111136&partnerID=40&md5=3cf19a03fd3c6366ab935c810f24ff15","Context: More than 78 countries have developed COVID contact-tracing apps to limit the spread of coronavirus. However, many experts and scientists cast doubt on the effectiveness of those apps. For each app, a large number of reviews have been entered by end-users in app stores. Objective: Our goal is to gain insights into the user reviews of those apps, and to find out the main problems that users have reported. Our focus is to assess the “software in society” aspects of the apps, based on user reviews. Method: We selected nine European national apps for our analysis and used a commercial app-review analytics tool to extract and mine the user reviews. For all the apps combined, our dataset includes 39,425 user reviews. Results: Results show that users are generally dissatisfied with the nine apps under study, except the Scottish (“Protect Scotland”) app. Some of the major issues that users have complained about are high battery drainage and doubts on whether apps are really working. Conclusion: Our results show that more work is needed by the stakeholders behind the apps (e.g., app developers, decision-makers, public health experts) to improve the public adoption, software quality and public perception of these apps. © 2021 Elsevier Inc.","Contact-tracing; COVID; Data mining; Mobile apps; Software engineering; Software in society; User reviews","Application programs; Computer software selection and evaluation; Decision making; App stores; Contact tracing; Coronaviruses; COVID; End-users; Exploratory analysis; Gain insight; Mobile app; Software in society; User reviews; Data mining",Article,Scopus,2-s2.0-85118981042
"Soltani-Mohammadi S., Hoseinian F.S., Abbaszadeh M., Khodadadzadeh M.","54929872400;56815160600;54419507900;36188365300;","Grade estimation using a hybrid method of back-propagation artificial neural network and particle swarm optimization with integrated samples coordinate and local variability",2022,"Computers and Geosciences","159",,"104981","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118827414&doi=10.1016%2fj.cageo.2021.104981&partnerID=40&md5=456f11d0a822717d45eb22d82d7e42a6","Grade estimation is a critical issue in mineral resource evaluation, being extensively investigated by data mining techniques. In this paper, a hybrid method composed of back-propagation artificial neural network (BPANN) and particle swarm optimization (PSO) algorithms is proposed to solve the grade estimation problem. The PSO algorithm is implemented to optimize the BPANN parameters by reducing the effects of a local minimum problem, which is one of the critical drawbacks of BPANN. The proposed BPANN-PSO algorithm is validated for Al2O3 grade estimation in one of Iran's largest Bauxite deposits. The performance of BPANN-PSO algorithm for grade estimation is compared with BPANN and ordinary kriging. The experimental results indicate that the BPANN-PSO model is more appropriate for estimating Al2O3 grade with a reasonable error. © 2021 Elsevier Ltd","Grade estimation; Input space configuration; Machine learning methods; Parameter optimization","Alumina; Aluminum oxide; Data mining; Neural networks; Parameter estimation; Particle swarm optimization (PSO); Back Propagation; Grade estimations; Hybrid method; Input space; Input space configuration; Machine learning methods; Neural network and particle swarm optimizations; Parameter optimization; Particle swarm optimization algorithm; Space configuration; Backpropagation; algorithm; artificial neural network; back propagation; bauxite; data mining; error analysis; estimation method; kriging; mineral resource; optimization; performance assessment; Iran",Article,Scopus,2-s2.0-85118827414
"Patriarca R., Di Gravio G., Cioponea R., Licu A.","56411874700;57216907224;25960591200;25825216400;","Democratizing business intelligence and machine learning for air traffic management safety",2022,"Safety Science","146",,"105530","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118746874&doi=10.1016%2fj.ssci.2021.105530&partnerID=40&md5=37bc50a62f292359b396f9fffccd2f13","The ways in which Air Navigation Service Providers (ANSPs) monitor safety performance is strongly influenced by international regulations, standards, and agreements, although each State may also add its own local requirements. Particularly in the case of more mature ANSPs, the regulatory safety performance obligations are merely the tip of the iceberg in the undertaken safety performance activities. Much of the indicators, methods and tools are over and above what is required by regulations, either national or international. In modern settings, the usage of Business Intelligence and Machine Learning solutions can be enumerated under the continuous chasing of strategies to foster ANSPs’ safety intelligence capacities towards higher standards. This manuscript shows the development process of an integrated data-driven framework for self-service BI and ML on safety reporting data for the air traffic management system. The proposed framework firstly focuses on the development process of a BI architecture to extract meaningful knowledge from multiple data sources. Then, it progresses discussing how ML solutions may support gaining a deeper understanding of system's performance and delineating specific safety recommendations. The explorative application of the proposed framework in multiple European ANSPs provides the basis for sharing lessons learned and outlining a possible path to start democratizing safety intelligence in aviation. © 2021 Elsevier Ltd","Aviation safety; Data mining; Safety intelligence; Self-service BI; Self-service ML","Air navigation; Air traffic control; Data mining; Information management; Safety engineering; Sea ice; Air navigation service providers; Air Traffic Management; Aviation safety; Development process; Indicator methods; International regulations; Safety intelligence; Safety performance; Self-service BI; Self-service ML; Machine learning; article; aviation; data mining; iceberg; intelligence",Article,Scopus,2-s2.0-85118746874
"Nanglia S., Ahmad M., Ali Khan F., Jhanjhi N.Z.","57325013000;57221463579;57324636300;36088700700;","An enhanced Predictive heterogeneous ensemble model for breast cancer prediction",2022,"Biomedical Signal Processing and Control","72",,"103279","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118548050&doi=10.1016%2fj.bspc.2021.103279&partnerID=40&md5=cda28861361ae23661dc1ee1d321594e","Breast Cancer is one of the most prevalent tumors after lung cancer and is common in both women and men. This disease is mostly asymptomatic in the early stages thus detection is difficult, and it becomes complicated and expensive to be treated in later stages resulting in increased fatality rates. There are comparatively very few pieces of literature that investigated breast cancer employing an ensemble learning for cancer prediction as compared to single classifier approaches. This paper presents a heterogeneous ensemble machine learning approach, to detect breast cancer in the early stages. The proposed approach follows the CRISP-DM process and uses Stacking for building the ensemble model using three different algorithms – K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and Decision Tree (DT). The performance of this meta classifier is compared with the individual performances of its base classifiers (KNN, SVM, DT) and other single classifiers – Logistic Regression (LR), Artificial Neural Network (ANN), Naïve Bayes (NB), Stochastic Gradient Descent (SGD) and a homogenous ensemble model of Random Forest (RF). The top 5 features – Glucose, Resistin, HOMA, Insulin, and BMI are derived by using Chi-Square. Evaluation of the model helps in estimating its consideration for early breast cancer prediction just by using the anthropometric data of humans. Performances of models are compared using metrics such as accuracy, AUC, ROC Curve, f1-score, precision, recall, log loss, and specificity using K-fold cross-validation of 2, 3, 5, 10, and 20 folds. The proposed ensemble model achieved the greatest accuracy of 78 % with the lowest log-loss of 0.56, at K = 20, thus rejecting the Null hypothesis. The derived p-value is 0.014, from the one-tailed t-test, which provides lower significance at ∝ = 0.05. © 2021","Breast cancer; Data mining; Heterogeneous ensemble learning; Homogenous ensemble learning; Machine learning; Meta classifiers","Anthropometry; Data mining; Diseases; Forecasting; Gradient methods; Nearest neighbor search; Neural networks; Stochastic models; Stochastic systems; Support vector machines; Support vector regression; Breast Cancer; Cancer prediction; Ensemble learning; Ensemble models; Heterogeneous ensemble learning; Heterogeneous ensembles; Homogenous ensemble learning; Meta-classifiers; Nearest-neighbour; Decision trees; adiponectin; glucose; insulin; leptin; monocyte chemotactic protein 1; resistin; anthropometric parameters; area under the curve; Article; artificial neural network; Bayesian learning; body mass; breast cancer; cancer diagnosis; cancer staging; classifier; clinical evaluation; comparative study; controlled study; data mining; decision tree; diagnostic accuracy; feature extraction; feature selection; female; homeostasis model assessment; human; k nearest neighbor; learning algorithm; logistic regression analysis; machine learning; null hypothesis; predictive value; random forest; recall; receiver operating characteristic; stochastic gradient descent; support vector machine",Article,Scopus,2-s2.0-85118548050
"Ghanbari E., Shakery A.","57068964800;18234225600;","A Learning to rank framework based on cross-lingual loss function for cross-lingual information retrieval",2022,"Applied Intelligence","52","3",,"3156","3174",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118543836&doi=10.1007%2fs10489-021-02592-z&partnerID=40&md5=de87001ddd0ae3a90090069385ca9293","Learning to Rank (LTR) techniques use machine learning to rank documents. In this paper, we propose a new LTR based framework for cross-language information retrieval (CLIR). The core idea of the proposed framework is the use of the knowledge of training queries in the target language as well as the training queries in the source language to extract features and to construct the ranking model instead of using only the training queries in the source language. The proposed framework is composed of two main components. The first component extracts monolingual and cross-lingual features from the queries and the documents. To extract the cross-lingual features, we introduce a general approach based on translation probabilities where translation knowledge, which is created from a combination of probabilistic dictionary extracted from translation resources with the translation knowledge available in the queries in the target language, is used to fill the gap between the documents and the queries. The second component of the proposed framework trains a ranking model to optimize the proposed loss function for an input LTR algorithm, and the features. The new loss function is proposed for any listwise LTR algorithm to construct a ranking model for CLIR. To this end, the loss function of the LTR algorithm is calculated for both training data in the target language and training data in the source language. We propose a linear interpolation of the harmonic mean of two loss functions (monolingual and cross-lingual) and the ratio of these two loss functions as the new loss function. The output of this framework is a cross-lingual ranking model that is created with the goal of minimizing the proposed loss function. Experimental results show that the proposed framework outperforms the baseline information retrieval methods and other LTR ranking models in terms of Mean Average Precision (MAP). The findings also indicate that the use of cross-lingual features considerably increases the efficiency of the framework in terms of MAP and Normalized Discounted Cumulative Gain (NDCG). © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Cross-lingual features; Cross-Lingual information retrieval (CLIR); Learning to rank (LTR)","Data mining; Information retrieval; Query processing; Translation (languages); Cross-language information retrieval; Cross-lingual information retrieval; Linear Interpolation; Loss functions; Source language; Target language; Translation knowledge; Translation resources; Learning to rank",Article,Scopus,2-s2.0-85118543836
"Dong Y., Zhu G.-Y., Hao L., Liang Q., Zhou J.-H., Shi Z.-D., Yu H., Ma W.-M., Fan T., Zhang W.-D., Zang G.-H., Han C.-H.","56434937100;57198451624;36480044500;57205334255;56477515400;55699926100;57221687220;57204425322;35760881100;55604050300;55252268700;8629930300;","High S100A7 expression is associated with early muscle invasion and poor survival in bladder carcinoma",2022,"Annals of Diagnostic Pathology","56",,"151847","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118535142&doi=10.1016%2fj.anndiagpath.2021.151847&partnerID=40&md5=8cb644ac83582e24a85097e4c7cd7665","Muscle-invasive bladder carcinoma (MIBC) accounts for 25% of newly diagnosed bladder carcinomas (BCs) and presents a high risk of progression and metastasis. This study aimed to identify reliable biomarkers associated with muscle invasion and prognosis to identify potential therapeutic targets for MIBC. Four gene datasets were downloaded from the Gene Expression Omnibus, and the integrated differentially expressed genes (DEGs) were then subjected to gene ontology (GO) terms and pathway enrichment analyses. Correlation analysis between the expression of the top-ranking DEGs and pathological T stages was performed to identify the genes associated with early muscle invasion. The corresponding prognostic values were evaluated, and co-expressed genes mined in the cBioPortal database were loaded into ClueGo in Cytoscape for pathway enrichment analysis. Using data mining from the STRING and TCGA databases, protein–protein interaction and competitive endogenous RNA networks were constructed. In total, 645 integrated DEGs were identified and these were mainly enriched in 26 pathways, including cell cycle, bladder cancer, DNA replication, and PPAR signaling pathway. S100A7 expression was significantly increased from the T2 stage and showed significantly worse overall survival and disease-specific survival in patients with BC. In total, 144 genes co-expressed with S100A7 in BC were significantly enriched in the IL-17 pathway. S100A7 was predicted to directly interact with LYZ, which potentially shows competitive binding with hsa-mir-140 to affect the expression of six lncRNAs in MIBC. In conclusion, high S100A7 expression was predicted to be associated with early muscle invasion and poor survival in patients with BC. © 2021","Bioinformatics; Bladder carcinoma; Muscle invasion; Prognosis; S100A7","anxa2 protein; azu1 protein; cell adhesion molecule; ctsg protein; cytokeratin 16; cytokeratin 6; ether lipid; fatty acid binding protein 5; glycerophospholipid; interleukin 17; isoleucine; leucine; long untranslated RNA; messenger RNA; microRNA; psoriasin; serpinb3 protein; ttr protein; unclassified drug; valine; aged; Article; binding competition; bladder cancer; cancer prognosis; cancer staging; cancer survival; cell nucleus membrane; clinical article; controlled study; cytoplasm; data base; data mining; differential gene expression; disease specific survival; DNA replication; female; gene expression; gene expression level; gene ontology; human; human tissue; male; muscle contraction; muscle invasive bladder cancer; muscle metastasis; oncogenomics; overall survival; pathway enrichment analysis; PPAR signaling; protein expression; protein protein interaction; tumor invasion; vascular smooth muscle",Article,Scopus,2-s2.0-85118535142
"Wahid A., Rao A.C.S.","57209486494;57190977830;","RDOF: An outlier detection algorithm based on relative density",2022,"Expert Systems","39","2","e12859","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118297105&doi=10.1111%2fexsy.12859&partnerID=40&md5=ba1d969169bdf2ecb067042850d17f96","An outlier has a significant impact on data quality and the efficiency of data mining. The outlier identification algorithm observes only data points that do not follow clearly defined meanings of projected behaviour in a data set. Several techniques for identifying outliers have been presented in recent years, but if outliers are located in areas where neighbourhood density varies substantially, it can result in an imprecise estimate. To address this problem, we provide a ‘Relative Density-based Outlier Factor (RDOF)’ algorithm based on the concept of mutual proximity between a data point and its neighbours. The proposed approach is divided into two stages: an influential space is created at a test point in the first stage. In the later stage, a test point is assigned an outlier-ness score. We have conducted experiments on three real-world data sets, namely the Johns Hopkins University Ionosphere, the Iris Plant, and Wisconsin Breast Cancer data sets. We have investigated three performance metrics for comparison: precision, recall, and rank power. In addition, we have compared our proposed method against a set of relevant baseline methods. The experimental results reveal that our proposed method detected all (i.e., 100%) outlier class objects with higher rank power than baseline approaches over these experimental data sets. © 2021 John Wiley & Sons Ltd.",,"Anomaly detection; Data handling; Data mining; Ionosphere; Nearest neighbor search; Signal detection; Data set; Density-based approaches; K-near neighbor; Local outlier detection; Local outliers; Nearest-neighbour; Outlier Detection; Outlier-ness score; Relative density; Reverse nearest neighbors; Statistics",Article,Scopus,2-s2.0-85118297105
"Dai P., Zhang S., Gong Y., Zhou Y., Hou H.","57205638339;8420622300;55122910400;55132880800;36642408800;","A crowd-sourced valuation of recreational ecosystem services using mobile signal data applied to a restored wetland in China",2022,"Ecological Economics","192",,"107249","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117828981&doi=10.1016%2fj.ecolecon.2021.107249&partnerID=40&md5=c21683a7cf43fc5c09cde8e42a5c52f3","The travel cost method is the main technique used to evaluate the monetary value of recreational cultural ecosystem services of wetlands. While questionnaires still provide an important way to obtain tourist travel distances, salary rate coefficients, and travel rate data, but they are restricted by the data scale and the need for long-term monitoring. Therefore, the present study introduces mobile phone signal data into the travel cost method and establishes an evaluation model for recreational ecosystem services. By mining mobile phone signal data, the visitors' duration of stay, starting and ending coordinates, age, and gender data were collected in order to estimate the potential costs. Data from 25,087 recreationists were sampled at Pan'an Lake Wetland Park, Xuzhou, China, in 2018. Data mining found that (1) mobile phone signal data have unique advantages in determining the travel distance and cost of overnight accommodations; (2) recreationists were distributed among 229 cities, although 87.2% came from cities located within 500 km of Pan'an Lake Wetland Park, while 72.04% of all recreationists were Xuzhou locals; (3) the consumer surplus associated with visiting Pan'an Lake Wetland Park was 129.79 Chinese Yuan (CNY) per person and the average recreational value was 448.11 CNY. The evaluation results showed that the recreational value of Pan'an Lake Wetland Park was about 836 million CNY in 2018. This suggests that mobile phone signal data can be used as a new data source when assessing the recreational value of wetlands via the travel cost method. © 2021 Elsevier B.V.","Cultural ecosystem service value; Mobile signal data; Recreational value assessment; Travel cost method","data mining; ecosystem service; mobile phone; recreational activity; China; Jiangsu; Xuzhou",Article,Scopus,2-s2.0-85117828981
"Diop L., Diop C.T., Giacometti A., Soulet A.","57225469969;36843222700;14015417400;8935563500;","Pattern on demand in transactional distributed databases",2022,"Information Systems","104",,"101908","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117770395&doi=10.1016%2fj.is.2021.101908&partnerID=40&md5=26bb63d7428ebd6440cc36b8881fe8ba","Many applications rely on distributed databases like sensor networks or the Semantic Web. However, only few methods exist to extract patterns without centralizing the data by following the exhaustive extraction paradigm. Their principle is to extract a unique large collection of frequent patterns that will be used for all downstream applications. Unfortunately, the communication of this large collection from the different nodes is often more expensive than the database centralization. Furthermore, this rigid principle is not suited to modern data analysis where data and analyst needs change daily. It is both too expensive to repeat the exhaustive extraction for each change and it is not possible to build the ideal collection of patterns to meet all the needs. To circumvent this difficulty, this paper revisits the problem of pattern mining in distributed databases by adopting the Pattern-On-Demand paradigm. This principle consists in instantly extracting the patterns at the moment when the analyst needs them. Specifically, we propose a new pattern sampling algorithm, named DDSAMPLING, that randomly draws a pattern from a transactional distributed database with a probability proportional to its interest. We demonstrate the soundness of DDSAMPLING and analyze its time complexity. Finally, experiments on benchmark datasets highlight its low communication cost and its robustness against network and node failures. We also illustrate its interest on real-world data from the Semantic Web for detecting outlier entities in DBpedia and Wikidata. In addition, our output space sampling method is more parsimonious in terms of communication cost than a baseline relying on input space sampling. © 2021 Elsevier Ltd","Knowledge base; Outlier detection; Pattern mining; Pattern on Demand; Pattern sampling","Anomaly detection; Data handling; Extraction; Knowledge based systems; Pattern recognition; Semantic Web; Sensor networks; Statistics; Communication cost; Distributed database; Downstream applications; Exhaustive extraction; On demands; Pattern mining; Pattern on demand; Pattern samplings; Sensors network; Space sampling; Data mining",Article,Scopus,2-s2.0-85117770395
"Huang D., Sun H., Zhang J., Zhao S., Zhou Q.","56408745600;7404827733;57276960400;56329953600;36926633300;","A data mining-based method for mining key factors affecting transient voltage stability for power systems with renewable energy sources",2022,"IET Generation, Transmission and Distribution","16","4",,"617","628",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116931959&doi=10.1049%2fgtd2.12314&partnerID=40&md5=c34bb29a1e87550fdc00b9efa179b592","Increasing penetration of renewable energy sources (RESs) into power systems has brought new challenges to guarantee transient voltage stability (TVS) of the system, due to complex and different characteristics of the RES compared with the synchronous generator. The related theories to the TVS for power systems with RES (PSRESs) are incomplete, and it is difficult to construct accurate physical model of the PSRES by using traditional TVS analysis method. Here a novel data mining-based approach for extracting key factors that affect the TVS of a PSRES is put forward. The original Relief algorithm is modified to deal with the imbalance of sample size between stable and unstable samples of the practical data set and improve the calculation accuracy. Then, a data mining scheme based on the modified Relief algorithm is presented to acquire key factors affecting the TVS. With the proposed scheme, the influence degrees of different factors on the TVS can be evaluated quantitatively by their weighting values, and then the key factors as well as the influence patterns can be determined. Test results which are conducted on the modified IEEE-39 test system with RESs are presented to demonstrate the accuracy and efficiency of the proposed method. © 2021 The Authors. IET Generation, Transmission & Distribution published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology",,"Data mining; Electric power system stability; Natural resources; Power quality; Transients; Data set; Key factors; Physical modelling; Power; Relief algorithm; Renewable energy source; Sample sizes; Stability analysis method; Transient voltage stability; Voltage stability analysis; Renewable energy resources",Article,Scopus,2-s2.0-85116931959
"Alsayat A.","57190838566;","Improving Sentiment Analysis for Social Media Applications Using an Ensemble Deep Learning Language Model",2022,"Arabian Journal for Science and Engineering","47","2",,"2499","2511",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116873543&doi=10.1007%2fs13369-021-06227-w&partnerID=40&md5=1908a65243e76625278162db5468ce5b","As data grow rapidly on social media by users’ contributions, specially with the recent coronavirus pandemic, the need to acquire knowledge of their behaviors is in high demand. The opinions behind posts on the pandemic are the scope of the tested dataset in this study. Finding the most suitable classification algorithms for this kind of data is challenging. Within this context, models of deep learning for sentiment analysis can introduce detailed representation capabilities and enhanced performance compared to existing feature-based techniques. In this paper, we focus on enhancing the performance of sentiment classification using a customized deep learning model with an advanced word embedding technique and create a long short-term memory (LSTM) network. Furthermore, we propose an ensemble model that combines our baseline classifier with other state-of-the-art classifiers used for sentiment analysis. The contributions of this paper are twofold. (1) We establish a robust framework based on word embedding and an LSTM network that learns the contextual relations among words and understands unseen or rare words in relatively emerging situations such as the coronavirus pandemic by recognizing suffixes and prefixes from training data. (2) We capture and utilize the significant differences in state-of-the-art methods by proposing a hybrid ensemble model for sentiment analysis. We conduct several experiments using our own Twitter coronavirus hashtag dataset as well as public review datasets from Amazon and Yelp. For concluding results, a statistical study is carried out indicating that the performance of these proposed models surpasses other models in terms of classification accuracy. © 2021, King Fahd University of Petroleum & Minerals.","Coronavirus; COVID-19; Data mining; Deep learning; Ensemble algorithms; Machine learning; Pandemic; Sentiment analysis; Social media",,Article,Scopus,2-s2.0-85116873543
"Tegin B., Hernandez E.E., Rini S., Duman T.M.","57217176202;57220898547;35410393500;7004018156;","Straggler Mitigation Through Unequal Error Protection for Distributed Approximate Matrix Multiplication",2022,"IEEE Journal on Selected Areas in Communications","40","2",,"468","483",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116821616&doi=10.1109%2fJSAC.2021.3118350&partnerID=40&md5=a0cfe882899bae62f2fd7911d4b9b412","Large-scale machine learning and data mining methods routinely distribute computations across multiple agents to parallelize processing. The time required for the computations at the agents is affected by the availability of local resources and/or poor channel conditions, thus giving rise to the 'straggler problem.' In this paper, we address this problem for distributed approximate matrix multiplication. In particular, we employ Unequal Error Protection (UEP) codes to obtain an approximation of the matrix product to provide higher protection for the blocks with a higher effect on the multiplication outcome. We characterize the performance of the proposed approach from a theoretical perspective by bounding the expected reconstruction error for matrices with uncorrelated entries. We also apply the proposed coding strategy to the computation of the back-propagation step in the training of a Deep Neural Network (DNN) for an image classification task in the evaluation of the gradients. Our numerical experiments show that it is indeed possible to obtain significant improvements in the overall time required to achieve DNN training convergence by producing approximation of matrix products using UEP codes in the presence of stragglers. © 1983-2012 IEEE.","approximate matrix multiplication; Distributed computation; stragglers; unequal error protection","Backpropagation; Codes (symbols); Data mining; Deep neural networks; Errors; Image coding; Multi agent systems; Network coding; Approximate matrix multiplication; Data mining methods; Distributed computations; Large-scale machine learning; Machine data; MAtrix multiplication; Matrix products; Multiple agents; Straggler; Unequal error protections; Matrix algebra",Article,Scopus,2-s2.0-85116821616
"Priyanga P., Nadira Banu Kamal A.R.","57290179400;56724236700;","Mobile App Usage Pattern Prediction Using Hierarchical Flexi-Ensemble Clustering (HFEC) for Mobile Service Rating",2022,"Wireless Personal Communications","122","4",,"3247","3268",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116805185&doi=10.1007%2fs11277-021-09048-0&partnerID=40&md5=5835a413e0e9d20e3daa5ab193ea545b","Nowadays, the mobile app market becomes rapidly increased in world wide. The mobile app marketers have smart enough to understand the requirements and demands of customers and perform their aspirations. They delight them. It provides growth, profitability, and creativity with lot of inventions. The main aim of this research is to analyze the customer interest and preferences of mobile service providers. This paper proposed the clustering model named as Hierarchical Flexi-Ensemble Clustering. It provides the final result with robustness and improved quality. Before clustering, the unwanted features are removed by using the Genetic Algorithm based on the Collective Materials technique. The customer preferences are analyzes with the clustering of mobile usage patterns. The analysis determined that the app usage pattern based on the most frequent word, rating category, rating character count, rating word count and content-based rating in the google play store app dataset. Finally, the results are compared with the existing methods to analyze the superior performance of proposed method. The comparison analysis is estimated based on the based on the average hit rate at different cache sizes. The work is concluded with the app pattern prediction in the form of clustering for app marketing service. From the marketing side, they can analyze the customer preferences and satisfaction. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","And customer rating; App behavior analysis; Data mining; Ensemble clustering; Genetic algorithm; Mobile service rating; Pattern prediction","Clustering algorithms; Commerce; Customer satisfaction; Data mining; Genetic algorithms; Mobile telecommunication systems; Sales; And customer rating; App behavior analyse; Behavior analysis; Clusterings; Ensemble clustering; Mobile app; Mobile service; Mobile service rating; Pattern prediction; Usage patterns; Forecasting",Article,Scopus,2-s2.0-85116805185
"Anuar A., Marwan N.F., Smith J., Siriyanun S., Sharif A.","57282669100;57206207624;57213085437;57204145921;56872517200;","Bibliometric analysis of immigration and environmental degradation: evidence from past decades",2022,"Environmental Science and Pollution Research","29","9",,"13729","13741",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116226489&doi=10.1007%2fs11356-021-16470-1&partnerID=40&md5=b5666cfa5ddd29b398a5aa802170dc0d","The aim of this paper is to examine immigration and environmental degradation using bibliometric analysis. This paper also analyzes sources of publication, authorship, citations, distributions publications and other bibliometric indicators. The study focuses on a total of 1372 articles published from 2000 to 2020. These articles were collected through an automated process from the Scopus database and later analyzed using techniques such as bibliometric indicators analysis, VOSviewer, and Perish or Publish. The research identified 991 articles from varieties of published sources. The topic of immigrants and environmental degradation has been an emerging topic since 1981. Starting in 2000, most of the scholars actively producing an articles pertinent to this topic. Most of the articles were published in journals, and English is the primary language of research. United States is the leading country in contributing the publications. Meanwhile, the most significant fields in which the sources were produced were environmental science, agricultural and biological sciences, arts and humanities and earth and planetary sciences. However, some limitations has been found. It has been suggested for future research, to lengthen this work to other databases, as well as bibliometric analyses of immigration and environmental degradation in developed and developing countries by adding a new keyword such as energy consumption and climate change. This paper aims to assess recent trends in the expansion of academic literature on immigration and environmental degradation using the bibliometric analysis method. Network visualization and bibliometric indicators are used in this paper to present the results. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Bibliometric analysis; Environmental degradation; Immigration; Scopus database","data mining; database; environmental degradation; immigration; research; visualization; United States; bibliometrics; factual database; language; migration; United States; Bibliometrics; Databases, Factual; Emigration and Immigration; Environmental Science; Language; United States",Article,Scopus,2-s2.0-85116226489
"Li J., Cheng X.","57210166358;57226181391;","Supervoxel-based extraction and classification of pole-like objects from MLS point cloud data",2022,"Optics and Laser Technology","146",,"107562","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116146694&doi=10.1016%2fj.optlastec.2021.107562&partnerID=40&md5=51caa8f818dbbb056f01f449f8f2b633","The three-dimensional (3D) instantiation expression of objects in road environments has significant applications in urban management and high-precision mapping. Pole-like objects, as the important part of road objects, their automatic and individual extraction and classification can reduce the cost of mapping and improve work efficiency. Therefore, this paper proposes a supervoxel-based method to automatically extract and classify individual pole-like objects from mobile laser scanning (MLS) point cloud data. First, supervoxels are generated through over-segmentation, and the vertical pole part of the pole-like objects are extracted via supervoxels region growing. Second, an uphill clustering method is used to individually segment the potential attachments on the vertical poles (the part of the pole-like objects except the vertical poles). Third, according to the spatial correspondence between the vertical poles and their attachments, the attachments are selected from the potential and matched with their corresponding vertical poles. Finally, the extracted individual pole-like objects are classified into four categories (street lights, cantilever traffic poles, street trees and others) according to their geometric characteristics. The proposed method was evaluated using two different MLS point cloud datasets. The experimental results demonstrate that the proposed method can efficiently extracted the pole-like objects from the two datasets, with the extraction rate of 92.4% and 98.6% respectively. Moreover, the proposed method can effectively classify the extracted pole-like objects. © 2021 Elsevier Ltd","Clustering; Extraction; MLS point cloud; Pole-like objects; Region growing","Data mining; Mapping; Poles; Roads and streets; Automatic extraction; Clusterings; High-precision; Laser scanning point clouds; Mobile laser scanning point cloud; Point cloud data; Pole-like object; Region growing; Road environment; Urban management; Extraction",Article,Scopus,2-s2.0-85116146694
"Zhang H., Dong Y., Xu D.","57218526582;8953365700;57278959900;","Accelerating exact nearest neighbor search in high dimensional Euclidean space via block vectors",2022,"International Journal of Intelligent Systems","37","2",,"1697","1722",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116031672&doi=10.1002%2fint.22692&partnerID=40&md5=6ee953d49bd8fb5610e812dc00b5a47f","The nearest neighbor search is an essential operation for many computer vision, data mining, and machine learning problems. Since it is so widely used, the nearest neighbor search should be as fast as possible. This paper explores lower bound-based approaches to speed up the exact nearest neighbor search in high dimensional Euclidean space. We compute the lower bound of Euclidean Distance by using the block vectors and Cauchy–Schwartz inequality. The proposed lower bound is calculated efficiently and is close to the real Euclidean Distance. Besides, the preprocessing step of the proposal has linear time complexity. Given a query, during the procedure of identifying the nearest neighbor, our method can eliminate many expensive actual distance computations using the lower bound to approximate Euclidean Distance. In addition, we develop a multilevel lower bound strategy, which calculates the lower bound step by step and utilizes the multistep filtering mechanism to improve the searching process further. Theoretical analysis is provided to show that the proposals can guarantee to obtain the same result as the brute-force search. Comprehensive experiments on 16 public data sets collected from various domains demonstrate that our approach performs well in finding the exact nearest neighbor compared to related competitors. The experimental results also illustrate that the multilevel lower bound strategy is effective. © 2021 Wiley Periodicals LLC.",,"Data mining; Geometry; Vector spaces; Block vectors; Cauchy–schwartz inequality; Euclidean distance; Exact near neighbor search; Low bound; Multilevel low bound strategy; Multilevels; Near neighbor searches; Schwartz inequality; Nearest neighbor search",Article,Scopus,2-s2.0-85116031672
"Wang X., Guo D., Cheng P.","35786636600;57279004900;36157657800;","Support structure representation learning for sequential data clustering",2022,"Pattern Recognition","122",,"108326","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115992886&doi=10.1016%2fj.patcog.2021.108326&partnerID=40&md5=43bd55bec2ad60d41c158ab7cc478261","Sequential data clustering is a challenging task in data mining (e.g., motion recognition and video segmentation). For good performance in dealing with complex local correlation and high-dimensional structure of sequential data, representation based methods have become one of the hot topics for sequential data clustering, in which subspace clustering is a representative tool. Subspace clustering methods divide the sequence into disjoint segments according to a locally continuous and connected representation of raw data. Although the subspace clustering methods maintain the successive property of sequential data well, there exist redundant connections in the intersection of two subsequences, which will destroy the integrity of a cluster and easily cause the chained partition of the sequence. So it is necessary to learn a more specific structure representation of a sequence to preserves both sequential information and efficient connections. Besides, the representation that conducive to clustering should have sparsity and connectivity under some assumptions. To this end, we propose a novel method to learn the support structure representation of sequence, which can extract sufficient information about instances and get the compact structure of sequential data. Furthermore, a new subspace clustering method is proposed based on the representation based method. Theoretical analysis and experimental results show the effectiveness of the proposed method. © 2021","Clustering; Sequential data; Support structure representation","Cluster analysis; Data mining; Ground supports; Motion estimation; Clustering methods; Clusterings; Learn+; Motion recognition; Motion video; Sequential data; Sequential data clustering; Subspace clustering; Support structure representation; Support structures; Clustering algorithms",Article,Scopus,2-s2.0-85115992886
"Jha A.K., Mithun S., Sherkhane U.B., Jaiswar V., Shi Z., Kalendralis P., Kulkarni C., Dinesh M.S., Rajamenakshi R., Sunder G., Purandare N., Wee L., Rangarajan V., Van Soest J., Dekker A.","57197687905;57163516700;57221671059;57221677446;57204588291;57204593162;57273601800;57204668762;25961228200;55920010400;7003293572;57213352753;6603448872;56156330000;57225379184;","Implementation of Big Imaging Data Pipeline Adhering to FAIR Principles for Federated Machine Learning in Oncology",2022,"IEEE Transactions on Radiation and Plasma Medical Sciences","6","2",,"207","213",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115751945&doi=10.1109%2fTRPMS.2021.3113860&partnerID=40&md5=5f59c0de39dade6fe16a82050e02c554","Cancer is a fatal disease and one of the leading causes of death worldwide. The cure rate in cancer treatment remains low; hence, cancer treatment is gradually shifting toward personalized treatment. Artificial intelligence (AI) and radiomics have been recognized as one of the potential areas of research in personalized medicine in oncology. Several researchers have identified the capabilities of AI and radiomics to characterize phenotype and there by predict the outcome of treatment in oncology. Although AI and radiomics have shown promising initial results in diagnosis and treatment in oncology, these technologies are also facing challenges of standardization and scalability. In the last few years, researchers have been trying to develop a research infrastructure for federated machine learning that increases the usability of Big Data for clinical research. These research infrastructures are based on the findable, accessible, interoperable, and reusable (i.e., FAIR) data principles. The India-Dutch 'big imaging data approach for oncology in a Netherlands India collaboration' (BIONIC) is a jointly funded initiative by the Dutch Research Council (NWO) and the Indian Ministry of Electronics and Information Technology (MeitY), aiming to introduce radiomic-based research into clinical environments using federated machine learning on geographically dispersed collections of FAIR data. This article described a prototype end-to-end research infrastructure implemented through the BIONIC partnership into a leading cancer care public hospital in India. © 2017 IEEE.","accessible; and reusable (FAIR) data; Artificial intelligence (AI); findable; interoperable; machine learning; natural language processing (NLP); radiomics","Big data; Clinical research; Diagnosis; Diseases; Hospitals; Learning algorithms; Machine learning; Medical imaging; Natural language processing systems; Oncology; Biomedical imaging; Data pipelines; FAIR data.; Fatal disease; Features extraction; Imaging data; Machine-learning; Radiomic; Research infrastructure; Data mining",Article,Scopus,2-s2.0-85115751945
"Wu J.M.-T., Srivastava G., Jolfaei A., Pirouz M., Lin J.C.-W.","57271797800;57202588447;36680369700;57193432915;56449520400;","Security and Privacy in Shared HitLCPS Using a GA-Based Multiple-Threshold Sanitization Model",2022,"IEEE Transactions on Emerging Topics in Computational Intelligence","6","1",,"16","25",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115721741&doi=10.1109%2fTETCI.2020.3032701&partnerID=40&md5=98bb3c81c7fb815445207180b343b371","In Cyber-Physical Systems (CPS), especially in human-in-the-loop situations (also known as HitLCPS), the security and privacy for keeping sensitive information private is considered an emerging topic in recent decades. Many techniques in privacy-preserving data mining (PPDM) can be applied directly to HitLCPS. However, most of them to date have focused on handling singular threshold problems for data sanitization. If a sensitive itemset includes more items, it has a higher probability of being identified due to its specificity. In this work, we propose a new concept of multiple support thresholds to assist in resolving this issue. The proposed method assigns a stricter threshold for an itemset. Furthermore, a genetic-algorithm (GA)-based model is involved in the designed algorithm to minimize side effects. In our experimental results, the GA-based PPDM approach is compared with traditional Greedy PPDM approaches. The strong experimental results clearly show that our proposed method can give similar performance to conventional algorithms while still maintaining higher-levels of security and privacy protection than previous methods. © 2017 IEEE.","genetic algorithm; multi-threshold; optimization; Privacy preservation; security","Data mining; Embedded systems; Genetic algorithms; Itemset; Multiple threshold; Multithreshold; Optimisations; Privacy preservation; Privacy-preserving data mining; Sanitization; Security; Security and privacy; Data privacy",Article,Scopus,2-s2.0-85115721741
"Yatracos Y.G.","6602642902;","Residual's influence index (RINFIN), bad leverage and unmasking in high dimensional L2-regression",2022,"Statistical Analysis and Data Mining","15","1",,"125","138",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115677894&doi=10.1002%2fsam.11550&partnerID=40&md5=d3f1be9bc4ec8ebbaf7f223c0a839458","In linear regression of Y on X(∈ Rp) with parameters β(∈ Rp+1), statistical inference is unreliable when observations are obtained from gross-error model, Fϵ,G = (1 − ϵ)F + ϵG, instead of the assumed probability F;G is gross-error probability, 0 &lt; ϵ &lt; 1. Residual's influence index (RINFIN) at (x, y) is introduced, with components measuring also the local influence of x in the residual and large value flagging a bad leverage case (from G), thus causing unmasking. Large sample properties of RINFIN are presented to confirm significance of the findings, but often the large difference in the RINFIN scores of the data is indicative. RINFIN is successful with microarray data, simulated, high dimensional data and classic regression data sets. RINFIN's performance improves as p increases and can be used in multiple response linear regression. © 2021 The Authors. Statistical Analysis and Data Mining published by Wiley Periodicals LLC.",,"Big data; Clustering algorithms; Data mining; Gross errors; High-dimensional; Higher-dimensional; Influence functions; Leverage; Local influence; Masking; P-values; Post-P-value era; Residual influence index; Linear regression",Article,Scopus,2-s2.0-85115677894
"Aarons M.F., Young C.M., Bruce L., Dwyer D.B.","57266209000;57192804054;7102225530;8273505500;","The effect of team formation on defensive performance in Australian football",2022,"Journal of Science and Medicine in Sport","25","2",,"178","182",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115305689&doi=10.1016%2fj.jsams.2021.09.002&partnerID=40&md5=50e047814f2735a61b76bb4b360b2e8f","Objectives: Understanding the successful characteristics of team formation during different scenarios in Australian Football matches can assist coaches in making important tactical match-day and training decisions. The aims of this study were to explore the outcomes of entries inside 50 m of the goal, in Australian Football and to determine whether there was an association between team formation and team defensive performance after a turnover. Design: Observational. Methods: Global Positioning System (GPS) data, technical event data and video files from 22 matches in one season were obtained from an elite Australian Football club. Of 1092 forward 50 entries, 392 possession chains that resulted in a turnover were analysed. Variables representing team formation of players at the occurrence of turnover were compared between positive and negative outcomes of the subsequent possession chain. Logistic regression and decision tree modelling were also used to explore associations and variable importance. Results: None of 18 team formation characteristics differed between positive and negative outcomes of turnovers. Multivariate modelling identified that having a team formation with greater width than length made it more likely to result in a positive outcome (Decision tree classification accuracy = 69.5%, AUROC = 0.72). Conclusions: No single characteristic of team formation affects the outcome of a turnover possession chain, however team formation that was wider than it was long may be associated with a more desirable outcome. The lack of association between most team formation characteristics and defensive outcomes, highlight the risk of over emphasising team formation in tactical planning for some phases of play. © 2021 Elsevier Ltd","Collective team behaviour; Data mining; Performance analysis; Sports analytics; Tactical","article; data mining; decision tree; football; global positioning system; human; receiver operating characteristic; season; turnover rate; videorecording; athletic performance; Australia; competitive behavior; soccer; Athletic Performance; Australia; Competitive Behavior; Football; Humans; Soccer",Article,Scopus,2-s2.0-85115305689
"Shahbazi N., Gryz J.","57188677691;6603426890;","Upper bounds for can-tree and FP-tree",2022,"Journal of Intelligent Information Systems","58","1",,"197","222",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115215910&doi=10.1007%2fs10844-021-00673-6&partnerID=40&md5=b29e676cfe0ee15ecd79c667fac9d20a","Two efficient tree structures known as Can-tree (Leung et al., Knowledge and Information Systems, 11(3), 287–311, 2007) and FP-tree (Han et al., 2000) are used to store a database in memory for mining frequent patterns. However, there has been no discussion on tight upper bound for the number of nodes in these trees. Instead, a very loose upper bound of 2n (where n is the number of distinct items in the database) is used. In this paper, we provide a tighter upper bound for the number of nodes in Can-tree and FP-tree. The upper bound on the number of nodes is provided through a greedy algorithm for the Can-tree and a closed form solution is derived for the FP-tree. These results are illustrated by examples both in graphical and mathematical form. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Data mining; Frequent item sets; Pattern mining; Upper bound","Data mining; Forestry; Closed form solutions; FP tree; Greedy algorithms; Mathematical forms; Tree structures; Upper Bound; Trees (mathematics)",Article,Scopus,2-s2.0-85115215910
"Li H., He L.-Y., Yang J.-J.","57060906400;57208528191;57222466777;","Forecasting the medium-term performance of restructured tourism firms with an adaptive integrated predictor",2022,"Tourism Management","88",,"104436","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115025592&doi=10.1016%2fj.tourman.2021.104436&partnerID=40&md5=026508a5434a803d358fc97dced66974","This paper investigates the forecasting of the medium-term performance of restructured tourism firms using a new adaptive integrated predictor. The predictor incorporates internal characteristics of firms (economic factors and management practice), restructuring characteristics (strategy types and payment means), and timing (e.g. calendar month) of the restructuring. The results show that, the firms that perform better after restructuring are those that: a) have experience of operating under liability, b) are able to acquire large amounts of undistributed profit and liquid assets, and c) are able to increase their turnover. Tourism firms that generate large proportion of profit with their equity value and rapid growth in their operational income will suffer losses due to restructuring. Forecasting the medium-term performance of restructured tourism firms needs to take account of time heterogeneity derived from the periodicity (seasonality) inherent to the tourism industry. The adaptive integrated predictor presented in this paper performs better than a series of benchmark models, because it is able to allow for both the high-risk nature and periodicity of the tourism industry. © 2021 Elsevier Ltd","Adaptive ensemble; Bayesian optimisation; Data mining; Merger and acquisition; Tourism forecasting","data mining; ensemble forecasting; industrial performance; merger; optimization; periodicity; tourism",Article,Scopus,2-s2.0-85115025592
"Tu B., Zhao Y., Yin G., Jiang N., Li G., Zhang Y.","57259025000;57259323300;57219225462;57200941236;55757220600;43362063400;","Research on intelligent calculation method of intelligent traffic flow index based on big data mining",2022,"International Journal of Intelligent Systems","37","2",,"1186","1203",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114933012&doi=10.1002%2fint.22665&partnerID=40&md5=24ffad42739c545c9483be54ae04d1a4","To understand the operating status of the road network and measure the traffic congestion problem, an intelligent calculation method for the intelligent traffic flow index based on big data mining is proposed. According to the error data discriminating rules, the error data in the traffic flow data is discriminated, all lanes are detected according to the data discriminating result, the traffic data of each lane are recorded in chronological order, and the traffic data is converted. Fuzzy data mining technology is used to predict the converted traffic flow, combined with traffic flow sequence segmentation and BP neural network model to realize the intelligent calculation of the smart traffic flow index. Experimental results show that the method can achieve accurate calculation of daily and weekly smart traffic index, and the calculation time is short, indicating that it can provide a reliable data basis for traffic operation state estimation and traffic early warning mechanism formulation. © 2021 Wiley Periodicals LLC",,"Backpropagation; Big data; Motor transportation; Traffic congestion; Accurate calculations; BP neural network model; Calculation time; Chronological order; Early warning mechanisms; Fuzzy-data mining; Intelligent traffics; Traffic operation; Data mining",Article,Scopus,2-s2.0-85114933012
"Mai T.T., Bezbradica M., Crane M.","57256575700;55372494000;8660401100;","Learning behaviours data in programming education: Community analysis and outcome prediction with cleaned data",2022,"Future Generation Computer Systems","127",,,"42","55",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114808128&doi=10.1016%2fj.future.2021.08.026&partnerID=40&md5=fa6db11cc9759cf339f3c2251d02bce3","Due to the COVID19 pandemic, more higher-level education programmes have moved to online channels, raising issues in monitoring students’ learning progress. Thanks to advances in online learning systems, however, student data can be automatically collected and used for the investigation and prediction of the students’ learning performance. In this article, we present a novel approach to analyse students’ learning behaviour, as well as the relationship between these behaviours and learning assessment results, in the context of programming education. A bespoke method has been built based on a combination of Random Matrix Theory, a Community Detection algorithm and statistical hypothesis tests. The datasets contain fine-grained information about students’ learning behaviours in two programming courses over two academic years with about 400 first-year students in a Medium-sized Metropolitan University in Dublin. The proposed method is a noval approach to data preprocessing which can improve the analysis and prediction based on learning behavioural datasets. The proposed approach deals with the issues of noise and trend effect in the data and has shown its success in detecting groups of students who have similar learning behaviours and outcomes. The higher performing groups have been found to be more active in practical-related activities throughout the course. Conversely, we found that the lower performing groups engage more with lecture notes instead of doing programming tasks. The learning behaviours data can also be used to predict students’ outcomes (i.e. Pass or Fail the terminal exams) at the early stages of the study, using popular machine learning classification techniques. © 2021 The Author(s)","Community detection; Educational data mining; Learning analytics; Machine learning; Random matrix theory","Forecasting; Machine learning; Matrix algebra; Online systems; Population dynamics; Random variables; Statistical tests; Students; Community analysis; Community detection; Education programmes; Educational data mining; Learning analytic; Learning behavior; Outcome prediction; Programming education; Random matrices theory; Student learning; Data mining",Article,Scopus,2-s2.0-85114808128
"Usmani M.S., Wang J., Ahmad N., Ullah Z., Iqbal M., Ismail M.","57221337120;39863638500;57193514375;57225724258;57209571359;57194153144;","Establishing a corporate social responsibility implementation model for promoting sustainability in the food sector: a hybrid approach of expert mining and ISM–MICMAC",2022,"Environmental Science and Pollution Research","29","6",,"8851","8872",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114306498&doi=10.1007%2fs11356-021-16111-7&partnerID=40&md5=e71ad95667b536be16bec7d8e49a486e","Corporate social responsibility (CSR) is significantly related to food companies due to its prominent impact and greater dependency on the environment, economy, and society (triple bottom line — TBL). The CSR-related threats and opportunities’ scale are shifting from single companies to networks and supply chains of the food sector. In this regard, this study empirically evaluates CSR initiatives by using Interpretive Structural Modeling (ISM) and Matrice d’Impacts Croisés Multiplication Appliqués à un Classement (MICMAC) methodology. So to develop an ISM–MICMAC-based framework, at first, CSR initiatives were chosen from existing literature with experts’ advice. Later, MICMAC analysis results showed that “employee trainings and workshops” and “employee welfare and empowerment” are significant CSR initiatives that could help CSR’s integration in the food sector of Pakistan, whereas CSR initiatives “community betterment” and “contribution towards economic development” proved least significant in the model. This study recommends that food sector firms should promote employee-based strategies in the firms. Moreover, the empirical findings of this study help to better understand CSR initiatives and their role in the implementation of CSR in the food sector of developing countries. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Corporate social responsibility; Developing countries; Food sector; Initiatives; ISM–MICMAC; Pakistan; Prioritizing; Triple bottom line","corporate social responsibility; data management; data mining; developing world; economic development; firm size; model; spatiotemporal analysis; sustainability; Pakistan; food; Pakistan; social responsibility; Food; Pakistan; Social Responsibility",Article,Scopus,2-s2.0-85114306498
"Sreekantha Reddy S., Yammani C.","57218687308;54396614700;","Parameter extraction of single-diode photovoltaic module using experimental current–voltage data",2022,"International Journal of Circuit Theory and Applications","50","2",,"753","771",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114296595&doi=10.1002%2fcta.3133&partnerID=40&md5=b86a703ddafc723cebec172e6328badb","Accurate modeling of photovoltaic (PV) module allows proper load scheduling and avoids the uncertainties in grid. This paper uses forward-backward linear least square error (FB-LLSE) curve fitting method to estimate the parameters of PV module. The proposed method does not require any data sheet information and initial values of parameters. The proposed method utilizes the experimental data on I–V curve to extract the parameters of PV module. To validate the proposed method, three different PV modules, namely, Mono-crystalline STM6-40/36, Poly-crystalline SMP6-120/36, PWP-201, and one Real Time Clock (R.T.C.) France solar cell has been considered. The obtained results are compared with three existing methods in the literature. The performance of the proposed FB-LLSE method is studied with root mean square error (RMSE), mean absolute error (MAE), normalized root mean square error (NRMSE), and normalized mean absolute error (NMAE). The calculated with proposed FB-LLSEM is reduced by 10%, 7%, 2.26%, and 1.89%, respectively, for PWP-201, STMP-120/36, SMP6-40/36, and R.T.C France solar cell when compared to the LLSEM method available in the literature. The proposed FB-LLSE method is found to exhibit low value of RMSE for all three PV modules and one PV cell. © 2021 John Wiley & Sons, Ltd.",,"Curve fitting; Data mining; Errors; Least squares approximations; Mean square error; Photovoltaic cells; Uncertainty analysis; Accurate modeling; Curve fitting methods; Linear least squares; Load scheduling; Mean absolute error; Photovoltaic modules; Polycrystalline; Root mean square errors; Solar cells",Article,Scopus,2-s2.0-85114296595
"Wang L., Lin B., Chen R., Lu K.-H.","57243946100;57244230400;57226816972;57244230500;","Using data mining methods to develop manufacturing production rule in IoT environment",2022,"Journal of Supercomputing","78","3",,"4526","4549",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114161603&doi=10.1007%2fs11227-021-04034-6&partnerID=40&md5=1d7dd59ccc8fc7a43264af13aae714e8","In order to meet the needs of customers in an Internet of Things (IoT) environment, the traditional manufacturing production strategy has gradually shifted from mass production to a small number of diverse forms. In traditional industry, when the production type changes to a small number of diverse forms, the complexity of scheduling increases and the rules of adaptability between products and production lines is not easy to judge. However, in traditional production management scheduling, the adaptability of production lines is mostly planned based on past experience. If the number of orders is too large or the production schedule changes, errors will increase. This situation will cause the actual production situation to be far removed from the planned result, which will affect the schedule achievement and delivery time. The present paper reports research using association rules to explore production lines and apply logic to solve the problem of production rules between production lines and products in the car manufacturing industry. The results show that the application of data mining association rules has an accuracy above 87%. The application of data mining can provide manufacturing production rules to assist managers to make better decisions in the IoT environment and to reduce the time required for manufacturing production. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Association rules; Car manufacturing production; Data mining; Internet of things (IoT)","Association rules; Automobile manufacture; Industrial research; Internet of things; Production control; Scheduling; Data mining methods; Internet of Things (IOT); Mining associations; Production management; Production schedule; Production strategy; Traditional industry; Traditional manufacturing; Data mining",Article,Scopus,2-s2.0-85114161603
"Meesala S.R., Subramanian S.","57230338700;57213723241;","Feature based opinion analysis on social media tweets with association rule mining and multi-objective evolutionary algorithms",2022,"Concurrency and Computation: Practice and Experience","34","3","e6586","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113353050&doi=10.1002%2fcpe.6586&partnerID=40&md5=2699d639c3d0b96640e6dcaa6019f665","Social media platform has achieved wide popularity in presenting the user-generated information online. The proliferation of user-generated content through social networking sites can enhance the existing transportation system. This work adds to the existing research by proposing a novel system to assess transit rider's reviews on the quality of transport services using Twitter information. A novel framework of a multi-objective evolutionary approach with association rule mining is proposed for feature-based opinion analysis on social media transit reviews. Feature-based opinion analysis is performed in two steps such as feature extraction and opinion analysis. First, the corpus of association rules is generated with opinion analysis, morphological and syntactic analysis, and semantic representation. Then, multi-objective flower pollination, multi-objective gray wolf, multi-objective moth flame, and multi-objective cat swarm optimization are applied to discover high-quality association rules on the transit user's opinion. The experimental results indicate that multi-objective cat swarm optimization using association rule mining performs better in terms of confidence, coverage, the interestingness of the rules, computational time, mean average support, and mean average confidence than the other proposed as existing approaches. It is observed that MCSO-ARM achieved an improvement in the range of 0.21%–0.27% in terms of confidence, 0.10%–0.13% in coverage, 0.10%–0.25% in interestingness, the computational time of 27.5 s, mean average support value of 0.10%–0.30% and mean of 0.20%–0.30% compared to the existing MPSO-ARM and other proposed MGWO-ARM, MMFO-ARM, and MFPO-ARM approaches. © 2021 John Wiley & Sons Ltd.",,"Association rules; Data mining; Multiobjective optimization; Semantics; Social networking (online); Syntactics; Computational time; Multi objective evolutionary algorithms; Multi-objective evolutionary; Semantic representation; Social media platforms; Social networking sites; Transportation system; User-generated content; Evolutionary algorithms",Article,Scopus,2-s2.0-85113353050
"Permiakova O., Burger T.","57200245016;23481175000;","Sketched Stochastic Dictionary Learning for large-scale data and application to high-throughput mass spectrometry",2022,"Statistical Analysis and Data Mining","15","1",,"43","56",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113158471&doi=10.1002%2fsam.11542&partnerID=40&md5=5bb80edbcca892d983ce8961dfec1c1e","Factorization of large data corpora has emerged as an essential technique to extract dictionaries (sets of patterns that are meaningful for sparse encoding). Following this line, we present a novel algorithm based on compressive learning theory. In this framework, the (arbitrarily large) dataset of interest is replaced by a fixed-size sketch resulting from a random sampling of the data distribution characteristic function. We apply our algorithm to the extraction of chromatographic elution profiles in mass spectrometry data, where it demonstrates its efficiency and interest compared to other related algorithms. © 2021 Wiley Periodicals LLC.",,"Distribution functions; Large dataset; Mass spectrometry; Stochastic systems; Data distribution; Dictionary learning; Elution profiles; Its efficiencies; Large scale data; Mass spectrometry data; Related algorithms; Sets of patterns; Data mining",Article,Scopus,2-s2.0-85113158471
"Wu J.M.-T., Wei M., Wu M.-E., Tayeb S.","57192413745;57215828588;18635615600;57192417165;","Top-k dominating queries on incomplete large dataset",2022,"Journal of Supercomputing","78","3",,"3976","3997",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112763974&doi=10.1007%2fs11227-021-04005-x&partnerID=40&md5=e56fb16177dd822e06b8b6f93bdd4d25","Top-k dominating (TKD) query is one of the methods to find the interesting objects by returning the k objects that dominate other objects in a given dataset. Incomplete datasets have missing values in uncertain dimensions, so it is difficult to obtain useful information with traditional data mining methods on complete data. BitMap Index Guided Algorithm (BIG) is a good choice for solving this problem. However, it is even harder to find top-k dominance objects on incomplete big data. When the dataset is too large, the requirements for the feasibility and performance of the algorithm will become very high. In this paper, we proposed an algorithm to apply MapReduce on the whole process with a pruning strategy, called Efficient Hadoop BitMap Index Guided Algorithm (EHBIG). This algorithm can realize TKD query on incomplete datasets through BitMap Index and use MapReduce architecture to make TKD query possible on large datasets. By using the pruning strategy, the runtime and memory usage are greatly reduced. What’s more, we also proposed an improved version of EHBIG (denoted as IEHBIG) which optimizes the whole algorithm flow. Our in-depth work in this article culminates with some experimental results that clearly show that our proposed algorithm can perform well on TKD query in an incomplete large dataset and shows great performance in a Hadoop computing cluster. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Big data framework; Dominance relationship; Incomplete data; MapReduce; Top-textitk dominating query","Cluster computing; Clustering algorithms; Data mining; Bitmap indexes; Computing clusters; Data mining methods; Large datasets; Missing values; Pruning strategy; Runtime and memory usage; Top-k dominating queries; Large dataset",Article,Scopus,2-s2.0-85112763974
"Wen H., Zheng W., Li M., Li Q., Liu Q., Zhou J., Liu Z., Chen X.","56770571700;57226772477;57226768481;56574506100;57221399237;36702347500;56730018900;56816037600;","Multiparametric Quantitative US Examination of Liver Fibrosis: A Feature-Engineering and Machine-Learning Based Analysis",2022,"IEEE Journal of Biomedical and Health Informatics","26","2",,"715","726",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112604116&doi=10.1109%2fJBHI.2021.3100319&partnerID=40&md5=8f240240bc6fd97c1530e8110cf6f6ae","Quantitative ultrasound (QUS), which attempts to extract quantitative features from the US radiofrequency (RF) or envelope data for tissue characterization, is becoming a promising technique for noninvasive assessments of liver fibrosis. However, the number of feature variables examined and finally used in the existing QUS methods is typically small, limiting the diagnostic performance. Therefore, this paper devises a new multiparametric QUS (MP-QUS) method which enables the extraction of a large number of feature variables from US RF signals and allows for the use of feature-engineering and machine-learning based algorithms for liver fibrosis assessment. In the MP-QUS, eighty-four feature variables were extracted from multiple QUS parametric maps derived from the RF signals and the envelope data. Afterwards, feature reduction and selection were performed in turn to remove the feature redundancy and identify the best combination of features in the reduced feature set. Finally, a variety of machine-learning algorithms were tested for fibrosis classification with the selected features, based on the results of which the optimal classifier was established. The performance of the proposed MP-QUS method for staging liver fibrosis was evaluated on an animal model, with histologic examination as the reference standard. The mean accuracy, sensitivity, specificity and area under the receiver-operating-characteristic curve achieved by MP-QUS are respectively 83.38%, 86.04%, 80.82%, and 0.891 for recognizing significant liver fibrosis, and 85.50%, 88.92%, 85.24%, and 0.924 for diagnosing liver cirrhosis. The proposed MP-QUS method paves a way for its future extension to assess liver fibrosis in human subjects. © 2013 IEEE.","feature engineering; Liver fibrosis; machine learning; quantitative ultrasound; radiomics","Biomedical signal processing; Data mining; Machine learning; Noninvasive medical procedures; Ultrasonic applications; Diagnostic performance; Feature engineerings; Non-invasive assessments; Quantitative features; Quantitative ultrasounds; Radio-frequency datum; Receiver operating characteristic curves; Tissue characterization; Learning algorithms; dimethylnitrosamine demethylase; pentobarbital; algorithm; animal cell; animal experiment; animal model; animal tissue; Article; attenuation; Bayesian learning; body weight; classifier; clinical examination; controlled study; decision tree; diffusion weighted imaging; discriminant analysis; echography; feature extraction; gray matter; histogram; histology; histopathology; human; image segmentation; inflammation; learning algorithm; liver biopsy; liver cell carcinoma; liver cirrhosis; liver fibrosis; liver injury; machine learning; male; minimum inhibitory concentration; nonhuman; quantitative analysis; radiomics; random forest; rat; support vector machine; tissue characterization; white matter",Article,Scopus,2-s2.0-85112604116
"Kishk A.M., Badawy M., Ali H.A., Saleh A.I.","57226645145;24723484700;57211770938;22434197300;","A new traffic congestion prediction strategy (TCPS) based on edge computing",2022,"Cluster Computing","25","1",,"49","75",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112176707&doi=10.1007%2fs10586-021-03377-2&partnerID=40&md5=1d9fb9bb09dd89185703eab480d581ff","Real-time accurate traffic congestion prediction can enable Intelligent traffic management systems (ITMSs) that replace traditional systems to improve the efficiency of traffic and reduce traffic congestion. The ITMS consists of three main layers, which are: Internet of Things (IoT), edge, and cloud layers. Edge can collect real-time data from different routes through IoT devices such as wireless sensors, and then it can compute and store this collected data before transmitting them to the cloud for further processing. Thus, an edge is an intermediate layer between IoT and cloud layers that can receive the transmitted data through IoT to overcome cloud challenges such as high latency. In this paper, a novel real-time traffic congestion prediction strategy (TCPS) is proposed based on the collected data in the edge’s cache server at the edge layer. The proposed TCPS contains three stages, which are: (i) real-time congestion prediction (RCP) stage, (ii) congestion direction detection (CD2) stage, and (iii) width change decision (WCD) stage. The RCP aims to predict traffic congestion based on the causes of congestion in the hotspot using a fuzzy inference system. If there is congestion, the CD2 stage is used to detect the congestion direction based on the predictions from the RCP by using the Optimal Weighted Naïve Bayes (OWNB) method. The WCD stage aims to prevent the congestion occurrence in which it is used to change the width of changeable routes (CR) after detecting the direction of congestion in CD2. The experimental results have shown that the proposed TCPS outperforms other recent methodologies. TCPS provides the highest accuracy, precision, and recall. Besides, it provides the lowest error, with values equal to 95%, 74%, 75%, and 5% respectively. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Congestion prediction; Data mining; Edge computing; IoT; Traffic management","Edge computing; Forecasting; Fuzzy inference; Internet of things; Real time systems; Congestion prediction; Direction detections; Fuzzy inference systems; Intelligent traffic management; Intermediate layers; Internet of Things (IOT); Traditional systems; Wireless sensor; Traffic congestion",Article,Scopus,2-s2.0-85112176707
"Shehab N., Badawy M., Ali H.A.","57211989263;24723484700;57211770938;","Toward feature selection in big data preprocessing based on hybrid cloud-based model",2022,"Journal of Supercomputing","78","3",,"3226","3265",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110958929&doi=10.1007%2fs11227-021-03970-7&partnerID=40&md5=e0bb9092ce1222f82e405b86d1d52892","Recently, big data are widely noticed in many fields like machine learning, pattern recognition, medical, financial, and transportation fields. Data analysis is crucial to converting data into more specific information fed to the decision-making systems. With the diverse and complex types of datasets, knowledge discovery becomes more difficult. One solution is to use feature subset selection preprocessing that reduces this complexity, so the computation and analysis become convenient. Preprocessing produces a reliable and suitable source for any data-mining algorithm. The effective features’ selection can improve a model’s performance and help us understand the characteristics and underlying structure of complex data. This study introduces a novel hybrid feature selection cloud-based model for imbalanced data based on the k nearest neighbor algorithm. The proposed model showed good performance compared with the simple weighted nearest neighbor. The proposed model combines the firefly distance metric and the Euclidean distance used in the k nearest neighbor. The experimental results showed good insights in both time usage and feature weights compared with the weighted nearest neighbor. It also showed improvement in the classification accuracy by 12% compared with the weighted nearest neighbor algorithm. And using the cloud-distributed model reduced the processing time up to 30%, which is deliberated to be substantial compared with the recent state-of-the-art methods. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Analysis; Big data; Classification; Cloud; Feature selection; Firefly; WKNN","Big data; Decision making; Feature extraction; Motion compensation; Nearest neighbor search; Classification accuracy; Data mining algorithm; Decision-making systems; Distributed modeling; Feature subset selection; Hybrid feature selections; K nearest neighbor algorithm; Nearest neighbor algorithm; Data mining",Article,Scopus,2-s2.0-85110958929
"Khushaba R.N., Melkumyan A., Hill A.J.","23390004100;55951358300;7403278832;","A Machine Learning Approach for Material Type Logging and Chemical Assaying from Autonomous Measure-While-Drilling (MWD) Data",2022,"Mathematical Geosciences","54","2",,"285","315",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110871945&doi=10.1007%2fs11004-021-09970-w&partnerID=40&md5=685ee085332822f1ce097f3b88504628","Understanding the structure and mineralogical composition of a region is an essential step in mining, both during exploration (before mining) and in the mining process. During exploration, sparse but high-quality data are gathered to assess the overall orebody. During the mining process, boundary positions and material properties are refined as the mine progresses. This refinement is facilitated through drilling, material logging, and chemical assaying. Material type logging suffers from a high degree of variability due to factors such as the diversity in mineralization and geology, the subjective nature of human measurement even by experts, and human error in manually recording results. While laboratory-based chemical assaying is much more precise, it is time-consuming and costly and does not always capture or correlate boundary positions between all material types. This leads to significant challenges and financial implications for the industry, as the accuracy of production blasthole logging and assaying processes is essential for resource evaluation, planning, and execution of mine plans. To overcome these challenges, this work reports on a pilot study to automate the process of material logging and chemical assaying. A machine learning approach has been trained on features extracted from measurement-while-drilling (MWD) data, logged from autonomous drilling systems (ADS). MWD data facilitate the construction of profiles of physical drilling parameters as a function of hole depth. A hypothesis is formed to link these drilling parameters to the underlying mineral composition. The results of the pilot study discussed in this paper demonstrate the feasibility of this process, with correlation coefficients of up to 0.92 for chemical assays and 93% accuracy for material detection, depending on the material or assay type and their generalization across the different spatial regions. The results achieved are significant, showing opportunities to guide further drilling processes, provide chemistry data with downhole resolution, and continuously update mine plans as the mine progresses. © 2021, International Association for Mathematical Geosciences.","Logging and assaying; Machine learning; Measurement-while-drilling; Mining","Chemical detection; Data mining; Drilling machines (machine tools); Infill drilling; Machine learning; Ore deposits; Turing machines; Correlation coefficient; Drilling parameters; Financial implications; High degree of variability; Machine learning approaches; Measurement while drillings; Mineral composition; Mineralogical compositions; Logging while drilling",Article,Scopus,2-s2.0-85110871945
"Jenkins P.R., Caballero W.N., Hill R.R.","57201478160;57204003678;7404752817;","Predicting success in United States Air Force pilot training using machine learning techniques",2022,"Socio-Economic Planning Sciences","79",,"101121","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110615532&doi=10.1016%2fj.seps.2021.101121&partnerID=40&md5=b8c4cbd9e39771b9af77893df29b9a22","The chronic pilot shortage that has plagued the United States Air Force over the past three years poses a national-level problem that senior military members are working to overcome. Unfortunately, not all pilot candidates successfully complete the necessary training requirements to become fully qualified Air Force pilots, which wastes critical time and resources and only further exacerbates the pilot shortage problem. Therefore, it is important for the Air Force to carefully consider whom they select to attend pilot training. This research examines historical specialized undergraduate pilot training (SUPT) candidate data leveraging a variety of machine learning techniques to obtain insights on candidate success. Computational experimentation is performed to determine how selected machine learning techniques and their respective hyperparameters affect solution quality. Results reveal that the extremely randomized tree machine learning technique can achieve nearly 94% accuracy in predicting candidate success. Additional analysis indicates degree type and commissioning source are the most important features in determining candidate success. Ultimately, this research can inform the modification of future SUPT candidate selection criteria and other related Air Force personnel policies. © 2021","Educational data mining; Human resource management; Machine learning; Pilot training","data mining; education; human resource; machine learning; United States",Article,Scopus,2-s2.0-85110615532
"Barrientos R.J., Riquelme J.A., Hernández-García R., Navarro C.A., Soto-Silva W.","14053682600;57218707943;56991037500;57197033980;56986493900;","Fast kNN query processing over a multi-node GPU environment",2022,"Journal of Supercomputing","78","2",,"3045","3071",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110291554&doi=10.1007%2fs11227-021-03975-2&partnerID=40&md5=752b1ba3a62467c206aaf810333eb307","The kNN (k nearest-neighbors) search is currently applied in a wide range of applications, such as data mining, multimedia, information retrieval, machine learning, pattern recognition, among others. Most of the solutions for this type of search are restricted to metric spaces or limited to use low dimension data. Our proposed algorithm uses as input a set of values (or measures) and returns the K lowest values from that set and can be used with measures obtained from metric and non-metric spaces or also from high dimensional databases. In this work, we introduce a novel GPU-based exhaustive algorithm to solve kNN queries, which is composed of two steps. The first is based on pivots to reduce the range of search, and the second one uses a set of heaps as auxiliary structures to return the final results. We also extended our algorithm to be able to use a multi-GPU platform and a multi-node/multi-GPU platform. To the best of our knowledge, taking account of the state-of-the-art technical literature, this work uses the most extensive database (in terms of data amount) to process a kNN query using up to 13,189 million of elements and achieving a speed-up up to 1843× when using a 5-nodes/20-GPUs platform. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Exhaustive search; GPU; kNN; Multi-GPU; Multi-node","Graphics processing unit; Nearest neighbor search; Pattern recognition; Query processing; Set theory; Topology; Dimension Data; High-dimensional; K-nearest neighbors; Metric spaces; Multi-nodes; Non-metric spaces; State of the art; Technical literature; Data mining",Article,Scopus,2-s2.0-85110291554
"Cui Y., Gan W., Lin H., Zheng W.","57222863442;56183769400;57222871151;56583141000;","FRI-miner: fuzzy rare itemset mining",2022,"Applied Intelligence","52","3",,"3387","3402",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109333168&doi=10.1007%2fs10489-021-02574-1&partnerID=40&md5=f9abbf7a2e8abc08ac85443c8929fabf","Data mining is a widely used technology for various real-life applications of data analytics and is important to discover valuable association rules in transaction databases. Interesting itemset mining plays an important role in many real-life applications, such as market, e-commerce, finance, and medical treatment. To date, various data mining algorithms based on frequent patterns have been widely studied, but there are a few algorithms that focus on mining infrequent or rare patterns. In some cases, infrequent or rare itemsets and rare association rules also play an important role in real-life applications. In this paper, we introduce a novel fuzzy-based rare itemset mining algorithm called FRI-Miner, which discovers valuable and interesting fuzzy rare itemsets in a quantitative database by applying fuzzy theory with linguistic meaning. Additionally, FRI-Miner utilizes the fuzzy-list structure to store important information and applies several pruning strategies to reduce the search space. The experimental results show that the proposed FRI-Miner algorithm can discover fewer and more interesting itemsets by considering the quantitative value in reality. Moreover, it significantly outperforms state-of-the-art algorithms in terms of effectiveness (w.r.t. different types of derived patterns) and efficiency (w.r.t. running time and memory usage). © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Fuzzy data mining; Fuzzy-set theory; Quantitative data; Rare pattern","Association rules; Data Analytics; Miners; Data mining algorithm; Medical treatment; Quantitative database; Quantitative values; Rare association rules; Real-life applications; State-of-the-art algorithms; Transaction database; Data mining",Article,Scopus,2-s2.0-85109333168
"Li J., Chiu B., Shang S., Shao L.","56609767400;57195415135;55803101900;55643855000;","Neural Text Segmentation and its Application to Sentiment Analysis",2022,"IEEE Transactions on Knowledge and Data Engineering","34","2",,"828","842",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108971827&doi=10.1109%2fTKDE.2020.2983360&partnerID=40&md5=d5fc7be978f95da142510ace66d66002","Text segmentation is a fundamental task in natural language processing. Depending on the levels of granularity, the task can be defined as segmenting a document into topical segments, or segmenting a sentence into elementary discourse units (EDUs). Traditional solutions to the two tasks heavily rely on carefully designed features. The recently proposed neural models do not need manual feature engineering, but they either suffer from sparse boundary tags or cannot efficiently handle the issue of variable size output vocabulary. In light of such limitations, we propose a generic end-to-end segmentation model, namely {mathrm{S}scriptstyle{mathrm{EG}}}{mathrm{B}scriptstyle{mathrm{OT}}}S EG B OT , which first uses a bidirectional recurrent neural network to encode an input text sequence. {mathrm{S}scriptstyle{mathrm{EG}}}{mathrm{B}scriptstyle{mathrm{OT}}}S EG B OT then uses another recurrent neural networks, together with a pointer network, to select text boundaries in the input sequence. In this way, {mathrm{S}scriptstyle{mathrm{EG}}}{mathrm{B}scriptstyle{mathrm{OT}}}S EG B OT does not require any hand-crafted features. More importantly, {mathrm{S}scriptstyle{mathrm{EG}}}{mathrm{B}scriptstyle{mathrm{OT}}}S EG B OT inherently handles the issue of variable size output vocabulary and the issue of sparse boundary tags. In our experiments, {mathrm{S}scriptstyle{mathrm{EG}}}{mathrm{B}scriptstyle{mathrm{OT}}}S EG B OT outperforms state-of-the-art models on two tasks: document-level topic segmentation and sentence-level EDU segmentation. As a downstream application, we further propose a hierarchical attention model for sentence-level sentiment analysis based on the outcomes of {mathrm{S}scriptstyle{mathrm{EG}}}{mathrm{B}scriptstyle{mathrm{OT}}}S EG B OT. The hierarchical model can make full use of both word-level and EDU-level information simultaneously for sentence-level sentiment analysis. In particular, it can effectively exploit EDU-level information, such as the inner properties of EDUs, which cannot be fully encoded in word-level features. Experimental results show that our hierarchical model achieves new state-of-the-art results on the Movie Review and Stanford Sentiment Treebank benchmarks. © 1989-2012 IEEE.","hierarchical attention; Natural language processing; pointer networks; sentiment analysis; text segmentation","Data mining; Hierarchical systems; Image segmentation; Recurrent neural networks; Hierarchical attention; Hierarchical model; ITS applications; Pointer network; Sentence level; Sentiment analysis; State of the art; Text segmentation; Variable sizes; Word level; Sentiment analysis",Article,Scopus,2-s2.0-85108971827
"Qian Y., Tan C., DIng D., Li H., Mamoulis N.","57195491391;57194212684;57221142892;57211503963;6701782749;","Fast and Secure Distributed Nonnegative Matrix Factorization",2022,"IEEE Transactions on Knowledge and Data Engineering","34","2",,"653","666",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108909645&doi=10.1109%2fTKDE.2020.2985964&partnerID=40&md5=f7e983b4aa57e65495f9675ce40f0f52","Nonnegative matrix factorization (NMF) has been successfully applied in several data mining tasks. Recently, there is an increasing interest in the acceleration of NMF, due to its high cost on large matrices. On the other hand, the privacy issue of NMF over federated data is worthy of attention, since NMF is prevalently applied in image and text analysis which may involve leveraging privacy data (e.g, medical image and record) across several parties (e.g., hospitals). In this paper, we study the acceleration and security problems of distributed NMF. First, we propose a distributed sketched alternating nonnegative least squares(DSANLS) framework for NMF, which utilizes a matrix sketching technique to reduce the size of nonnegative least squares subproblems with a convergence guarantee. For the second problem, we show that DSANLS with modification can be adapted to the security setting, but only for one or limited iterations. Consequently, we propose four efficient distributed NMF methods in both synchronous and asynchronous settings with a security guarantee. We conduct extensive experiments on several real datasets to show the superiority of our proposed methods. The implementation of our methods is available at https://github.com/qianyuqiu79/DSANLS. © 1989-2012 IEEE.","Distributed nonnegative matrix factorization; matrix sketching; privacy","Data mining; Data privacy; Matrix factorization; Medical imaging; Data mining tasks; Distributed nonnegative matrix factorization; High costs; matrix; Matrix sketching; Nonnegative least squares; Nonnegative matrix factorization; Privacy; Privacy issue; Sketchings; Matrix algebra",Article,Scopus,2-s2.0-85108909645
"Balasundaram B., Borrero J.S., Pan H.","9243894200;37006468400;57224907699;","Graph signatures: Identification and optimization",2022,"European Journal of Operational Research","296","3",,"764","775",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108562216&doi=10.1016%2fj.ejor.2021.03.051&partnerID=40&md5=477ba03370cf1b765e50f6255b285af7","We introduce a new graph-theoretic paradigm called a graph signature that describes persistent patterns in a sequence of graphs. This framework is motivated by the need to detect subgraphs of significance in temporal networks, e.g., social and biological networks that evolve over time. Because the subgraphs of interest may not all “look alike” in the snapshots of the temporal network, the framework deems a subgraph to be persistent if it satisfies one of several preselected properties in each snapshot of a consecutive subsequence. The persistency requirement is parameterized by the length of this subsequence. This discrete mathematical framework can be viewed more broadly as a way to generalize classical graph properties and invariants associated with a single graph to a sequence of graphs. In this introductory article, we formulate the graph signature identification problem as a mixed-integer program and propose an algorithmic framework based on dynamic programming. This methodology is applicable to any collection of mixed-integer representable graph properties. We also demonstrate how this framework can be tailored to exploit property-specific decomposition and scale reduction techniques through three different computational case-studies. Our experiments show that the dynamic programming algorithm solves this problem across most instances in our test bed to optimality. Moreover, for the instances in our test bed, the optimal signature sizes are comparable to those of their static counterparts, suggesting that our new framework can identify subgraphs of significance in complex dynamic networks. © 2021 Elsevier B.V.","Cross-graph mining; Frequent subgraph mining; Networks; Relational; Temporal","Dynamic programming; Equipment testing; Graph theory; Integer programming; Networks (circuits); Cross-graph mining; Frequent subgraph mining; Graph properties; Network; Property; Relational; Signature identification; Subgraphs; Temporal; Temporal networks; Data mining",Article,Scopus,2-s2.0-85108562216
"Zhang Y., Wu B., Tan L., Liu J.","57200519275;57223049002;57218304007;57196290878;","Information visualization analysis based on historical data",2022,"Multimedia Tools and Applications","81","4",,"4735","4751",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106300438&doi=10.1007%2fs11042-021-11030-8&partnerID=40&md5=da76fe4183d494420a1a9d1534873fb7","Visual expression is increasingly used in historical research due to its intuitiveness and distinctness. However, most of the common research contents focus on the spatial concept, but lack the visualization analysis of the attribute characteristics of the research elements. In order to achieve this goal, based on a case study of the coastal military defense system in Ming Dynasty, the Geographic Information System (GIS) platform was adopted to reconstruct the historical map and its spatial data were extracted. On this foundation, the attribute characteristics of the military settlements, accessibility, was quantified by constructing a hierarchy evaluation model, and then the results were projected into the spatial geographic coordinates to realize the visualization of the accessibility of the military settlements in Ming Dynasty. The results showed that the combined method of quantification and visualization not only enabled more comprehensive and intuitive display of historical information, but also promoted data extraction and correlation analysis, creating a possibly for more in-depth future research. © 2021, The Author(s).","Coastal defense system; Data quantification; GIS; Historical data; Visualization","Data mining; Information systems; Military mapping; Visualization; Correlation analysis; Evaluation modeling; Geographic coordinates; Historical information; Historical research; Information visualization; Military defense; Visualization analysis; Data visualization",Article,Scopus,2-s2.0-85106300438
"Sprecher B., Verhagen T.J., Sauer M.L., Baars M., Heintz J., Fishman T.","56096074600;57217824023;57221784463;57223181666;24279572400;56125365000;","Material intensity database for the Dutch building stock: Towards Big Data in material stock analysis",2022,"Journal of Industrial Ecology","26","1",,"272","280",,5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105154109&doi=10.1111%2fjiec.13143&partnerID=40&md5=5dd3541636c7fb8314d3c2c7fe931984","Re-use and recycling in the construction sector is essential to keep resource use in check. Data availability about the material contents of buildings is significant challenge for planning future re-use potentials. Compiling material intensity (MI) data is time and resource intensive. Often studies end up with only a handful of datapoints. In order to adequately cover the diversity of buildings and materials found in cities, and accurately assess material stocks at detailed spatial scopes, many more MI datapoints are needed. In this work, we present a database on the material intensity of the Dutch building stock, containing 61 large-scale demolition projects with a total of 781 datapoints, representing more than 306,000 square meters of built floor space. This dataset is representative of the types of buildings being demolished in the Netherlands. Our data were empirically sourced in collaboration with a demolition company that explicitly focuses on re-using and recycling materials and components. The dataset includes both the structural building materials and component materials, and covers a wide range of building types, sizes, and construction years. Compared to the existing literature, this paper adds significantly more datapoints, and more detail to the different types of materials found in demolition streams. This increase in data volume is a necessary step toward enabling big data methods, such as data mining and machine learning. These methods could be used to uncover previously unrecognized patters in material stocks, or more accurately estimate material stocks in locations that have only sparse data available. This article met the requirements for a Gold-Gold JIE data openness badge described at http://jie.click/badges. © 2021 The Authors. Journal of Industrial Ecology published by Wiley Periodicals LLC on behalf of Yale University","circular economy; construction; industrial ecology; material stocks and flows; urban mining","Big data; Construction industry; Demolition; Gold; Recycling; Building stocks; Component materials; Construction sectors; Data availability; Demolition projects; Material content; Recycling materials; Structural buildings; Data mining; building; building code; building construction; data mining; database; demolition; resource use; Netherlands",Article,Scopus,2-s2.0-85105154109
"Hamdi A., Shaban K., Erradi A., Mohamed A., Rumi S.K., Salim F.D.","57218862397;8938806700;11641389000;55763482600;57204282860;7801420000;","Spatiotemporal data mining: a survey on challenges and open problems",2022,"Artificial Intelligence Review","55","2",,"1441","1488",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104809912&doi=10.1007%2fs10462-021-09994-y&partnerID=40&md5=a0b77951535fa762095d1a4f0dc73f6d","Spatiotemporal data mining (STDM) discovers useful patterns from the dynamic interplay between space and time. Several available surveys capture STDM advances and report a wealth of important progress in this field. However, STDM challenges and problems are not thoroughly discussed and presented in articles of their own. We attempt to fill this gap by providing a comprehensive literature survey on state-of-the-art advances in STDM. We describe the challenging issues and their causes and open gaps of multiple STDM directions and aspects. Specifically, we investigate the challenging issues in regards to spatiotemporal relationships, interdisciplinarity, discretisation, and data characteristics. Moreover, we discuss the limitations in the literature and open research problems related to spatiotemporal data representations, modelling and visualisation, and comprehensiveness of approaches. We explain issues related to STDM tasks of classification, clustering, hotspot detection, association and pattern mining, outlier detection, visualisation, visual analytics, and computer vision tasks. We also highlight STDM issues related to multiple applications including crime and public safety, traffic and transportation, earth and environment monitoring, epidemiology, social media, and Internet of Things. © 2021, The Author(s), under exclusive licence to Springer Nature B.V.","Challenges Issues; Data Mining; Research Problems; Spatial; Spatiotemporal","Open Data; Surveys; Visualization; Data characteristics; Environment monitoring; Interdisciplinarity; Multiple applications; Spatio-temporal data; Spatio-temporal data mining; Spatio-temporal relationships; Traffic and transportation; Data mining",Article,Scopus,2-s2.0-85104809912
"Pinar A., Boran F.E.","57215021733;25723017700;","A novel distance measure on q-rung picture fuzzy sets and its application to decision making and classification problems",2022,"Artificial Intelligence Review","55","2",,"1317","1350",,4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103661336&doi=10.1007%2fs10462-021-09990-2&partnerID=40&md5=ee524bf9dab62a9b5c23d310ab287bbd","In recent years, different higher order fuzzy sets have been introduced to better handle the uncertainty in many practical decision making and data mining problems. The recent proposal of higher order fuzzy set is q-rung picture fuzzy set (q-RPFS) modeled by three parameters positive, negative, and neutral membership function. One of the important topics in q-RPFS is distance measures which play a crucial role in many multi criteria decision making methods and data mining applications. In this paper, we introduce a novel distance measure for q-RPFS which is the combination of q-rung orthopair fuzzy set (q-ROFS) and picture fuzzy set (PFS). The proposed distance measure is used in q-rung picture fuzzy (q-RPF) ELECTRE integrated with TOPSIS as a new approach for group decision making in q-RPF environment. To demonstrate the effectiveness of our proposed method, a comparison is made with the q-RPF approach based on aggregation operators using a numerical example for decision making problem. Furthermore, the proposed distance measure is utilized in a q-RPF k nearest neighborhood (kNN) algorithm for classification. The proposed classification algorithm is applied to twenty UCI machine learning classification data sets. A comparison with other algorithms is performed and the results show that the proposed classification algorithm has the highest average classification accuracy. © 2021, The Author(s), under exclusive licence to Springer Nature B.V.","Classification; Distance measure; Multi-criteria decision making; Q-rung picture fuzzy set","Classification (of information); Clustering algorithms; Data mining; Fuzzy sets; Learning algorithms; Machine learning; Mathematical operators; Membership functions; Nearest neighbor search; Numerical methods; Classification accuracy; Classification algorithm; Data mining applications; Decision-making problem; Higher-order fuzzy sets; K-nearest neighborhoods; Machine learning classification; Multi-criteria decision making methods; Decision making",Article,Scopus,2-s2.0-85103661336
"Zeng Y., Yang Z., Zhang W., Li C.","57191256837;56171367500;57200954753;57194011327;","Application of processing technology based on skyline query in computer network",2022,"Neural Computing and Applications","34","4",,"2637","2647",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103430960&doi=10.1007%2fs00521-021-05931-w&partnerID=40&md5=cbc800408ca313d17e375bd77e61471c","With the rapid development of network data communication technology, the complexity of the network environment makes the data in the data stream have uncertain characteristics, and a large number of data streams are generated in many fields. Therefore, it is necessary to find an efficient and accurate processing method to process a large amount of data in a computer network. This paper takes the skyline probability calculation and the actual efficiency of the unknown object index structure in the skyline query processing technology on the uncertain data stream as the research object. Based on the skyline query technology, we study an efficient skyline query processing method on the unknown network traffic based on the model SUMG. The method includes two algorithms: dynamic modeling algorithm DMG and skyline query algorithm GST. The DMG algorithm samples the data in the sliding window of the uncertain data stream and establishes a model to convert the data stream into the parameter stream in the uncertain object probability density function, the GST algorithm establishes the R-tree index structure, which is in order to reduce the amount of calculation, it uses the parameter flow of the model. In both methods, the set of local skyline results is first obtained at the distributed node, and the skyline query is performed again on the union of the local skyline results to obtain the global skyline result set. The experimental results show that compared with the skyline query method BNL for unknown network traffic without an index structure, the SUMG method can not only effectively model the link-type unknown object to assist the skyline query, but also effectively prune the uncertain data object and improve the skyline. The distributed skyline query method can cope with the skyline query task on the distributed data stream. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Cauchy model; Computer network; E-Dsud algorithm; Skyline query","Computer networks; Data mining; Decision trees; Probability density function; Processing; Query processing; Trees (mathematics); Uncertainty analysis; Distributed data streams; Network environments; Processing method; Processing technologies; R-tree index structure; Skyline probabilities; Skyline query processing; Uncertain data streams; Data streams",Article,Scopus,2-s2.0-85103430960
"Ye C., Zhao H., Ma L., Jiang H., Li H., Wang R., Chapman M.A., Junior J.M., Li J.","23013109600;57190386810;57190372479;57192705984;57221311290;55717757900;57203027250;55640064500;57235557700;","Robust Lane Extraction from MLS Point Clouds Towards HD Maps Especially in Curve Road",2022,"IEEE Transactions on Intelligent Transportation Systems","23","2",,"1505","1518",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098760391&doi=10.1109%2fTITS.2020.3028033&partnerID=40&md5=901407be8561413734721b51b51ccb24","This article presents a semi-automated method to extract the lane features along the curved roads from mobile laser scanning (MLS) point clouds. The proposed method consists of four steps. After data pre-processing, a road edge detection algorithm is performed to distinguish road curbs and extract road surfaces. Then, textual and directional road markings such as arrows, symbols, and words, to inform drivers in necessary cases, are detected by intensity thresholding and conditional Euclidean clustering algorithms. Furthermore, lane markings are extracted by local intensity analysis and distance thresholding methods according to road design standards, because they are more regular along the road. Finally, centerline points on lanes are estimated based on the coordinates of extracted lane markings. Our method shows strong feasibility and robustness when creating high-definition (HD) maps from MLS data, by increasing the number of blocks in the curve and the distance threshold control in curved lane centerline extraction. Quantitative evaluations show that the average recall, precision, and F1-score obtained from four datasets for road marking extraction are 93.87%, 93.76%, and 93.73%, respectively. The generated lane centerlines are evaluated by overlaying them on manually labeled reference buffers from 4 cm resolution orthoimagery. The comparative study indicates that the proposed methods can achieve higher accuracy and robustness than most state-of-the-art methods. © 2000-2011 IEEE.","centerline; High-definition map; lane-level navigation; mobile laser scanning; point clouds; road marking","Clustering algorithms; Data mining; Digital television; Edge detection; Extraction; Highway markings; Highway planning; Roads and streets; Centerline extraction; Comparative studies; Data preprocessing; Intensity analysis; Quantitative evaluation; Reference buffers; State-of-the-art methods; Thresholding methods; Road and street markings",Article,Scopus,2-s2.0-85098760391
"Sun X., Ze B., Zhang L.-J., BaiMa Y.-Z., Zuo W., Zhao B., GeSang L.-B.","57219990889;57219993136;57219991798;57337252900;57202783232;55362268600;57219992158;","Hemorrhage Risk Profiles among Different Antithrombotic Regimens: Evidence from a Real-World Analysis of Postmarketing Surveillance Data",2022,"Cardiovascular Drugs and Therapy","36","1",,"103","112",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096437164&doi=10.1007%2fs10557-020-07110-w&partnerID=40&md5=3fac6c00a31eb3d55e0d5091dca9c27b","Background: Although the use of direct oral anticoagulants (DOACs) has been reported in patients with atrial fibrillation (AF), there is currently no consensus on the occurrence or characteristics of the hemorrhage risk in different antithrombotic regimens. Methods: Disproportionality and Bayesian analyses were performed in mining data of suspected hemorrhagic events after antithrombotic drug use from the FDA Adverse Event Reporting System (FAERS) from January 2004 to September 2019. The time to onset and fatality rate of hemorrhage following different antithrombotic regimens were also compared. Results: A total of 84,998 reports of hemorrhage-related adverse events with the use of antithrombotic drugs were identified. The patients included were mostly from the Americas (80.87%) and Europe (13.22%), with most data submitted by nonhealthcare professionals. Among the seven antithrombotic drug monotherapies, betrixaban had the highest association with hemorrhage based on the highest reporting odds ratio (ROR, 829.95; 95% CI = 113.61-6063.15), proportional reporting ratio (PRR, 24.68, χ2 = 804.24), and multi-item gamma Poisson shrinker (MGPS, 24.68, 95% one-sided CI = 4.67). The combination therapies of clopidogrel plus new oral anticoagulants had higher RORs, PRRs, and empirical Bayesian geometric means (EBGMs) than the antithrombotic drug monotherapies. Hemorrhage associated with rivaroxaban plus clopidogrel appeared to have an earlier onset (171 days vs 219 days, 95% two-sided CI =68.68-27.34, p < 0.0001) and a lower fatality rate (15.30% vs 17.74%, p<0.05) than that associated with rivaroxaban monotherapy. Conclusion: This study provides a relevant overview of the hemorrhagic complications/fatalities associated with different antithrombotic regimens in their real-world use. Among the combination therapies, clopidogrel plus DOACs were found to have stronger associations with hemorrhage than traditional dual antithrombotic therapies. Rivaroxaban showed a stronger association with hemorrhage than other antithrombotic drug monotherapies, and apixaban monotherapy appeared to have weaker associations with hemorrhage than others. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Adverse drug events; Antithrombotic; Data mining; Hemorrhage; Spontaneous reporting system","antithrombocytic agent; blood clotting factor 10a inhibitor; fibrinolytic agent; adolescent; adult; aged; Bayes theorem; bleeding; combination drug therapy; drug surveillance program; female; Food and Drug Administration; human; male; middle aged; postmarketing surveillance; retrospective study; United States; very elderly; young adult; Adolescent; Adult; Adverse Drug Reaction Reporting Systems; Aged; Aged, 80 and over; Bayes Theorem; Drug Therapy, Combination; Factor Xa Inhibitors; Female; Fibrinolytic Agents; Hemorrhage; Humans; Male; Middle Aged; Platelet Aggregation Inhibitors; Product Surveillance, Postmarketing; Retrospective Studies; United States; United States Food and Drug Administration; Young Adult",Article,Scopus,2-s2.0-85096437164
"Belay Gebremeskel G., Hailu B., Biazen B.","57212586387;57212585863;57212585669;","Architecture and optimization of data mining modeling for visualization of knowledge extraction: Patient safety care",2022,"Journal of King Saud University - Computer and Information Sciences","34","2",,"468","479",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077170654&doi=10.1016%2fj.jksuci.2019.12.001&partnerID=40&md5=a4d86f98a819aece1ba2e7a95148fcb9","Visualization of the knowledge extraction process is a front line to reveal the detail process and data structure, which is an advanced technique for the presentation of data modeling. However, the mechanisms for healthcare are challenging and dynamic processes to gain a clear insight or understanding of patient care. In this paper, we proposed a new approach of architecture and optimization of data mining modeling for visualization of knowledge extraction by analyzing clinical data sets to define the determinant attributes through modeling techniques. Therefore, architecture for the visualization of the knowledge extraction process is a systematic approach to support users to the best of their knowledge of the issues over the challenge of visualizing techniques. The proposed approach is capable and dynamic to handle and analyze large-scale data in its dimension and context. Such a variable is defined using various techniques to characterize them towards the detection of determinant variables as its influential circumstance. We focused on modeling based visualization as model representation, factor's interaction and integration. The detection process experimented in a different approach and justification as discussed in section five. The finding showed a deep understandability for an advanced and dynamic data mining modeling techniques to integrate applications with domain contexts for the optimal and understandable decision process. The strength of this approach is the depth for visualization towards the knowledge extraction process and its understandability for users as per their background and circumstances. It is also essential to inference for architecture based modeling and visualization for large scale data. Researchers, physicians, experts, and other users are the potentials to refer to these novel ideas and findings. © 2019 The Authors","Architectures; Clinical datasets; Data mining; Data visualization; Decision tree; Knowledge extraction; Pattern analysis",,Article,Scopus,2-s2.0-85077170654
"Wang Y., Hu L., Chen J., Ren Y.","55961816100;57211609425;57211600383;57211600649;","Health status diagnosis of distribution transformers based on big data mining",2022,"Transactions on Emerging Telecommunications Technologies","33","2","e3759","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074583531&doi=10.1002%2fett.3759&partnerID=40&md5=fd37c0338bdf00c5fe4ad944cb5a8166","In the fault detection technology of distribution transformer, the traditional artificial intelligence algorithm has been unable to achieve its efficient analysis and processing, but also unable to achieve the elimination of misdiagnosis caused by improper interval segmentation in the process of fault diagnosis of distribution transformer. In a word, the problem that always exists in transformer fault diagnosis technology is the problem of discretization of fault data. In order to solve this problem, this paper creatively proposes a health condition diagnosis method of distribution transformer based on large data. This method innovatively proposes that the dissolved gas analysis value of distribution transformer is the conditional attribute, and the fault type is the decision attribute, and the fault decision table is established. The continuous attribute data in the decision table are discretized by using the optimization behavior of large data sets. Subsequently, the discretized decision table is simplified by using the big data theory, and the decision table of fault diagnosis rules is established, which greatly simplifies the difficulty of attribute simplification of decision table and makes diagnosis more convenient. Finally, an example shows that the proposed method can effectively discretize and reduce samples. Compared with traditional methods, it improves the accuracy of fault diagnosis. © 2019 John Wiley & Sons, Ltd.",,"Artificial intelligence; Big data; Data mining; Decision tables; Decision theory; Electric transformers; Failure analysis; Artificial intelligence algorithms; Continuous attribute; Decision attribute; Detection technology; Dissolved gas analysis; Distribution transformer; Efficient analysis; Transformer fault diagnosis; Fault detection",Article,Scopus,2-s2.0-85074583531
"Ermatita, Sanmorino A., Samsuryadi, Rini D.P.","49961277000;55444319800;57203454126;55946417500;","Analyzing Factors Contributing to Research Performance using Backpropagation Neural Network and Support Vector Machine",2022,"KSII Transactions on Internet and Information Systems","16","1",,"153","172",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124588407&doi=10.3837%2ftiis.2022.01.009&partnerID=40&md5=a10def9fcc329e7ab7ff28c8716a8240","In this study, the authors intend to analyze factors contributing to research performance using Backpropagation Neural Network and Support Vector Machine. The analyzing factors contributing to lecturer research performance start from defining the features. The next stage is to collect datasets based on defining features. Then transform the raw dataset into data ready to be processed. After the data is transformed, the next stage is the selection of features. Before the selection of features, the target feature is determined, namely research performance. The selection of features consists of Chi-Square selection (U), and Pearson correlation coefficient (CM). The selection of features produces eight factors contributing to lecturer research performance are Scientific Papers (U: 154.38, CM: 0.79), Number of Citation (U: 95.86, CM: 0.70), Conference (U: 68.67, CM: 0.57), Grade (U: 10.13, CM: 0.29), Grant (U: 35.40, CM: 0.36), IPR (U: 19.81, CM: 0.27), Qualification (U: 2.57, CM: 0.26), and Grant Awardee (U: 2.66, CM: 0.26). To analyze the factors, two data mining classifiers were involved, Backpropagation Neural Networks (BPNN) and Support Vector Machine (SVM). Evaluation of the data mining classifier with an accuracy score for BPNN of 95 percent, and SVM of 92 percent. The essence of this analysis is not to find the highest accuracy score, but rather whether the factors can pass the test phase with the expected results. The findings of this study reveal the factors that have a significant impact on research performance and vice versa. Copyright © 2022 KSII.","Backpropagation; Factors contributing to research performance; Selection of features; Support vector machine","Backpropagation; Classification (of information); Correlation methods; Data mining; Feature extraction; Neural networks; Vectors; Back-propagation neural networks; Factor contributing to research performance; High-accuracy; Neural network and support vector machines; Pearson correlation coefficients; Research performance; Scientific papers; Selection of feature; Support vectors machine; Target feature; Support vector machines",Article,Scopus,2-s2.0-85124588407
"Ye X., Han M.-M.","57200435940;7402604202;","An improved feature extraction algorithm for insider threat using hidden Markov model on user behavior detection",2022,"Information and Computer Security","30","1",,"19","36",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099772532&doi=10.1108%2fICS-12-2019-0142&partnerID=40&md5=65af5bf379de0687f10fde7de5e9a750","Purpose: By using a new feature extraction method on the Cert data set and using a hidden Markov model (HMM) to model and analyze the behavior of users to distinguish whether the behavior is normal within a continuous period. Design/methodology/approach: Feature extraction of five parts of the time series by rules and sorting in chronological order. Use the obtained features to calculate the probability parameters required by the HMM model and establish a behavior model for each user. When the user has abnormal behavior, the model will return a very low probability value to distinguish between normal and abnormal information. Findings: Generally, HMM parameters are obtained by supervised learning and unsupervised learning, but the hidden state cannot be clearly defined. When the hidden state is determined according to the data set, the accuracy of the model will be improved. Originality/value: This paper proposes a new feature extraction method and analysis mode, which determines the shape of the hidden state according to the situation of the data set, making subsequent HMM modeling simple and efficient and in turn improving the accuracy of user behavior detection. © 2020, Emerald Publishing Limited.","Anomaly detection; Hidden Markov model; Insider threat detection; Viterbi algorithm","Behavioral research; Data mining; Extraction; Hidden Markov models; Abnormal behavior; Behavior model; Chronological order; Design/methodology/approach; Feature extraction algorithms; Feature extraction methods; Insider Threat; Low probability; Feature extraction",Article,Scopus,2-s2.0-85099772532
"Sameer S., Behadili S.F., Abd M.S., Salam A.","57291653200;57207243874;57459743600;57459743700;","CART_based Approach for Discovering Emerging Patterns in Iraqi Biochemical Dataset",2022,"Iraqi Journal of Science","63","1",,"353","362",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124964977&doi=10.24996%2fijs.2022.63.1.33&partnerID=40&md5=4bd2f50f99157c608869f2ca7b6163b6","This paper is intended to apply data mining techniques for real Iraqi biochemical dataset to discover hidden patterns within tests relationships. It is worth noting that preprocessing steps take remarkable efforts to handle this type of data, since it is pure data set with so many null values reaching a ratio of 94.8%, then it becomes 0% after achieving these steps. However, in order to apply Classification And Regression Tree (CART) algorithm, several tests were assumed as classes, because of the dataset was unlabeled. Which then enabled discovery of patterns of tests relationships, that consequently, extends its impact on patients' health, since it will assist in determining test values by performing only relevant tests. Therefore decreases the number of tests for patients. © 2022 University of Baghdad-College of Science. All rights reserved.","Biochemical; CART; Data mining; Iraq",,Article,Scopus,2-s2.0-85124964977
"Restrepo D.S., Pérez L.E., López D.M., Vargas-Cañas R., Osorio-Valencia J.S.","57450635800;57450097600;57451353300;35753897400;56994031300;","Multi-Dimensional Dataset of Open Data and Satellite Images for Characterization of Food Security and Nutrition",2022,"Frontiers in Nutrition","8",,"796082","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124527802&doi=10.3389%2ffnut.2021.796082&partnerID=40&md5=45d2dc6192b678b11c1b5504c47a42b6","Background: Nutrition is one of the main factors affecting the development and quality of life of a person. From a public health perspective, food security is an essential social determinant for promoting healthy nutrition. Food security embraces four dimensions: physical availability of food, economic and physical access to food, food utilization, and the sustainability of the dimensions above. Integrally addressing the four dimensions is vital. Surprisingly most of the works focused on a single dimension of food security: the physical availability of food. Objective: The paper proposes a multi-dimensional dataset of open data and satellite images to characterize food security in the department of Cauca, Colombia. Methods: The food security dataset integrates multiple open data sources; therefore, the Cross-Industry Standard Process for Data Mining methodology was used to guide the construction of the dataset. It includes sources such as population and agricultural census, nutrition surveys, and satellite images. Results: An open multidimensional dataset for the Department of Cauca with 926 attributes and 9 rows (each row representing a Municipality) from multiple sources in Colombia, is configured. Then, machine learning models were used to characterize food security and nutrition in the Cauca Department. As a result, The Food security index calculated for Cauca using a linear regression model (Mean Absolute Error of 0.391) is 57.444 in a range between 0 and 100, with 100 the best score. Also, an approach for extracting four features (Agriculture, Habitation, Road, Water) of satellite images were tested with the ResNet50 model trained from scratch, having the best performance with a macro-accuracy, macro-precision, macro-recall, and macro-F1-score of 91.7, 86.2, 66.91, and 74.92%, respectively. Conclusion: It shows how the CRISP-DM methodology can be used to create an open public health data repository. Furthermore, this methodology could be generalized to other types of problems requiring the creation of a dataset. In addition, the use of satellite images presents an alternative for places where data collection is challenging. The model and methodology proposed based on open data become a low-cost and effective solution that could be used by decision-makers, especially in developing countries, to support food security planning. Copyright © 2022 Restrepo, Pérez, López, Vargas-Cañas and Osorio-Valencia.","data mining; dataset; food security; machine learning; remote sensing; satellite imagery",,Article,Scopus,2-s2.0-85124527802
"Kim S., Jee Seol S., Byun J., Oh S.","57338334700;7003771512;7102897422;56991773300;","Extraction of diffractions from seismic data using convolutional U-net and transfer learning",2022,"Geophysics","87","2",,"V117","V129",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124345573&doi=10.1190%2fgeo2020-0847.1&partnerID=40&md5=f2c19998913a91276b650c5eceac321d","Diffraction images can be used for modeling reservoir heterogeneities at or below the seismic wavelength scale. However, the extraction of diffractions is challenging because their amplitude is weaker than that of overlapping reflections. Recently, deep-learning (DL) approaches have been used as a powerful tool for diffraction extraction. Most DL approaches use a classification algorithm that classifies pixels in the seismic data as diffraction, reflection, noise, or diffraction with reflection and takes whole values for the classified diffraction pixels. Thus, these DL methods cannot extract diffraction energy from pixels for which diffractions are masked by reflections. We have developed a DL-based diffraction extraction method that preserves the amplitude and phase characteristics of diffractions. Through the systematic generation of a training data set using synthetic modeling based on t-distributed stochastic neighbor embedding analysis, this technique extracts not only faint diffractions but also diffraction tails overlapped by strong reflection events. We also determine that the DL model pretrained with a basic synthetic data set can be applied to seismic field data through transfer learning. Because the diffractions extracted by our method preserve the amplitude and phase, they can be used for velocity model building and high-resolution diffraction imaging. © 2021 Society of Exploration Geophysicists.","common offset; diffraction; machine learning; separation; signal processing","Classification (of information); Data mining; Deep learning; Extraction; Geophysical prospecting; Historic preservation; Pixels; Seismic response; Seismic waves; Stochastic systems; Classification algorithm; Classifieds; Common offset; Diffraction images; Diffraction reflections; Learning approach; Reservoir heterogeneity; Signal-processing; Transfer learning; Wavelength scale; Diffraction; algorithm; classification; data set; machine learning; pixel; seismic data; seismic noise; seismic reflection; signal processing; wave diffraction",Article,Scopus,2-s2.0-85124345573
"Zhang C., Cai M., Zhao X., Wang D.","57216358943;57210170141;57223790143;57219486081;","Research on case preprocessing based on deep learning",2022,"Concurrency and Computation: Practice and Experience","34","2","e6214","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100721140&doi=10.1002%2fcpe.6214&partnerID=40&md5=213037fd9c1cc1e51c0ed0ef6a4abd87","Considering the problem of missing fields in the criminal case system, this article proposes a deep learning algorithm to extract the features of the case description and fill in the missing value. Due to Chinese expressions and characteristics of criminal cases, we make both character vectors and word vectors to present text embedding. Character vectors are from bert model. Word vector is trained by long short-term memory model with attention. The experiment uses 13,890 data totally. This work is an extension of our short conference proceeding paper. The results show that the combination of characters and words can effectively improve the accuracy of the conference paper by 9%. This is the first time to cascade the character and word dimensions on the criminal case information preprocess and it can provide higher quality data especially for the crime data mining. © 2021 John Wiley & Sons, Ltd.","case classification; data preprocessing; deep learning; theft cases","Crime; Data mining; Learning algorithms; Vectors; Case description; Character vectors; Conference papers; Crime data mining; Criminal case; Missing values; Quality data; Short term memory; Deep learning",Article,Scopus,2-s2.0-85100721140
"Hou W., Ji Z.","56465347500;57351900400;","Unbiased visualization of single-cell genomic data with SCUBI",2022,"Cell Reports Methods","2","1","100135","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123185696&doi=10.1016%2fj.crmeth.2021.100135&partnerID=40&md5=5fdba7d425d0b24b28b1afe4b2c5130e","Visualizing low-dimensional representations with scatterplots is a crucial step in analyzing single-cell genomic data. However, this visualization has significant biases. The first bias arises when visualizing the gene expression levels or the cell identities. The scatterplot only shows a subset of cells plotted last, and the cells plotted earlier are masked and unseen. The second bias arises when comparing the cell-type compositions across samples. The scatterplot is biased by the unbalanced total number of cells across samples. We developed SCUBI, an unbiased method that visualizes the aggregated information of cells within non-overlapping squares to address the first bias and visualizes the differences of cell proportions across samples to address the second bias. We show that SCUBI presents a more faithful visual representation of the information in a real single-cell RNA sequencing (RNA-seq) dataset and has the potential to change how low-dimensional representations are visualized in single-cell genomic data. © 2021 The Authors","data mining; single-cell genomics; unbiased; visualization",,Article,Scopus,2-s2.0-85123185696
"Kim J.-Y., Cho S.-B.","56526721100;7404884741;","A deep neural network ensemble of multimodal signals for classifying excavator operations",2022,"Neurocomputing","470",,,"290","299",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100594980&doi=10.1016%2fj.neucom.2020.01.127&partnerID=40&md5=d287b9b866f5a6f949005c88404e7480","The prognostics and health management (PHM) aims to provide a comprehensive solution for equipment health care. Classifying the operation mode of excavator, one of the tasks in the PHM, is important to evaluate the remaining useful lifetime. Several studies have been conducted to classify the operations with either video or sensor data, but they have several limitations to use only one type of data. A model trained with sensor data cannot classify the similar operations such as “digging” and “ditch digging”, whereas a model with video data is vulnerable to surrounding condition like weather. In this paper, to overcome these shortcomings, we propose a deep neural network ensemble called FusionNet that classifies the operations of excavator. Two models are trained with sensor data and video frames respectively, where the feature extractors are transferred to the FusionNet. The proposed network ensemble performs a flexible and well-optimized classification by automatically calculating weights according to the extracted feature vectors and combining them. To verify the proposed model, several experiments are conducted with the real-world data. The proposed model achieves the accuracy of 99.17% which outperforms the conventional methods. We also confirm that the proposed model can address the shortcomings of using only one type of data and maximize the benefits through the automatic weighting of extracted features. © 2021 Elsevier B.V.","Autoencoder; Classification; CNN-LSTM; Deep learning; Multimodal data; Sensor data; Video data","Construction equipment; Data mining; Deep neural networks; Excavation; Excavators; Conventional methods; Feature extractor; Feature vectors; Network ensemble; Neural network ensembles; Operation mode; Prognostics and health managements; Useful lifetime; Neural networks; accuracy; Article; autoencoder; classification; construction work and architectural phenomena; controlled study; convolutional neural network; deep learning; deep neural network; ensemble model; excavator operation; feature extraction; health care management; prognosis; sensor based model; statistical model; video based model",Article,Scopus,2-s2.0-85100594980
"Perez-Vilar S., Dores G.M., Marquez P.L., Ng C.S., Cano M.V., Rastogi A., Lee L., Su J.R., Duffy J.","36519543700;57205682974;57209009414;57222747115;7101754618;57364351200;7404388305;26639858900;37010893400;","Safety surveillance of meningococcal group B vaccine (Bexsero®), Vaccine Adverse Event Reporting System, 2015–2018",2022,"Vaccine","40","2",,"247","254",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123651947&doi=10.1016%2fj.vaccine.2021.11.071&partnerID=40&md5=608f5f17190aa593e43546f406936cd7","Background: Bexsero® (GlaxoSmithKline) is a four-component Neisseria meningitidis serogroup B vaccine (MenB-4C). It was licensed in the United States in 2015 for use among individuals ages 10–25 years. We aimed to assess the post-licensure safety profile of MenB-4C by examining reports received in the Vaccine Adverse Event Reporting System (VAERS). Methods: VAERS is a national passive surveillance system for adverse events (AEs) following immunization that uses the Medical Dictionary for Regulatory Activities to code reported AEs and the Code of Federal Regulations to classify reports by seriousness. In this case series, we analyzed U.S. reports involving MenB-4C received between January 23, 2015 through December 31, 2018. We used Empirical Bayesian data mining to identify MenB-4C/AE combinations reported at least twice as often as expected. Results: VAERS received 1,867 reports following MenB-4C administration, representing 332 reports per million doses distributed. Most reports were for females (59%), with a median age of 17 years (interquartile range: 16–18 years); 40% of reports described simultaneous administration of other vaccines. The majority of reports were classified as non-serious (96%). The most commonly reported AEs were injection site pain (22%), pyrexia (16%), and headache (16%). Data mining identified disproportionate reporting for “injected limb mobility decreased” secondary to injection site reactions, including extensive swelling of the vaccinated limb and injection site pain. Conclusions: Analysis of passive surveillance data from over 5.6 million doses of MenB-4C distributed in the United States did not reveal new safety concerns. The large majority of reports were classified as non-serious and the reported AEs were generally consistent with the safety experience described in clinical studies and the product's package insert. While our results are reassuring, continued post-marketing surveillance is warranted. © 2021","4CMenB vaccine; Meningococcal vaccines; Neisseria meningitidis serogroup B; Pharmacovigilance; Vaccine Adverse Event Reporting System","azithromycin; eculizumab; eltrombopag; hepatitis A vaccine; hepatitis B vaccine; Human papilloma virus vaccine; immunoglobulin; influenza vaccine; Meningococcus vaccine; Pneumococcus vaccine; propylthiouracil; 4CMenB vaccine; Meningococcus vaccine; acute disseminated encephalomyelitis; acute sinusitis; adolescent; adult; allergic reaction; anaphylaxis; Article; asthenia; bursitis; cellulitis; child; chill; data mining; dizziness; drug safety; dyspnea; erythema; faintness; fatigue; female; fever; gait disorder; Graves disease; Guillain Barre syndrome; headache; human; hypesthesia; idiopathic thrombocytopenic purpura; injection site erythema; injection site pain; injection site reaction; injection site swelling; injection site warmth; limb pain; limb swelling; major clinical study; malaise; male; medication error; meningococcosis; multiple sclerosis; muscle spasm; myalgia; nausea; neck pain; optic neuritis; otalgia; pain; pallor; paresthesia; passive surveillance; pharmacovigilance; sore throat; tendinitis; thrombocytopenia; United States; upper respiratory tract infection; urticaria; vaccination reaction; vaccine failure; virus infection; vomiting; Waterhouse-Friderichsen syndrome; Bayes theorem; drug surveillance program; epidemiology; Neisseria meningitidis; young adult; Adolescent; Adult; Adverse Drug Reaction Reporting Systems; Bayes Theorem; Child; Female; Humans; Meningococcal Vaccines; Neisseria meningitidis, Serogroup B; United States; Young Adult",Article,Scopus,2-s2.0-85123651947
"Nazari-Ghanbarloo V.","57220734378;","A dynamic performance measurement system for supply chain management",2022,"International Journal of Productivity and Performance Management","71","2",,"576","597",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098498982&doi=10.1108%2fIJPPM-01-2020-0023&partnerID=40&md5=7caed8cb0d4202b034deb4847b062568","Purpose: This paper aims to propose a dynamic model for measurement supply chain performance (SCP) based on a dynamic balanced scorecard (DBSC). Balanced scorecard (BSC) can be defined as a popular performance measurement method that can translate the strategy into a set of performance indicators and manage the status of implementing the various strategies. However, BSC is unable to simulate the complicated environment and the dynamic behavior of performance metrics. Therefore, the author combines BSC with system dynamics (SD) to explore a more efficient tool for measurement SCP. Design/methodology/approach: A dynamic causal model is proposed based on the causal hypotheses. The developed DBSC enables managers to evaluate and measure the SCP in a much-balanced way. Using DBSC makes it possible that different SCP metrics to be reviewed and distributed into the four above-mentioned perspectives. It also enables supply chain (SC) managers to evaluate different strategies to improve SCP. Findings: investigates two strategies to improve SCP as follows: (1) competitive strategy and (2) harvesting big data and using data mining techniques to determine the customer's expectations and then compares the results of these two strategies based on the four perspectives of DBSC and introduce the best strategy. Finally, harvesting big data and data mining is selected as the best strategy. Originality/value: This study proposes a novel strategic management tool for measurement SCP and simulation of the complicated environment and the dynamic behavior of performance metrics. The proposed DBSC model enables managers to compare different strategies and select the best strategy. © 2020, Emerald Publishing Limited.","Big data; Data mining; Dynamic balanced scorecard; Strategy; Supply chain performance; System dynamics",,Article,Scopus,2-s2.0-85098498982
"Wang S., Xu J., Feng Y., Peng M., Ma K.","55714418100;57223849284;57195972211;57223849322;57223874128;","A Markov logic network method for reconstructing association rule-mining tasks in library book recommendation",2022,"Information Discovery and Delivery","50","1",,"34","44",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106286269&doi=10.1108%2fIDD-09-2020-0110&partnerID=40&md5=ba652885c2b9f12aaa977bd076bacc6f","Purpose: This study aims to overcome the problem of traditional association rules relying almost entirely on expert experience to set relevant interest indexes in mining. Second, this project can effectively solve the problem of four types of rules being present in the database at the same time. The traditional association algorithm can only mine one or two types of rules and cannot fully explore the database knowledge in the decision-making process for library recommendation. Design/methodology/approach: The authors proposed a Markov logic network method to reconstruct association rule-mining tasks for library recommendation and compared the method proposed in this paper to traditional Apriori, FP-Growth, Inverse, Sporadic and UserBasedCF algorithms on two history library data sets and the Chess and Accident data sets. Findings: The method used in this project had two major advantages. First, the authors were able to mine four types of rules in an integrated manner without having to set interest measures. In addition, because it represents the relevance of mining in the network, decision-makers can use network visualization tools to fully understand the results of mining in library recommendation and data sets from other fields. Research limitations/implications: The time cost of the project is still high for large data sets. The authors will solve this problem by mapping books, items, or attributes to higher granularity to reduce the computational complexity in the future. Originality/value: The authors believed that knowledge of complex real-world problems can be well captured from a network perspective. This study can help researchers to avoid setting interest metrics and to comprehensively extract frequent, rare, positive, and negative rules in an integrated manner. © 2021, Emerald Publishing Limited.","Association rule; Data mining; Decision-making; Library recommendation",,Article,Scopus,2-s2.0-85106286269
"Chaudhari A.Y., Mulay P.","57226548827;55356029600;","Unleashing analytics to reduce electricity consumption using incremental clustering algorithm",2022,"International Journal of Energy Sector Management","16","2",,"357","371",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111907437&doi=10.1108%2fIJESM-11-2019-0016&partnerID=40&md5=140c83260af6a4e9d3ff695bfcb20883","Purpose: To reduce the electricity consumption in our homes, a first step is to make the user aware of it. Reading a meter once in a month is not enough, instead, it requires real-time meter reading. Smart electricity meter (SEM) is capable of providing a quick and exact meter reading in real-time at regular time intervals. SEM generates a considerable amount of household electricity consumption data in an incremental manner. However, such data has embedded load patterns and hidden information to extract and learn consumer behavior. The extracted load patterns from data clustering should be updated because consumer behaviors may be changed over time. The purpose of this study is to update the new clustering results based on the old data rather than to re-cluster all of the data from scratch. Design/methodology/approach: This paper proposes an incremental clustering with nearness factor (ICNF) algorithm to update load patterns without overall daily load curve clustering. Findings: Extensive experiments are implemented on real-world SEM data of Irish Social Science Data Archive (Ireland) data set. The results are evaluated by both accuracy measures and clustering validity indices, which indicate that proposed method is useful for using the enormous amount of smart meter data to understand customers’ electricity consumption behaviors. Originality/value: ICNF can provide an efficient response for electricity consumption patterns analysis to end consumers via SEMs. © 2021, Emerald Publishing Limited.","Electricity data analytics; Incremental clustering algorithm; Incremental clustering with nearness factor algorithm; Pattern recognition; Smart electricity meter; Smart meter","Consumer behavior; Data mining; Electric power measurement; Electric power utilization; Smart meters; Clustering validity index; Design/methodology/approach; Electricity consumption patterns; Electricity-consumption; Household electricity consumption; Incremental clustering; Incremental clustering algorithm; Social science data; Clustering algorithms",Article,Scopus,2-s2.0-85111907437
"Caraka R.E., Hudaefi F.A., Ugiana P., Toharudin T., Tyasti A.E., Goldameir N.E., Chen R.C.","57190489490;57209250605;57298757100;23467616300;57298757200;57207728434;12759739100;","Indonesian Islamic moral incentives in credit card debt repayment: a feature selection using various data mining",2022,"International Journal of Islamic and Middle Eastern Finance and Management","15","1",,"100","124",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117166731&doi=10.1108%2fIMEFM-08-2020-0408&partnerID=40&md5=669902dee4d13c0ba812565a9a7e57c2","Purpose: Despite the practice of credit card services by Islamic financial institutions (IFIs) is debatable, Islamic banks (IBs) have been offering this product. Both Muslim and non-Muslim customers have subscribed to the products. Thus, it is critical to analyse the strategy of IBs’ moral messages in reminding their Muslim and non-Muslim customers to repay their credit card debts. This paper aims to investigate this issue in Indonesia using data mining via machine learning. Design/methodology/approach: This study examines the IBs’ customers across the 32 provinces of Indonesia regarding their moral status in credit card debt repayment. This work considers 6,979 observations of the variables that affect the moral status of the IBs’ customers in repaying their debt. The five types of data mining via machine learning (i.e. Boruta, logistic regression, Bayesian regression, random forest, XGBoost and spatial cluster) are used. Boruta, random forest and XGBoost are used to select the important features to investigate the moral aspects. Bayesian regression is used to get the odds and opportunity for the transition of each variable and spatially formed based on the information from the logistical intercepts. The best method is selected based on the highest accuracy value to deliver the information on the relationship between moral status categories in the selected 32 provinces in Indonesia. Findings: A different variable on moral status in each province is found. The XGBoost finds an accuracy value of 93.42%, which the three provincial groups have the same information based on the importance of the variables. The strategy of IBs’ moral messages by sending the verse of al-Qur’an and al-Hadith (traditions or sayings of the Prophet Muhammad PBUH) and simple messages reminders do not impact the customers’ repaying their debts. Both Muslim and non-Muslim groups are primarily found in the non-moral group. Research limitations/implications: This study does not consider socio-economic demographics and culture. This limitation calls future works to consider such factors when conducting a similar topic. Practical implications: The industry professionals can take benefit from this study to understand the Indonesian customers’ moral status in repaying credit card debt. In addition, future works may advance the recent findings by considering socio-cultural factors to investigate the moral status approach to Islamic credit warnings that is not covered by this study. Social implications: This work finds that religious text of credit card repayment reminders sent to Muslims in several provinces of Indonesia does not affect their decision to repay their debts. To some extent, this finding draws a social issue that the local IBs need to consider when implementing the strategy of credit card repayment reminders. Originality/value: This study credits a novelty in the discourse of data science for Islamic finance practices. Specifically, this study pioneers an example of using data mining to investigate Islamic-moral incentives in credit card debt repayment. © 2021, Emerald Publishing Limited.","Credit card; Data mining; Feature selection; Islamic credit card; Machine learning; Moral status; Repayment",,Article,Scopus,2-s2.0-85117166731
"Saputra A., Wang G., Zhang J.Z., Behl A.","57263346900;57214836304;57208659110;56554009500;","The framework of talent analytics using big data",2022,"TQM Journal","34","1",,"178","198",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115180339&doi=10.1108%2fTQM-03-2021-0089&partnerID=40&md5=d889eb22578fc0017fcef7640e53a542","Purpose: The era of work 4.0 demands organizations to expedite their digital transformation to sustain their competitive advantage in the market. This paper aims to help the human resource (HR) department digitize and automate their analytical processes based on a big-data-analytics framework. Design/methodology/approach: The methodology applied in this paper is based on a case study and experimental analysis. The research was conducted in a specific industry and focused on solving talent analysis problems. Findings: This research conducts digital talent analysis using data mining tools with big data. The talent analysis based on the proposed framework for developing and transforming the HR department is readily implementable. The results obtained from this talent analysis using the big-data-analytics framework offer many opportunities in growing and advancing a company's talents that are not yet realized. Practical implications: Big data allows HR to perform analysis and predictions, making more intelligent and accurate decisions. The application of big data analytics in an HR department has a significant impact on talent management. Originality/value: This research contributes to the literature by proposing a formal big-data-analytics framework for HR and demonstrating its applicability with real-world case analysis. The findings help organizations develop a talent analytics function to solve future leaders' business challenges. © 2021, Emerald Publishing Limited.","Big data; Human resource; Talent; Talent analytics","Big data; Competition; Data Analytics; Data mining; Analysis problems; Analytical process; Business challenges; Competitive advantage; Design/methodology/approach; Digital transformation; Experimental analysis; Talent management; Advanced Analytics",Article,Scopus,2-s2.0-85115180339
"Abbas A., Moosa I., Ramiah V.","57223871585;7006511199;34168330300;","The contribution of human capital to foreign direct investment inflows in developing countries",2022,"Journal of Intellectual Capital","23","1",,"9","26",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106248463&doi=10.1108%2fJIC-12-2020-0388&partnerID=40&md5=4f8b996c5b684699619cbdcce7c101ed","Purpose: This paper is about the effect of human capital on foreign direct investment (FDI). The purpose of this paper is to find out if developing countries with high levels of human capital (educated people and well-trained labour force) are more successful in attracting FDI. The underlying hypothesis has been tested repeatedly without reaching a consensus view or providing an answer to the basic question. This is to be expected because FDI is determined by a large number of factors, making the results sensitive to the selected set of explanatory variables, which forms the basis of the Leamer (1983) critique of the use of multiple regression to derive inference. Furthermore, confirmation bias and publication bias entice researchers to be selective in choosing the set of results they report. Design/methodology/approach: The technique of extreme bounds analysis, as originally suggested by Leamer (1983) and modified by Sala-i-Martin (1997), is used to determine the importance of human capital for the ability of developing countries to attract FDI. The authors use a cross-sectional sample covering 103 developing and transition countries. Findings: The results show no contradiction between firms seeking human capital and cheap labour. No matter what proxy is used to represent human capital, it turns out that the most important factor for attracting FDI is the variable “employee compensation”, which is the wage bill, implying that multinational firms look for cheap and also skilled labour in the host country. Originality/value: In this paper, the authors follow the procedure prescribed by Leamer (1983), and modified by Sala-i-Martin (1997), using extreme bounds analysis to distinguish between robust and fragile determinants of FDI, with particular emphasis on human capital. Instead of deriving inference from one regression equation by determining the statistical significance of the coefficient on the variable of interest, the extreme bounds or the distribution of estimated coefficients are used to distinguish between robust and fragile variables. This means that emphasis is shifted from significance, as implied by a single regression equation, to robustness, which is based on a large number of equations. The authors conduct tests on three proxies for human capital to find out if they are robust determinants of FDI and also judge the degree of robustness relative to other determinants. © 2021, Emerald Publishing Limited.","Data mining; Extreme bounds analysis; Foreign direct investment; Human capital; Sensitivity analysis",,Article,Scopus,2-s2.0-85106248463
"Adil B., Abdelhadi F.","57215669187;55935734000;","A BIG DATA ANALYTICS FRAMEWORK FOR COMPETITIVE INTELLIGENCE SYSTEMS",2022,"Journal of Theoretical and Applied Information Technology","100","1",,"149","169",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124492743&partnerID=40&md5=16221110efb119b3d4a492fabd4011cb","Nowadays, companies are facing a lot of challenges due to the volume and velocity of data available online, the nature of this data which comes in different formats structured, semi-structured or unstructured, forces the adoption of new tools and techniques to process and transform data to knowledge, competitive intelligence systems aims at setting-up tools and software to handle this stream of data from data collection, data analysis, data visualization to the results dissemination for stakeholders to enhance the decision making process of companies. In this paper, we present a big data analytics layer/framework for competitive intelligence systems and we implement it in the case of XEW 2.0 system relying on Apache Spark capabilities and big data analytics technologies, we validate the proposed framework with a case study about Research in Morocco in order to achieve a technological surveillance, the framework shows promising results in providing analysts with a toolbox to extract strategic information. © 2021 Little Lion Scientific","Apache Spark; Big Data; Big Data Analytics; Competitive Intelligence; Data Mining",,Article,Scopus,2-s2.0-85124492743
"Song Q.H., Jiang H.Y., Song Q.J., Xiao L.J., Wang Y.","57195770737;56764865400;57387919700;7202696642;7601507488;","Study on vessel–pipe coupling dynamic behavior under regular waves in deep sea mining process",2022,"Ocean Engineering","244",,"110401","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121847758&doi=10.1016%2fj.oceaneng.2021.110401&partnerID=40&md5=a0eda97757c4eeeede652c3fd10b2f89","A indirect time-domain coupling dynamic mathematical model of mining vessel–lifting pipe was established by combining the analysis method of mining vessel and lifting pipe in this paper. The vessel–pipe coupling experiments under various wave conditions were performed at the China National Ocean Technology Center, and a dynamic computer simulation of the coupled model was conducted to verify the effectiveness of the experiment and simulation. To simulate the coupling dynamic behavior, we established the hydrodynamic model of the vessel using AQWA software to determine the response amplitude operators (RAOs). Subsequently, the obtained RAO matrix was merged into an OrcaFlex model to analyze the dynamics of the mining vessel–lifting pipe interaction. In addition, the influencing factors of vessel–pipe coupling dynamics were estimated quantitatively, including regular wave, buffer mass, and sailing velocity. The findings show that the coupling effect has a significant impact on the vessel–pipe dynamic behavior, which has a strong relevance with surface waves, but a weak relevance with sailing velocity. In the process of changing the wave direction from 180° to 90°, the tension at the top of the pipeline increases by 19.31% and the coupling time decreases by 66.67%. © 2021 Elsevier Ltd","Deep-sea mining (DSM); Dynamic characteristics; Lifting pipe; Sea state; Vessel–pipe coupling","Data mining; Surface waves; Time domain analysis; Coupling dynamics; Deep-sea mining; Dynamic behaviors; Dynamics characteristic; Lifting pipe; Pipe-coupling; Regular waves; Sea state; Vessel–pipe coupling; Ocean currents; dynamic property; numerical model; ocean wave; pipeline; sea state; submarine mining; vessel; China",Article,Scopus,2-s2.0-85121847758
"Hou S.-Z., Guo W., Wang Z.-Q., Liu Y.-T.","7201471361;55542576700;57212119454;57218902766;","Deep-Learning-Based Fault Type Identification Using Modified CEEMDAN and Image Augmentation in Distribution Power Grid",2022,"IEEE Sensors Journal","22","2",,"1583","1596",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121339781&doi=10.1109%2fJSEN.2021.3133352&partnerID=40&md5=266ac63e7f831c5f6a831e71b06fdca7","The data mining method is limited to be applied in the distribution network fault diagnosis because of the unbalanced fault sample problem. Aiming at this problem, combining the modified complete ensemble empirical mode decomposition with adaptive noise (MCEEMDAN) and conditional generative adversarial network (CGAN), a fault identification method for the distribution network was proposed. At first, the CEEMDAN was modified by the partial mean of multi-scale permutation entropy. The MCEEMDAN may decompose the electric signal into a series of intrinsic mode functions. The raw time-domain signal can be transformed into the two-dimensional gray-level image by the pseudo-color coding of the intrinsic mode functions. Then, the fault gray image can be labeled and put into CGAN to generate a large number of new samples to achieve data augmentation. In order to improve the quality of the generated samples, the least square loss function is introduced into the original CGAN network to make the generated samples close to the raw samples. Finally, the convolutional neural network (CNN) is used to mine the fault features autonomously. The Softmax classifier is used to achieve distribution network fault classification. The experiments show that the proposed method can effectively learn the distribution characteristics of the original sample. Furthermore, the fault recognition accuracy can be effectively improved. The proposed method has good stability, fast convergence speed, and high precision, and it can effectively complete the fault identification of the distribution network. © 2001-2012 IEEE.","conditional generative adversarial network; convolutional neural network; fault recognition; Partial mean of multi-scale permutation entropy; two-dimensional gray-level image","Convolution; Convolutional neural networks; Data mining; Deep learning; Electric power transmission networks; Failure analysis; Fault detection; Generative adversarial networks; Image coding; Time domain analysis; Time series analysis; Conditional generative adversarial network; Convolutional neural network; Fault recognition; Faults diagnosis; Features extraction; Gray level image; Grey-level images; Multi-scales; Partial mean of multi-scale permutation entropy; Permutation entropy; Time-series analysis; Two-dimensional; Two-dimensional gray-level image; Entropy",Article,Scopus,2-s2.0-85121339781
"Walker V.R., Schmitt C.P., Wolfe M.S., Nowak A.J., Kulesza K., Williams A.R., Shin R., Cohen J., Burch D., Stout M.D., Shipkowski K.A., Rooney A.A.","7102764346;7202056836;7202794886;57195615254;57224140437;57220567603;57372197400;57371436700;57193190092;57372197500;56781526100;7003764422;","Evaluation of a semi-automated data extraction tool for public health literature-based reviews: Dextr",2022,"Environment International","159",,"107025","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121141085&doi=10.1016%2fj.envint.2021.107025&partnerID=40&md5=14b95c904570b0a3d75a4826d08b6db2","Introduction: There has been limited development and uptake of machine-learning methods to automate data extraction for literature-based assessments. Although advanced extraction approaches have been applied to some clinical research reviews, existing methods are not well suited for addressing toxicology or environmental health questions due to unique data needs to support reviews in these fields. Objectives: To develop and evaluate a flexible, web-based tool for semi-automated data extraction that: 1) makes data extraction predictions with user verification, 2) integrates token-level annotations, and 3) connects extracted entities to support hierarchical data extraction. Methods: Dextr was developed with Agile software methodology using a two-team approach. The development team outlined proposed features and coded the software. The advisory team guided developers and evaluated Dextr's performance on precision, recall, and extraction time by comparing a manual extraction workflow to a semi-automated extraction workflow using a dataset of 51 environmental health animal studies. Results: The semi-automated workflow did not appear to affect precision rate (96.0% vs. 95.4% manual, p = 0.38), resulted in a small reduction in recall rate (91.8% vs. 97.0% manual, p < 0.01), and substantially reduced the median extraction time (436 s vs. 933 s per study manual, p < 0.01) compared to a manual workflow. Discussion: Dextr provides similar performance to manual extraction in terms of recall and precision and greatly reduces data extraction time. Unlike other tools, Dextr provides the ability to extract complex concepts (e.g., multiple experiments with various exposures and doses within a single study), properly connect the extracted elements within a study, and effectively limit the work required by researchers to generate machine-readable, annotated exports. The Dextr tool addresses data-extraction challenges associated with environmental health sciences literature with a simple user interface, incorporates the key capabilities of user verification and entity connecting, provides a platform for further automation developments, and has the potential to improve data extraction for literature reviews in this and other fields. © 2021","Automation; Literature review; Machine learning; Natural language processing; Scoping review; Systematic evidence map; Systematic review; Text mining","Automation; Clinical research; Data mining; Extraction; Learning algorithms; Machine learning; User interfaces; Automated data; Data extraction; Environmental health; Extraction time; Literature reviews; Scoping review; Systematic evidence map; Systematic Review; User verification; Work-flows; Natural language processing systems; automation; data mining; literature review; machine learning; map; public health; accuracy; Article; automation; data extraction; data processing; environmental health; extraction time; feedback system; gold standard; information processing; literature; machine learning; natural language processing; nonhuman; public health; recall; software; usability; workflow",Article,Scopus,2-s2.0-85121141085
"Zhou H., Zhang F., Du Z., Liu R.","57221630058;56434720200;57226071076;57355612900;","A theory-guided graph networks based PM2.5 forecasting method",2022,"Environmental Pollution","293",,"118569","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120079043&doi=10.1016%2fj.envpol.2021.118569&partnerID=40&md5=4c1d67e0b131d364b24f09ff04a18e04","The theory-guided air quality model solves the mathematical equations of chemical and physical processes in pollution transportation numerically. While the data-driven model, as another scientific research paradigm with powerful extraction of complex high-level abstractions, has shown unique advantages in the PM2.5 prediction applications. In this paper, to combine the two advantages of strong interpretability and feature extraction capability, we integrated the partial differential equation of PM2.5 dispersion with deep learning methods based on the newly proposed DPGN model. We extended its ability to perform long-term multi-step prediction and used advection and diffusion effects as additional constraints for graph neural network training. We used hourly PM2.5 monitoring data to verify the validity of the proposed model, and the experimental results showed that our model achieved higher prediction accuracy than the baseline models. Besides, our model significantly improved the correct prediction rate of pollution exceedance days. Finally, we used the GNNExplainer model to explore the subgraph structure that is most relevant to the prediction to interpret the results. We found that the hybrid model is more biased in selecting stations with Granger causality when predicting. © 2021 Elsevier Ltd","Graph neural network; LSTM; Partial differential equation; PM2.5concentration prediction","Air quality; Data mining; Extraction; Graph neural networks; Graph theory; Long short-term memory; Partial differential equations; Air quality models; Concentration prediction; Forecasting methods; Graph networks; Graph neural networks; LSTM; Mathematical equations; Network-based; PM 2.5; PM2.5concentration prediction; Forecasting; air quality; artificial neural network; concentration (composition); numerical model; particulate matter; pollution monitoring; advection; article; deep learning; diffusion; feature extraction; forecasting; particulate matter 2.5; prediction; theoretical study; validity; air pollutant; air pollution; environmental monitoring; forecasting; particulate matter; Air Pollutants; Air Pollution; Environmental Monitoring; Forecasting; Particulate Matter",Article,Scopus,2-s2.0-85120079043
"Im S., Shang S.-L., Smith N.D., Krajewski A.M., Lichtenstein T., Sun H., Bocklund B.J., Liu Z.-K., Kim H.","57219142368;57353001300;57192938409;57221146537;56433249200;57214725544;57195284475;57352777500;37063220300;","Thermodynamic properties of the Nd-Bi system via emf measurements, DFT calculations, machine learning, and CALPHAD modeling",2022,"Acta Materialia","223",,"117448","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119900630&doi=10.1016%2fj.actamat.2021.117448&partnerID=40&md5=f8cda98974e945559d70d98357ee1ffc","Thermodynamic properties of the Nd-Bi system were investigated using a combination of experimental measurements, first-principles calculations based on density functional theory (DFT), data mining and machine learning (DM + ML) predictions, and calculation of phase diagrams (CALPHAD) modeling. The electromotive force (emf) of Nd-Bi alloys in molten LiCl-KCl-NdCl3 at 773–973 K was measured via coulometric titration of Nd into Bi for the determination of thermochemical properties such as activity coefficients and solubilities of Nd in Bi. A new peritectic reaction of [liquid + NdBi2 = Nd3Bi7] at 774 K was confirmed using differential scanning calorimetry, structural (X-ray diffraction), and microstructural (scanning electron microscopy) analyses. The unknown crystal structure of NdBi2 was suggested to be a mixture of the anti-La2Sb configuration and the La2Te-type configuration based on ML predictions for over 26,000 data-mined AB2-type configurations together with DFT-based verifications. Using the newly acquired experimental data and DFT-based calculations, the thermodynamic description of the Nd-Bi system was remodeled, and a more complete Nd-Bi phase diagram was calculated, including the Nd3Bi7 compound, invariant transition reactions, and liquidus temperatures. © 2021 Acta Materialia Inc.","CALPHAD modeling; Emf measurement; First-principles calculations; Machine learning; Nd-Bi phase diagram; Rare-earth alloys","Calculations; Chlorine compounds; Data mining; Density functional theory; Differential scanning calorimetry; Lanthanum alloys; Lithium alloys; Lithium compounds; Neodymium alloys; Phase diagrams; Potassium compounds; Rare earths; Scanning electron microscopy; Thermodynamic properties; Bi-phase diagram; Calculation of phase diagram modeling; Calculation of phase diagrams; Density-functional-theory; E.m.f. measurements; Electromotive force measurements; EMF measurement; First principle calculations; Measurement density; Nd-bi phase diagram; Binary alloys",Article,Scopus,2-s2.0-85119900630
"Hong Z., Peng Z., Zhang L.","57329588800;57193428305;37040045000;","Game analysis on the choice of emission trading among industrial enterprises driven by data",2022,"Energy","239",,"122447","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118891332&doi=10.1016%2fj.energy.2021.122447&partnerID=40&md5=95f51922239dd934af4afbc242dee221","The construction and promotion of emission trading information platform makes it possible for enterprises to collect and use emission rights and other data. How to conduct game analysis for industrial enterprises' emission trading under data driven has become an effective basis and inevitable trend to assist enterprises to achieve emission reduction and optimal decision-making. However, existing game methods are not used for comprehensive optimal decision for enterprises based on these data. Therefore, this paper integrates dynamic game and data to effectively solve optimal choice in the process of emission trading among industrial enterprises. The bargaining dynamic game model and forward reasoning method are proposed to realize the game analysis of emission trading among enterprises in the secondary market based on the data mining or evaluation of pollutant emissions, market price and marginal revenue of emission rights and initial emission rights by Support Vector Regression (SVR), Linear Regression (LR) and Analytical Hierarchy Process (AHP). Taking six industrial enterprises in Tianjin as an example, this paper analyzes the optimal trading price, trading volume and object of emission trading among different enterprises under different loss factors. © 2021 Elsevier Ltd","Dynamic bargaining game; Emission trading; Forward reasoning; Support vector regression (SVR)","Commerce; Data mining; Decision making; Game theory; Regression analysis; Bargaining game; Dynamic bargaining game; Dynamic game; Emission rights; Emissions Trading; Forward reasoning; Game analysis; Industrial enterprise; Support vector regression; Support vector regressions; Emission control; data mining; data set; emission control; environmental economics; industrial enterprise; market conditions; spatiotemporal analysis; China; Tianjin",Article,Scopus,2-s2.0-85118891332
"Kumar R., Swarnkar M., Singal G., Kumar N.","57216257328;57188665441;56892765700;57206866080;","IoT Network Traffic Classification Using Machine Learning Algorithms: An Experimental Analysis",2022,"IEEE Internet of Things Journal","9","2",,"989","1008",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118584349&doi=10.1109%2fJIOT.2021.3121517&partnerID=40&md5=f1e1663549131865bc45e5d96787f15e","Internet of Things (IoT) refers to a wide variety of embedded devices connected to the Internet, enabling them to transmit and share information in smart environments with each other. The regular monitoring of IoT network traffic generated from IoT devices is important for their proper functioning and detection of malicious activities. One such crucial activity is the classification of IoT devices in the network traffic. It enables the administrator to monitor the activities of IoT devices which can be useful for proper implementation of Quality of Service, detect malicious IoT devices, etc. In the literature, various methods are proposed for IoT traffic classification using various machine learning algorithms. However, the accuracy of these machine learning algorithms depends on the data generated from various IoT devices, features extracted from network traffic, site at which IoT is deployed, etc. Moreover, the selection of features and machine learning algorithms are manual operations that are prone to error. Therefore, it is important to study the network traffic characteristics as well as suitable machine learning algorithms for accurate and optimized IoT traffic classification. In this article, we perform an in-depth comparative analysis of various popular machine learning algorithms using different effective features extracted from IoT network traffic. We utilize a public data set having 20 days of network traces generated from 20 popular IoT devices. Network traces are first processed to extract the significant features. We then selected state-of-the-art machine learning algorithms based on the recent survey papers for the IoT traffic classification. We then comparatively evaluated the performance of those machine learning algorithms on the basis of classification accuracy, speed, training time, etc. Finally, we provided a few suggestions for selecting the machine learning algorithm for different use cases based on the obtained results. © 2014 IEEE.","Internet of Things (IoT) devices; machine learning algorithms; network flow; network packet; network traffic classification","Artificial intelligence; Classification (of information); Data mining; Feature extraction; Intelligent buildings; Internet of things; Learning algorithms; Learning systems; Quality of service; Site selection; Features extraction; Internet of thing device; Machine learning algorithms; Network packet.; Network packets; Network traffic classification; Networks flows; Performances evaluation; Smart homes; Automation",Article,Scopus,2-s2.0-85118584349
"Chen J., Lin D., Wu J.","57287948800;57218105399;55760584700;","Do cryptocurrency exchanges fake trading volumes? An empirical analysis of wash trading based on data mining",2022,"Physica A: Statistical Mechanics and its Applications","586",,"126405","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116591323&doi=10.1016%2fj.physa.2021.126405&partnerID=40&md5=b74fe93e44f3e259dc4664c0a8a930a5","Cryptocurrency exchanges, which act as a platform for cryptocurrency trading, play a vital role in the ever-growing cryptocurrency market. However, with the rapid development of this emerging market, some unethical phenomena including faking trading volume have also appeared in cryptocurrency exchanges. To this end, this paper proposes a data mining-based method based on off-chain data and on-chain transaction data to detect the exchanges that fake trading volume. In particular, we first collect off-chain data from the websites of five exchanges and the on-chain data provided by a blockchain browser, and then analyze them from two perspectives, including transaction number and transaction amount. The empirical results suggest that Huobi exchange fakes trading volume most obviously, while Binance trading is relatively the most honest. In addition, different exchanges adopt distinct counterfeiting strategies when creating wash trading. © 2021 Elsevier B.V.","Blockchain; Cryptocurrency; Data mining; Exchange","Blockchain; Electronic money; Block-chain; Emerging markets; Empirical analysis; Exchange; Trading volumes; Transaction data; Data mining",Article,Scopus,2-s2.0-85116591323
"Zhu X., Xu Z., You S., Komárek M., Alessi D.S., Yuan X., Palansooriya K.N., Ok Y.S., Tsang D.C.W.","56459195300;57198999895;57220033618;8568354900;57240441700;57240282800;57201908597;7003403766;12760921200;","Machine learning exploration of the direct and indirect roles of Fe impregnation on Cr(VI) removal by engineered biochar",2022,"Chemical Engineering Journal","428",,"131967","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113914269&doi=10.1016%2fj.cej.2021.131967&partnerID=40&md5=fc95ae06447ac4cadc10486dc57225c1","Data mining and knowledge discovery by machine learning (ML) have recently come into application in environmental remediation, especially the exploration for the multifactorial process such as hexavalent chromium [Cr(VI)] removal by iron-biochar composite (Fe-BC). The Cr(VI) removal capacity of Fe-BC was concurrently controlled by impregnated iron species (Fe0/Fe2+/Fe3+), carbon properties, and iron-carbon interactions, while the current lab-scale research could hardly untangle the overall relationships with the Cr(VI) removal experiments of one or several Fe-BCs under different research frameworks. Herein, we investigated the impacts of various microscopic material properties of Fe-BC on aqueous Cr(VI) removal by ML approach and highlighted the variations of biochar properties after iron impregnation. Our results suggested that the direct impacts of impregnated-iron contents on the Cr(VI) removal were limited, possibly related to undistinguished Fe species in the ML models, in which the roles of different iron species on Cr(VI) removal might be counteracted. Instead, the impacts of impregnated iron on the Cr(VI) removal were embodied indirectly by altering the biochar properties. Surface oxygen-containing functional groups (SOFGs) contents on biochar played a pivotal role in Cr(VI) removal according to the ML models. The condensed polyaromatic carbon matrices of BC and Fe-BC with a high content of non-polar carbon were also proved to be conducive to Cr(VI) removal. The ML models developed in this study consider surface functionalities information of BC and Fe-BC and offer a more accurate prediction for Cr(VI) removal, and the information mining behind models can act as a vital reference for the rational design of engineered biochar to remove aqueous Cr(VI). © 2021 Elsevier B.V.","Environmental remediation; Hexavalent chromium; Iron-biochar composite; Machine learning; Redox properties; Sustainable waste management","Carbon; Chemicals removal (water treatment); Chromium compounds; Data mining; Impregnation; Iron compounds; Machine learning; Waste management; Bio chars; Engineered biochar; Environmental remediation; Hexavalent chromium; Iron-biochar composite; Machine learning models; Machine-learning; Property; Redox property; Sustainable waste management; Iron",Article,Scopus,2-s2.0-85113914269
"Hou R., Jeong S., Lynch J.P., Ettouney M.M., Law K.H.","57190580657;56893042300;57199678735;7004428697;55671078700;","Data-driven analytical load rating method of bridges using integrated bridge structural response and weigh-in-motion truck data",2022,"Mechanical Systems and Signal Processing","163",,"108128","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109836572&doi=10.1016%2fj.ymssp.2021.108128&partnerID=40&md5=c7e6ba4c5b3d5b9c29428dbfc7c8370d","Load rating is a widely used approach for evaluating the load-carrying capacity of bridges in an effort to ensure safe bridge operation under expected traffic loads. Load rating often relies on simplified analytical models including empirically derived model parameters that do not reflect bridge-specific information resulting in conservative ratings. To reduce this conservatism, this study proposes a novel data-driven framework that utilizes long-term bridge response data to extract bridge-specific model parameters that can be used within in the Load and Resistance Factor Rating (LRFR) process. The data-driven LRFR (DD-LRFR) framework is empowered by a cyber-physical system (CPS) architecture that uses Internet connectivity to integrate measured bridge responses with truck weights measured by a weigh-in-motion (WIM) station. The CPS architecture uses computer vision of camera images to confirm trucks observed at a WIM station are identical to those observed at a bridge. Bridge response and axle weight data are then used to extract probabilistic models of dynamic load allowances and unit influence lines. The DD-LRFR method is validated using a 20-mile (32.2-km) segment of the I-275 northbound highway in Michigan that is monitored continuously by the CPS architecture. Six girders associated with two bridges along I-275 are rated using the proposed DD-LRFR methodology with rating factors compared to those obtained using conventional and refined load rating methods. The DD-LRFR method yields inventory- and operational-level rating factors that are less conservative than those from the approximate LRFR method and comparable to those using finite element modeling of the bridge. © 2021 Elsevier Ltd","Cyber-physical system; Data-driven load rating; Dynamic load allowance; Influence line; Structural monitoring; Weigh-in-motion","Automobiles; Cyber Physical System; Data mining; Dynamics; Embedded systems; Trucks; Weigh-in-motion (WIM); Bridge response; Cybe-physical systems; Cyber-physical systems; Data driven; Data-driven load rating; Dynamic load allowance; Influence lines; Load and resistance factor ratings; Load ratings; Structural monitoring; Dynamic loads",Article,Scopus,2-s2.0-85109836572
"Song J., Wei P., Valdebenito M.A., Faes M., Beer M.","55803046900;53878880900;16025949100;56785926300;25935731000;","Data-driven and active learning of variance-based sensitivity indices with Bayesian probabilistic integration",2022,"Mechanical Systems and Signal Processing","163",,"108106","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108119561&doi=10.1016%2fj.ymssp.2021.108106&partnerID=40&md5=f232309308d087826d957ab72c1eddc1","Variance-based sensitivity indices play an important role in scientific computation and data mining, thus the significance of developing numerical methods for efficient and reliable estimation of these sensitivity indices based on (expensive) computer simulators and/or data cannot be emphasized too much. In this article, the estimation of these sensitivity indices is treated as a statistical inference problem. Two principle lemmas are first proposed as rules of thumb for making the inference. After that, the posterior features for all the (partial) variance terms involved in the main and total effect indices are analytically derived (not in closed form) based on Bayesian Probabilistic Integration (BPI). This forms a data-driven method for estimating the sensitivity indices as well as the involved discretization errors. Further, to improve the efficiency of the developed method for expensive simulators, an acquisition function, named Posterior Variance Contribution (PVC), is utilized for realizing optimal designs of experiments, based on which an adaptive BPI method is established. The application of this framework is illustrated for the calculation of the main and total effect indices, but the proposed two principle lemmas also apply to the calculation of interaction effect indices. The performance of the development is demonstrated by an illustrative numerical example and three engineering benchmarks with finite element models. © 2021 Elsevier Ltd","Adaptive experiment design; Bayesian probabilistic integration; Data-driven; Gaussian process regression; Posterior variance contribution; Variance-based sensitivity","Benchmarking; Data integration; Data mining; Numerical methods; Adaptive experiment design; Bayesian; Bayesian probabilistic integration; Data driven; Gaussian process regression; Posterior variance contribution; Probabilistic integration; Sensitivity indices; Variance-based sensitivity; Variance-based sensitivity indices; Integration",Article,Scopus,2-s2.0-85108119561
"Wang S., Xie Z., Wu Z.","57431360500;57431561500;57208700583;","Establishment and Validation of a Ferroptosis-Related Gene Signature to Predict Overall Survival in Lung Adenocarcinoma",2022,"Frontiers in Genetics","12",,"793636","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123686066&doi=10.3389%2ffgene.2021.793636&partnerID=40&md5=1df99e0e9a7db02b5c9aab436cde49f8","Background: Lung adenocarcinoma (LUAD) is the most common and lethal subtype of lung cancer. Ferroptosis, an iron-dependent form of regulated cell death, has emerged as a target in cancer therapy. However, the prognostic value of ferroptosis-related genes (FRGs)x in LUAD remains to be explored. Methods: In this study, we used RNA sequencing data and relevant clinical data from The Cancer Genome Atlas (TCGA) dataset and Gene Expression Omnibus (GEO) dataset to construct and validate a prognostic FRG signature for overall survival (OS) in LUAD patients and defined potential biomarkers for ferroptosis-related tumor therapy. Results: A total of 86 differentially expressed FRGs were identified from LUAD tumor tissues versus normal tissues, of which 15 FRGs were significantly associated with OS in the survival analysis. Through the LASSO Cox regression analysis, a prognostic signature including 11 FRGs was established to predict OS in the TCGA tumor cohort. Based on the median value of risk scores calculated according to the signature, patients were divided into high-risk and low-risk groups. Kaplan–Meier analysis indicated that the high-risk group had a poorer OS than the low-risk group. The area under the curve of this signature was 0.74 in the TCGA tumor set, showing good discrimination. In the GEO validation set, the prognostic signature also had good predictive performance. Functional enrichment analysis showed that some immune-associated gene sets were significantly differently enriched in two risk groups. Conclusion: Our study unearthed a novel ferroptosis-related gene signature for predicting the prognosis of LUAD, and the signature may provide useful prognostic biomarkers and potential treatment targets. Copyright © 2022 Wang, Xie and Wu.","data mining; ferroptosis; genes; immune infiltration; Lung Adenocarcinoma","biological marker; cyclin dependent kinase inhibitor 1A; glucose transporter 1; interleukin 33; messenger RNA; toll like receptor 4; transcriptome; adult; aged; area under the curve; Article; B lymphocyte; cancer patient; cancer prognosis; cell death; cell infiltration; cohort analysis; dendritic cell; diagnostic test accuracy study; drug metabolism; female; ferroptosis; functional enrichment analysis; gene expression; gene ontology; genetic marker; helper cell; human; human tissue; Kaplan Meier method; least absolute shrinkage and selection operator; lung adenocarcinoma; major clinical study; male; metabolomics; overall survival; prognosis; protein protein interaction; quantitative structure activity relation; receiver operating characteristic; regulatory T lymphocyte; risk factor; RNA sequence; RNA sequencing; signal transduction; survival analysis; tumor associated leukocyte; tumor volume; Wnt signaling",Article,Scopus,2-s2.0-85123686066
"Liu W., Guo X., Chen B., He W.","57211041217;57425808200;55253884300;55764155800;","Potential of Overcomplete Wavelet Frame Expansion for Facilitating Electroencephalogram Information Mining",2022,"Frontiers in Neuroscience","15",,"782918","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123417820&doi=10.3389%2ffnins.2021.782918&partnerID=40&md5=f66222aade158a43da4aee16289ef77d",[No abstract available],"brain imaging; EEG; feature extraction; overcomplete wavelet frame expansion; wavelet transform","Article; artificial intelligence; clinical practice; data mining; diagnostic accuracy; discrete wavelet transform; electroencephalogram; entropy; feature extraction; human; neuroimaging; overcomplete wavelet frame expansion; seizure; signal noise ratio; wavelet analysis; wavelet transform",Article,Scopus,2-s2.0-85123417820
"Guo Y., Wang T., Chen W., Kaptchuk T.J., Li X., Gao X., Yao J., Tang X., Xu Z.","57204419336;57437439600;55716044700;26643513100;57437873400;57212969386;57202690493;57437731400;57438019300;","Acceptability of Traditional Chinese Medicine in Chinese People Based on 10-Year's Real World Study With Mutiple Big Data Mining",2022,"Frontiers in Public Health","9",,"811730","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123920893&doi=10.3389%2ffpubh.2021.811730&partnerID=40&md5=7de0d59d053f7bd0bf40694fa70ee2af","In the past decades, numerous clinical researches have been conducted to illuminate the effects of traditional Chinese medicine for better inheritance and promotion of it, which are mostly clinical trials designed from the doctor's point of view. This large-scale data mining study was conducted from real-world point of view in up to 10 years' big data sets of Traditional Chinese Medicine (TCM) in China, including both medical visits to hospital and cyberspace and contemporaneous social survey data. Finally, some important and interesting findings appear: (1) More Criticisms vs. More Visits. The intensity of criticism increased by 2.33 times over the past 10 years, while the actual number of visits increased by 2.41 times. (2) The people of younger age, highly educated and from economically developed areas have become the primary population for utilizing TCM, which is contrary to common opinions on the characteristics of TCM users. The discovery of this phenomenon indicates that TCM deserves further study on how it treats illness and maintains health. Copyright © 2022 Guo, Wang, Chen, Kaptchuk, Li, Gao, Yao, Tang and Xu.","big data; China; data mining; social review; TCM; traditional Chinese medicine","article; big data; China; Chinese; Chinese medicine; data mining; human; mining",Article,Scopus,2-s2.0-85123920893
"Zhao M.-H., Zhu C., Sun Z., Xia T., Han Y., Zeng Y., Gao Z., Gong Y., Wang X., Hong J., Zhang W.-X., Wang Y., Yao D.-X., Li M.-R.","57205507018;57215547002;57221135470;57391856700;57208569239;55646790800;57208564614;57392374900;57211543838;23388823300;36063203900;57205480982;24382122800;7405261524;","Methodological Approach to the High-Pressure Synthesis of Nonmagnetic Li2B4+B′6+O6Oxides",2022,"Chemistry of Materials","34","1",,"186","196",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121996848&doi=10.1021%2facs.chemmater.1c03073&partnerID=40&md5=dfce20f67a7166111beb85fbd6c53ea0","High-pressure solid-state synthesis advances boost discoveries of new materials and unusual phenomena but endures stringent recipe conditions, poor yield, and high cost. A methodological approach for accelerated and precisely high-pressure synthesis is therefore highly desired. Here, we take the exotic double-perovskite-related nonmagnetic Li2B+4B′+6O6 as an example to show the pipeline of data-mining, high-throughput calculations, experimental realization, and chemical interception of metastable phases. A total of 140 compounds in 7 polymorph categories were initially screened by the convex hull, which left ∼50% candidates in chemical space on the phase diagram of pressure-dependent polymorph evolution. Li2TiWO6 and Li2TiTeO6 were singled out for experimental testing according to the predicted map of crystal structure, function, and synthesis parameters. Computation on surface energy effect and interfacial chemical strain suggested that the as-made high-pressure R3-Li2TiTeO6 polymorph cannot be intercepted below a critical nanoscale but can be stabilized in heterojunction film on a selected compressive substrate at ambient pressure. The developed methodology is expected to accelerate the big-data-driven discovery of generic chemical formula-based new materials beyond perovskites by high-pressure synthesis and shed light on the large-scale stabilization of metastable phases under mild conditions. © 2021 American Chemical Society.",,"Crystal structure; Data mining; Heterojunctions; High pressure effects; High pressure engineering; Metastable phases; Perovskite; Tungsten compounds; Condition; Double perovskites; High costs; High-pressure solids; High-pressure synthesis; Metastable phase; Methodological approach; Nonmagnetics; Solid-state synthesis; Stringents; Lithium compounds",Article,Scopus,2-s2.0-85121996848
"Pan Y., Zou J., Qiu J., Wang S., Hu G., Pan Z.","57216386448;57193922285;56349097300;57210136560;57310969500;8426014800;","Joint network embedding of network structure and node attributes via deep autoencoder",2022,"Neurocomputing","468",,,"198","210",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117760854&doi=10.1016%2fj.neucom.2021.10.032&partnerID=40&md5=2e0409c7a35efe648172f6d6a5f711e4","Network embedding aims to learn a low-dimensional vector for each node in networks, which is effective in a variety of applications such as network reconstruction and community detection. However, the majority of the existing network embedding methods merely exploit the network structure and ignore the rich node attributes, which tend to generate sub-optimal network representation. To learn more desired network representation, diverse information of networks should be exploited. In this paper, we develop a novel deep autoencoder framework to fuse topological structure and node attributes named FSADA. We firstly design a multi-layer autoencoder which consists of multiple non-linear functions to capture and preserve the highly non-linear network structure and node attribute information. Particularly, we adopt a pre-processing procedure to pre-process the original information, which can better facilitate to extract the intrinsic correlations between topological structure and node attributes. In addition, we design an enhancement module that combines topology and node attribute similarity to construct pairwise constraints on nodes, and then a graph regularization is introduced into the framework to enhance the representation in the latent space. Our extensive experimental evaluations demonstrate the superior performance of the proposed method. © 2021 Elsevier B.V.","Data mining; Deep autoencoder; Deep learning; Network analysis; Network embedding; Pattern recognition","Data mining; Deep learning; Embeddings; Functions; Pattern recognition; Topology; Auto encoders; Deep autoencoder; Deep learning; Learn+; Network embedding; Network node; Network representation; Network structures; Node attribute; Topological structure; Network embeddings; article; autoencoder; data mining; deep learning; embedding; network analysis; pattern recognition",Article,Scopus,2-s2.0-85117760854
"Cai R., Lin Z., Chen W., Hao Z.","57304240700;57304392100;57087444800;7201992728;","Shared state space model for background information extraction and time series prediction",2022,"Neurocomputing","468",,,"85","96",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117601287&doi=10.1016%2fj.neucom.2021.10.010&partnerID=40&md5=05c82015789e802234d96fa556f23ffa","Time series prediction is important for financial analysis, climate forecasting, and so on. Existing works mainly focus on the prediction of target time series based on the prior of the sequence itself, but ignore the background information behind the sequences. Such ignored information is usually essential to build a robust prediction model in complex real-world applications. However, how to extract the shared background information behind multiple sequences and how to incorporate the extracted information in the prediction model are two main challenges. To address the above two challenges, we propose a shared state space model (SSSM) by introducing a shared background information component into the state space model. In SSSM, we consider all sequences as a whole and model each target series by utilizing a state space model with shared same parameters and background information. First, we employ two recurrent neural networks to extract the temporal characteristic of the target sequence as well as the background information. Second, the above extracted information is integrated into a state space model in the form of a linear Gaussian component, whose inference procedure is accomplished by Kalman Filter. Finally, the model is optimized following a log-likelihood of the model with the above two components. Experiments on real-world applications show that our model can extract the share information behind the data and outperforms the state-of-the-art methods. © 2021 Elsevier B.V.","Kalman filter; Shared background information; State space model; Time series prediction","Data mining; Forecasting; Recurrent neural networks; State space methods; Time series; Time series analysis; Background information; Climate forecasting; Financial analysis; Prediction modelling; Real-world; Shared background information; Shared state; State-space models; Time series prediction; Times series; Kalman filters",Article,Scopus,2-s2.0-85117601287
"Rodriguez-Esteban R.","14825664700;","The speed of information propagation in the scientific network distorts biomedical research",2022,"PeerJ","10",,"e12764","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122731809&doi=10.7717%2fpeerj.12764&partnerID=40&md5=9dea0062a3c484489567f303f92a1095","Delays in the propagation of scientific discoveries across scientific communities have been an oft-maligned feature of scientific research for introducing a bias towards knowledge that is produced within a scientist's closest community. The vastness of the scientific literature has been commonly blamed for this phenomenon, despite recent improvements in information retrieval and text mining. Its actual negative impact on scientific progress, however, has never been quantified. This analysis attempts to do so by exploring its effects on biomedical discovery, particularly in the discovery of relations between diseases, genes and chemical compounds. Results indicate that the probability that two scientific facts will enable the discovery of a new fact depends on how far apart these two facts were originally within the scientific landscape. In particular, the probability decreases exponentially with the citation distance. Thus, the direction of scientific progress is distorted based on the location in which each scientific fact is published, representing a path-dependent bias in which originally closely-located discoveries drive the sequence of future discoveries. To counter this bias, scientists should open the scope of their scientific work with modern information retrieval and extraction approaches. Copyright 2022 Rodriguez-Esteban.","Citation network; Information science; Knowledge diffusion; Knowledge management; Scientometrics","Article; data mining; information retrieval; information science; knowledge management; medical research; probability; scientometrics; validation study; velocity",Article,Scopus,2-s2.0-85122731809
"Deng S., Zhang N., Chen H., Tan C., Huang F., Xu C., Chen H.","57201556430;55923601900;57221150709;57221151395;57210150087;57225936783;35268022500;","Low-resource extraction with knowledge-aware pairwise prototype learning[Formula presented]",2022,"Knowledge-Based Systems","235",,"107584","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119588924&doi=10.1016%2fj.knosys.2021.107584&partnerID=40&md5=a5f3fa4b7c1a91c27345b3260399e756","Knowledge Extraction (KE) aims at extracting structured information from raw texts, such as relation extraction and event extraction. One of the major issues for KE is the low-resource problem due to deficient samples. Previous work addresses the low-resource issue mostly via data-driven methods, such as transfer learning, while neglecting correlation knowledge among classes. For example, inherent correlation of entailment between the relation pair and causality between the event pair can also be utilized for low-resource KE. Consequently, we propose to leverage correlation knowledge via pairwise prototype learning on the hypersphere with a novel framework called Knowledge-aware Hyperspherical Prototype Network (K-HPN). K-HPN is able to recognize inherent correlation among classes, where each class is represented as a prototype on the hypersphere. The experimental results demonstrate that K-HPN outperforms previous methods of KE, particularly with low-resource training data regimes. © 2021 Elsevier B.V.","Knowledge extraction; Knowledge-aware; Low-resource; Pairwise prototype learning","Data mining; Knowledge management; Hyper-spheres; Hyperspherical; Knowledge extraction; Knowledge-aware; Low-resource; Pairwise prototype learning; Prototype learning; Relation extraction; Resource extraction; Structured information; Extraction",Article,Scopus,2-s2.0-85119588924
"Kaadoud I.C., Rougier N.P., Alexandre F.","57195229730;12344787800;7006444724;","Knowledge extraction from the learning of sequences in a long short term memory (LSTM) architecture",2022,"Knowledge-Based Systems","235",,"107657","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118842260&doi=10.1016%2fj.knosys.2021.107657&partnerID=40&md5=6e8cba919b9277f9e1667ea847abc345","Transparency and trust in machine learning algorithms have been deemed to be fundamental and yet, from a practical point of view, they remain difficult to implement. Particularly, explainability and interpretability are certainly among the most difficult capabilities to be addressed and imply to be able to understand a decision in terms of simple cues and rules. In this article, we address this specific problem in the context of sequence learning by recurrent neuronal models (and more specifically Long Short Term Memory model). We introduce a general method to extract knowledge from the latent space based on the clustering of the internal states. From these hidden states, we explain how to build and validate an automaton that corresponds to the underlying (unknown) grammar, and allows to predict if a given sequence is valid or not. Finally, we show that it is possible for such complex recurrent model, to extract the knowledge that is implicitly encoded in the sequences and we report a high rate of recognition of the sequences extracted from the original grammar. This method is illustrated on artificial grammars (Reber grammar variants) as well as on a real use-case in the electrical domain, whose underlying grammar is unknown. © 2021","Implicit learning; Knowledge extraction; Latent space; Long Short Term Memory; Recurrent Neural Networks; Sequence learning","Brain; Data mining; Extraction; Learning algorithms; Implicit learning; Interpretability; Knowledge extraction; Latent space; Machine learning algorithms; Memory modeling; Neuronal modeling; Sequence learning; Simple++; Specific problems; Long short-term memory",Article,Scopus,2-s2.0-85118842260
"Li X., Jiang H., Liu Y., Wang T., Li Z.","57196405148;27167524700;55367210600;57324834700;57323898200;","An integrated deep multiscale feature fusion network for aeroengine remaining useful life prediction with multisensor data",2022,"Knowledge-Based Systems","235",,"107652","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118534932&doi=10.1016%2fj.knosys.2021.107652&partnerID=40&md5=4ac75f2bf97695be81afbf332949d343","Most RUL prediction methods can only extract single-scale features, ignoring significant details at other scales and layers. These methods are all constructed using one type of model, and do not use the advantages of different models. An integrated deep multiscale feature fusion network (IDMFFN) for aeroengine RUL prediction using multisensor data is proposed in this study. Two-dimensional samples are constructed using multisensor data with multiple time cycles. Multiscale feature extraction blocks are designed to learn different-scale features using convolutional filters of different sizes. A multiscale feature concatenated block is constructed to integrate multiscale features from different layers. A GRU-based high-level feature fusion block is built to replace the traditional fully connected layer, and can leverage powerful temporal feature learning for feature fusion. A novel activation function Mish is used to construct the network. A simulated turbofan engine dataset was used to verify the effectiveness of the network. The results suggest that the IDMFFN can predict RUL more accurately than existing methods. © 2021","Aeroengine; Fusion; Multiscale; Multisensor; Remaining useful life prediction","Aircraft engines; Data mining; Forecasting; Aero-engine; Features fusions; Multi sensor; Multi-scale features; Multi-sensor data; Multiscale; Prediction methods; Remaining useful life predictions; Time cycles; Two-dimensional; Turbofan engines",Article,Scopus,2-s2.0-85118534932
"Zhu X., Lou Y., Deng H., Ji D.","57215840522;57310249200;35232817000;8698003700;","Leveraging bilingual-view parallel translation for code-switched emotion detection with adversarial dual-channel encoder",2022,"Knowledge-Based Systems","235",,"107436","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117793867&doi=10.1016%2fj.knosys.2021.107436&partnerID=40&md5=82af23a0dcfcd6543cc5602aea542cbf","Code-switched emotion detection, a task analyzing the emotion in code-switched texts, has gain increasing research attention within recent years. Prior works utilize various neural models with sophisticated features to pursuit high performance of the task, while they still overlook some crucial characteristics of the code-switched texts. In this work, we present an innovative approach for improving code-switched emotion detection. We first consider a bilingual-view parallel translation for code-switched text enhancement, i.e., translating the code-switched texts into two languages. Then we propose an adversarial dual-channel encoder architecture, where two private encoders take as inputs the parallel texts in two languages, respectively. The private encoders and the shared encoder work collaboratively, and effectively retrieve the features from monolingual and bilingual perspectives under adversarial training. We conduct extensive experiments on five code-switched benchmark datasets. Results show that our model outperforms the strongly-performing baselines that leverage external code-switched or bilingual word embedding with over 1.5% F1 score on the Chinese–English, Spanish–English and Hindi–English code-mixed data, becoming the new state-of-the-art system. Further analyses including ablation, qualitative and error studies, demonstrate the effectiveness of our proposed encoder for code-switched texts, as well as the bilingual-view parallel translation strategy. © 2021","Adversarial training; Bilingual-view translation; Code-switched text; Data mining; Emotion detection; Natural language processing; Sentiment analysis","Channel coding; Codes (symbols); Data mining; Signal encoding; Adversarial training; Bilingual-view translation; Bilinguals; Channel encoder; Code-switched text; Dual channel; Emotion detection; Neural modelling; Performance; Sentiment analysis; Sentiment analysis",Article,Scopus,2-s2.0-85117793867
"Lobo G.P., Laraway J., Gadgil A.J.","57225091214;57248746600;7006432543;","Identifying schools at high-risk for elevated lead in drinking water using only publicly available data",2022,"Science of the Total Environment","803",,"150046","","",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114386536&doi=10.1016%2fj.scitotenv.2021.150046&partnerID=40&md5=2afc5f9504a6fcd9b866b995ec1a095d","Estimating the risk of lead contamination of schools' drinking water at the State level is a complex, important, and unexplored challenge. Variable water quality among water systems and changes in water chemistry during distribution affect lead dissolution rates from pipes and fittings. In addition, the locations of lead-bearing plumbing materials are uncertain. We tested the capability of six machine learning models to predict the likelihood of lead contamination of drinking water at the schools' taps using only publicly available datasets. The predictive features used in the models correspond to those with a proven correlation to the dominant, but commonly unavailable, factors that govern lead leaching: the presence of lead-bearing plumbing materials and water quality conducive to lead corrosion. By combining water chemistry data from public reports, socioeconomic information from the US census, and spatial features using Geographic Information Systems, we trained and tested models to estimate the likelihood of lead contaminated tap water in over 8,000 schools across California and Massachusetts. Our best-performing model was a Random Forest, with a 10-fold cross validation score of 0.88 for Massachusetts and 0.78 for California using the average Area Under the Receiver Operating Characteristic Curve (ROC AUC) metric. The model was then used to assign a lead leaching risk category to half of the schools across California (the other half was used for training). There was good agreement between the modeled risk categories and the actual lead leaching outcomes for every school; however, the model overestimated the lead leaching risk in up to 17% of the schools. This model is the first of its kind to offer a tool to predict the risk of lead leaching in schools at the State level. Further use of this model can help deploy limited resources more effectively to prevent childhood lead exposure from school drinking water. © 2021","Environmental justice; Lead in school drinking water; Lead leaching; Machine learning; Public data mining","Contamination; Data mining; Decision trees; Information use; Leaching; Machine learning; Risk perception; Taps; Water quality; California; Environmental justice; Lead contamination; Lead in school drinking water; Lead leaching; Machine-learning; Massachusetts; Public data mining; Risk categories; Water chemistry; Potable water; drinking water; lead; tap water; lead; data mining; drinking water; environmental justice; leachate; lead; machine learning; risk assessment; water chemistry; Article; childhood; correlational study; corrosion; cross validation; environmental exposure; geographic information system; human; leaching; machine learning; predictive value; risk factor; school; socioeconomics; United States; water pollution; water quality; sanitation; school; California; Massachusetts; United States; Drinking Water; Lead; Machine Learning; Sanitary Engineering; Schools",Article,Scopus,2-s2.0-85114386536
"Jacob M.S., Selvi Rajendran P.","57226791592;57226792085;","Fuzzy artificial bee colony-based CNN-LSTM and semantic feature for fake product review classification",2022,"Concurrency and Computation: Practice and Experience","34","1","e6539","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112676165&doi=10.1002%2fcpe.6539&partnerID=40&md5=45b66f5004d75f05b957e1cfddad1ce1","In recent years, online reviews are considered as the most significant resource for consumers to make a decision regarding the purchase of a particular product. The reviews can either encourage or relegate a product; therefore posting fake reviews has turned into a money-spinning business in the modern period. The detection of fake reviews has become a center of attraction for various business people. This research study aims in detecting fake product reviews using four significant phases namely the data pre-processing, feature extraction, feature selection, and classification. The features obtained in the pre-processing phase are extracted and selected using chi-squared technique to obtain a delegate subset among all data and to reduce the complication issues. Then a CNNLSTM-FABC approach classifies and detects the review as fake or real. Finally, the performance evaluation and the comparative analysis are carried out to determine the effectiveness of the proposed approach. The results reveal that the proposed approach performs well irrespective of the product type and sentiment polarity. © 2021 John Wiley & Sons Ltd.","CNN; dataset; FABC; fake reviews; features; LSTM; product; real reviews","Data handling; Data mining; Feature extraction; Long short-term memory; Optimization; Purchasing; Semantics; Artificial bee colonies; Comparative analysis; Data preprocessing; Online reviews; Pre-processing; Product reviews; Research studies; Semantic features; Classification (of information)",Article,Scopus,2-s2.0-85112676165
"Nallusamy P., Reshmi T.R., Krishnan M.","57219592240;55581064200;57197856904;","AGFT: Adaptive entries aggregation scheme to prevent overflow in multiple flow table environment",2022,"Concurrency and Computation: Practice and Experience","34","1","e6491","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110263650&doi=10.1002%2fcpe.6491&partnerID=40&md5=026a0f80b2bf2266ca1f6281669f212a","The revolutionary architecture termed Software-Defined Network provides flexible network management by detaching the control logic from the underlying data plane. The flow table resides in Ternary Content Addressable Memory imparts rules for the incoming flows with a limitation of high cost, limited storage, and consumes high power. In data center networks, when the traffic rate is high the overflow occurs due to storage limitation with high packet drop, frequent rule miss, and severe controller overhead. To overcome these challenges and to provide Quality of Service to the current network design sketch-based entry reduction scheme is proposed. It incorporates three concurrent modules integrated to function sequentially where (1) Periodical analysis in Multiple Flow Tables is performed to ensure the availability of redundant entries using the robust data mining algorithm called Term Frequency. (2) Recurrent entries are further classified and clustered using the Boyer–Moore pattern matching algorithm to facilitate the forthcoming aggregation process. (3) A compact flow table is achieved with a customized multibit trie using Huffman coding compression technique. The experimental outcomes prove that this work prevents overflow by 99.98% with 98.99% enhanced flow table space and provides a significant reduction of controller overhead than the existing schemes. © 2021 John Wiley & Sons, Ltd.","flow table; Huffman coding algorithm; Multibit trie; SDN; TCAM; term frequency","Data mining; Digital storage; Memory architecture; Pattern matching; Quality of service; Recurrent neural networks; Aggregation process; Aggregation schemes; Compression techniques; Data center networks; Entry reductions; Reduction of controller; Storage limitation; Ternary content addressable memory; Information management",Article,Scopus,2-s2.0-85110263650
"Huang H.-Y., Lin Y.-C.-D., Cui S., Huang Y., Tang Y., Xu J., Bao J., Li Y., Wen J., Zuo H., Wang W., Li J., Ni J., Ruan Y., Li L., Chen Y., Xie Y., Zhu Z., Cai X., Chen X., Yao L., Chen Y., Luo Y., LuXu S., Luo M., Chiu C.-M., Ma K., Zhu L., Cheng G.-J., Bai C., Chiang Y.-C., Wang L., Wei F., Lee T.-Y., Huang H.-D.","37025368700;57215286625;57215271361;57221544358;57215280110;57215283977;57424780700;57424780800;57424730700;55336499000;57424709800;57276168100;57424709900;57424747400;57424675700;57424693700;57215280084;57424710000;57215280046;57221544964;57376848600;57215302396;57424765500;57424730800;57424710100;57424710200;57424730900;57424710600;57424710700;57424660700;57424710800;57424765600;57424675900;8503006200;7405615434;","miRTarBase update 2022: an informative resource for experimentally validated miRNA-target interactions",2022,"Nucleic acids research","50","D1",,"D222","D230",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123389461&doi=10.1093%2fnar%2fgkab1079&partnerID=40&md5=9c3abbed4d72d85d46bb31db2476a3c6","MicroRNAs (miRNAs) are noncoding RNAs with 18-26 nucleotides; they pair with target mRNAs to regulate gene expression and produce significant changes in various physiological and pathological processes. In recent years, the interaction between miRNAs and their target genes has become one of the mainstream directions for drug development. As a large-scale biological database that mainly provides miRNA-target interactions (MTIs) verified by biological experiments, miRTarBase has undergone five revisions and enhancements. The database has accumulated >2 200 449 verified MTIs from 13 389 manually curated articles and CLIP-seq data. An optimized scoring system is adopted to enhance this update's critical recognition of MTI-related articles and corresponding disease information. In addition, single-nucleotide polymorphisms and disease-related variants related to the binding efficiency of miRNA and target were characterized in miRNAs and gene 3' untranslated regions. miRNA expression profiles across extracellular vesicles, blood and different tissues, including exosomal miRNAs and tissue-specific miRNAs, were integrated to explore miRNA functions and biomarkers. For the user interface, we have classified attributes, including RNA expression, specific interaction, protein expression and biological function, for various validation experiments related to the role of miRNA. We also used seed sequence information to evaluate the binding sites of miRNA. In summary, these enhancements render miRTarBase as one of the most research-amicable MTI databases that contain comprehensive and experimentally verified annotations. The newly updated version of miRTarBase is now available at https://miRTarBase.cuhk.edu.cn/. © The Author(s) 2021. Published by Oxford University Press on behalf of Nucleic Acids Research.",,"biological marker; microRNA; untranslated RNA; 3' untranslated region; animal; binding site; chemistry; classification; computer interface; data mining; exosome; gene expression regulation; gene regulatory network; genetics; human; Internet; metabolism; molecular genetics; mouse; neoplasm; nucleic acid database; pathology; single nucleotide polymorphism; tumor cell culture; 3' Untranslated Regions; Animals; Binding Sites; Biomarkers; Data Mining; Databases, Nucleic Acid; Exosomes; Gene Expression Regulation; Gene Regulatory Networks; Humans; Internet; Mice; MicroRNAs; Molecular Sequence Annotation; Neoplasms; Polymorphism, Single Nucleotide; RNA, Untranslated; Tumor Cells, Cultured; User-Computer Interface",Article,Scopus,2-s2.0-85123389461
"Cai L., Xuan J., Lin Q., Wang J., Liu S., Xie F., Zheng L., Li B., Qu L., Yang J.","57424700100;57200335590;57424753300;57221548473;57191495755;57221545602;56060056000;57424736900;7103292439;57424753400;","Pol3Base: a resource for decoding the interactome, expression, evolution, epitranscriptome and disease variations of Pol III-transcribed ncRNAs",2022,"Nucleic acids research","50","D1",,"D279","D286",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123388926&doi=10.1093%2fnar%2fgkab1033&partnerID=40&md5=089d014b2dd9b99feff594d4e079d3aa","RNA polymerase III (Pol III) transcribes hundreds of non-coding RNA genes (ncRNAs), which involve in a variety of cellular processes. However, the expression, functions, regulatory networks and evolution of these Pol III-transcribed ncRNAs are still largely unknown. In this study, we developed a novel resource, Pol3Base (http://rna.sysu.edu.cn/pol3base/), to decode the interactome, expression, evolution, epitranscriptome and disease variations of Pol III-transcribed ncRNAs. The current release of Pol3Base includes thousands of regulatory relationships between ∼79 000 ncRNAs and transcription factors by mining 56 ChIP-seq datasets. By integrating CLIP-seq datasets, we deciphered the interactions of these ncRNAs with >240 RNA binding proteins. Moreover, Pol3Base contains ∼9700 RNA modifications located within thousands of Pol III-transcribed ncRNAs. Importantly, we characterized expression profiles of ncRNAs in >70 tissues and 28 different tumor types. In addition, by comparing these ncRNAs from human and mouse, we revealed about 4000 evolutionary conserved ncRNAs. We also identified ∼11 403 tRNA-derived small RNAs (tsRNAs) in 32 different tumor types. Finally, by analyzing somatic mutation data, we investigated the mutation map of these ncRNAs to help uncover their potential roles in diverse diseases. This resource will help expand our understanding of potential functions and regulatory networks of Pol III-transcribed ncRNAs. © The Author(s) 2021. Published by Oxford University Press on behalf of Nucleic Acids Research.",,"DNA directed RNA polymerase III; RNA binding protein; transcription factor; transfer RNA; untranslated RNA; animal; classification; data mining; gene expression regulation; gene regulatory network; genetic database; genetic transcription; genetics; human; information processing; Internet; metabolism; molecular evolution; mouse; mutation; neoplasm; pathology; software; Animals; Data Mining; Databases, Genetic; Datasets as Topic; Evolution, Molecular; Gene Expression Regulation; Gene Regulatory Networks; Humans; Internet; Mice; Mutation; Neoplasms; RNA Polymerase III; RNA, Transfer; RNA, Untranslated; RNA-Binding Proteins; Software; Transcription Factors; Transcription, Genetic",Article,Scopus,2-s2.0-85123388926
"Walsh A.T., Triant D.A., Le Tourneau J.J., Shamimuzzaman M., Elsik C.G.","57213181659;57229611900;57213181975;37010130900;6602625086;","Hymenoptera Genome Database: new genomes and annotation datasets for improved go enrichment and orthologue analyses",2022,"Nucleic acids research","50","D1",,"D1032","D1039",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123388152&doi=10.1093%2fnar%2fgkab1018&partnerID=40&md5=d9cfab6e74f9c93f938a32693544ef87","We report an update of the Hymenoptera Genome Database (HGD; http://HymenopteraGenome.org), a genomic database of hymenopteran insect species. The number of species represented in HGD has nearly tripled, with fifty-eight hymenopteran species, including twenty bees, twenty-three ants, eleven wasps and four sawflies. With a reorganized website, HGD continues to provide the HymenopteraMine genomic data mining warehouse and JBrowse/Apollo genome browsers integrated with BLAST. We have computed Gene Ontology (GO) annotations for all species, greatly enhancing the GO annotation data gathered from UniProt with more than a ten-fold increase in the number of GO-annotated genes. We have also generated orthology datasets that encompass all HGD species and provide orthologue clusters for fourteen taxonomic groups. The new GO annotation and orthology data are available for searching in HymenopteraMine, and as bulk file downloads. © The Author(s) 2021. Published by Oxford University Press on behalf of Nucleic Acids Research.",,"animal; biology; classification; genetic database; genetics; genomics; Hymenoptera; insect genome; molecular genetics; software; Animals; Computational Biology; Databases, Genetic; Genome, Insect; Genomics; Hymenoptera; Molecular Sequence Annotation; Software",Article,Scopus,2-s2.0-85123388152
"Tang G., Cho M., Wang X.","57424782700;57424677600;57358532600;","OncoDB: an interactive online database for analysis of gene expression and viral infection in cancer",2022,"Nucleic acids research","50","D1",,"D1334","D1339",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121721577&doi=10.1093%2fnar%2fgkab970&partnerID=40&md5=618ef8067f74b739d7e7e9778651551a","Large-scale multi-omics datasets, most prominently from the TCGA consortium, have been made available to the public for systematic characterization of human cancers. However, to date, there is a lack of corresponding online resources to utilize these valuable data to study gene expression dysregulation and viral infection, two major causes for cancer development and progression. To address these unmet needs, we established OncoDB, an online database resource to explore abnormal patterns in gene expression as well as viral infection that are correlated to clinical features in cancer. Specifically, OncoDB integrated RNA-seq, DNA methylation, and related clinical data from over 10 000 cancer patients in the TCGA study as well as from normal tissues in the GTEx study. Another unique aspect of OncoDB is its focus on oncoviruses. By mining TCGA RNA-seq data, we have identified six major oncoviruses across cancer types and further correlated viral infection to changes in host gene expression and clinical outcomes. All the analysis results are integratively presented in OncoDB with a flexible web interface to search for data related to RNA expression, DNA methylation, viral infection, and clinical features of the cancer patients. OncoDB is freely accessible at http://oncodb.org. © The Author(s) 2021. Published by Oxford University Press on behalf of Nucleic Acids Research.",,"computer interface; data mining; DNA methylation; gene expression profiling; gene expression regulation; genetic database; genetics; human; Internet; neoplasm; procedures; software; virology; virus infection; Data Mining; Databases, Genetic; DNA Methylation; Gene Expression Profiling; Gene Expression Regulation, Neoplastic; Humans; Internet; Neoplasms; RNA-Seq; Software; User-Computer Interface; Virus Diseases",Article,Scopus,2-s2.0-85121721577
"Zhang E., Li H., Huang Y., Hong S., Zhao L., Ji C.","48362054400;57292445400;57292214400;57291756500;57222902172;57292669600;","Practical multi-party private collaborative k-means clustering",2022,"Neurocomputing","467",,,"256","265",,4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116855097&doi=10.1016%2fj.neucom.2021.09.050&partnerID=40&md5=e093d0dc4c9207795053d4ab1900a074","k-means clustering is widely used in many fields such as data mining, machine learning, and information retrieval. In many cases, users need to cooperate to perform k-means clustering tasks. How to perform clustering without revealing privacy has become a hot research topic. However, the existing k-means scheme based on secure multi-party computation cannot effectively protect the privacy of the output results. The multi-party k-means scheme based on differential privacy may lead to loss of data availability. In this article, we propose a practical protocol for k-means clustering in a collaborative manner, while protecting the privacy of each data record. Our protocol is the first to combine secure multi-party computing and differential privacy technology to train a privacy-preserving k-means clustering model. We design a novel algorithm, which is suitable for multi-party collaboration to update cluster centers without leaking data privacy. The algorithm guarantees that noise is added only once in each iteration, regardless of the number of participants. The protocol achieve the ”best of both worlds”, which can simultaneously achieves both the input privacy and the output privacy in the k-means clustering scheme. Evaluation of real data sets shows that our scheme has comparable running time compared with the k-means clustering scheme without privacy protection. © 2021 Elsevier B.V.","Differential privacy; k-means clustering; Privacy preserving; Secure multi-party computation","Data mining; Iterative methods; K-means clustering; Clustering scheme; Clusterings; Data availability; Differential privacies; Hot research topics; K-means; K-means++ clustering; Privacy preserving; Secure multi-party computation; User need; Data privacy; adult; article; data availability; data mining; data privacy; human; information retrieval; k means clustering; noise; running",Article,Scopus,2-s2.0-85116855097
"Balashunmugaraja B., Ganeshbabu T.R.","57219329507;57369489500;","Privacy preservation of cloud data in business application enabled by multi-objective red deer-bird swarm algorithm",2022,"Knowledge-Based Systems","236",,"107748","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120965694&doi=10.1016%2fj.knosys.2021.107748&partnerID=40&md5=802eca181b9c8037684ce4b4d35161bf","Decision-making is one of the important deals in knowledge transfer and it can be useful for the multi-source domains. Meanwhile, the existing knowledge transfer schemes do not use privacy-preserving techniques for preserving security. This can be a problem for critical domains like financial market forecasting as the misuse of security can lead to legal and financial implications. In recent years, cloud services have revolutionized various technological applications. Cloud computing has become more popular with digital technologies as it provides uninterrupted services like transmission, storage, and intensive computing of data. The architecture of the cloud is also cost-efficient. Besides, various promising services from the cloud, some challenges need to be addressed to secure the privacy of the cloud users as millions of users access its services. Privacy preservation is an important aspect in the field of data mining, and the necessity of securing important data in the cloud from hackers is on the rise. Privacy-preserving data mining algorithms have been analyzed over recent years to provide sufficient solutions for securing the privacy of the data in the cloud. This paper plans to introduce a new hybrid meta-heuristic concept for developing a privacy preservation strategy towards business data under the cloud sector. The main objective of this paper is to design a new hybrid red deer-bird swarm algorithm (RD-BSA) to ensure higher convergence and since the use of control parameters over the solution generation is minimized. The proposed privacy preservation scheme on three financial databases is evaluated with the performance against the existing privacy preservation schemes. Different analyses like statistical, key sensitivity, Known-Plaintext Attack (KPA), and Chosen-Plaintext Attack (CPA) are used for evaluating the efficiency of the algorithm. The comparative analysis of the proposed model over the conventional models demonstrates its effective performance via diverse analysis. © 2021 Elsevier B.V.","Business application; Cloud computing; Data restoration; Data sanitization; Hybrid red deer-bird swarm algorithm; Optimal key generation; Privacy-preserving data mining","Birds; Cloud computing; Cloud data security; Data mining; Decision making; Digital storage; Finance; Personal computing; Privacy-preserving techniques; Business applications; Cloud-computing; Data restoration; Data sanitization; Hybrid red deer-bird swarm algorithm; Key generation; Optimal key generation; Privacy-preserving data mining; Sanitization; Swarm algorithms; Knowledge management",Article,Scopus,2-s2.0-85120965694
"Yang K., Qi H., Huang Q.","57210595411;57221723305;57217827202;","The impact of task description linguistic style on task performance: a text mining of crowdsourcing contests",2022,"Industrial Management and Data Systems","122","1",,"322","344",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119008864&doi=10.1108%2fIMDS-03-2021-0178&partnerID=40&md5=c3f78739e62349128dc9852c9971b19c","Purpose: Existing studies on the relationship between task description and task performance are insufficient, with many studies considering description length rather than content to measure quality or only evaluating a single aspect of task performance. To address this gap, this study analyzes the linguistic styles of task descriptions from 2,545 tasks on the Taskcn.com crowdsourcing platform. Design/methodology/approach: An empirical analysis was completed for task description language styles and task performance. The paper used text mining tool Simplified Chinese Linguistic Inquiry and Word Count to extract eight linguistic styles, namely readability, self-distancing, cognitive complexity, causality, tentative language, humanizing personal details, normative information and language intensity. And it tests the relationship between the eight language styles and task performance. Findings: The study found that more cognitive complexity markers, tentative language, humanized details and normative information increase the quantity of submissions for a task. In addition, more humanized details and normative information in a task description improves the quality of task. Conversely, the inclusion of more causal relationships in a task description reduces the quantity of submissions. Poorer readability of the task description, less self-estrangement and higher language intensity reduces the quality of the task. Originality/value: This study first reveals the importance of the linguistic styles used in task descriptions and provides a reference for how to attract more task solvers and achieve higher quality task performance by improving task descriptions. The research also enriches existing knowledge on the impact of linguistic styles and the applications of text mining. © 2021, Emerald Publishing Limited.","Linguistic styles; Online crowdsourcing contests; Open innovation; Task description; Task performance; Text mining","Data mining; Linguistics; Cognitive complexity; Crowdsourcing platforms; Design/methodology/approach; Empirical analysis; Linguistic styles; Online crowdsourcing contest; Open innovation; Task description; Task performance; Text-mining; Crowdsourcing",Article,Scopus,2-s2.0-85119008864
"Bijalwan V., Semwal V.B., Gupta V.","57201040008;56081893700;57218941470;","Wearable sensor-based pattern mining for human activity recognition: deep learning approach",2022,"Industrial Robot","49","1",,"21","33",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103658765&doi=10.1108%2fIR-09-2020-0187&partnerID=40&md5=e21dc2bcd15a2cc94745f4fa963cba04","Purpose: This paper aims to deal with the human activity recognition using human gait pattern. The paper has considered the experiment results of seven different activities: normal walk, jogging, walking on toe, walking on heel, upstairs, downstairs and sit-ups. Design/methodology/approach: In this current research, the data is collected for different activities using tri-axial inertial measurement unit (IMU) sensor enabled with three-axis accelerometer to capture the spatial data, three-axis gyroscopes to capture the orientation around axis and 3° magnetometer. It was wirelessly connected to the receiver. The IMU sensor is placed at the centre of mass position of each subject. The data is collected for 30 subjects including 11 females and 19 males of different age groups between 10 and 45 years. The captured data is pre-processed using different filters and cubic spline techniques. After processing, the data are labelled into seven activities. For data acquisition, a Python-based GUI has been designed to analyse and display the processed data. The data is further classified using four different deep learning model: deep neural network, bidirectional-long short-term memory (BLSTM), convolution neural network (CNN) and CNN-LSTM. The model classification accuracy of different classifiers is reported to be 58%, 84%, 86% and 90%. Findings: The activities recognition using gait was obtained in an open environment. All data is collected using an IMU sensor enabled with gyroscope, accelerometer and magnetometer in both offline and real-time activity recognition using gait. Both sensors showed their usefulness in empirical capability to capture a precised data during all seven activities. The inverse kinematics algorithm is solved to calculate the joint angle from spatial data for all six joints hip, knee, ankle of left and right leg. Practical implications: This work helps to recognize the walking activity using gait pattern analysis. Further, it helps to understand the different joint angle patterns during different activities. A system is designed for real-time analysis of human walking activity using gait. A standalone real-time system has been designed and realized for analysis of these seven different activities. Originality/value: The data is collected through IMU sensors for seven activities with equal timestamp without noise and data loss using wirelessly. The setup is useful for the data collection in an open environment outside the laboratory environment for activity recognition. The paper also presents the analysis of all seven different activity trajectories patterns. © 2020, Emerald Publishing Limited.","Deep learning; Gait analysis; Human activity recognition (HAR); IMU sensor; Wearable sensor","Accelerometers; Agricultural robots; Data acquisition; Data handling; Data mining; Deep learning; Deep neural networks; Filtration; Gait analysis; Gyroscopes; Interactive computer systems; Inverse kinematics; Joints (anatomy); Long short-term memory; Magnetometers; Pattern recognition; Physiological models; Real time systems; Walking aids; Activities recognition; Convolution neural network; Design/methodology/approach; Human activity recognition; Inertial measurement unit; Inverse kinematics algorithms; Real-time activity recognition; Three axis accelerometers; Wearable sensors",Article,Scopus,2-s2.0-85103658765
"Lu H., Wang R., Huang Z.","57465676800;57465097800;57465239300;","Application of Data Mining in Performance Management of Public Hospitals",2022,"Mobile Information Systems","2022",,"2412928","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125203195&doi=10.1155%2f2022%2f2412928&partnerID=40&md5=bb233760c6000ce234bc86fcfe2b20c5","With the rapid development of computer technology, information technology covers all aspects of daily life, and the medical industry is also paying more attention to information construction. Conventional management methods have been unable to further improve the hospital's management capabilities. At the same time, countries that are better in terms of hospital management practices have set a benchmark for mainland hospitals and reformed hospitals in order to stand out in the future. In addition to evaluating the economic benefits and work efficiency of doctors, hospitals must also consider that hospitals, as a special service industry, cannot be measured by economic indicators. Therefore, there is a multiparty game in the performance appraisal of hospitals, and it is necessary to consider not only economic factors but also the characteristics of public services. This article is based on the case of a large domestic tertiary hospital, combined with the hospital's performance management reform plan, through the design idea of performance management and incentive performance pay distribution, using data mining technology as an auxiliary means. It successfully helped the hospital complete the performance and incentive performance pay aspects reform. The main research work of this paper is divided into the following three aspects. (1) Using data mining technology, according to each nursing unit's workload, risk level, the difficulty of internship, and other objective factors in the past year for patient outpatient visits, surgery implementation, critical first aid, etc., are classified in line with the actual situation and provide a reliable basis for the reasonable and efficient allocation of hospital human resources. (2) In the performance management system, we integrate the third-party data mining tool weka to assist in the evaluation of the performance distribution plan and the calculation of the follow-up incentive performance pay. (3) We use the mathematical model of data mining to measure and evaluate the reasonableness of historical workload and performance appraisal, determine a new incentive performance pay distribution model, and use the software as a calculation tool for the internal distribution of performance wages to provide monthly incentive performance wage statistics in the future. © 2022 Hindawi Limited. All rights reserved.",,"Data mining; Information management; Medical computing; Medical information systems; Wages; Computer technology; Daily lives; Data mining technology; Hospital management; Information construction; Medical industries; Performance; Performance appraisal; Performance management; Performance pays; Hospitals",Article,Scopus,2-s2.0-85125203195
"Bavarsad Salehpour H., Asghari P., Haj Seyyed Javadi H., Shiri M.E.","57465435100;56719715600;57465580900;22735248800;","IAMnet: Presentation of Parallelization Approach for Repetitive Mining on Network Topologies with an Improved Apriori Method",2022,"Wireless Communications and Mobile Computing","2022",,"8217774","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125170818&doi=10.1155%2f2022%2f8217774&partnerID=40&md5=affcb19fffcb4a017a8de31e299fe372","Recently, the discovery of association rules and the consequent mining frequent patterns have attracted the attention of many researchers to discover unknown relationships in big data, especially in networking and distributed environments. In this research, a parallelization-based approach is proposed to improve the performance of the Apriori algorithm in repetitive mining patterns on network topologies. The proposed approach includes two main features: (1) combining centrality criteria of the node and the Apriori algorithm to identify repetitive patterns and (2) using the mapping/reduction method to create parallel processing and achieve optimal values in the shortest time. This approach also pursues three main objectives: reducing the temporal and spatial complexity of the Apriori algorithm, improving the association rules mining process and identifying repetitive patterns, and comparing the proposed approach's performance on different network topologies to determine the advantages and disadvantages of each topology. Comparing our proposed method and the basic Apriori algorithm, it is concluded that our approach provides acceptable efficiency in terms of evaluation criteria such as energy consumption, network lifetime, and runtime compared to other methods. Experimental results also show that when using our proposed method compared to the basic Apriori algorithm, network life is increased by 7.1%, the runtime is reduced by 43.2%, and the energy consumption is saved by about 41.2%. © 2022 Hindawi Limited. All rights reserved.",,"Association rules; Data mining; Energy utilization; Topology; Apriori; Apriori algorithms; Distributed environments; Energy-consumption; Network topology; Networking environment; Parallelizations; Performance; Repetitive pattern; Runtimes; Learning algorithms",Article,Scopus,2-s2.0-85125170818
"Huang X.","57447062600;","Application of Computer Data Mining Technology Based on AKN Algorithm in Denial of Service Attack Defense Detection",2022,"Wireless Communications and Mobile Computing","2022",,"4729526","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125147781&doi=10.1155%2f2022%2f4729526&partnerID=40&md5=3ad7915d38b131a1d70e5492e3ba4815","Denial of service attacks have become one of the most difficult network security problems because they are easy to implement, difficult to prevent, and difficult to track, and they have brought great harm to the network society. Denial of service (Dos) is a phenomenon in which a large number of useless data packets or obstructive content are maliciously transmitted to the target server, which makes the target server unable to provide users with normal services. Denial of service attack (Dos attack) is a very typical network attack method, and the main harm of Dos attack is to exhaust service resources, making the computer or network unable to provide normal services. And AKN (adaptive Kohonen network) is an adaptive neural network proposed in recent years, and an algorithm summarized by using the characteristics of the neural network is called the AKN algorithm. This algorithm can realize fast, low-consumption, and high-precision denial of service detection in complex networks. In the era of big data, network security is becoming more and more important, and in order to maintain the security of network data, this article studies the common forms and principles of Dos attacks, as well as the current corresponding defense detection methods. It also investigates several commonly used algorithms of computer data mining technology, such as clustering algorithms, classification algorithms, neural network algorithms, regression algorithms, website data mining, and association algorithms, and proposes a computer data mining model based on the AKN algorithm. In addition, the computer data mining technology based on the AKN algorithm is used to conduct defensive detection experiments under Dos attacks and compares with classic algorithms. Experimental results show that experiments based on the AKN algorithm have better defensive detection effects than classic algorithms, with a detection accuracy rate of more than 97% and a detection efficiency improvement of more than 20%. © 2022 Xiang Huang.",,"Clustering algorithms; Complex networks; Data mining; Network security; Classic algorithm; Data mining technology; Data packet; Denial of Service; Denialof- service attacks; Kohonen network; Network algorithms; Network security problems; Network society; Technology-based; Denial-of-service attack",Article,Scopus,2-s2.0-85125147781
"Xiong S.","57464301200;","The Innovative Development Path of Financial Media Based on Mobile Edge Computing Technology from the Perspective of Rural Revitalization",2022,"Wireless Communications and Mobile Computing","2022",,"5671393","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125135474&doi=10.1155%2f2022%2f5671393&partnerID=40&md5=8b1ac9d453b4f907912dea4193593f94","Rural Revitalization is a systematic and complex strategic project that must keep up with the times while also taking into account the times' characteristics and social development. The media should be at the forefront of rural revitalization efforts. The interaction of various ""forces""is required for the successful implementation of the rural revitalization strategy. The county-level financial media center, as a ""force of the media,""will encourage mainstream media to overcome their own development challenges, become an important mainstream public opinion publicity platform, and aid in the realization of the great cause of rural revitalization. For rural comprehensive revitalization, big data provides new means and tools. It is not only a powerful support for rural comprehensive revitalization, but also an important means of realizing the integrated development of the digital economy and rural revitalization, as well as improving the quality and level of rural revitalization. We need to accelerate the realization of agricultural and rural modernization to implement the rural revitalization strategy, and the construction of smart countryside is an important embodiment of agricultural and rural modernization. Mobile edge computing technology has become ingrained in many aspects of social production and daily life. With the power of MEC, urban radio and television must accelerate the pace of multiscreen and multifrequency interaction, as well as online and offline integration; create an all-media instant communication and interactive platform; make new media products suitable for rural audiences; realize integrated development of urban and rural areas; and improve the influence and credibility of new media on agricultural communication. From the perspective of rural revitalization, this paper will investigate the development path of financial media under the influence of data mining (DM) and mobile edge computing technology. © 2022 Siyu Xiong.",,"Agriculture; Data mining; Finance; Mobile edge computing; Social aspects; Characteristic development; Computing technology; County level; Development path; Integrated development; Mainstream media; Media center; New media; Social development; Time characteristics; Rural areas",Article,Scopus,2-s2.0-85125135474
"Cai L.","57463836000;","A Novel Recognition Model of University Students' Psychological Crisis Based on DM",2022,"Scientific Programming","2022",,"8908848","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125127337&doi=10.1155%2f2022%2f8908848&partnerID=40&md5=e12700cdaed00e6996775579e8d7532c","The research on identifying college students' psychological crises can assist college administrators in better understanding of students' psychological changes and planning of intervention measures ahead of time to ensure college students' mental health. When confronted with unexpected events or major setbacks and difficulties, people experience psychological crises, which they can neither avoid nor solve using their own resources and stress methods. This paper proposes a method of college students' psychological crisis identification based on data mining technology to solve the problems that exist in the process of identifying psychological crises in college students and to improve the correct rate of identifying psychological crises in college students. In comparison to other technologies, data mining can uncover the link between students' psychological problems and their basic information, as well as the main causes of psychological problems, using statistical data. © 2022 Lihuang Cai.",,"Data mining; College students; Data mining technology; Intervention measures; Mental health; Model of university; Recognition models; Student's psychological crisis; Technology data; Unexpected events; University students; Students",Article,Scopus,2-s2.0-85125127337
"An S., Yu Z.","57464005100;57463727400;","Mental and Emotional Recognition of College Students Based on Brain Signal Features and Data Mining",2022,"Security and Communication Networks","2022",,"4198353","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125115530&doi=10.1155%2f2022%2f4198353&partnerID=40&md5=531e60858b702327ef0b31ccadaa47e3","Nowadays, people pay more and more attention to the psychological situation of college students. Using data mining technology to model and analyze the collected psychological data of college students is a research hotspot in psychology and computer science. In addition, the essence of human emotional change is the higher nervous activity in the cerebral cortex. Electroencephalography (EEG) has become an important feature signal for emotion recognition because of its high time resolution and portability and practicality. Therefore, to solve the problem that the accuracy and generalization of the existing research models are not ideal, a method of college students' psychological emotion recognition based on EEG signal features and data mining is proposed. Firstly, a feature selection method based on sparse learning is used to find out a few features from the high-dimensional feature space that contribute greatly to the reconstruction of category information so as to quickly acquire a few key emotion-related features. Then, the entropy-weighted clustering algorithm is combined with sparse learning feature selection, and the local structure of heterogeneous data is divided. Experimental results show that, compared with traditional methods, the proposed method has stronger applicability and higher accuracy of five categories of emotions, which provides a valuable reference for the evaluation of depression and anxiety of college students based on brain signal characteristics. © 2022 Shaobo An and Zhifen Yu.",,"Clustering algorithms; Data mining; Electrophysiology; Feature extraction; Speech recognition; Students; Brain signals; College students; Data mining technology; Emotion recognition; Emotional recognition; Hotspots; Modelling and analysis; Signal data; Signal features; Student-based; Electroencephalography",Article,Scopus,2-s2.0-85125115530
"Hu H., Yin M., Li J.","57216870779;57462341100;57211027132;","Evolution of Business Intelligence: An Analysis from the Perspective of Social Network",2022,"Tehnicki Vjesnik","29","2",,"497","503",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125077828&doi=10.17559%2fTV-20210819071232&partnerID=40&md5=2472717bccda5d908e685f9b920a05ad","Based on CiteSpace, Pajek and other software, this paper makes a visual analysis of the knowledge graph of the related literature of Business Intelligence and explores the future development trend of business intelligence. Taking the core periodicals of CNKI as the data source, key words are drawn and analyzed with the help of software. The total number of articles was 2938 from 2006 to 2020, and the number of articles published in the past 15 years was gradually levelled off. Among the 607 researchers, Yang Bingru is the representative; there are 424 journals, Journal of Information is the first, and 787 keywords are the most frequently used data mining. Our country still needs in-depth research in the field of business intelligence. Through the atlas, it directly shows that big data and machine learning are the frontier hot spots of future development, which provides research direction for researchers. © 2022, Strojarski Facultet. All rights reserved.","Business Intelligence; Knowledge Atlas; Research Status; Social Network Analysis","Knowledge graph; Social networking (online); Social sciences computing; Business-intelligence; Citespace; Data-source; Development trends; Key words; Knowledge atlas; Knowledge graphs; Research status; Social Network Analysis; Visual analysis; Data mining",Article,Scopus,2-s2.0-85125077828
"Mastromattei M., Ranaldi L., Fallucchi F., Zanzotto F.M.","57290356800;57217633494;23392345500;7801422424;","Syntax and prejudice: ethically-charged biases of a syntax-based hate speech recognizer unveiled",2022,"PeerJ Computer Science","8",,"e859","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125058825&doi=10.7717%2fpeerj-cs.859&partnerID=40&md5=87e1288c4534e9bfa93078e4d8a0c7c8","Hate speech recognizers (HSRs) can be the panacea for containing hate in social media or can result in the biggest form of prejudice-based censorship hindering people to express their true selves. In this paper, we hypothesized how massive use of syntax can reduce the prejudice effect in HSRs. To explore this hypothesis, we propose Unintended-bias Visualizer based on Kermit modeling (KERM-HATE): a syntax-based HSR, which is endowed with syntax heat parse trees used as a post-hoc explanation of classifications. KERM-HATE significantly outperforms BERT-based, RoBERTa-based and XLNet-based HSR on standard datasets. Surprisingly this result is not sufficient. In fact, the post-hoc analysis on novel datasets on recent divisive topics shows that even KERM-HATE carries the prejudice distilled from the initial corpus. Therefore, although tests on standard datasets may show higher performance, syntax alone cannot drive the ‘‘attention“ of HSRs to ethically-unbiased features. © 2022 Mastromattei et al. All Rights Reserved.","Artificial Intelligence; Bias; Data Mining and Machine Learning; Explainability; Hate speech; Natural Language and Speech; Neural networks; Syntax","Digital storage; Machine learning; Speech; Speech recognition; Syntactics; Bias; Data mining and machine learning; Explainability; Hate speech; Machine-learning; Natural languages; Natural speech; Neural-networks; Social media; Speech recognizer; Data mining",Article,Scopus,2-s2.0-85125058825
"Folorunsho O., Adegbola I.A., Jimoh R.G.","57211334651;57462806300;57192954046;","AN ENHANCED FEATURE SELECTION AND CLASSIFICATION MODEL FOR NETWORK INTRUSION DETECTION SYSTEM USING DATA MINING TECHNIQUES",2022,"Indian Journal of Computer Science and Engineering","13","1",,"145","156",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125044804&doi=10.21817%2findjcse%2f2022%2fv13i1%2f221301081&partnerID=40&md5=0bcfa885a330ecf86da28fc7321c2a0d","Security of information in this Information Technology (IT) era has been one of the challenges facing individuals and organisations. Among the measures developed by security experts to counter security threats is the Intrusion Detection System (IDS). Despite earlier research efforts to develop formidable IDSs, the existing systems still suffer from a high false alarm and inability to detect new (novel) attacks because of the high volume of features in network traffic. Therefore, this study aimed at developing IDS with an enhanced feature selection and classification method using two stages of attack identification. The feature selection phase employed Particle Swarm Optimization (PSO) to optimally select relevant features from Principal Component Analysis (PCA)'s projected principal space. The reduced dataset was passed into the misuse detector using C4.5 to classify network traffic into normal and attack. The ""assumed"" normal traffic further passed to the anomaly detector, the second-level classifier using Support Vector Machine (SVM) for detecting new attacks that the misuse detector has not previously detected. The proposed model was demonstrated on the KDD Cup’99 and NSL-KDD intrusion datasets, with the system achieving a false alarm rate of 0.53% and detection rate of 99.43% for NSL KDD dataset. The results show that enhancing the feature selection phase and classification method reduces the false alarm and improves the system's ability to detect zero-day attacks. © 2022, Engg Journals Publications. All rights reserved.","Classification; Data Mining; Feature selection; Network traffic",,Article,Scopus,2-s2.0-85125044804
"Zhang L., Chen H., Zheng M.","57461623800;57461901800;57222297864;","Research on risk assessment method of energy system based on data mining",2022,"International Journal of Global Energy Issues","44","1",,"47","64",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125010806&partnerID=40&md5=f448f06d8c95d69a6b5bb996170536fa","In order to overcome the problem of data index confusion and index weight ambiguity in the traditional energy system risk assessment process, this paper proposes an energy system risk assessment method based on data mining. This method USES data mining technology and quantitative index processing method to select risk assessment index of energy system, construct risk assessment index system of energy system, determine the weight of risk assessment index of energy system, and build risk assessment model of energy system on this basis. The experimental results show that the weighing accuracy and evaluation accuracy of the proposed method are above 90%, and the skewness coefficient is always close to 0. The method has a high degree of rationality in energy risk index selection, high precision in index weight and high accuracy in evaluation results, which can effectively guarantee the safety of urban energy system. Copyright © 2022 Inderscience Enterprises Ltd.","Data mining; Energy system; Indicator system; Risk assessment","Risk assessment; Assessment index; Assessment process; Data mining technology; Energy systems; Index weight; Indicators systems; Risk assessment methods; Risks assessments; System risk assessment; Technology indices; Data mining",Article,Scopus,2-s2.0-85125010806
"Wang R., Han L.","57460669500;57460694400;","Analysis of the Model for Sports Enhancing Human Health Using Data Mining",2022,"Journal of healthcare engineering","2022",,,"3416255","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125002581&doi=10.1155%2f2022%2f3416255&partnerID=40&md5=e75ed585d34f1a9ffdf43fdfd7deaa28","The problems of low reliability and the high fitting degree of mutual information feature extraction of traditional sports to human health enhancement model are analyzed. We analyze and study the sports to human health enhancement model using data mining. The model consists of a data layer, a logic layer, and a presentation layer. Sports project data, real-time sports data, and health monitoring data are collected in the data layer, and the collected data are transmitted to the logic layer. The logical layer uses the dynamic difference feature classification algorithm of data mining to fuse human health data, extract the mutual information features of human health, and input the features into the long short-term memory (LSTM) neural network, which outputs the pattern recognition results of sports health after forward and reverse operations. The results of sports health pattern recognition are input into the display layer, and the enhancing effect of sports on human health is presented for users by constructing a model of sports on human health. The results show that the effect of sports on human health enhancement analyzed by the model in this paper is extremely accurate, which can significantly improve the health level of community residents and college students. When the number of data is about 600, it remains at about 0.05, indicating that this model has high reliability, and the fitting degree of mutual information feature extraction is up to 99.82%. It has certain practical application value. Copyright © 2022 Ruiqing Wang and Lei Han.",,"adult; article; classification algorithm; college student; data mining; feature extraction; health data; human; logic; long short term memory network; reliability; resident; sport",Article,Scopus,2-s2.0-85125002581
"Ou T.-Y., Lin G.-Y., Fu H.-P., Wei S.-C., Tsai W.-L.","57459813800;57274792800;57255666900;57459415200;57217379499;","An Intelligent Recommendation System for Real Estate Commodity",2022,"Computer Systems Science and Engineering","42","3",,"881","897",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124990654&doi=10.32604%2fcsse.2022.022637&partnerID=40&md5=b07b42eacb22013b065eaf809f4b272c","Most real estate agents develop new objects by visiting unfamiliar clients, distributing leaflets, or browsing other real estate trading website platforms, whereas consumers often rely on websites to search and compare prices when purchasing real property. In addition to being time consuming, this search process renders it difficult for agents and consumers to understand the status changes of objects. In this study, Python is used to write web crawler and image recognition programs to capture object information from the web pages of real estate agents; perform data screening, arranging, and cleaning; compare the text of real estate object information; as well as integrate and use the convolutional neural network of a deep learning algorithm to implement image recognition. In this study, data are acquired from two business-to-consumer real estate agency networks, i.e., the Sinyi real estate agent and the Yungching real estate agent, and one consumer-to-consumer real estate agency platform, i.e., the, FiveNineOne real estate agent. The results indicate that text mining can reveal the similarities and differences between the objects, list the number of days that the object has been available for sale on the website, and provide the price fluctuations and fluctuation times during the sales period. In addition, 213,325 object amplification images are used as a database for training using deep learning algorithms, and the maximum image recognition accuracy achieved is 95%. The dynamic recommendation system for real estate objects constructed by combining text mining and image recognition systems enables developers in the real estate industry to understand the differences between their commodities and other businesses in approximately 2 min, as well as rapidly determine developable objects via comparison results provided by the system. Meanwhile, consumers require less time in searching and comparing prices after they have understood the commodity dynamic information, thereby allowing them to use the most efficient approach to purchase real estate objects of their interest. © 2022 CRL Publishing. All rights reserved.","Deep learning; Image comparison; Real estate agency; Real estate object dynamic recommendation system; Text mining; Web crawler","Character recognition; Costs; Data mining; Deep learning; Image recognition; Learning algorithms; Recommender systems; Sales; Web crawler; Deep learning; Dynamic recommendations; Image comparison; Object dynamics; Object information; Real estate agents; Real estate object dynamic recommendation system; Real-estate agencies; Real-estates; Text-mining; Websites",Article,Scopus,2-s2.0-85124990654
"Eltahir M.M.E., Ahmed T.M.","57224529665;57460073700;","Diagnosing Breast Cancer Accurately Based on Weighting of Heterogeneous Classification Sub-Models",2022,"Computer Systems Science and Engineering","42","3",,"1257","1272",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124960679&doi=10.32604%2fcsse.2022.022942&partnerID=40&md5=0f701dea4fb480d8ebc95d7afad13762","In developed and developing countries, breast cancer is one of the leading forms of cancer affecting women alike. As a consequence of growing life expectancy, increasing urbanization and embracing Western lifestyles, the high prevalence of this cancer is noted in the developed world. This paper aims to develop a novel model that diagnoses Breast Cancer by using heterogeneous datasets. The model can work as a strong decision support system to help doctors to make the right decision in diagnosing breast cancer patients. The proposed model is based on three datasets to develop three sub-models. Each sub-model works independently. The final diagnosis decision is taken by the three sub-models independently. The power of the model comes from the diversity checks of patients and this reduces the risk of wrong diagnosing. The model has been developed by conducting intensive experiments. Several classification algorithms were used to select the best one in each sub-model. As the final results, the sub-model accuracies were 72%, 74% and 97%. © 2022 CRL Publishing. All rights reserved.","Breast cancer; Classification; Data mining","Artificial intelligence; Data mining; Decision support systems; Developing countries; Diagnosis; Breast Cancer; Cancer patients; Classification algorithm; Diagnosis decision; Heterogeneous datasets; Life expectancies; Modeling accuracy; Power; Submodels; Diseases",Article,Scopus,2-s2.0-85124960679
"Najafi H., Nourani V., Sharghi E., Roushangar K., Dąbrowska D.","57201943565;13906150400;56071592200;53880403000;57113331500;","Application of Z-numbers to teleconnection modeling between monthly precipitation and large scale sea surface temperature",2022,"Hydrology Research","53","1",,"1","13",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124946868&doi=10.2166%2fNH.2021.025&partnerID=40&md5=138ea4abdc6435e28a2c9e68e9dec93c","The teleconnection modeling of hydro-climatic events is a complex problem with highly uncertain circumstances. In contrast to the classic fuzzy logic methods, by using the Z-number in addition to the constraint of information, and by evaluating the data reliability, it is possible to characterize the degree of ambiguity of data. In this regard, this study investigates the performance of the Z-number-based model (ZBM) in prediction of classified monthly precipitation (MP) events of two synoptic stations in Iran (up to five months in advance). To this end, the sea surface temperature (SST) of adjacent seas was used as a predictor. The suggested model, by using Z-number directly and applying fuzzy Hausdorff distance to determine weights of if-then rules, predicted MP events of both the stations with over 70% confidence. Analysis of the results in the test step showed that the ZBM compared to the traditional fuzzy approach improved the results by 69% for Kermanshah and 112% for Tabriz. Overall, the Z-number concept by assessing events reliability can be used in various sectors of water resources management such as decision-making and drought monitoring. © 2022 The Authors.","Data mining; Fuzzy modeling; Hydro-climatologic processes; Monthly precipitation; Sea surface temperature; Z-numbers","Atmospheric temperature; Data mining; Decision making; Fuzzy logic; Submarine geophysics; Surface properties; Surface waters; Uncertainty analysis; Climatic events; Complex problems; Fuzzy logic method; Fuzzy modeling; Hydro-climatologic process; Large-scales; Monthly precipitation; Precipitation events; Teleconnections; Z-number; Oceanography; data; decision making; fuzzy mathematics; modeling; precipitation intensity; sea surface temperature; East Azerbaijan; Iran; Kermanshah; Tabriz",Article,Scopus,2-s2.0-85124946868
"Mohamed W., Abdel-Fattah M.A.","57225108217;55545556200;","A proposed hybrid algorithm for mining frequent patterns on Spark",2022,"International Journal of Business Intelligence and Data Mining","20","2",,"146","169",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124800195&doi=10.1504%2fIJBIDM.2022.120833&partnerID=40&md5=99dd1c62d10c4acd44627c40e055d58c","Frequent itemset mining is one of the most important data mining techniques applied to discover frequent itemset, interesting information, and correlation from data. Many algorithms such as Apriori, Fp-growth and Eclat have been adjusted and implemented to deal with big data. Those algorithms are implemented on big data processing engines such as MapReduce and Spark. However, the existing implementations have limitations. Consequently, this paper proposes a hybrid algorithm to mine frequent patterns on sparse big dataset over Spark platform. The proposed hybrid algorithm uses Apriori in the first few levels then switches to use Eclat for the rest of levels. The proposed hybrid algorithm consists of four phases. Experiments for testing the performance of the proposed algorithm are conducted, and the elapsed time of the proposed hybrid algorithm is compared with parallel fp-growth, YAFIM and Eclat-Spark. The proposed algorithm outperforms YAFIM, Eclat, and fp-growth with a high degree of minimum support. © 2022 Inderscience Enterprises Ltd.","Apriori; Big data; Eclat; Frequent pattern mining; Spark","Big data; Data handling; Apriori; Data-mining techniques; Eclat; FP growths; Frequent itemset; Frequent itemset mining; Frequent patterns minings; Hybrid algorithms; Interesting information; Processing engine; Data mining",Article,Scopus,2-s2.0-85124800195
"Shah K., Patel K.S.","57456892900;57457206800;","A comparative study and performance analysis of multirelational classification algorithms",2022,"International Journal of Business Intelligence and Data Mining","20","2",,"121","145",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124798622&doi=10.1504%2fIJBIDM.2022.120835&partnerID=40&md5=e0038ca311a1d6a34ae0a1a61bb80f8e","Classification is one of the important tasks in data mining in which a model is generated-based on training dataset and that model is used to predict class label of unknown dataset. Many propositional classification algorithms exist to build accurate and scalable classifiers, applied to single table dataset only. Most real-world data are structured and stored in relational format and single table classification algorithms that cannot deal directly with relational data. Hence, the need for a multirelational classification algorithm that learns relational data and predicts class labels for relational tuple arises. For relational classification, various techniques are available that include flattening relational data, upgrading existing algorithm, and multiview learning. This paper presents comparative analysis of these techniques and algorithms in detail and shows that multiview-based algorithms outperform other algorithms. By implementing multiview-based algorithms it demonstrated that these algorithms achieve higher accuracy for binary class classification than multiclass classification. © 2022 Inderscience Enterprises Ltd.","Binary class data; Classification; Data mining; Multi class data; Multirelational classification; Multiview learning; Relational data","Data mining; Learning algorithms; Binary class data; Class labels; Classification algorithm; Comparative performance; Comparatives studies; Multi class data; Multi-relational classifications; Multi-view learning; Multi-views; Relational data; Classification (of information)",Article,Scopus,2-s2.0-85124798622
"Nagarajan R., Jothi J.A.A.","57457264200;57456888600;","Analysing traveller ratings for tourist satisfaction and tourist spot recommendation",2022,"International Journal of Business Intelligence and Data Mining","20","2",,"208","234",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124798594&doi=10.1504%2fIJBIDM.2022.120828&partnerID=40&md5=e25ddcea450c916f4d2c64b4b098fc2c","In this study, we propose an automated system to classify traveller ratings on travel destinations in ten categories across East Asia using the UCI travel reviews dataset. The automated system developed in this study is called traveller rating classification system (TRCS). Since the travel reviews dataset is an unlabelled dataset, K-means clustering algorithm is used to group the samples from the dataset into three clusters. The cluster numbers obtained from K-means clustering are assigned as class labels for the samples and the dataset is converted into a labelled dataset. Popular individual classifiers and ensemble classifiers are used to classify the samples present in the labelled dataset. In this study, bagging with decision tree classifier achieved the best classification accuracy of 97.95%. The study further analyses the attributes in the dataset using visualisation techniques to draw inferences by performing small transformations on them. The proposed system will be useful to understand traveller satisfaction and as a tourist spot recommendation system. © 2022 Inderscience Enterprises Ltd.","Classification problem; Data mining; Ensemble learning; K-means clustering; Recommender systems; Supervised learning; Travel and tourism","Automation; Classification (of information); Data mining; Decision trees; Machine learning; Recommender systems; Automated systems; Classification problem; Classification system; East Asia; Ensemble learning; K-means++ clustering; Labeled dataset; Rating classifications; Tourist spots; Travel and tourism; K-means clustering",Article,Scopus,2-s2.0-85124798594
"Jiang W., Xiao Y., Liu Y., Liu Q., Li Z.","57457095800;7403260082;8879913300;35183549200;57211079419;","Bi-GRCN: A Spatio-Temporal Traffic Flow Prediction Model Based on Graph Neural Network",2022,"Journal of Advanced Transportation","2022",,"5221362","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124792614&doi=10.1155%2f2022%2f5221362&partnerID=40&md5=8d4c93e3d09f3eb19d2d444575866223","Because traffic flow data has complex spatial dependence and temporal correlation, it is a challenging problem for researchers in the field of Intelligent Transportation to accurately predict traffic flow by analyzing spatio-temporal traffic data. Based on the idea of spatio-temporal data fusion, fully considering the correlation of traffic flow data in the time dimension and the dependence of spatial structure, this paper proposes a new spatio-temporal traffic flow prediction model based on Graph Neural Network (GNN), which is called Bidirectional-Graph Recurrent Convolutional Network (Bi-GRCN). First, aiming at the spatial dependence between traffic flow data and traffic roads, Graph Convolution Network (GCN) which can directly analyze complex non-Euclidean space data is selected for spatial dependence modeling, to extract the spatial dependence characteristics. Second, considering the temporal dependence of traffic flow data on historical data and future data in its time-series period, Bidirectional-Gate Recurrent Unit (Bi-GRU) is used to process historical data and future data at the same time, to learn the temporal correlation characteristics of data in the bidirectional time dimension from the input data. Finally, the full connection layer is used to fuse the extracted spatial features and the learned temporal features to optimize the prediction results so that the Bi-GRCN model can better extract the spatial dependence and temporal correlation of traffic flow data. The experimental results show that the model can not only effectively predict the short-term traffic flow but also get a good prediction effect in the medium- and long-term traffic flow prediction. © 2022 Wenhao Jiang et al.",,"Complex networks; Convolution; Convolutional neural networks; Data fusion; Data mining; Flow graphs; Graph neural networks; Recurrent neural networks; Convolutional networks; Flow data; Graph neural networks; Model-based OPC; Prediction modelling; Spatial dependence; Spatio-temporal; Temporal correlations; Traffic flow; Traffic flow prediction; Forecasting",Article,Scopus,2-s2.0-85124792614
"Joseph N., Lindblad I., Zaker S., Elfversson S., Albinzon M., Ødegård Ø., Hantler L., Hellström P.M.","57456475600;57456560000;57456603500;57456560100;57456344100;57456344200;57456603600;7101653405;","Automated data extraction of electronic medical records: Validity of data mining to construct research databases for eligibility in gastroenterological clinical trials",2022,"Upsala Journal of Medical Sciences","127",,"e8260","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124775827&doi=10.48101%2fUJMS.V127.8260&partnerID=40&md5=a6197d6ccf019bd875b635b0c2fda675","Background: Electronic medical records (EMRs) are adopted for storing patient-related healthcare information. Using data mining techniques, it is possible to make use of and derive benefit from this massive amount of data effectively. We aimed to evaluate validity of data extracted by the Customized eXtraction Program (CXP). Methods: The CXP extracts and structures data in rapid standardised processes. The CXP was programmed to extract TNFα-native active ulcerative colitis (UC) patients from EMRs using defined International Classification of Disease-10 (ICD-10) codes. Extracted data were read in parallel with manual assessment of the EMR to compare with CXP-extracted data. Results: From the complete EMR set, 2,802 patients with code K51 (UC) were extracted. Then, CXP extracted 332 patients according to inclusion and exclusion criteria. Of these, 97.5% were correctly identified, resulting in a final set of 320 cases eligible for the study. When comparing CXP-extracted data against manually assessed EMRs, the recovery rate was 95.6–101.1% over the years with 96.1% weighted average sensitivity. Conclusion: Utilisation of the CXP software can be considered as an effective way to extract relevant EMR data without significant errors. Hence, by extracting from EMRs, CXP accurately identifies patients and has the capacity to facilitate research studies and clinical trials by finding patients with the requested code as well as funnel down itemised individuals according to specified inclusion and exclusion criteria. Beyond this, medical procedures and laboratory data can rapidly be retrieved from the EMRs to create tailored databases of extracted material for immediate use in clinical trials. © 2022 The Author(s).","Big data; Data analytics; Data extraction; Data mining; Electronic medical records","data mining; electronic health record; factual database; human; International Classification of Diseases; procedures; Data Mining; Databases, Factual; Electronic Health Records; Humans; International Classification of Diseases",Article,Scopus,2-s2.0-85124775827
"Xiao D., Zhao A., Li F.","57218551061;57455765400;57215221981;","Robust Watermarking Scheme for Encrypted Images Based on Scrambling and Kronecker Compressed Sensing",2022,"IEEE Signal Processing Letters","29",,,"484","488",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124765105&doi=10.1109%2fLSP.2022.3143038&partnerID=40&md5=4f6906a6446e485b8ede611d0c5bb9e9","Data hiding in encrypted images (DHEI) embeds data in the cipher state, which not only protects the confidentiality of the cover image content, but also hides the existence of the secret information, and has a broad application prospect. However, many DHEI schemes pursue high capacity at the expense of robustness. In this letter, we propose a robust watermarking scheme for encrypted images based on scrambling and Kronecker compressed sensing (CS). The scheme makes full use of the encryption and compression characteristics of CS to realize the image encryption and preprocessing at the same time, and utilizes the democratic characteristic of CS, Most Significant Bits (MSBs) replacement, image global scrambling and adjusted uniform quantization to improve the robustness. In addition, the two-dimensional Kronecker CS reduces the computational complexity of the encoding end and increases the encoding efficiency. Experimental results show that our scheme achieves good robustness against various noise attacks and clipping attacks, while maintaining high-capacity embedding and stable recovery. © 1994-2012 IEEE.","encoding efficiency; Encrypted domain; information hiding; Kronecker compressed sensing; robustness","Compressed sensing; Data mining; Efficiency; Encoding (symbols); Image coding; Image compression; Image enhancement; Image watermarking; Signal encoding; Watermarking; Compressed-Sensing; Encoding efficiency; Encrypted domain; Encrypted images; Information hiding; Kronecke compressed sensing; Kronecker; Robustness; Cryptography",Article,Scopus,2-s2.0-85124765105
"Maake B.M., Ojo S.O., Zuva K., Mzee F.A.","57119932700;23089206100;56407164400;57455907600;","A Bisociated Research Paper Recommendation Model using BiSOLinkers",2022,"International Journal on Advanced Science, Engineering and Information Technology","12","1",,"121","130",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124722182&doi=10.18517%2fijaseit.12.1.14163&partnerID=40&md5=3fdfd274018c805bc5a031ca98add823","In the current days of information overload, it is nearly impossible to obtain a form of relevant knowledge from massive information repositories without using information retrieval and filtering tools. The academic field daily receives lots of research articles, thus making it virtually impossible for researchers to trace and retrieve important articles for their research work. Unfortunately, the tools used to search, retrieve and recommend relevant research papers suggest similar articles based on the user profile characteristic, resulting in the overspecialization problem whereby recommendations are boring, similar, and uninteresting. We attempt to address this problem by recommending research papers from domains considered unrelated and unconnected. This is achieved through identifying bridging concepts that can bridge these two unrelated domains through their outlying concepts – BiSOLinkers. We modeled a bisociation framework using graph theory and text mining technologies. Machine learning algorithms were utilized to identify outliers within the dataset, and the accuracy achieved by most algorithms was between 96.30% and 99.49%, suggesting that the classifiers accurately classified and identified the outliers. We additionally utilized the Latent Dirichlet Allocation (LDA) algorithm to identify the topics bridging the two unrelated domains at their point of intersection. BisoNets were finally generated, conceptually demonstrating how the two unrelated domains were linked, necessitating cross-domain recommendations. Hence, it is established that recommender systems' overspecialization can be addressed by combining bisociation, topic modeling, and text mining approaches. © 2022,International Journal on Advanced Science, Engineering and Information Technology. All Rights Reserved.","Bisociation; Data mining; Knowledge discovery; Recommender system; Serendipity; Text mining; Topic modeling",,Article,Scopus,2-s2.0-85124722182
"Ding Y., Li P.","57454958900;57336826100;","Predictive Skills and Reading Efficiency of College English Based on Multimedia Technology",2022,"Scientific Programming","2022",,"9882409","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124698295&doi=10.1155%2f2022%2f9882409&partnerID=40&md5=632b8e95f09a53b9ad5011f623d1af50","Slow reading speed is a common problem among Chinese college students' English learning, limiting the amount of reading done and thus directly affecting English-level improvement. Teachers in English classes place a premium on improving students' reading skills but the results are not immediately apparent. Even if many students have strong vocabulary and grammar skills, they will never be able to read English fluently. Teachers should prioritize not only students' knowledge acquisition and intelligence development but also the development of students' interpersonal skills and emotional intelligence in order to maximize their potential. This paper uses multimedia technology to investigate the role of multimedia technology in improving college English-reading efficiency and analyzes how teachers and students can play a positive role in improving the efficiency of college multimedia teaching using data mining technology, which shows that students' predictive reading skills can help them improve their reading efficiency. Teachers can also supervise and ask questions about students' learning progress via computers, communicate effectively with students, and provide timely feedback on students' learning results using multimedia technology. © 2022 Yuexia Ding and Ping Li.",,"Data mining; Efficiency; Engineering education; Multimedia systems; College English; College students; English Learning; Multimedia technologies; Reading efficiencies; Reading skills; Reading speed; Student knowledge; Student learning; Teachers'; Students",Article,Scopus,2-s2.0-85124698295
"Zhang M., Fan J., Sharma A., Kukkar A.","57454805400;57454805500;57217481903;57202704922;","Data mining applications in university information management system development",2022,"Journal of Intelligent Systems","31","1",,"207","220",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124684691&doi=10.1515%2fjisys-2022-0006&partnerID=40&md5=9b2a613e930fbff5d2c2e06e98c568f4","Nowadays, the modern management is promoted to resolve the issue of unreliable information transmission and to provide work efficiency. The basic aim of the modern management is to be more effective in the role of the school to train talents and serve the society. This article focuses on the application of data mining (DM) in the development of information management system (IMS) in universities and colleges. DM provides powerful approaches for a variety of educational areas. Due to the large amount of student information that can be used to design valuable patterns relevant to student learning behavior, research in the field of education is continuously expanding. Educational data mining can be used by educational institutions to assess student performance, assisting the institution in recognizing the student’s accomplishments. In DM, classification is a well-known technique that has been regularly used to determine student achievement. In this study, the process of DM and the application research of association rules is introduced in the development of IMS in universities and colleges. The results show that the curriculum covers the whole field and the minimum transaction support count be 2, minconf = 70%. The results also suggested that students who choose one course also tend to choose the other course. The application of DM theory in university information will greatly upsurge the data analysis capability of administrators and improve the management level. © 2022 Minshun Zhang et al., published by De Gruyter","data mining; information management system; universities and colleges","Behavioral research; Curricula; Information management; Students; Data mining applications; Educational area; Information management systems; Information transmission; Large amounts; Management IS; Modern management; System development; Universities and colleges; Work efficiency; Data mining",Article,Scopus,2-s2.0-85124684691
"Shao D., Li C., Huang C., An Q., Xiang Y., Guo J., He J.","42662301600;57454041900;57453969800;57454183500;55836565000;57327754100;55714993700;","The short texts classification based on neural network topic model",2022,"Journal of Intelligent and Fuzzy Systems","42","3",,"2143","2155",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124669421&doi=10.3233%2fJIFS-211471&partnerID=40&md5=dc5f5f7ce6aed7af9e8ef1d0e2d9741f","Aiming at the low effectiveness of short texts feature extraction, this paper proposes a short texts classification model based on the improved Wasserstein-Latent Dirichlet Allocation (W-LDA), which is a neural network topic model based on the Wasserstein Auto-Encoder (WAE) framework. The improvements of W-LDA are as follows: Firstly, the Bag of Words (BOW) input in the W-LDA is preprocessed by Term Frequency-Inverse Document Frequency (TF-IDF); Subsequently, the prior distribution of potential topics in W-LDA is replaced from the Dirichlet distribution to the Gaussian mixture distribution, which is based on the Variational Bayesian inference; And then the sparsemax function layer is introduced after the hidden layer inferred by the encoder network to generate a sparse document-Topic distribution with better topic relevance, the improved W-LDA is named the Sparse Wasserstein-Variational Bayesian Gaussian mixture model (SW-VBGMM); Finally, the document-Topic distribution generated by SW-VBGMM is input to BiGRU (Bidirectional Gating Recurrent Unit) for the deep feature extraction and the short texts classification. Experiments on three Chinese short texts datasets and one English dataset represent that our model is better than some common topic models and neural network models in the four evaluation indexes (accuracy, precision, recall, F1 value) of text classification. © 2022-IOS Press. All rights reserved.","BiGRU (Bidirectional Gating Recurrent Unit); neural network topic model; Short texts classification; sparsemax; Variational Bayesian Gaussian mixture model (VBGMM)","Bayesian networks; Classification (of information); Data mining; Extraction; Feature extraction; Inference engines; Information retrieval systems; Recurrent neural networks; Signal encoding; Statistics; Text processing; Bidirectional gating recurrent unit; Gaussian Mixture Model; Latent Dirichlet allocation; Neural network topic model; Neural-networks; Short text classifications; Sparsemax; Topic Modeling; Variational bayesian; Variational bayesian gaussian mixture model; Gaussian distribution",Article,Scopus,2-s2.0-85124669421
"Wu G., Li Q.","57199960977;57218101048;","Population economic function model based on data mining algorithm",2022,"Journal of Intelligent and Fuzzy Systems","42","3",,"2369","2382",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124669192&doi=10.3233%2fJIFS-211663&partnerID=40&md5=80930eab173804b2e0b05b79b79c8120","Population structure changes interact with economic development, moderate population and reasonable population structure are important guarantees for sustainable social and economic development. The research ignores the specific impact of the change of population age structure on economic growth, and proposes and establishes a population economic function model based on data mining algorithm. Based on the changes of population structure in Liaoning Province in the past 20 years, Grey correlation analysis method is selected. The analysis shows that there is a close relationship between population structure and economic growth. Based on this research, the econometric method is used to construct a multiple linear regression model to further analyze the specific impact of population structure changes on economic growth. The analysis results show that the total population of urban areas, the total number of employed people in the primary industry, the number of middle school students per 10,000 people, and the total number of employed people in the tertiary industry are the four most significant demographic indicators for the per capita GDP of the study area. There is a significant positive correlation between the total number of employed people in the tertiary industry and per capita GDP and there is a significant negative correlation between the total number of employed people in the primary industry and the number of middle school students per capita and per capita GDP. The impact of other indicators on per capita GDP is not significant. According to the conclusion, countermeasures and suggestions to ease population structure change and promote the coordinated development of population and economy in the study area are put forward. © 2022-IOS Press. All rights reserved.","data mining algorithm; Grey correlation analysis method; model; population economic function","Correlation methods; Data mining; Economic analysis; Economic and social effects; Industrial economics; Linear regression; Data mining algorithm; Economic functions; Economic growths; Function modelling; Gray correlation analysis methods; Model-based OPC; Per capita; Population economic function; Population structures; Structure change; Population statistics",Article,Scopus,2-s2.0-85124669192
"Shi M., Wang Z., Xu L.","55757230100;57454121600;57303706700;","A fuzzy clustering algorithm based on hybrid surrogate model",2022,"Journal of Intelligent and Fuzzy Systems","42","3",,"1963","1976",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124668192&doi=10.3233%2fJIFS-211340&partnerID=40&md5=7af0f828d15fadf65d47ee1185b5202b","Data clustering based on regression relationship is able to improve the validity and reliability of the engineering data mining results. Surrogate models are widely used to evaluate the regression relationship in the process of data clustering, but there is no single surrogate model that always performs the best for all the regression relationships. To solve this issue, a fuzzy clustering algorithm based on hybrid surrogate model is proposed in this work. The proposed algorithm is based on the framework of fuzzy c-means algorithm, in which the differences between the clusters are evaluated by the regression relationship instead of Euclidean distance. Several surrogate models are simultaneously utilized to evaluate the regression relationship through a weighting scheme. The clustering objective function is designed based on the prediction errors of multiple surrogate models, and an alternating optimization method is proposed to minimize it to obtain the memberships of data and the weights of surrogate models. The synthetic datasets are used to test single surrogate model-based fuzzy clustering algorithms to choose the surrogate models used in the proposed algorithm. It is found that support vector regression-based and response surface-based fuzzy clustering algorithms show competitive clustering performance, so support vector regression and response surface are used to construct the hybrid surrogate model in the proposed algorithm. The experimental results of synthetic datasets and engineering datasets show that the proposed algorithm can provide more competitive clustering performance compared with single surrogate model-based fuzzy clustering algorithms for the datasets with regression relationships. © 2022-IOS Press. All rights reserved.","Data clustering; engineering data; fuzzy clustering; hybrid surrogate model; regression relationship","Cluster analysis; Clustering algorithms; Copying; Data mining; Professional aspects; Regression analysis; Surface properties; Data clustering; Engineering data; Fuzzy clustering algorithm; Hybrid surrogate model; Model-based OPC; Regression relationship; Response surface; Support vector regressions; Surrogate modeling; Synthetic datasets; Fuzzy clustering",Article,Scopus,2-s2.0-85124668192
"Sarfaraz A.H., Yazdi A.K., Hanne T., Gizem Ö., Khalili-Damghani K., Husseinagha S.M.","49662353100;51664396200;6602279467;57453801600;36806209600;57454021600;","Analyzing the Investment Behavior in the Iranian Stock Exchange during the COVID-19 Pandemic Using Hybrid DEA and Data Mining Techniques",2022,"Mathematical Problems in Engineering","2022",,"1667618","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124667682&doi=10.1155%2f2022%2f1667618&partnerID=40&md5=5e4f282a4b9ef70698c9c2b66700eee9","The main purpose of this paper is to investigate the effects of COVID-19 regarding the efficiency of industries based on data in the Tehran stock market. A hybrid model of Data Envelopment Analysis (DEA) and data mining techniques is used to analyze the investment behavior in Tehran stock market. Particularly during the COVID-19 pandemic, many companies face financial crises. That is why companies with inferior performance must be benchmarked with efficient companies. First, the financial data of investments on selective companies are analyzed using data mining approaches to recognize the behavioral patterns of investors and securities. Second, customers are clustered into 3 selling and 4 buying groups using data mining techniques. Then, the efficiency of active companies in stock exchange is evaluated using input-oriented DEA. The results indicate that, among 23 industries listed on the stock market in Iran, solely nine were efficient in 2019. Moreover, in 2020, the number of efficient industries further decreased to six industries. Comparing the obtained results with those of another study which was conducted in 2018 by other researchers revealed that COVID-19 strongly affects the performance of an industry and some industries which were efficient in the past such as the bank industry became inefficient in the following year. © 2022 Amir Homayoun Sarfaraz et al.",,"Commerce; Data mining; Efficiency; Financial markets; Investments; Behavioral patterns; Data-mining techniques; Financial crisis; Financial data; Hybrid datum; Hybrid model; Industry based; Iranian stock exchanges; Performance; Stock exchange; Data envelopment analysis",Article,Scopus,2-s2.0-85124667682
"Akbar S.B., Thanupillai K., Govindarajan V.","57454121800;57454190600;57454049600;","Forecasting Bitcoin price using time opinion mining and bi-directional GRU",2022,"Journal of Intelligent and Fuzzy Systems","42","3",,"1825","1833",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124664625&doi=10.3233%2fJIFS-211217&partnerID=40&md5=704b5b5b4e2964d6862b190775b490b6","Bitcoin is an innovative decentralized digital currency without intermediaries. Bitcoin price prediction is a demanding need in the present situation. This paper makes an investigation on the Bitcoin price forecast with a Bi-directional Gated Recurrent Unit (GRU) time series method, combined with opinion mining based on Twitter and Reddit feeds. An hourly basis sentimental analysis through the implementation of Natural Language Processing presents a positive impact of sentimental analysis on the Bitcoin price prediction. For prediction, RNN, long-short memory, GRU has been utilized. Unidirectional and Bi-directional versions of all three networks with and without sentimental analysis were implemented for comparison. Of all the techniques implemented Bi-directional GRU along with sentimental analysis gives a minimum RMSE and Minimum absolute percentage error of 1108.33 and 7.384%. Thus, the framework including Bi-Directional GRU along with Sentimental Analysis provides better results than the State-of-Art methods. © 2022-IOS Press. All rights reserved.","Bitcoin; GRU; MAPE; mining; neural network; RMSE","Bitcoin; Data mining; Sentiment analysis; Bi-directional; Decentralised; Gated recurrent unit; Mape; Neural-networks; Present situation; Price forecasts; Price prediction; RMSE; Time series method; Forecasting",Article,Scopus,2-s2.0-85124664625
"She C., Zeng S.","57454183700;23391480700;","An efficient local outlier detection optimized by rough clustering",2022,"Journal of Intelligent and Fuzzy Systems","42","3",,"2071","2082",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124650403&doi=10.3233%2fJIFS-211433&partnerID=40&md5=d88400b170ddc38cba5047b39d0119b5","Outlier detection is a hot issue in data mining, which has plenty of real-world applications. LOF (Local Outlier Factor) can capture the abnormal degree of objects in the dataset with different density levels, and many extended algorithms have been proposed in recent years. However, the LOF needs to search the nearest neighborhood of each object on the whole dataset, which greatly increases the time cost. Most of these extended algorithms only consider the distance between an object and its neighborhood, but ignore the local distribution of an object within its neighborhood, resulting in a high false-positive rate. To improve the running speed, a rough clustering based on triple fusion is proposed, which divides a dataset into several subsets and outlier detection is performed only on each subset. Then, considering the local distribution of an object within its neighborhood, a new local outlier factor is constructed to estimate the abnormal degree of each object. Finally, the experimental results indicate that the proposed algorithm has better performance and lower running time than the others. © 2022-IOS Press. All rights reserved.","local outlier factor; Outlier detection; rough Clustering","Anomaly detection; Data handling; Data mining; Nearest neighbor search; Statistics; Density levels; Different densities; Local distributions; Local Outlier Factor; Local outliers; Nearest neighborhood; Neighbourhood; Outlier Detection; Real-world; Rough clustering; Geographical distribution",Article,Scopus,2-s2.0-85124650403
"He Y., Feng Q., Yan L., Lu X.-Y.","56373631700;57453576800;56323872100;7404839911;","Visualization and Analysis of Mapping Knowledge Domain of Heterogeneous Traffic Flow",2022,"Computational Intelligence and Neuroscience","2022",,"7754961","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124635978&doi=10.1155%2f2022%2f7754961&partnerID=40&md5=c4d0bcef02c52a9dfcaf8e54a3607653","Mapping knowledge domain (MKD) is an important application in bibliometrics, which is a method of visually presenting and explaining newly developed interdisciplinary scientific fields using data mining, information analysis, scientific measurement, and graphic rendering. This study combines applied mathematics, visual analysis technology, information science, and scientometrics to systematically analyze the development status, research distribution, and future trend of the heterogeneous traffic flow by using the MKD software tools VOSviewer and CiteSpace. Based on the MKD and Bibliometrics approaches, 4709 articles have been studied, which were published by Science Citation Index Expanded (SCIE) and Social Sciences Citation Index (SSCI) from 2004 to 2021 in the field of heterogeneous traffic flows. Firstly, this paper presents the annual numbers of articles, origin countries, main research organizations, and groups as well as the source journals on heterogeneous traffic flow studies. Then, cocitation analysis is used to divide heterogeneous traffic flow into three main research directions, which include ""heterogeneous traffic flow model,""""traffic flow capacity analysis,""and ""traffic flow stability analysis.""The keyword cooccurrence analysis is applied to identify five dominant clusters: ""modeling and optimization methods,""""traffic flow characteristics analysis,""""driving behavior analysis,""""simulation experiment,""and ""policies and barriers.""Finally, burst keywords were studied according to the publication date to present more clearly the change of research focus and direction over time. © 2022 Yi He et al.",,"Computer aided software engineering; Data visualization; Indexing (of information); Mapping; Applied mathematics; Bibliometric; Development status; Graphics rendering; Heterogeneous traffic flow; Knowledge domains; Scientific fields; Scientometrics; Visual analysis; Visualization and analysis; Data mining; bibliometrics; car driving; data mining; knowledge; methodology; Automobile Driving; Bibliometrics; Data Mining; Knowledge; Research Design",Article,Scopus,2-s2.0-85124635978
"Rahman Z.U., Ullah S.I., Salam A., Rahman T., Khan I., Niazi B.","57452397400;57211516274;57209091500;57214596936;57189853778;57189027746;","Automated Detection of Rehabilitation Exercise by Stroke Patients Using 3-Layer CNN-LSTM Model",2022,"Journal of Healthcare Engineering","2022",,"1563707","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124603583&doi=10.1155%2f2022%2f1563707&partnerID=40&md5=a451d4857735fb8f731522f8fbf260d7","According to statistics, stroke is the second or third leading cause of death and adult disability. Stroke causes losing control of the motor function, paralysis of body parts, and severe back pain for which a physiotherapist employs many therapies to restore the mobility needs of everyday life. This research article presents an automated approach to detect different therapy exercises performed by stroke patients during rehabilitation. The detection of rehabilitation exercise is a complex area of human activity recognition (HAR). Due to numerous achievements and increasing popularity of deep learning (DL) techniques, in this research article a DL model that combines convolutional neural network (CNN) and long short-Term memory (LSTM) is proposed and is named as 3-Layer CNN-LSTM model. The dataset is collected through RGB (red, green, and blue) camera under the supervision of a physiotherapist, which is resized in the preprocessing stage. The 3-layer CNN-LSTM model takes preprocessed data at the convolutional layer. The convolutional layer extracts useful features from input data. The extracted features are then processed by adjusting weights through fully connected (FC) layers. The FC layers are followed by the LSTM layer. The LSTM layer further processes this data to learn its spatial and temporal dynamics. For comparison, we trained CNN model over the prescribed dataset and achieved 89.9% accuracy. The conducted experimental examination shows that the 3-Layer CNN-LSTM outperforms CNN and KNN algorithm and achieved 91.3% accuracy. © 2022 Zia Ur Rahman et al.",,"Convolution; Convolutional neural networks; Data mining; Patient rehabilitation; Patient treatment; Automated detection; Back pain; Body parts; Causes of death; Convolutional neural network; Memory layers; Memory modeling; Motor function; Rehabilitation exercise; Stroke patients; Long short-term memory",Article,Scopus,2-s2.0-85124603583
"Zhang P., Xu J., Nie H., Gao Z., Nie K.","57213839962;7407007459;57195519192;56647252000;36604916400;","Motion detection for high-speed high-brightness objects based on a pulse array image sensor [基于脉冲阵列图像传感器的高速高亮度目标检测]",2022,"Frontiers of Information Technology and Electronic Engineering","23","1",,"113","122",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124491534&doi=10.1631%2fFITEE.2000407&partnerID=40&md5=844e756bc88c776662a18d91810dad11","We describe a method of optical flow extraction for high-speed high-brightness targets based on a pulse array image sensor (PAIS). PAIS is a retina-like image sensor with pixels triggered by light; it can convert light into a series of pulse intervals. This method can obtain optical flow from pulse data directly by accumulating continuous pulses. The triggered points can be used to filter redundant data when the target is brighter than the background. The method takes full advantage of the rapid response of PAIS to high-brightness targets. We applied this method to extract the optical flow of high-speed turntables with different background brightness, with the sensor model and actual data, respectively. Under the sampling condition of 2×104 frames/s, the optical flow could be extracted from a high-speed turntable rotating at 1000 r/min. More than 90% of redundant points could be filtered by our method. Experimental results showed that the optical flow extraction algorithm based on pulse data can extract the optical flow information of high-brightness objects efficiently without the need to reconstruct images. © 2022, Zhejiang University Press.","High-speed targets; Optical flow; Pulse triggered; Retina-like image sensor; TP301.6; Vision processing","Data mining; Extraction; Luminance; Optical flows; Phonographs; Array images; Flow extraction; High brightness; High Speed; High speed targets; Motion detection; Pulse triggered; Retina-like image sensor; Tp301.6; Vision processing; Image sensors",Article,Scopus,2-s2.0-85124491534
"Ahasan R., Alam M.S., Chakraborty T., Hossain M.M.","57220811460;55731178500;57226308988;57207311791;","Applications of GIS and geospatial analyses in COVID-19 research: A systematic review",2022,"F1000Research","9",,"1379","","",,4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124466556&doi=10.12688%2ff1000research.27544.2&partnerID=40&md5=f3e1a722181d06f98917b0486bb696ef","Background: Geographic information science (GIS) has established itself as a distinct domain and incredibly useful whenever the research is related to geography, space, and other spatio-temporal dimensions. However, the scientific landscape on the integration of GIS in COVID-related studies is largely unknown. In this systematic review, we assessed the current evidence on the implementation of GIS and other geospatial tools in the COVID-19 pandemic. Methods: We systematically retrieved and reviewed 79 research articles that either directly used GIS or other geospatial tools as part of their analysis. We grouped the identified papers under six broader thematic groups based on the objectives and research questions of the study- environmental, socio-economic, and cultural, public health, spatial transmission, computer-aided modeling, and data mining. Results: The interdisciplinary nature of how geographic and spatial analysis was used in COVID-19 research was notable among the reviewed papers. Geospatial techniques, especially WebGIS, have even been widely used to visualize the data on a map and were critical to informing the public regarding the spread of the virus, especially during the early days of the pandemic. This review not only provided an overarching view on how GIS has been used in COVID-19 research so far but also concluded that geospatial analysis and technologies could be used in future public health emergencies along with statistical and other socio-economic modeling techniques. Our review also highlighted how scientific communities and policymakers could leverage GIS to extract useful information to make an informed decision in the future. Conclusions: Despite the limited applications of GIS in identifying the nature and spatio-temporal pattern of this raging pandemic, there are opportunities to utilize these techniques in handling the pandemic. The use of spatial analysis and GIS could significantly improve how we understand the pandemic as well as address the underserviced demographic groups and communities. © 2022 Ahasan R et al.","Coronavirus; COVID-19; Evidence-based practice; GIS; Spatial analysis; Systematic review","Article; big data; computer analysis; coronavirus disease 2019; cultural factor; data mining; environmental factor; epidemiological surveillance; geographic distribution; geographic information system; geographic mapping; human; medical information; medical research; pandemic; public health; remote sensing; social media; socioeconomics; spatial analysis; statistical analysis; systematic review; virus transmission",Article,Scopus,2-s2.0-85124466556
"Pan Q., Yang G.","57449634800;57449738100;","Application of mining algorithm in personalized Internet marketing strategy in massive data environment",2022,"Journal of Intelligent Systems","31","1",,"237","244",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124466014&doi=10.1515%2fjisys-2022-0014&partnerID=40&md5=94a4f0f1f5510b7a36175f65b93882ca","Internet marketing requires a personalized marketing strategy. In this study, the application of data mining in personalized Internet marketing was studied. Based on the mining algorithm, a personalized marketing method was designed. Through the calculation of frequent closed item sets and support counts of positive and negative samples, the interval with a high success rate for marketing was obtained. With performance analysis, it was found that the success rate of the marketing method proposed in this study improved 8% compared with the traditional marketing method and had a better performance under the smaller interval number and smaller minimum success number. After applying the designed method in telecommunication enterprise A, it was found that after adopting the marketing method of this study, the marketing success rate of enterprise A increased from 2.72 to 6.31%, which indicated the effectiveness of the method. The research results of this study verify the role of data mining algorithms in Internet marketing, which is conducive to the further application of mining algorithms in personalized marketing and innovation of business modes. © 2022 Qianqian Pan and Gang Yang, published by De Gruyter.","big data; data mining; internet marketing; personalized marketing","Big data; Commerce; Marketing; Strategic planning; Data environment; Internet marketing; Internet marketing strategies; Itemset; Marketing IS; Marketing strategy; Massive data; Mining algorithms; Personalized marketings; Support count; Data mining",Article,Scopus,2-s2.0-85124466014
"Hou J., Ding F.","57447425000;57447296800;","Design and Analysis of Network Virus Defense System Based on Multimodal Data Mining Technology",2022,"Scientific Programming","2022",,"8587906","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124418419&doi=10.1155%2f2022%2f8587906&partnerID=40&md5=06a837eecdcee78482811f7e09a8566b","This study analyzes and designs a network security management system based on the K-means algorithm. Through the investigation of network security managers, this article uses structured modeling technology to derive the system's logical business functions, which are data acquisition function, data analysis function, and post-processing function. This study uses a three-tier architecture to design the system, describes the system data business processing flow, and gives the key flow of the K-means algorithm application. In addition, the system database is designed to provide support for system data processing. This study proposes a dual-network text matching model based on local interactive components. This model composes two modalities of single-modal short text data: a positional structural modal for describing local interactions and a global semantic understanding modal for extracting global semantic information. Two heterogeneous networks are used to extract two modalities. The principle of complementarity of modalities is realized by constructing the differences of each modal. At the same time, through the attention mechanism, the position information of the position structure modal is transferred to the global semantic understanding modal to obtain consistent comprehensive information so as to realize the principle of modal consistency. On this basis, by designing low-order interaction functions and high-order interaction functions, and using long- and short-term memory networks, we have, respectively, improved the position structure modal and global semantic understanding modal. The theoretical analysis and simulation results of the model show that the propagation characteristics of the network worm are completely determined by the threshold R0 required for its existence. When R0 >1, the network worm will become popular even with a small initial infection number; conversely, even if the initial infection machine is large, the network worm will eventually become extinct. The research results of this article also show that the introduction of mobile devices has an important impact on the defense technology of network worms. To curb network worms that use both the network and mobile devices to spread, and to increase the proportion of machines that have never been infected with the virus, the most effective method is to control the number of mobile devices and reduce the possibility of cross-infection between mobile devices and machines. The simulation results show that, regardless of whether dynamic isolation and antivirus software are considered, the local area network-based choke method given in this study can effectively curb intelligent network worms that have slow scanning rates and clear scanning targets. In addition, the choke method presented in this article can determine the choke threshold without affecting the normal network scanning behavior of users in the local area network. © 2022 Jinhuan Hou and Fuqiang Ding.",,"Client server computer systems; Computer software; Computer viruses; Data mining; Heterogeneous networks; K-means clustering; Network security; Semantics; Data mining technology; Defence systems; Design and analysis; Interaction functions; K-mean algorithms; Multi-modal data; Network virus; Network worms; Semantics understanding; Structure modal; Modal analysis",Article,Scopus,2-s2.0-85124418419
"Che L.","57447663400;","Copyright Protection of Literary Works Based on Data Mining Algorithms",2022,"Scientific Programming","2022",,"2847590","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124402226&doi=10.1155%2f2022%2f2847590&partnerID=40&md5=33e02bfd3738efec7bdc084931ea719c","In order to improve the copyright protection effect of literary works and improve the healthy dissemination of digitized literary works, this paper combines data mining technology to conduct research on the copyright protection of literary works and constructs a literary copyright protection system. In digital literary works, watermarking algorithms can be used to watermark the characteristics of literary works to obtain digital literary works that have been watermarked. After that, this paper can combine data mining algorithms to perform text feature recognition and feature classification and improve the copyright protection effect of literary works. The experimental research results verify that the effect of the copyright protection system of literary works based on data mining algorithms is very good. © 2022 Liying Che.",,"Character recognition; Data mining; Text processing; Copyright protections; Data mining algorithm; Data mining technology; Feature classification; Features recognition; Literary works; Protection effect; Protection systems; Text feature; Watermarking algorithms; Copyrights",Article,Scopus,2-s2.0-85124402226
"Jiang L.","57223395710;","Development and Implementation Path of Kindergarten Stem Educational Activities Based on Data Mining",2022,"Computational Intelligence and Neuroscience","2022",,"2700674","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124401482&doi=10.1155%2f2022%2f2700674&partnerID=40&md5=5f8fea33a3bd1382b2b8dc57ec7c3429","Early childhood education in China has given stem education constant attention and study. On the one hand, it has introduced many foreign research findings on stem education, such as curriculum practice, evaluation systems, teacher training, and so on; on the other hand, this paper investigates the localization implementation path of stem education based on the realities of kindergartens. This paper investigates the development and implementation path of kindergarten stem education activities using data mining, analyzes how the kindergarten stem education monitoring index system is developed and further improved using data mining algorithm, and determines the function path and mode of data mining algorithm in kindergarten stem education. It is expected to be used as a reference in the development and implementation of stem education and teaching activities. The development and implementation path of kindergarten stem educational activities based on data mining algorithm using data technology to realize continuous audit can not only improve the audit means and scope but also provide new research ideas for the expansion and innovation of audit work, which is useful in building a path model of kindergarten stem educational activities development and implementation. © 2022 Liangfu Jiang.",,"Curricula; Personnel training; Activity-based; Data mining algorithm; Early childhood educations; Educational activities; Indices systems; Localisation; Monitoring index; STEM education; Teacher training; Teaching activities; Data mining; China; curriculum; data mining; human; preschool child; school; Child, Preschool; China; Curriculum; Data Mining; Humans; Schools",Article,Scopus,2-s2.0-85124401482
"Ma Y.","57447822300;","New progress in international nanotechnology research in the past ten years - visual analysis based on CitesSpace",2022,"Journal of Computational Methods in Sciences and Engineering","22","1",,"265","277",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124394858&doi=10.3233%2fJCM-215639&partnerID=40&md5=d7623daaee975d61350cdbc68e69108a","With the continuous development of high-tech industry, Moore's law is close to the limit. People urgently need nano science and technology to trigger a new scientific and technological revolution to meet the needs of life, military and so on. Nanotechnology covers almost all industries and has made achievements in the industries such as medical, materials, manufacturing, and information technology. It has changed the production and life of human beings and subverted many industries. In recent years, more and more people have conducted data mining on nanotechnology research. By combing the literature, this paper summarizes the core authors, keyword changes, important authors and emergent words of the existing literature. Contributing to analyzing the research status of this field and revealing research hotspots in this field. It is of great significance for scholars to sort out the development process of nano field and predict the future development trend. Using CitesSpace bibliometric analysis software, 44002 pieces of literature about nanotechnology in SCI and SSCI journals in the core collection of the Web of Science database were analyzed in this paper. The results indicated that countries such as the United States, Germany, China, and Japan have issued more articles; However, the centrality of articles published in European countries such as the UK, Germany, and France was relatively strong; High-yield units mainly included Chinese Acad Sci and Russian Acad Sci; The main research scholars were Wei Wang, Peixuan Guo, Thomas J Webster, Hao Yan; Research emergent words primarily included polymer, particle, dynamics, mechanical properties and silver nanoparticle. On this basis, countermeasure suggestions and prospects are proposed. © 2022 - IOS Press. All rights reserved.","Citation burst; emergence; keywords; knowledge graph; research hotspots","Binary alloys; Knowledge graph; Potassium alloys; Silver nanoparticles; Citation burst; Continuous development; Emergence; High tech industry; Hotspots; Keyword; Knowledge graphs; Nanotechnology research; Research hotspot; Visual analysis; Data mining",Article,Scopus,2-s2.0-85124394858
"Xue Z., Yu X., Tan X., Liu B., Yu A., Wei X.","57192194290;12141515900;56074915200;57196891847;56337728400;56074247000;","Multiscale Deep Learning Network With Self-Calibrated Convolution for Hyperspectral and LiDAR Data Collaborative Classification",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124394603&doi=10.1109%2fTGRS.2021.3106025&partnerID=40&md5=5ac0319778c1794bcfaf99dbb51edf42","In this article, we propose a novel multiscale deep learning network with self-calibrated convolution (MSNetSC) for hyperspectral and light detection and ranging (LiDAR) data collaborative classification. Conventional deep learning methods have limitations in extracting multiscale features at a granular level from multimodality data and fusing these features in a context-awareness way, which will severely restrict the performance of hyperspectral and LiDAR data joint classification. The proposed multiscale deep learning network utilizes a hierarchical residual structure combined with self-calibrated convolution to extract features with different receptive fields, and this can enhance the model's capability to represent the multimodality data. Besides, we employ spectral and spatial self-attention modules to adaptively calibrate weights of features with different scales, thereby enhancing the discriminative ability of extracted multiscale features. Furthermore, the attentional feature fusion module can dynamically and adaptively fuse the features from multimodality data in a contextual scale-aware way, and this attention-based feature fusion method will further improve the collaborative classification performance of hyperspectral and LiDAR data. Four benchmark multimodality data (i.e., hyperspectral and LiDAR data) sets collected by different sensors and at different acquisition times are employed for joint classification experiments. These comparative classification results and ablation study sufficiently certify the superiority of the proposed model in terms of collaborative classification accuracy when compared with other state-of-the-art methods. © 2021 IEEE.","Collaboration; Convolution; Data mining; Feature extraction; Fuses; Hyperspectral imaging; Laser radar","Classification (of information); Convolution; Data mining; Deep learning; Hyperspectral imaging; Spectroscopy; Collaboration; Collaborative classifications; Features extraction; Granular levels; Hyperspectral detections; Learning methods; Learning network; Light detection and ranging; Multi-modality; Multi-scale features; Optical radar; artificial neural network; calibration; data mining; lidar; machine learning; satellite data; spectral analysis",Article,Scopus,2-s2.0-85124394603
"Li H.","57118953000;","The method of repairing OHE damaged ancient painted murals based on machine vision",2022,"Journal of Computational Methods in Sciences and Engineering","22","1",,"305","319",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124388815&doi=10.3233%2fJCM-215654&partnerID=40&md5=54322c39773148c21a62a5861ca7d8b5","Traditional mural repair methods only observe the texture of murals when segmenting the repair area, but ignore the extraction of a mural damage data, resulting in incomplete damage crack information. For this reason, the method of repairing the damaged murals based on machine vision is studied. Using machine vision, it can get two-dimensional image of a mural, preprocess the image, extract the damaged data of a mural, and then divide the repair area and repair degree index. According to different types of damage, it can choose the corresponding repair methods to achieve the repair of damaged mural. The results show: Compared with the OPTICS-based unsupervised method and the machine vision for orchard navigation method, the number of repair points and repair cracks extracted by the proposed method is more than that of the two traditional methods, which can more accurately and comprehensively extract the repair information of murals. © 2022 - IOS Press. All rights reserved.","damaged image; Machine vision; painted mural; painting grouting; repair method","Computer vision; Computerized tomography; Data mining; Textures; Damage crack; Damaged image; Machine-vision; On-machines; Painted mural; Painting grouting; Preprocess; Repair methods; Two dimensional images; Unsupervised method; Repair",Article,Scopus,2-s2.0-85124388815
"Liang J.","57447172100;","Problems and Solutions of Art Professional Service Rural Revitalization Strategy Based on Random Forest Algorithm",2022,"Wireless Communications and Mobile Computing","2022",,"9752512","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124383445&doi=10.1155%2f2022%2f9752512&partnerID=40&md5=ef6254e09d4284b3f6aca017e6dd33d4","The rural revitalization strategy proposal is not a passive water, but rather one with its own internal historical logic. The rural revitalization strategy has been implemented in a consistent manner with China's rural construction practice over the last 100 years. Since modern times, the Chinese agricultural civilization has been severely harmed by the collision and friction with western industrial and commercial civilizations, which has directly aroused the strong reflection of people with lofty ideals from all walks of life in Chinese society and led them down the path of rural construction aimed at ""rejuvenating""the countryside. Accelerating the development of rural culture is beneficial to the overall improvement of social culture and the happiness of the Chinese people. As a result, the development and construction of rural culture will have a significant impact on China. The random forest algorithm simulates the data learning process based on data mining, creates a model, and returns a judgment result. The strategy of art professional service is examined in this study. The random forest algorithm was used to revitalize rural areas. The random forest algorithm's learning process is very quick, and it is still very effective in dealing with art professional services. The current random forest algorithm weighs the importance of all variables to determine the significance of switching from independent to dependent variables. © 2022 Jianhui Liang.",,"Data mining; Decision trees; Learning algorithms; Learning systems; Professional aspects; Chinese people; Construction practice; Learning process; Modern time; Problems and Solutions; Process-based; Professional services; Random forest algorithm; Rural constructions; Social culture; Rural areas",Article,Scopus,2-s2.0-85124383445
"Wang Q.","57446767400;","Application of Six Sigma Management-based Teaching Method in Financial Management Course Online Teaching",2022,"International Journal of Emerging Technologies in Learning","17","1",,"60","73",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124356137&doi=10.3991%2fijet.v17i01.28269&partnerID=40&md5=68b356b18432151d8a1a7881d48339e3","Driven by huge demand for talents in the market, economics major in colleges and universities need to take the initiative to reform the teaching of finance specialty in accordance with the demand for finance talents in the era of big data, cultivate versatile innovative competent talents, and output high-quality talents for social enterprises, satisfy enterprises' needs of recruiting financial talents, and give full play to the social education value of university finance specialty. Financial management is the core of economic organization, and the management effectiveness affects the overall operational feasibility of the organization. In order to promote the high-quality development of the market economy, universities should devote greater effort to the teaching reform of financial management, enhance talent cultivation conversion efficiency, and give full play to the value of the financial specialty in colleges and universities. However, the existing course teaching can hardly achieve the expected goals due to insufficient monitoring of financial management teaching process, insufficient self-learning resources, and defective student learning support service, imperfect teacher's training, single assessment method, and small proportion of practical teaching, etc. In view of the current status, this paper innovatively introduced six sigma management theory to improve the online teaching of financial management courses. Firstly, the teaching process involving five stages of define-measure-analyze-improve-control was designed according to the course characteristics of financial management and at the same time, the financial management teaching model was improved by online and offline inquiry-based teaching method. In order to improve the application effectiveness of financial management online teaching resources, a precise teaching resource push system was established using Web data mining so as to achieve the goal of recommending different teaching resources to different students. It was proved by the teaching practice that the new teaching method proposed in this study can help students improve learning efficiency and enhance their problem-solving ability. Thus, this teaching model is worthy of application and promotion in courses of finance specialty. © 2022. All Rights Reserved.","financial management; online teaching; precise push; six sigma management","Commerce; Curricula; Data mining; E-learning; Education computing; Efficiency; Information management; Personnel training; Process monitoring; Six sigma; Students; Teaching; Colleges and universities; Financial managements; High quality; Management course; Online teaching; Precise push; Six sigma management; Teaching methods; Teaching process; Teaching resources; Work simplification",Article,Scopus,2-s2.0-85124356137
"Sengupta S., Banerjee A., Chakrabarti S.","57222749612;57222748529;57193560456;","Efficient Data Mining Model for Question Retrieval and Question Analytics Using Semantic Web Framework in Smart E-learning Environment",2022,"International Journal of Emerging Technologies in Learning","17","1",,"4","17",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124336117&doi=10.3991%2fijet.v17i01.25909&partnerID=40&md5=0113a11519489ad74d50cf52ba748c6e","In the field of Information recovery, the fundamental target is to discover important just as most applicable data concerning a few questions. However, the essential issue regarding recuperation has reliably been, that the request an area is enormous so much that it has gotten very difficult to recuperate applicable information capably. In any case, with the latest progressions in profound learning and AI models, calculations, applications brilliant and computerized data recovery component matched with text examination to decide different characterizing boundaries alongside intricacy and weight-age assurance of inquiries. By focusing, the cutoff points and hardships, like CPU cost, efficiency, automation and congruity, we have assigned our information recuperation structure particularly towards Academic Institutional Domain to consider the interest of various association related inquiries. The aim is to make an efficient data mining and analytical model that can automate an efficient question retrieval and analysis for complexity and weight-age determination. © 2022. All Rights Reserved.","data mining; deep learning; smart and automated information retrieval mechanism; weight-age determination","Computer aided instruction; Deep learning; E-learning; Semantic Web; Age determination; Automated information; Data mining models; Deep learning; E-learning environment; Retrieval mechanisms; Semantic-Web; Smart and automated information retrieval mechanism; Web frameworks; Weight-age determination; Data mining",Article,Scopus,2-s2.0-85124336117
"Wei Z., Yan L.","57445124600;57445773900;","Construction of an Intelligent Evaluation Model of Mental Health Based on Big Data",2022,"Journal of Sensors","2022",,"4378718","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124326262&doi=10.1155%2f2022%2f4378718&partnerID=40&md5=d25f9be77b9934c6e04d2ef4ba560e00","In this paper, mental health data were used to evaluate the educational effects, in which the high and low scorers of three emotions, autism, positivity, and anxiety, are compared separately to explore the subtle differences in the long-term trends of the sensing traits of people with opposite characteristics. Based on the fusion of multiple kinds of sensing traits, the differences in physical and mental health assessment of positive and negative emotions by different fusion trait approaches are explored, and speech and behavioural traits are fused to build a physical and mental health assessment system for positive and negative emotions. Energy gravity uses physical distance to estimate the residual energy of nodes and considers the energy distribution of downstream nodes. The main work is to combine the data of mental health of higher education students using data mining techniques, to analyze the feasibility study of mental health education of college students. Relevant definitions, classifications, tasks, processes, and application areas of data mining techniques are introduced, and the basic principles of data mining are analyzed in detail. Taking the mental health assessment data of new students as the research object, the decision tree algorithm is used to construct a decision tree model for students with depressive symptoms, and an association rule algorithm is used to data mine the relationship between factors of psychological dimensions. Finally, it can find out the hidden laws and knowledge behind the data information and analyze the relationship that exists between psychological problems and students. © 2022 Zhang Wei and Liang Yan.",,"Big data; Decision trees; Education computing; Health; Students; Behavioral traits; Data-mining techniques; Evaluation models; Health assessments; Health data; Intelligent evaluation; Long-term trend; Mental health; Physical health; Positive and negative emotions; Data mining",Article,Scopus,2-s2.0-85124326262
"Shang T.","57444946900;","An Analysis of the Motivation Mechanism of the Formation of Corporate Health Strategic Innovation Capability Based on the K-Means Algorithm",2022,"Computational Intelligence and Neuroscience","2022",,"3647549","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124316422&doi=10.1155%2f2022%2f3647549&partnerID=40&md5=83ffca168783848877b81ac0b05af0da","Improving enterprises' independent innovation capability is critical to improving their competitive strength, industries' independent innovation capability, industries' international competitiveness, and countries' independent innovation capability, as well as building an innovative country. It is now the era of strategic innovation. Many growing businesses are focused on strategic innovation, but they overlook the issue of ensuring that strategic innovation is implemented. Management and innovation are perennial themes in the long-term development of businesses. The most pressing issue in enterprises' strategic innovation activities is how to combine their strategic direction with their superior ability and choose a strategic innovation path that is appropriate for their development. This paper uses data mining theory to establish the K-means clustering algorithm to identify the best strategic orientation of enterprise innovation strategic direction selection based on the existing advantages and capabilities of enterprises based on the analysis of the direction of enterprise strategic innovation. © 2022 Tingting Shang.",,"Competition; Data mining; Corporates; Independent innovation; Innovation capability; International competitiveness; K-mean algorithms; Long-term development; Motivation mechanism; Pressung; Strategic direction; Strategic innovations; K-means clustering; algorithm; China; data mining; industry; invention; motivation; Algorithms; China; Data Mining; Industry; Inventions; Motivation",Article,Scopus,2-s2.0-85124316422
"Kulkarni D.S., Rodd S.S.","57202986724;55735233300;","Sentiment Analysis in Hindi A Survey on the State-of-The-Art Techniques",2022,"ACM Transactions on Asian and Low-Resource Language Information Processing","21","1","3469722","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124293074&doi=10.1145%2f3469722&partnerID=40&md5=83acda65b3bbff78e0710fbf58ddf9c6","Sentiment Analysis (SA) has been a core interest in the field of text mining research, dealing with computational processing of sentiments, views, and subjective nature of the text. Due to the availability of extensive web-based data in Indian languages such as Hindi, Marathi, Kannada, Tamil, and so on. It has become extremely significant to analyze this data and recover valuable and relevant information. Hindi being the first language of the majority of the population in India, SA in Hindi has turned out to be a critical task particularly for companies and government organizations. This research portrays a systematic review specifically in the field of Hindi SA. The major contribution of this article includes the categorization of numerous articles based on techniques that have attracted researchers in performing SA tasks in Hindi language. This survey classifies these state-of-The-Art computational intelligence techniques into four major categories namely lexicon-based techniques, machine learning techniques, deep learning techniques, and hybrid techniques. It discusses the importance of these techniques based on different aspects such as their impact on the issues of SA, levels of analysis, and performance evaluation measures. The research puts forward a comprehensive overview of the majority of the work done in Hindi SA. This study will help researchers in finding out resources such as annotated datasets, linguistic resources, and lexical resources. This survey delivers some significant findings and presents overall future research directions in the field of Hindi SA. © 2021 Association for Computing Machinery.","hindi language; lexicon technique; opinion mining; Sentiment analysis; systematic review","Data mining; Deep learning; Learning algorithms; Surveys; Computational processing; Critical tasks; Hindi language; Indian languages; Lexicon technique; Sentiment analysis; State-of-the-art techniques; Systematic Review; Text-mining; Web based; Sentiment analysis",Article,Scopus,2-s2.0-85124293074
"Zhou N., Shi W., Liang R., Zhong N.","57445984700;57221310911;57221310733;57220782654;","TextRank Keyword Extraction Algorithm Using Word Vector Clustering Based on Rough Data-Deduction",2022,"Computational Intelligence and Neuroscience","2022",,"5649994","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124283762&doi=10.1155%2f2022%2f5649994&partnerID=40&md5=dc3fc95900673bdc6f767ac0704e1131","When TextRank algorithm based on graph model constructs graph associative edges, the co-occurrence window rules only consider the relationships between local terms. Using the information in the document itself is limited. In order to solve the above problems, an improved TextRank keyword extraction algorithm based on rough data reasoning combined with word vector clustering, RDD-WRank, was proposed. Firstly, the algorithm uses rough data reasoning to mine the association between candidate keywords, expands the search scope, and makes the results more comprehensive. Then, based on Wikipedia online open knowledge base, word embedding technology is used to integrate Word2Vec into the improved algorithm, and the word vector of TextRank lexical graph nodes is clustered to adjust the voting importance of nodes in the cluster. Compared with the traditional TextRank algorithm and the Word2Vec algorithm combined with TextRank, the experimental results show that the improved algorithm has significantly improved the extraction accuracy, which proves that the idea of using rough data reasoning can effectively improve the performance of the algorithm to extract keywords. © 2022 Ning Zhou et al.",,"Clustering algorithms; Data mining; Extraction; Graph theory; Clusterings; Co-occurrence; Data reasoning; Extraction algorithms; Graph model; Improved * algorithm; Keywords extraction; Modeling construct; Rough data; Word vectors; Knowledge based systems; algorithm; cluster analysis; knowledge base; problem solving; Algorithms; Cluster Analysis; Knowledge Bases; Problem Solving",Article,Scopus,2-s2.0-85124283762
"Hou M.","57445123200;","English Education Translation System Based on 5G Network Virtualization",2022,"Journal of Sensors","2022",,"8763849","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124270497&doi=10.1155%2f2022%2f8763849&partnerID=40&md5=daf48e8b85550c4a13c954e6a72ba5ec","In order to improve the effect of English education translation, this paper improves the 5G network virtualization algorithm and uses the typical correlation analysis method in data mining to extract the correlation relationship between the data content contained in the massive English translation user equipment. Furthermore, this paper adopts the similarity-based clustering method to realize the self-organization construction of English translation virtual community. On this basis, this paper proposes a spectrum allocation algorithm based on convex optimization, which effectively solves the problem of spectrum allocation in virtual cells. Finally, this paper constructs the functional architecture of the English education translation system based on 5G network virtualization and conducts experimental research. The experimental research results show that the English education translation system based on 5G virtualization constructed in this paper can play an important role in smart English teaching. © 2022 Manman Hou.",,"Convex optimization; Data mining; Queueing networks; Virtual reality; Virtualization; Analysis method; Clustering methods; Correlation analysis; Data contents; English educations; Experimental research; Network virtualization; Spectrum allocation; Translation systems; User equipments; 5G mobile communication systems",Article,Scopus,2-s2.0-85124270497
"Zhang Y., Hou J., Zeng Z.","57219445924;57445211100;57446074500;","Analysis of Prescription Medication Rules of Traditional Chinese Medicine for Diabetes Treatment Based on Data Mining",2022,"Journal of Healthcare Engineering","2022",,"7653765","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124261332&doi=10.1155%2f2022%2f7653765&partnerID=40&md5=8ee2e162ee4d30a116268ec0a22d8a5d","In this study, we have used TCM medical record management platform and SAS statistical software to analyze the cases of a professor in the treatment of type 2 diabetes, in order to explore the medication rules for the treatment of type 2 diabetes, so as to enrich and optimize the diagnosis and treatment plan for type 2 diabetes based on the experience of famous doctors. Chinese medicine treatment provides more diagnosis and treatment ideas. We have collected the professor's treatment of type 2 diabetes, screened out 100 patients, from a total of 285 prescriptions, and entered them into the TCM medical record management platform. We used the TCM medical record management platform and SAS statistical software to analyze his professor's experience in the treatment of type 2 diabetes. The medication analysis is as follows: (1) frequency of medication: 285 cases that met the inclusion criteria used a total of 187 traditional Chinese medicines. Among them, Salvia miltiorrhiza was used most frequently; (2) drug frequency analysis: 285 cases met the inclusion criteria, and the four Qi were mainly cold and warm medicines. The five flavors are mainly sweet, bitter, and pungent drugs. The main meridians are the liver, spleen, and kidney; (3) drug efficacy and classification analysis: 285 cases met the inclusion criteria, and 285 cases met the inclusion criteria and the Chinese medicines involved are the most common medicines used for tonic, heat clearing, blood circulation, and stasis; (4) clustering of medications: 35 Chinese medicines with medication frequency ≥20% are divided into 11 categories. (5) Using the improved Apriori algorithm, the minimum confidence level is selected to be 0.5, data mining is conducted on all type 2 diabetes cases, and a total of 21 recipes are dug out involving 10 Chinese medicines. In this study, with the aid of the TCM medical record management platform and SAS statistical software, cluster analysis, improved Apriori algorithm, and other data mining methods were used to systematically and objectively analyze the professor's treatment of type 2 diabetes cases. The results have clinical significance and can be used for traditional Chinese medicine treatment of type 2 diabetes, which provides an objective basis and provides a certain reference for the current inheritance of traditional Chinese medical experience and summary research. © 2022 Yi Zhang et al.",,"Cardiovascular system; Cluster analysis; Clustering algorithms; Data mining; Diagnosis; Patient treatment; Chinese medicines; Improved apriori algorithms; Management platforms; Medical record; Record management; Salvia miltiorrhiza; Statistical software; Traditional Chinese Medicine; Treatment plans; Type-2 diabetes; Learning algorithms",Article,Scopus,2-s2.0-85124261332
"Tian Y., Sun Y., Zhang L., Qi W.","57432723500;57445336100;57433586200;39262265600;","Research on MOOC Teaching Mode in Higher Education Based on Deep Learning",2022,"Computational Intelligence and Neuroscience","2022",,"8031602","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124257541&doi=10.1155%2f2022%2f8031602&partnerID=40&md5=f0c7bcb4144deefab71ee335726a124a","With the rapid development of computer technology and network technology and the widespread popularity of electronic equipment, communication among people is more dependent on the Internet. The Internet has brought great convenience to people's lives and work, and the Internet data is constantly being recorded. People's data information and behavior information, which provides the basis for data mining and recommendation systems, mining users' information and behaviors, and providing ""user portraits""for each user, can provide better services to users and it is also an important part of the recommendation system. In one step, this article takes MOOC education resources as the research goal. In order to improve the effective management of MOOC platform resources based on traditional methods, this article combines relevant data sets and recommendation techniques to initially build a learning platform, implements a deep neural network algorithm, and recommends related services. The request and response data were explained, and through the online learning data set, based on the learner's historical learning records, the learning resources were simulated and recommended to the learners. The resource customization module was elaborated. Through the results of resource recommendation, a personalized learning resource recommendation platform was initially realized, which more intuitively demonstrated the recommendation effect and better realized the teaching management of the MOOC platform. © 2022 Yuan Tian et al.",,"Data mining; Deep neural networks; Information management; Oscillators (electronic); Computer technology; Data informations; Data set; High educations; Internet data; Learning resource; Network technologies; Resource recommendation; Teaching modes; User information; Recommender systems; algorithm; data mining; education; human; Algorithms; Data Mining; Deep Learning; Education, Distance; Humans; Neural Networks, Computer",Article,Scopus,2-s2.0-85124257541
"Alzubi O.A.","50460949000;","A deep learning-based frechet and dirichlet model for intrusion detection in IWSN",2022,"Journal of Intelligent and Fuzzy Systems","42","2",,"873","883",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124252005&doi=10.3233%2fJIFS-189756&partnerID=40&md5=4a1fe85164b5d7c95f40b0d7a3aa278c","Industrial Wireless Sensor Network (IWSN) includes numerous sensor nodes that collect data about target objects and transmit to sink nodes (SN). During data transmission among nodes, intrusion detection is carried to improve data security and privacy. Intrusion detection system (IDS) examines the network for intrusions based on user activities. Several works have been done in the field of intrusion detection and different measures are carried out to increase data security from the issues related to black hole, Sybil attack, Worm hole, identity replication attack and etc. In various existing approaches, secure data transmission is not achieved, therefore resulted in compromising the security and privacy of IWSNs. Accurate intrusion detection is still challenging task in terms of improving security and intrusion detection rate. In order to improve intrusion detection rate (IDR) with minimum time, generalized Frechet Hyperbolic Deep and Dirichlet Secured (FHD-DS) data communication model is introduced. At first, Frechet Hyperbolic Deep Traffic (FHDT) feature extraction method is designed to extract more relevant network activities and inherent traffic features. With the help of extracted features, anomalous or normal data is predicted. Followed by Statistical Dirichlet Anomaly-based Intrusion Detection model is applied to discover intrusion. Here, Dirichlet distribution is evaluated to attain secure data transmission and significantly detect intrusions in WSNs. Experimental evaluation is carried out with KDD cup 99 dataset on factors such as IDR, intrusion detection time (IDT) and data delivery rate (DDR). The observed results show that the generalized FHD-DS data communication method achieves higher IDR with minimum time. © 2022-IOS Press. All rights reserved.","Deep learning; Frechet hyperbolic; industrial wireless sensor networks; intrusion detection; IWSN security; machine learning; security; statistical dirichlet distribution","Computer crime; Convolutional codes; Data communication systems; Data mining; Data privacy; Data transfer; Deep learning; Network security; Sensor nodes; Deep learning; Dirichlet distributions; Frechet; Frechet hyperbolic; Industrial wireless sensor network; Industrial wireless sensor network security; Industrial wireless sensors; Intrusion-Detection; Security; Statistical dirichlet distribution; Wireless sensor network security; Intrusion detection",Article,Scopus,2-s2.0-85124252005
"Cariou C., Le Moan S., Chehdi K.","55948839000;36666943700;6602079185;","A Novel Mean-Shift Algorithm for Data Clustering",2022,"IEEE Access","10",,,"14575","14585",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124242722&doi=10.1109%2fACCESS.2022.3147951&partnerID=40&md5=75a7d231c59a3f6795807259a44fb626","We propose a novel Mean-Shift method for data clustering, called Robust Mean-Shift (RMS). A new update equation for point iterates is proposed, mixing the ones of the standard Mean-Shift (MS) and the Blurring Mean-Shift (BMS). Despite its simplicity, the proposed method has not been studied so far. RMS can be set up in both a kernel-based and a nearest-neighbor (NN)-based fashion. Since the update rule of RMS is closer to BMS, the convergence of point iterates is conjectured based on the Chen's BMS convergence theorem. Experimental results on synthetic and real datasets show that RMS in several cases outperforms MS and BMS in the clustering task. In addition, RMS exhibits larger attraction basins than MS and BMS for identical parametrization; consequently, its kernel variant requires a lower aperture of the kernel function, and its NN variant a lower number of nearest neighbors compared to MS or BMS, to achieve optimal clustering results. In addition, the NN version of RMS does not need to specify a convergence threshold to stop the iterations, contrarily to the NN-BMS algorithm. © 2013 IEEE.","data clustering; Data mining; kernel-based method; mean-shift; nearest neighbors","Cluster analysis; Clustering algorithms; Iterative methods; Job analysis; Convergence; Data clustering; Kernel; Kernel based methods; Mean shift; Mean shift algorithm; Nearest-neighbour; Sensitivity; Shift-and; Task analysis; Data mining",Article,Scopus,2-s2.0-85124242722
"Demirović E., Lukina A., Hebrard E., Chan J., Bailey J., Leckie C., Ramamohanarao K., Stuckey P.J.","55321177600;57191832219;55897451800;16068112600;7404350735;7003524629;56891817800;7006033659;","MurTree: Optimal Decision Trees via Dynamic Programming and Search",2022,"Journal of Machine Learning Research","23",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124234775&partnerID=40&md5=083cd4d55436419ab8f39358aacc50ca","Decision tree learning is a widely used approach in machine learning, favoured in applications that require concise and interpretable models. Heuristic methods are traditionally used to quickly produce models with reasonably high accuracy. A commonly criticised point, however, is that the resulting trees may not necessarily be the best representation of the data in terms of accuracy and size. In recent years, this motivated the development of optimal classification tree algorithms that globally optimise the decision tree in contrast to heuristic methods that perform a sequence of locally optimal decisions. We follow this line of work and provide a novel algorithm for learning optimal classification trees based on dynamic programming and search. Our algorithm supports constraints on the depth of the tree and number of nodes. The success of our approach is attributed to a series of specialised techniques that exploit properties unique to classification trees. Whereas algorithms for optimal classification trees have traditionally been plagued by high runtimes and limited scalability, we show in a detailed experimental study that our approach uses only a fraction of the time required by the state-of-the-art and can handle datasets with tens of thousands of instances, providing several orders of magnitude improvements and notably contributing towards the practical use of optimal decision trees. © 2022 Emir Demirović, Anna Lukina, Emmanuel Hebrard, Jeffrey Chan, James Bailey, Christopher Leckie, Kotagiri Ramamohanarao, and Peter J. Stuckey.","Combinatorial optimisation; Decision trees; Dynamic programming; Search","Classification (of information); Combinatorial optimization; Data mining; Dynamic programming; Heuristic methods; Learning algorithms; Learning systems; Classification trees; Decision tree learning; High-accuracy; Novel algorithm; ON dynamics; Optimal classification; Optimal decisions; Search; Tree algorithms; Tree-based; Decision trees",Article,Scopus,2-s2.0-85124234775
"Mohamed I., Fouda M.M., Hosny K.M.","57192916377;35955834000;57205214086;","Machine Learning Algorithms for COPD Patients Readmission Prediction: A Data Analytics Approach",2022,"IEEE Access","10",,,"15279","15287",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124233695&doi=10.1109%2fACCESS.2022.3148600&partnerID=40&md5=368ac303ecc9588f2b6f059a78d1c789","Patients' readmission can be considered as a critical factor affecting cost reduction while maintaining a high-quality treatment of patients. Therefore, predicting and controlling patients' readmission rates would significantly improve the healthcare service. In this study, we aim at predicting the readmission of COPD (Chronic Obstructive Pulmonary Disease) patients through the deployment of machine learning algorithms. Area Under Curve (AUC) and ACCuracy (ACC) were considered as the main criteria for evaluating models' prediction power in each time frame. Then, the importance of the variables for each outcome was explicitly identified, and defined important variables have then been differentiated. Our study could achieve the highest accuracy in predicting readmission with %91 ACC. © 2013 IEEE.","Classification algorithms; COPD readmission; Data mining; Decision support systems; Healthcare data analytics","Cost reduction; Data mining; Decision support systems; Forecasting; Learning algorithms; Patient treatment; Predictive analytics; Pulmonary diseases; Chronic obstructive pulmonary disease; Chronic obstructive pulmonary disease readmission; Classification algorithm; Data analytics; Healthcare data analytic; Machine learning algorithms; Medical services; Predictive models; Support vectors machine; Support vector machines",Article,Scopus,2-s2.0-85124233695
"Ou J., Zhang J.","57443311700;57355761600;","Data Mining and Meta-Analysis of Psoriasis Based on Association Rules",2022,"Journal of Healthcare Engineering","2022",,"9188553","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124229653&doi=10.1155%2f2022%2f9188553&partnerID=40&md5=116d2a1c618945480c0deba3d34de1c9","Psoriasis is a common chronic and recurrent disease in dermatology, which has a great impact on the physical and mental health of patients. Meta-analysis can evaluate the effectiveness and safety of defubao in the treatment of psoriasis vulgaris. This article observes psoriasis skin lesions treated with topical defubao and the changes in blood vessels under dermoscopy. Considering that the Apriori algorithm and the existing improved algorithm have the problems of ignoring the weight and repeatedly scanning the database, this paper proposes a matrix association rule method based on random forest weighting. This method uses the random forest algorithm to assign weights to each item in the data set, and introduces matrix theory to convert the transaction data set into a matrix form and store it, thereby improving operating efficiency. This article included 11 studies, of which 7 studies used the indicator ""Researcher's Overall Assessment""(IGA) to evaluate the efficacy, 5 studies used the ""Patient Overall Assessment""(PGA) as the efficacy evaluation index, and Loss Area and Severity Index (PASI) was used as an observation index to evaluate the efficacy. Seven studies conducted safety comparisons. In this paper, IGA and PGA were used as evaluation indicators. The treatment effect of the defubao group was better than the calcipotriol group and the betamethasone group. The differences were statistically significant. The effect of the Fubao treatment for 8 weeks is significantly better than that of 4 weeks and 2 weeks, and the differences are statistically different. Using PASI as the evaluation index, a descriptive study was carried out, and it was found that after 4 weeks of treatment for psoriasis vulgaris, the average PASI reduction rate of patients was higher than that of the calcipotriol group and the betamethasone group. The safety evaluation found that after 8 weeks of treatment, the incidence of adverse events in the defubao group was significantly lower than that in the calcipotriol group. © 2022 Jiarui Ou and Jianglin Zhang.",,"Blood vessels; Data handling; Data mining; Decision trees; Dermatology; Diagnosis; Matrix algebra; Patient treatment; Betamethasone; Data set; Dermoscopy; Evaluation index; Mental health; Meta-analysis; Physical health; Psoriasis skin; Psoriasis vulgaris; Skin lesion; Association rules; betamethasone; betamethasone dipropionate; calcipotriol; defubao; placebo; unclassified drug; algorithm; Apriori algorithm; Article; blood vessel; Chinese medicine; controlled study; data mining; drug efficacy; epiluminescence microscopy; follow up; human; loss area and severity index; meta analysis; patient overall assessment; psoriasis; psoriasis vulgaris; random forest; randomized controlled trial (topic); researcher overall assessment; unspecified side effect",Article,Scopus,2-s2.0-85124229653
"Yu T., Zhou H.","57444191400;57443610800;","Unsupervised Data Mining and Effect of Fast Rehabilitation Nursing Intervention in Fracture Surgery",2022,"Journal of Healthcare Engineering","2022",,"7087844","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124225441&doi=10.1155%2f2022%2f7087844&partnerID=40&md5=919c04ef212285a95dd3c61ff976b2d1","At present, the most commonly used surgical treatment for fractures caused by external force injury is clinical, and unsupervised data mining is more advantageous in the face of the unknown format of perioperative network data. Therefore, this research aims to explore the application effect of unsupervised data mining in the concept of rapid rehabilitation nursing intervention after fracture surgery. 80 patients who underwent fracture surgery in the Department of Orthopedics of XXX Hospital were determined as the subjects, who were rolled into a research group (group R, 40 cases) and a control group (group C, 40 cases) by drawing lots. An unsupervised data mining algorithm based on unsupervised data mining for support vector machines (VDMSVMs) was proposed and applied to two groups of patients undergoing perioperative fracture surgery with the rapid rehabilitation nursing intervention and basic routine nursing. The results showed that the number of important features selected by the VDMSVM algorithm (5) was obviously more than that of the compressed edge fragment sampling (CEFS) algorithm (1) and the multicorrelation forward searching (MCFS) algorithm (2) (P<0.05). The number of noise features screened by the VDMSVM algorithm (3) was much less in contrast to that of the CEFS algorithm and the MCFS algorithm, which was 8 and 10, respectively (P < 0.05). The Visual Analogue Scale (VAS) scores of the fracture site at the 4th, 8th, 12th, and 16th hour after surgery in group R were all lower than the scores in group C (P<0.05). The length of hospital stay (LoHS) in group R was greatly shorter than that in group C (P<0.05). After different nursing methods, the World Health Organization Quality of Life (WHOQOL-BREF) score of patients in group R (89.64 points) was greatly higher than the score in group C (61.45 points) (P<0.05). The nursing satisfaction score of group R was 92.35 ± 3.65 points, and that in group C was 2.14 ± 1.25 points, respectively (P<0.05). The test results verified the effectiveness of the feature selection of the VDMSVM algorithm. The rapid rehabilitation nursing intervention was conductive to reducing the postoperative pain of fracture patients, shortening the LoHS of patients, improving the quality of life (QOL) of fracture surgery patients, and increasing the patient's satisfaction with nursing. © 2022 Tongyao Yu and Haihong Zhou.",,"Data mining; Fracture; Hospitals; Patient rehabilitation; Support vector machines; Surgery; Clinical data; Compressed edge fragment sampling algorithms; External force; Length of hospital stays; Multi-correlations; Nursing interventions; Quality of life; Searching algorithms; Surgical treatment; Unsupervised data; Nursing; adult; Article; controlled study; data mining; fat embolism; feature selection algorithm; female; fracture; hospitalization; human; incision; length of stay; major clinical study; male; muscle atrophy; nursing intervention; patient satisfaction; perioperative period; postoperative complication; postoperative infection; postoperative pain; postoperative thrombosis; rehabilitation nursing; shock; support vector machine; unsupervised machine learning; visual analog scale; WHOQOL-BREF",Article,Scopus,2-s2.0-85124225441
"Luma-Osmani S., Ismaili F., Pathak P., Zenuni X.","57200914157;35179510900;57205670746;26421842500;","Identifying Causal Structures from Cyberstalking: Behaviors Severity and Association",2022,"Journal of Communications Software and Systems","18","1",,"1","8",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124191483&doi=10.24138%2fjcomss-2021-0139&partnerID=40&md5=aa2ab004ab13fb0b86e316861becafd1","This paper presents an etiological cyberstalking study, meaning the use of various technologies and internet in general to harass or to stalk someone. The novelty of the paper is the multivariate empirical approach of cyberstalking victimization that has received less attention from the research community. Also, there is a lack of such studies from the causal perspective. It happens, since in most of the studies, a priority is given on a single causation identification, whereas the data examination used for mining causal relationships in this paper presents a novel and great potential to detect combined or multiple cause factors. The paper focuses in the impact that variables such as age, gender and the fact whether the participant has ever harassed someone, is related to the fact of being victim of cyberstalking. The research aims to find the causes of cyberstalking in high school’s teenagers. Furthermore, an exploratory data analysis has been performed. A weak and moderate correlation between the factors on the dataset is emphasized. The odds ratio among the variables has been calculated, which implies that girls are twice as likely as boys to be cyberstalked. Similarly, concerning outcomes related to cyberstalking frequency recidivism are noticed. © 2022 CCIS.","Causal rules; Causality; Cyberstalking; Data mining",,Article,Scopus,2-s2.0-85124191483
"Wang B.","57443050900;","Data Feature Extraction Method of Wearable Sensor Based on Convolutional Neural Network",2022,"Journal of Healthcare Engineering","2022",,"1580134","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124181509&doi=10.1155%2f2022%2f1580134&partnerID=40&md5=48fbd7547e8045d8760ce4d9eef564c3","With the rapid development of society and science technology, human health issues have attracted much attention due to wearable devices' ability to provide high-quality sports, health, and activity monitoring services. This paper proposes a method for feature extraction of wearable sensor data based on a convolutional neural network (CNN). First, it uses the Kalman filter to fuse the data to obtain a preliminary state estimation, and then it uses CNN to recognize human behavior, thereby obtaining the corresponding behavior set. Moreover, this paper conducts experiments on 5 datasets. The experimental results show that the method in this paper extracts data features at multiple scales while fully maintaining data independence, can effectively extract corresponding feature data, and has strong generalization ability, which can adapt to different learning tasks. © 2022 Baoying Wang.",,"Behavioral research; Convolutional neural networks; Data mining; Extraction; Feature extraction; Wearable sensors; Convolutional neural network; Data feature; Feature extraction methods; Health issues; Health monitoring; High quality; Human health; Science technologies; Sport monitoring; Wearable devices; Convolution; acceleration; Article; artificial intelligence; behavior; convolutional neural network; data accuracy; data collection method; facial recognition; feature extraction; heart rate; human; measurement",Article,Scopus,2-s2.0-85124181509
"Ahmed M., Khan A., Ahmed M., Tahir M., Jeon G., Fortino G., Piccialli F.","57442159100;55209201300;57196745090;57442159200;15022497800;6602895297;42762051900;","Energy Theft Detection in Smart Grids: Taxonomy, Comparative Analysis, Challenges, and Future Research Directions",2022,"IEEE/CAA Journal of Automatica Sinica","PP","99",,"1","23",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124157800&doi=10.1109%2fJAS.2022.105404&partnerID=40&md5=2fc50292e69c19f866ed01eaad6d312f","Electricity theft is one of the major issues in developing countries which is affecting their economy badly. Especially with the introduction of emerging technologies, this issue became more complicated. Though many new energy theft detection (ETD) techniques have been proposed by utilising different data mining (DM) techniques, state network (SN) based techniques, and game theory (GT) techniques. Here, a detailed survey is presented where many state-of-the-art ETD techniques are studied and analysed for their strengths and limitations. Three levels of taxonomy are presented to classify state-of-the-art ETD techniques. Different types and ways of energy theft and their consequences are studied and summarised and different parameters to benchmark the performance of proposed techniques are extracted from literature. The challenges of different ETD techniques and their mitigation are suggested for future work. It is observed that the literature on ETD lacks knowledge management techniques that can be more effective, not only for ETD but also for theft tracking. This can help in the prevention of energy theft, in the future, as well as for ETD. © 2014 Chinese Association of Automation.","Challenges; comparative analysis; energy theft detection; future research directions; smart grid; taxonomy","Benchmarking; Data mining; Developing countries; Electric power transmission networks; Game theory; Knowledge management; Smart power grids; Taxonomies; Challenge; Comparative analyzes; Electricity theft; Emerging technologies; Energy; Energy theft detection; Future research directions; Smart grid; State of the art; Theft detections; Crime",Article,Scopus,2-s2.0-85124157800
"Hu L.","55267373100;","Research on English Achievement Analysis Based on Improved CARMA Algorithm",2022,"Computational Intelligence and Neuroscience","2022",,"8687879","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124111466&doi=10.1155%2f2022%2f8687879&partnerID=40&md5=dd4d1b3cc556e9105172801d19a1d862","This paper uses data mining technology to analyze students' English scores. In view of the influence of many factors on students' English performance, the analysis is realized by using the association rule algorithm. The thesis analyzes and applies students' English scores based on association rules and mainly does the following work: (1) at present, the problem of the CARMA algorithm is low operating efficiency. The combination of the genetic algorithm's crossover, mutation, and the CARMA algorithm realizes the fast search of the algorithm. The simulation results show that the operation performance of the algorithm is greatly improved after the crossover and mutation operations in the genetic algorithm are applied to the CARMA algorithm. The simulation results show that the mining accuracy of the improved algorithm is 97.985%, and the mining accuracy before the improvement is 92.221%, indicating that the improved algorithm can improve the accuracy of mining. (2) By comparing the mining time of the improved CARMA algorithm, the traditional CARMA algorithm, the FP-Growth algorithm, and the Apriori algorithm, the results show that when the number is 6,500, the mining efficiency of the improved CARMA algorithm is twice that of the other three algorithms. As the amount of data increases, the effect of improving mining efficiency gradually increases. (3) By using the improved CARMA algorithm to analyze students' English performance, it is found that the quality of student performance is strongly related to the quality of daily homework, and if it is related to the teacher's gender, professional title, etc., it is recommended that schools should pay more attention to homework during the teaching process. © 2022 Lin Hu.",,"Association rules; Data mining; Efficiency; Genetic algorithms; Association rule algorithm; Crossover and mutation; Crossover operations; Data mining technology; Fast search; Improved * algorithm; Mutation operations; Operating efficiency; Operation performance; Performance; Students; achievement; Apriori algorithm; article; attention; data mining; female; gender; genetic algorithm; human; male; mining; simulation; teacher; teaching; Algorithms; Computer Simulation; Data Mining; Humans; Schools; Students",Article,Scopus,2-s2.0-85124111466
"Tranin H., Godet O., Webb N., Primorac D.","57219761686;10239961100;57215511180;57193919573;","Probabilistic classification of X-ray sources applied to Swift -XRT and XMM-Newton catalogs",2022,"Astronomy and Astrophysics","657",,"A138","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124109087&doi=10.1051%2f0004-6361%2f202141259&partnerID=40&md5=c86aa63ec54c19a4077bfea074e414ad","Context. Serendipitous X-ray surveys have proven to be an efficient way to find rare objects, for example tidal disruption events, changing-look active galactic nuclei (AGN), binary quasars, ultraluminous X-ray sources, and intermediate mass black holes. With the advent of very large X-ray surveys, an automated classification of X-ray sources becomes increasingly valuable. Aims. This work proposes a revisited naive Bayes classification of the X-ray sources in the Swift-XRT and XMM-Newton catalogs into four classes - AGN, stars, X-ray binaries (XRBs), and cataclysmic variables (CVs) - based on their spatial, spectral, and timing properties and their multiwavelength counterparts. An outlier measure is used to identify objects of other natures. The classifier is optimized to maximize the classification performance of a chosen class (here XRBs), and it is adapted to data mining purposes. Methods. We augmented the X-ray catalogs with multiwavelength data, source class, and variability properties. We then built a reference sample of about 25 000 X-ray sources of known nature. From this sample, the distribution of each property was carefully estimated and taken as reference to assign probabilities of belonging to each class. The classification was then performed on the whole catalog, combining the information from each property. Results. Using the algorithm on the Swift reference sample, we retrieved 99%, 98%, 92%, and 34% of AGN, stars, XRBs, and CVs, respectively, and the false positive rates are 3%, 1%, 9%, and 15%. Similar results are obtained on XMM sources. When applied to a carefully selected test sample, representing 55% of the X-ray catalog, the classification gives consistent results in terms of distributions of source properties. A substantial fraction of sources not belonging to any class is efficiently retrieved using the outlier measure, as well as AGN and stars with properties deviating from the bulk of their class. Our algorithm is then compared to a random forest method; the two showed similar performances, but the algorithm presented in this paper improved insight into the grounds of each classification. Conclusions. This robust classification method can be tailored to include additional or different source classes and can be applied to other X-ray catalogs. The transparency of the classification compared to other methods makes it a useful tool in the search for homogeneous populations or rare source types, including multi-messenger events. Such a tool will be increasingly valuable with the development of surveys of unprecedented size, such as LSST, SKA, and Athena, and the search for counterparts of multi-messenger events. © H. Tranin et al. 2022.","Catalogs; Methods: statistical; X-rays: binaries; X-rays: galaxies; X-rays: general","Classification (of information); Data mining; Decision trees; Electron sources; Galaxies; Probability distributions; Stars; Statistics; X ray apparatus; Active galactic nuclei; Catalog; Methods:statistical; Property; X-ray general; X-ray sources; X-ray surveys; X-rays: Binaries; X-rays: Galaxies; XMM-Newton; Surveys",Article,Scopus,2-s2.0-85124109087
"Wang B., Chen Z., Wu L., Yang X., Zhou Y.","57216356539;26322275400;57440977700;55683811800;57224485899;","SADA-Net: A Shape Feature Optimization and Multiscale Context Information-Based Water Body Extraction Method for High-Resolution Remote Sensing Images",2022,"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15",,,"1744","1759",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124102501&doi=10.1109%2fJSTARS.2022.3146275&partnerID=40&md5=785997b7e1bcb61c79008b0a4af3c5bd","Convolutional neural networks (CNNs) have significance in remote sensing image mapping, and pixel-level representation allows refined results. Due to inconsistencies within a class and different scales of water bodies, the water body mapping has challenges, such as insufficient integrity and rough shape segmentation. To resolve these issues, we proposed an intelligent water bodies extraction method (named SADA-Net) for high-resolution remote sensing images. This method considers multiscale information, context dependence, and shape features. The network framework integrates three critical components: shape feature optimization (SFO), atrous spatial pyramid pooling, and dual attention modules. SADA-Net can accurately extract an extensive range of water bodies in complex scenarios. SADA-Net has certain advantages regarding small and dense water bodies extraction, as the SFO module effectively solves the defects of the unified processing of low-level features in the encoder stage of CNNs, which highlights the shape information of a water body. Two data types (red, green, and blue bands and multispectral images) are employed to verify the performance of the proposed network. The best result achieved an evaluation index F1-Score of 96.14% in large-scale image segmentation, and the structural similarity index measure reached 94.70%. Overall, the proposed method achieves the purpose of maximizing the integrity and optimizing the shape of a water body. Additionally, the SADA-Net proposed in this article has a specific reference value for high-resolution remote sensing image water bodies mapping. © 2008-2012 IEEE.","Atrous spatial pyramid pooling; dual attention; multispectrum; shape feature optimization; small water bodies","Data mining; Extraction; Mapping; Neural networks; Remote sensing; Water resources; Atrous spatial pyramid pooling; Dual attention; Features extraction; Features optimizations; Images segmentations; Index; Multi-spectrum; Remote-sensing; Shape; Shape feature optimization; Shape features; Small water body; Spatial pyramids; Waterbodies; Waters resources; Image segmentation",Article,Scopus,2-s2.0-85124102501
"Guo L., Luo R., Li X., Zhou Y., Juanjuan T., Lei C.","57355997500;15757674300;57355576600;57212241291;57393511900;57355438300;","Seismic Random Noise Removal Based on a Multiscale Convolution and Densely Connected Network for Noise Level Evaluation",2022,"IEEE Access","10",,,"13911","13925",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124100955&doi=10.1109%2fACCESS.2022.3147242&partnerID=40&md5=5bb83e8d88779310236b64c3f2c9653e","Traditional denoising methods for seismic exploration data design a corresponding mathematical denoising model batch according to the different properties of different random noises, which is a tedious and time-consuming process. To solve this problem, this paper proposes a deep convolutional neural network denoising model based on noise estimation (MCD-DCNN). This model is primarily composed of two modules, the noise estimation module and the denoising module. The noise estimation module uses a multiscale convolutional neural network to better extract the characteristics of random noise in the seismic data. To make full use of the extracted features, a dense connection method is adopted between the multiscale convolutions in the noise estimation module. In the denoising module, we use multiscale convolutions and dense connections to replace the original convolutional neural network and use the residual structure (ResNet) and batch normalization (BN) to improve the denoising effect and running speed of the model. In this experiment, single trace and simple and complex profile data are used as input to simulate the real data processing environment. Finally, we compare the denoising effects of the MCD-DCNN model proposed in this paper with the current mainstream feed-forward denoising convolutional neural network (DnCNN) and a fast and flexible denoising convolutional neural network (FFDNet) models. The comprehensive results show that under the condition of a given prior noise level, the denoising performance of the FFDNet and MCD-DCNN models are comparable. In the absence of a priori noise level, the denoising performance of the FFDNet model drops sharply, while the denoising performance of MCD-DCNN is not affected; therefore, MCD-DCNN is more in line with actual seismic denoising. © 2013 IEEE.","Deep learning; MCD-DCNN; noise estimate; seismic data denoising","Convolution; Data handling; Deep neural networks; Estimation; Noise abatement; Seismic design; Seismic response; Seismic waves; Convolutional neural network; De-noising; Deep learning; Features extraction; MCD-DCNN; Noise estimates; Noise estimation; Noise levels; Random noise; Seismic data denoising; Data mining",Article,Scopus,2-s2.0-85124100955
"Uddin M.N., Hafi M.F.B., Hossain S., Islam S.M.M.","57208989719;57440967600;35955923900;57441173300;","Drug Sentiment Analysis using Machine Learning Classifiers",2022,"International Journal of Advanced Computer Science and Applications","13","1",,"92","100",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124095051&doi=10.14569%2fIJACSA.2022.0130112&partnerID=40&md5=691aea50690f8a4a58f7b1117ef4700e","In recent times, one of the most emerging sub-dimensions of natural language processing is sentiment analysis which refers to analyzing opinion on a particular subject from plain text. Drug sentiment analysis has become very significant in present times as classifying medicines based on their effectiveness through analyzing reviews from users can assist potential future consumers in gaining knowledge and making better decisions about a particular drug. The objective of this proposed research is to measure the effectiveness level of a particular drug. Currently most of the text mining researches are based on unsupervised machine learning methods to cluster data. When supervised learning methods are used for text mining, the usual primary concern is to classify the data into two classes. Lack of technical terms in similar datasets make the categorization even more challenging. The proposed research focuses on finding out the keywords through tokenization and lemmatization so that better accuracy can be achieved for categorizing the drugs based on their effectiveness using different algorithms. Such categorization can be instrumental for treating illness as well as improve one’s health and well-being. Four machine learning algorithms have been applied for binary classification and one for multiclass classification on the drug review dataset acquired from the UCI machine learning repository. The machine learning algorithms used for binary classification are naive Bayes classifier, random forest, support vector classifier (SVC), and multilayer perceptron; among these machine learning algorithms, linear SVC was used for multiclass classification. Results obtained from these four classifier algorithms have been analyzed to evaluate their performances. The random forest has been proven to have the best performance among these four algorithms. However, multiclass classification was found to have low performance when applied to natural language processing. On the contrary, the applied linear SVC algorithm performed better for class 2 with AUC 0.82 in this research © 2022,International Journal of Advanced Computer Science and Applications.All Rights Reserved","Drugs sentiment analysis; Machine learning algorithms; Natural language processing; Text mining","Classification (of information); Classifiers; Data mining; Decision trees; Learning algorithms; Static Var compensators; Support vector machines; Binary classification; Classifier algorithms; Drug sentiment analyse; Machine learning algorithms; Multi-class classification; Performance; Random forests; Sentiment analysis; Support vector classifiers; Text-mining; Sentiment analysis",Article,Scopus,2-s2.0-85124095051
"Wu S., Dong Z.","57441119400;57441319700;","An Auxiliary Decision-Making System for Electric Power Intelligent Customer Service Based on Hadoop",2022,"Scientific Programming","2022",,"5165718","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124091285&doi=10.1155%2f2022%2f5165718&partnerID=40&md5=fa90ec23b2421304da7445e99e3bf113","Aiming at the problems of low security, high occupancy rate, and long response time in the current power intelligent customer service assistant decision-making system, a power intelligent customer service assistant decision-making system based on the Hadoop big data framework is designed. By analyzing the Hadoop big data framework, according to the characteristics and core elements of the HDFS distributed file system, the MapReduce programming model, and the data mining algorithm, the basic process of power intelligent customer service assistance decision-making is established. We analyze the overall and functional requirements of the system, design the overall architecture and application architecture of the system, design the E-R diagram and table structure of the database according to the database design principle, and realize the design of power intelligent customer service auxiliary decision-making system based on Hadoop big data framework. The test results show that the proposed method has high system security and low system occupancy and can effectively shorten the system response time. The systems run more flawlessly as compared to the existing methods and give impressing results with lesser CPU utilization. The response time was recorded to be about 12.2 seconds for 1000 power intelligent customer servers, which is much lower than that of the competitors. © 2022 Shisong Wu and Zhaojie Dong.",,"Big data; Data mining; Decision making; Distributed computer systems; Distributed database systems; Response time (computer systems); Sales; Assistant decision-making system; Characteristic elements; Current power; Customer-service; Data framework; Decision-making systems; Electric power; Occupancy rate; Power; Service-based; File organization",Article,Scopus,2-s2.0-85124091285
"Wu W., Deng Z.","57433295100;7402665968;","The Analysis of Public Opinion in Colleges and Universities Oriented to Wireless Networks under the Application of Intelligent Data Mining",2022,"Wireless Communications and Mobile Computing","2022",,"7597366","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124087546&doi=10.1155%2f2022%2f7597366&partnerID=40&md5=5d884f18cc5f5b4031cd754de11e65e9","In recent years, the incidence of public opinion in colleges and universities has been high. Monitoring, forecasting, and responding to public opinion of students in colleges and universities have increasingly become the work that education management departments at all levels attach great importance to. For each university, how to understand the sensation of teachers and students in real time in the era of informationization entering the intelligent campus has become an urgent problem. How to collect college campus network information, analyze and manage this information, and find hot topics from it has a profound impact on the reform of colleges and universities. Hence, in this paper, we propose a public opinion analysis framework based on intelligent data mining technique. Its advantage lies in the fact that it can withdraw the needed and unknown knowledge and regularities from the massive network data and host log data. It is a new attempt to use data mining in achieving public opinion. At present, data mining algorithm applied to public opinion analysis mainly has four basic patterns: association, sequence, classification, and clustering. Data mining technology is advanced for: it can process large amount of data. It does not need the users' subjective evaluation and is more likely to discover the ignored and hidden information. Here, initially, the dataset is collected, which is preprocessed and divided into a training set and test set. Feature extraction of the text is done using Linear Discriminant Analysis (LDA). After that, text cosine similarity calculation is performed to compute the similarity between text vectors obtained from the LDA. Convolutional neural network (CNN) is used for classification purpose. We proposed Krill Herd Harmony Search Optimization Algorithm (KHHSOA) for optimizing the CNN and classifying the text into positive and negative opinion. The proposed system is simulated using MATLAB simulation tool, and the performance is analyzed in terms of metrics like accuracy, precision, recall, F-measure, kappa coefficient, and error rate. The proposed method is proved to be better when compared with the existing techniques. © 2022 WenNing Wu and ZhengHong Deng.",,"Data mining; Discriminant analysis; Information management; MATLAB; Statistical tests; Students; Wireless networks; Colleges and universities; Convolutional neural network; Education management; Informationization; Intelligent data minings; Linear discriminant analyze; Opinion analysis; Public opinions; Real- time; Teachers'; Social aspects",Article,Scopus,2-s2.0-85124087546
"Calderon-Valenzuela J., Payihuanca-Mamani K., Bedregal-Alpaca N.","57440763200;57441465900;57204841602;","Educational Data Mining to Identify the Patterns of Use made by the University Professors of the Moodle Platform",2022,"International Journal of Advanced Computer Science and Applications","13","1",,"321","328",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124086708&doi=10.14569%2fIJACSA.2022.0130140&partnerID=40&md5=d503d9c8d02cd57876512a4957f8314e","Due to the events caused by the COVID-19 pandemic and social distancing measures, learning management systems have gained importance, preserving quality standards, they can be used to implement remote education or as support for face-to-face education. Consequently, it is important to know how teachers and students use them. In this work, clustering techniques are used to analyze the use, made by university professors, of the resources and activities of the Moodle platform. The CRISP-DM methodology was applied to implement a data mining process, based on the Simple K-Means algorithm; to identify associated groups of teachers it was necessary to categorize the data obtained from the platform. The Apriori algorithm was applied to identify associations in the use of resources and activities. Performance scales were established in the use of Moodle functionalities, the results show the use made by teachers was very low. Rules were generated to identify the associations between activities and resources. As a result the functionalities that need to be enhanced in the teacher training processes were identified. Having identified the patterns of use of the Moodle platform, it is concluded that it was necessary to use a Likert scale to transform the frequency of use of activities and resources and identify the rules of association that establish profiles of teachers and tools that should be promoted in future training actions © 2022,International Journal of Advanced Computer Science and Applications.All Rights Reserved","A priori algorithm; Clustering; Educational data mining; K-means algorithm; Moodle; Usage patterns","K-means clustering; Personnel training; Technology transfer; A priori algorithm; Clusterings; Educational data mining; K-mean algorithms; Learning management system; Moodle; MOODLE platform; Quality standard; Teachers'; Usage patterns; Data mining",Article,Scopus,2-s2.0-85124086708
"An Y., Meng S., Wu H.","57441078500;57441180300;57440874200;","Discover Customers’ Gender From Online Shopping Behavior",2022,"IEEE Access","10",,,"13954","13965",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124082097&doi=10.1109%2fACCESS.2022.3147447&partnerID=40&md5=321e0c3f8e7593998d5206ffb8971ca7","Gender information is very important for the recommendation system in the online shopping website. However, gender data often face label missing and incorrect labelling problems caused by consumers’ unwillingness to actively disclose personal information, which leads to gender estimation results that cannot meet the needs of the product recommendation system. To discover the customers’ gender information, we explore the customers’ online shopping behavior, especially the items viewed in the shopping session, from the dataset provided by Vietnam FPT Group. The dataset is very imbalanced while the number of female samples is 3\times of the male samples. To address the imbalance issue, we cluster the female samples into three subsets and then train a two-layer classifier model to estimate the customers’ gender. Experimental results demonstrate that our proposed method could achieve a combined accuracy 78% on average, and takes less than 6 seconds on average. As a data mining model for gender prediction, our approach has a lightweight network structure and less time consumption. © 2013 IEEE","Data models; Electronic commerce; Estimation; Predictive models; Privacy; Training; Training data","Electronic commerce; Recommender systems; Sales; Cluster; Estimation results; Gender classification; Gender data; Gender estimations; Labelings; Online shopping; Personal information; Shopping behaviour; Shopping websites; Data mining",Article,Scopus,2-s2.0-85124082097
"Karrar A.E.","57208438049;","Investigate the Ensemble Model by Intelligence Analysis to Improve the Accuracy of the Classification Data in the Diagnostic and Treatment Interventions for Prostate Cancer",2022,"International Journal of Advanced Computer Science and Applications","13","1",,"181","188",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124081444&doi=10.14569%2fIJACSA.2022.0130122&partnerID=40&md5=91da0a788b90a0479ca93bad155fa092","Class imbalance problem become greatest issue in data mining, imbalanced data appears in daily application, especially in the health care. This research aims at investigating the application of ensemble model by intelligence analysis to improving the classification accuracy of imbalanced data sets on prostate cancer. The primary requirements obtained for this study included the datasets, relevant tools for pre-processing to identify the missing values, models for attribute selection and cross validation, data resembling framework, and intelligent algorithms for base classification. Additionally, the ensemble model and meta-learning algorithms were acquired in preparation for performance evaluation by embedding feature selecting capabilities into the classification model. The experimental results led to the conclusion that the application of ensemble learning algorithm on resampled data sets provides highly accurate classification results on single classifier J48. The study further suggests that gain ratio and ranker techniques are highly effective for attribute selection in the analysis of prostate cancer data. The lowest error rate and optimal performance accuracy in the classification of imbalanced prostate cancer data is achieved using when Adaboost algorithm is combined with single classifier J48 © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.","Classification of imbalanced data; Ensemble model; Intelligence analysis; Prostate cancer","Adaptive boosting; Classification (of information); Data mining; Urology; Attribute selection; Cancer data; Class imbalance problems; Classification accuracy; Classification of imbalanced data; Ensemble models; Imbalanced data; Imbalanced dataset; Intelligence analysis; Prostate cancers; Diseases",Article,Scopus,2-s2.0-85124081444
"Patil S.S., Vidyavathi B.M.","57440958300;57194610382;","A Machine Learning Approach to Weather Prediction in Wireless Sensor Networks",2022,"International Journal of Advanced Computer Science and Applications","13","1",,"254","259",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124079361&doi=10.14569%2fIJACSA.2022.0130131&partnerID=40&md5=8251c8dad8bcb089c4387d3182b9ea0e","Weather prediction is the key requirement to save many lives from environmental disasters like landslides,earthquake, flood, forest fire, tsunami etc. Disaster monitoringand issuing forewarning to people, living in disaster-prone places,can help protect lives. In this paper, the Multiple LinearRegression (MLR) model is proposed for humidity prediction.After exploratory data analysis and outlier treatment, MultipleLinear Regression technique was applied to predict humidity.Intel lab dataset, collected by deploying 54 sensors, to form awireless sensor network, an advanced networking technologythat existed in the frontier of computer networks, is used forsolution build. Inputs to the model are various meteorologicalvariables, for predicting weather precisely. The model isevaluated using metrics - Mean Absolute Error (MAE), RootMean Square Error (RMSE) and Mean Absolute PercentageError (MAPE). From experimentation, the applied methodgenerated results with a minimum error of 11%, hence the modelis statistically significant and predictions more reliable thanother methods © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.","Adjusted r-square; Data mining; Multiple linear regression; Outliers treatment; R-square; Wireless sensor network","Data mining; Deforestation; Disasters; Errors; Machine learning; Statistics; Weather forecasting; Wireless sensor networks; Adjusted r-square; Environmental disasters; Exploratory data analysis; Forest fires; Machine learning approaches; Multiple linear regression models; Outlier treatment; R square; Weather prediction; Multiple linear regression",Article,Scopus,2-s2.0-85124079361
"Pursalim M., Keong K.C.","57440210600;57211226493;","An Efficient Multiresolution Clustering for Motif Discovery in Complex Networks",2022,"IEEE/ACM Transactions on Computational Biology and Bioinformatics","19","1",,"284","294",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124054945&doi=10.1109%2fTCBB.2020.3003018&partnerID=40&md5=3c780c707dcae9f73f1c26e4476bc409","Motif discovery and network clustering in complex networks have received a lot of attention in recent years, also they are still challenging tasks in bioinformatics, big data analytics and data mining applications. Motif discovery in big data networks has a lot of important applications in different domains such as engineering, bioinformatics, cheminformatics, genomics, sociology and ecology for revealing hidden frequent structures, functional building blocks, or knowledge discovery. In this paper, a motif localization method based on a novel clustering algorithm in complex networks is presented. In our method, for each complex network, a novel structure so-called Augmented Multiresolution Network (AMN) is generated, then it is adaptively partitioned into several clusters and their corresponding subnets. Then top ranked subnets are chosen to discover network motifs. We show that the proposed method provides an efficient solution for clustering and motif discovery; It speeds up current motif discovery algorithms by pruning non-promising regions of complex networks. Experimental results show our algorithm efficiently deals with complex networks representing large datasets with high-dimensionality such as big scientific data. Our method also provides motivations for future studies in big data and complex networks. © 2004-2012 IEEE.","complex networks; knowledge discovery; Motif discovery; multiresolution network clustering","Bioinformatics; Clustering algorithms; Complex networks; Large dataset; Cheminformatics; Data mining applications; Data network; Different domains; Motif discovery; Multi-resolution clustering; Multiresolution; Multiresolution network clustering; Network Clustering; Subnets; Data mining; algorithm; biology; cluster analysis; genomics; Algorithms; Cluster Analysis; Computational Biology; Genomics",Article,Scopus,2-s2.0-85124054945
"Zhao F., Skums P., Zelikovsky A., Sevigny E.L., Swahn M.H., Strasser S.M., Huang Y., Wu Y.","57197867147;55919194000;57219004836;24400169900;7003475674;7004981363;57196142018;55817537700;","Computational Approaches to Detect Illicit Drug Ads and Find Vendor Communities Within Social Media Platforms",2022,"IEEE/ACM Transactions on Computational Biology and Bioinformatics","19","1",,"180","191",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124051625&doi=10.1109%2fTCBB.2020.2978476&partnerID=40&md5=3baca938bcdb0c14fb7df119d5992b9a","The opioid abuse epidemic represents a major public health threat to global populations. The role social media may play in facilitating illicit drug trade is largely unknown due to limited research. However, it is known that social media use among adults in the US is widespread, there is vast capability for online promotion of illegal drugs with delayed or limited deterrence of such messaging, and further, general commercial sale applications provide safeguards for transactions; however, they do not discriminate between legal and illegal sale transactions. These characteristics of the social media environment present challenges to surveillance which is needed for advancing knowledge of online drug markets and the role they play in the drug abuse and overdose deaths. In this paper, we present a computational framework developed to automatically detect illicit drug ads and communities of vendors. The SVM- and CNN- based methods for detecting illicit drug ads, and a matrix factorization based method for discovering overlapping communities have been extensively validated on the large dataset collected from Google+, Flickr and Tumblr. Pilot test results demonstrate that our computational methods can effectively identify illicit drug ads and detect vendor-community with accuracy. These methods hold promise to advance scientific knowledge surrounding the role social media may play in perpetuating the drug abuse epidemic. © 2004-2012 IEEE.","community detection; deep learning; illicit drug ads; Opioid abuse epidemic; social media; text mining","Commerce; Computational methods; Crime; Data mining; Deep learning; Epidemiology; Factorization; Health risks; Large dataset; Numerical models; Community detection; Deep learning; Drug abuse; Illicit drug; Illicit drug ads; Opioid abuse epidemic; Opioids; Social media; Text-mining; Vendor community; Social networking (online); adult; advertising; human; methodology; social media; Adult; Advertising; Humans; Illicit Drugs; Research Design; Social Media",Article,Scopus,2-s2.0-85124051625
"Flamm C., Hellmuth M., Merkle D., Nojgaard N., Stadler P.F.","7004463099;24922528300;7003688971;57195558033;7102702401;","Generic Context-Aware Group Contributions",2022,"IEEE/ACM Transactions on Computational Biology and Bioinformatics","19","1",,"429","442",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124051403&doi=10.1109%2fTCBB.2020.2998948&partnerID=40&md5=1251558bb398b5bd4190d63563718c47","Many properties of molecules vary systematically with changes in the structural formula and can thus be estimated from regression models defined on small structural building blocks, usually functional groups. Typically, such approaches are limited to a particular class of compounds and requires hand-curated lists of chemically plausible groups. This limits their use in particular in the context of generative approaches to explore large chemical spaces. Here we overcome this limitation by proposing a generic group contribution method that iteratively identifies significant regressors of increasing size. To this end, LASSO regression is used and the context-dependent contributions are 'anchored' around a reference edge to reduce ambiguities and prevent overcounting due to multiple embeddings. We benchmark our approach, which is available as 'Context AwaRe Group cOntribution' ($\mathsf {CARGO}_{\mathrm{}}$CARGO), on artificial data, typical applications from chemical thermodynamics. As we shall see, this method yields stable results with accuracies comparable to other regression techniques. As a by-product, we obtain interpretable additive contributions for individual chemical bonds and correction terms depending on local contexts. © 2004-2012 IEEE.","cheminformatics; frequent subgraph mining; Group contributions; lasso regression; thermodynamics","Benchmarking; Bond strength (chemical); Data mining; Iterative methods; Thermodynamics; Cheminformatics; Context-aware groups; Frequent subgraph mining; Generic contexts; Group contributions; Lasso regressions; Property; Regression modelling; Structural buildings; Structural formula; Regression analysis; regression analysis; thermodynamics; Regression Analysis; Thermodynamics",Article,Scopus,2-s2.0-85124051403
"Xiang Z.","57439947000;","Visual Dynamic Simulation Model of Unstructured Data in Social Networks",2022,"Security and Communication Networks","2022",,"9095330","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124048505&doi=10.1155%2f2022%2f9095330&partnerID=40&md5=82510b00e4d929d1770506a98e0068af","Social networks contain a large amount of unstructured data. To ensure the stability of unstructured big data, this study proposes a method for visual dynamic simulation model of unstructured data in social networks. This study uses the Hadoop platform and data visualization technology to establish a univariate linear regression model according to the time correlation between data, estimates and approximates perceptual data, and collects unstructured data of social networks. Then, the unstructured data collected from the original social network are processed, and an adaptive threshold is designed to filter out the influence of noise. The unstructured data of social network after feature analysis are processed to extract its visual features. Finally, this study carries out the Hadoop cluster design, implements data persistence by HDFS, uses MapReduce to extract data clusters for distributed computing, builds a visual dynamic simulation model of unstructured data in social network, and realizes the display of unstructured data in social network. The experimental results show that this method has a good visualization effect on unstructured data in social networks and can effectively improve the stability and efficiency of unstructured data visualization in social networks. © 2022 Zhang Xiang.",,"Cluster computing; Data mining; MapReduce; Regression analysis; Social networking (online); Social sciences computing; Visualization; Adaptive thresholds; Data estimates; Dynamic simulation models; Hadoop platforms; Large amounts; Linear regression modelling; Time correlations; Univariate; Unstructured data; Visualization technologies; Data visualization",Article,Scopus,2-s2.0-85124048505
"Miah M.B.A., Awang S., Azad M.S., Rahman M.M.","57193541086;35118601300;57439627600;57212184310;","Keyphrases Concentrated Area Identification from Academic Articles as Feature of Keyphrase Extraction: A New Unsupervised Approach",2022,"International Journal of Advanced Computer Science and Applications","13","1",,"788","796",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124045433&doi=10.14569%2fIJACSA.2022.0130192&partnerID=40&md5=f1a60bb6b84975b40d3b154e18d7cc44","The extraction of high-quality keywords and summarising documents at a high level has become more difficult in current research due to technological advancements and the exponential expansion of textual data and digital sources. Extracting high-quality keywords and summarising the documents at a highlevel need to use features for the keyphrase extraction, becoming more popular. A new unsupervised keyphrase concentrated area (KCA) identification approach is proposed in this study as a feature of keyphrase extraction: corpus, domain and language independent; document length-free; utilized by both supervised and unsupervised techniques. In the proposed system, there are three phases: data pre-processing, data processing, and KCA identification. The system employs various text pre-processing methods before transferring the acquired datasets to the data processing step. The pre-processed data is subsequently used during the data processing step. The statistical approaches, curve plotting, and curve fitting technique are applied in the KCA identification step. The proposed system is then tested and evaluated using benchmark datasets collected from various sources. To demonstrate our proposed approach’s effectiveness, merits, and significance, we compared it with other proposed techniques. The experimental results on eleven (11) datasets show that the proposed approach effectively recognizes the KCA from articles as well as significantly enhances the current keyphrase extraction methods based on various text sizes, languages, and domains. © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.","Curve fitting; Data processing; Feature extraction; Kca identification; Keyphrase concentrated area; Keyphrase extraction","Character recognition; Data handling; Data mining; Extraction; 'current; Curves fittings; Features extraction; High quality; Kca identification; Key-phrase; Key-phrases extractions; Keyphrase concentrated area; Processing steps; Unsupervised approaches; Curve fitting",Article,Scopus,2-s2.0-85124045433
"Jia X., Wang L.","57216866955;57192099417;","Attention enhanced capsule network for text classification by encoding syntactic dependency trees with graph convolutional neural network",2022,"PeerJ Computer Science","7",,"e831","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124040238&doi=10.7717%2fPEERJ-CS.831&partnerID=40&md5=a06dcdd6dd53e10bb5435d6127963f2d","Text classification is a fundamental task in many applications such as topic labeling, sentiment analysis, and spam detection. The text syntactic relationship and word sequence are important and useful for text classification. How to model and incorporate them to improve performance is one key challenge. Inspired by human behavior in understanding text. In this paper, we combine the syntactic relationship, sequence structure, and semantics for text representation, and propose an attention-enhanced capsule network-based text classification model. Specifically, we use graph convolutional neural networks to encode syntactic dependency trees, build multihead attention to encode dependencies relationship in text sequence, merge with semantic information by capsule network at last. Extensive experiments on five datasets demonstrate that our approach can effectively improve the performance of text classification compared with state-of-the-art methods. The result also shows capsule network, graph convolutional neural network, and multi-headed attention has integration effects on text classification tasks. © 2022 Jia and Wang. All Rights Reserved.","Artificial Intelligence; Capsule network; Computational Linguistics; Data Mining and Machine Learning; Graph convolutional neural network; Multi-headed attention; Natural Language and Speech; Syntactic dependency tree; Text classification","Behavioral research; Classification (of information); Computational linguistics; Data mining; Encoding (symbols); Forestry; Graph neural networks; Machine learning; Network coding; Semantics; Sentiment analysis; Trees (mathematics); Capsule network; Data mining and machine learning; Labelings; Machine-learning; Multi-headed attention; Natural languages; Natural speech; Sentiment analysis; Syntactic dependency trees; Text classification; Convolution",Article,Scopus,2-s2.0-85124040238
"Patel A.A., Kathiriya D.","57439108200;56826169600;","Cotton Crop Yield Prediction using Data Mining Technique",2022,"International Journal of Advanced Computer Science and Applications","13","1",,"725","731",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124033704&doi=10.14569%2fIJACSA.2022.0130184&partnerID=40&md5=50b0d9ad8be9bb1c8b275f8985771755","Cotton is a very important crop, as India leads it in terms of production in the world; and also that a vast number of manpower is engaged in farming as well as post-harvest processing and management of different derivatives of it. Weather is crucial for the productivity of the crop. The challenges of climate change; availability of limited land and water for farming; lake of knowledge for good cultivation practices and judicious use of agricultural inputs with farmers are critical hindrances for improving productivity. This requires thorough research on land preparation and use, how to improve fertility of soil, good agronomic practices in lieu of variable climatic conditions, etc. All the talukas of the three districts of North Gujarat where cotton is cultivated have been selected purposively for this study. The effect of soil type, soil pH, soil organic carbon, phosphorous, potassium, precipitation and temperature were selected as independent factors. The yield of cotton crop has positive correlation with the selected parameters. The data sets were applied for analytical process to WEKA. The difference between average of predicted and actual yields of all talukas for high rainfall year 2013 was only 1.55 per cent. The difference between actual and predicted yield for the low temperature year (2015) in different talukas of all talukas was only 0.44 per cent © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.","Agriculture; Cotton crop yield prediction; Data mining; Data processing; Data visualization","Climate change; Cotton; Crops; Cultivation; Data handling; Data visualization; Organic carbon; Productivity; Soils; Temperature; Agronomic practices; Climatic conditions; Cotton crop yield prediction; Crop yield; Data-mining techniques; Gujarat; Postharvest processing; Soil pH; Soil types; Yield prediction; Data mining",Article,Scopus,2-s2.0-85124033704
"Ahmed H.A., Hameed A., Bawany N.Z.","57207737338;57207736332;10044512100;","Network intrusion detection using oversampling technique and machine learning algorithms",2022,"PeerJ Computer Science","8",,"e820","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124023733&doi=10.7717%2fPEERJ-CS.820&partnerID=40&md5=1139f1c329a80d16bb1e2a8d4b2b04bb","The expeditious growth of the World Wide Web and the rampant flow of network traffic have resulted in a continuous increase of network security threats. Cyber attackers seek to exploit vulnerabilities in network architecture to steal valuable information or disrupt computer resources. Network Intrusion Detection System (NIDS) is used to effectively detect various attacks, thus providing timely protection to network resources from these attacks. To implement NIDS, a stream of supervised and unsupervised machine learning approaches is applied to detect irregularities in network traffic and to address network security issues. Such NIDSs are trained using various datasets that include attack traces. However, due to the advancement in modern-day attacks, these systems are unable to detect the emerging threats. Therefore, NIDS needs to be trained and developed with a modern comprehensive dataset which contains contemporary common and attack activities. This paper presents a framework in which different machine learning classification schemes are employed to detect various types of network attack categories. Five machine learning algorithms: Random Forest, Decision Tree, Logistic Regression, K-Nearest Neighbors and Artificial Neural Networks, are used for attack detection. This study uses a dataset published by the University of New South Wales (UNSW-NB15), a relatively new dataset that contains a large amount of network traf fi c data with nine categories of network attacks. The results show that the classification models achieved the highest accuracy of 89.29% by applying the Random Forest algorithm. Further improvement in the accuracy of classification models is observed when Synthetic Minority Oversampling Technique (SMOTE) is applied to address the class imbalance problem. After applying the SMOTE, the Random Forest classifier showed an accuracy of 95.1% with 24 selected features from the Principal Component Analysis method. © 2022 Ahmed et al. All Rights Reserved.","Computer Networks and Communications; Data Mining and Machine Learning; Imbalanced classes; Network attack; Network intrusion detection system (NIDS); Security and Privacy; Synthetic minority over-sampling technique (SMOTE); UNSW-NB15 dataset","Computer crime; Data mining; Decision trees; Intrusion detection; Large dataset; Learning algorithms; Machine learning; Nearest neighbor search; Network architecture; Neural networks; Principal component analysis; Computer network and communication; Data mining and machine learning; Imbalanced class; Machine-learning; Network attack; Network intrusion detection system; Network intrusion detection systems; Security and privacy; Synthetic minority over-sampling technique; Synthetic minority over-sampling techniques; UNSW-nb15 dataset; Network security",Article,Scopus,2-s2.0-85124023733
"Abdelhadi A., Zainudin S., Sani N.S.","57439626000;24479069300;57196190931;","A Regression Model to Predict Key Performance Indicators in Higher Education Enrollments",2022,"International Journal of Advanced Computer Science and Applications","13","1",,"454","460",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124018142&doi=10.14569%2fIJACSA.2022.0130156&partnerID=40&md5=ae89506c1a6063cb9eef6820f6873e89","Key Performance Indicators (KPIs) are essential factors for the success of an organization. KPIs measure the current performance and identify the ongoing progress for specified business objectives. The Ministry of Higher Education (MoHE) in Palestine used established formulas to predict the KPI. These KPIs are vital for charting the organization aims. This study applies regression models for student enrollment data sets to predict accurate KPIs that can be used and adapted for any higher education system. The predictive engine will determine the KPI based on linear regression techniques such as Lasso, Elastic Net, and non-linear regression such as Support Vector Regression (SVR), and K-Nearest Neighbor (KNN). The Ministry of Higher Education (MoHE) in Palestine provided the datasets related to enrollments and graduations data for different Higher Education Institutions (HEIs). The regression algorithms were evaluated by mean absolute error, mean square error (MSE), root mean square error (RMSE) and the R Squared. The experiment demonstrates that the 40% training with 60% testing splitting using linear regression shows the best result. © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.","Data mining; Higher education; Kpi; Prediction model; Regression","Data mining; Education computing; Errors; Forecasting; Linear regression; Mean square error; Nearest neighbor search; Business objectives; Current performance; Data set; High educations; Key performance indicators; Kpi; Palestine; Prediction modelling; Regression modelling; Student enrollments; Benchmarking",Article,Scopus,2-s2.0-85124018142
"Liu Y.L., Chen Y.K., Li W.X., Zhang Y.","57439097200;57438755600;57439437900;57225167117;","Model design and parameter optimization of CNN for side-channel cryptanalysis",2022,"PeerJ Computer Science","7",,"e829","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124016721&doi=10.7717%2fPEERJ-CS.829&partnerID=40&md5=b1320b160f6e3abc9ea3909395ea0525","Background: The side-channel cryptanalysis method based on convolutional neural network (CNNSCA) can effectively carry out cryptographic attacks. The CNNSCA network models that achieve cryptanalysis mainly include CNNSCA based on the VGG variant (VGG-CNNSCA) and CNNSCA based on the Alexnet variant (Alex-CNNSCA). The learning ability and cryptanalysis performance of these CNNSCA models are not optimal, and the trained model has low accuracy, too long training time, and takes up more computing resources. In order to improve the overall performance of CNNSCA, the paper will improve CNNSCA model design and hyperparameter optimization. Methods: The paper first studied the CNN architecture composition in the SCA application scenario, and derives the calculation process of the CNN core algorithm for side-channel leakage of one-dimensional data. Secondly, a new basic model of CNNSCA was designed by comprehensively using the advantages of VGG-CNNSCA model classification and fitting efficiency and Alex-CNNSCA model occupying less computing resources, in order to better reduce the gradient dispersion problem of error back propagation in deep networks, the SE (Squeeze-and-Excitation) module is newly embedded in this basic model, this module is used for the first time in the CNNSCA model, which forms a new idea for the design of the CNNSCA model. Then apply this basic model to a known first-order masked dataset from the side-channel leak public database (ASCAD). In this application scenario, according to the model design rules and actual experimental results, exclude non-essential experimental parameters. Optimize the various hyperparameters of the basic model in the most objective experimental parameter interval to improve its cryptanalysis performance, which results in a hyper-parameter optimization scheme and a final benchmark for the determination of hyper-parameters. Results: Finally, a new CNNSCA model optimized architecture for attacking unprotected encryption devices is obtained—CNNSCAnew. Through comparative experiments, CNNSCAnew's guessing entropy evaluation results converged to 61. From model training to successful recovery of the key, the total time spent was shortened to about 30 min, and we obtained better performance than other CNNSCA models. © 2022 Liu et al. All Rights Reserved.","Alexnet; Algorithms and Analysis of Algorithms; Artificial Intelligence; CNN; Cryptography; Data Mining and Machine Learning; Hyperparameter; SEnet; Side-channel analysis; VGG","Backpropagation; Benchmarking; Convolutional neural networks; Network architecture; Side channel attack; Alexnet; Algorithm and analyse of algorithm; Analysis of algorithms; CNN; Data mining and machine learning; Hyper-parameter; Machine-learning; Senet; Side-channel analysis; VGG; Data mining",Article,Scopus,2-s2.0-85124016721
"Yang B.","57439292600;","Traffic accident frequency prediction method based on deep data mining",2022,"Advances in Transportation Studies","1","Special issue",,"97","107",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123987474&doi=10.53136%2f979125994811310&partnerID=40&md5=9fef52acc8c32298c17272ce17744ead","In order to overcome the problem of low prediction accuracy of traditional traffic accident frequency prediction methods, a new traffic accident frequency prediction method based on deep data mining is proposed in this paper. Firstly, the association rule algorithm is used to deeply mine and collect traffic accident data. Secondly, based on the data collection results, the proportional sequence of traffic accident data is constructed and tested. Finally, the grey prediction model is used to construct the traffic accident frequency prediction function, and the traffic accident frequency prediction results are obtained. The experimental results show that this method can achieve high-precision traffic accident data collection, and get accurate frequency prediction results. The prediction results of traffic accident frequency basically reach 99%. © 2022, Aracne Editrice. All rights reserved.","Association rules; Deep data mining; Prediction; Traffic accident frequency","Association rules; Data acquisition; Forecasting; Highway accidents; Accident data; Accident frequency; Association rule algorithm; Data collection; Deep data mining; Frequency prediction; Grey prediction model; Prediction accuracy; Prediction methods; Traffic accident frequency; Data mining",Article,Scopus,2-s2.0-85123987474
"Kawtar A., Benlahmar H., Naji M.A., Sanaa E., Banou Z.","57439627100;57439798200;57439971800;57439109100;57439798300;","Extract Concept using Subtitles in MOOC",2022,"International Journal of Advanced Computer Science and Applications","13","1",,"633","638",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123987091&doi=10.14569%2fIJACSA.2022.0130176&partnerID=40&md5=307d7cdc4b40174ddf4ae55ad519a0e8","Massive open online courses (MOOCs) are a variety of courses offered through the online mode, paid orunpaid and has evolved as an excellent learning resource forstudents. The structure of the course design is mainly linearwhere there are a few video lectures provided by eitherprofessors of several universities, or people with expertise in theparticular subject. They are usually graded on a weekly basisthrough quizzes or peer-graded assignments. The objective ofthis paper is to extract the concepts taught in the videos from thesubtitles, which could later be used to enhance recommendationsof the learners using their clickstream data. The teachers couldalso use this to see the demand for their courses. Evaluate twokeyword extraction methods, which are BERT and LDA usingdifferent Coursera courses. The experimental results show thatBERT outperforms LDA in terms of Coherence © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.","Bert; Lda; Overlap coefficient; Topic coherence","Data mining; E-learning; Bert; Course design; Graded assignments; Lda; Learning resource; Massive open online course; Online modes; Overlap coefficients; Topic coherence; Video lectures; Curricula",Article,Scopus,2-s2.0-85123987091
"Du J., Pi Y.","57439774700;57439252700;","Research on Privacy Protection Technology of Mobile Social Network Based on Data Mining under Big Data",2022,"Security and Communication Networks","2022",,"3826126","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123981401&doi=10.1155%2f2022%2f3826126&partnerID=40&md5=c2328e13fcc024c0168ee32676c6a813","With the advent of the era of big data, people's lives have undergone earth-shaking changes, not only getting rid of the cumbersome traditional data collection but also collecting and sorting information directly from people's footprints on social networks. This paper explores and analyzes the privacy issues in current social networks and puts forward the protection strategies of users' privacy data based on data mining algorithms so as to truly ensure that users' privacy in social networks will not be illegally infringed in the era of big data. The data mining algorithm proposed in this paper can protect the user's identity from being identified and the user's private information from being leaked. Using differential privacy protection methods in social networks can effectively protect users' privacy information in data publishing and data mining. Therefore, it is of great significance to study data publishing, data mining methods based on differential privacy protection, and their application in social networks. © 2022 Jiawen Du and Yong Pi.",,"Big data; Data mining; Social networking (online); Social sciences computing; Data collection; Data mining algorithm; Data publishing; Differential privacies; Mobile social networks; Network-based; Privacy issue; Privacy protection; Protection technologies; User privacy; Data privacy",Article,Scopus,2-s2.0-85123981401
"Dong Y., Liu Q., Du B., Zhang L.","56585094400;57438356400;55020400300;8359720900;","Weighted Feature Fusion of Convolutional Neural Network and Graph Attention Network for Hyperspectral Image Classification",2022,"IEEE Transactions on Image Processing","31",,,"1559","1572",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123968888&doi=10.1109%2fTIP.2022.3144017&partnerID=40&md5=266bb326c134b1ab6430fdd8f619fe3f","Convolutional Neural Networks (CNN) and Graph Neural Networks (GNN), such as Graph Attention Networks (GAT), are two classic neural network models, which are applied to the processing of grid data and graph data respectively. They have achieved outstanding performance in hyperspectral images (HSIs) classification field, which have attracted great interest. However, CNN has been facing the problem of small samples and GNN has to pay a huge computational cost, which restrict the performance of the two models. In this paper, we propose Weighted Feature Fusion of Convolutional Neural Network and Graph Attention Network (WFCG) for HSI classification, by using the characteristics of superpixel-based GAT and pixel-based CNN, which proved to be complementary. We first establish GAT with the help of superpixel-based encoder and decoder modules. Then we combined the attention mechanism to construct CNN. Finally, the features are weighted fusion with the characteristics of two neural network models. Rigorous experiments on three real-world HSI data sets show WFCG can fully explore the high-dimensional feature of HSI, and obtain competitive results compared to other state-of-the art methods. © 1992-2012 IEEE.","attention mechanism; convolutional neural network; graph attention network; Hyperspectral image classification; weighted feature fusion","Classification (of information); Convolution; Data handling; Decoding; Hyperspectral imaging; Image classification; Image fusion; Neural networks; Spectroscopy; Superpixels; Attention mechanisms; Convolutional neural network; Features extraction; Features fusions; Graph attention network; Hyperspectral image classification; Networks and graphs; Weighted feature fusion; Weighted features; Data mining; article; artificial neural network; attention; attention network; convolutional neural network; Neural Networks, Computer",Article,Scopus,2-s2.0-85123968888
"Li S., Zheng C., Li L.","57438095900;57438096000;57305879300;","The Relationship between the Mechanism of Sarcopenia and Exercise Based on Data Mining",2022,"Computational and Mathematical Methods in Medicine","2022",,"9339905","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123965464&doi=10.1155%2f2022%2f9339905&partnerID=40&md5=19b577100de13f64648e189db6160e3c","Due to the increasing prosperity of human life science and technology, many huge research results have been obtained, and the scientific research of molecular biology is developing rapidly. Therefore, the output of biological genome data has increased exponentially, which constitutes a huge amount of data analysis. The seemingly chaotic and massive amount of data information actually contains a large amount of data and information of great key scientific significance and value. Therefore, this kind of genomic data information not only contains the information content that describes the characteristics of human life but also contains the information content that can express the essence of the biological organism. It includes macroeconomic information that can reflect the basic structure and capabilities of living organisms and microinformation in related fields of molecular biology. This massive amount of genetic data is usually closely related to each other, can influence each other, and does not exist alone. In the article, the causes of uncertain data and the classification of uncertain data are introduced, and the basic concepts and related algorithms of data mining are explained. Focusing on the research and analysis of abnormal point detection and clustering algorithms in uncertain data mining technology, this paper solves the problem of how to obtain more diverse and accurate outlier detection and cluster analysis results in uncertain data. The results showed that whether it was related to obesity or not, the Lp(a) level of the sarcopenia group was significantly higher than that of the nonsarcopenia group. At the same time, the correlation analysis showed that ASM/height was negatively correlated with Lp(a). ASM/height is one of the criteria for diagnosing sarcoidosis, and it is also the core of the analysis. Among the 1956 tumor patients collected in this study, 432 had sarcopenia, accounting for 22.08%, and the incidence of sarcopenia in patients with gastrointestinal tumors increased. © 2022 Shengnan Li et al.",,"Classification (of information); Cluster analysis; Clustering algorithms; Data mining; Diagnosis; Molecular biology; Chaotics; Data informations; Human lives; Information contents; Life-sciences; Research results; Sarcopenia; Science and Technology; Scientific researches; Uncertain datas; Tumors",Article,Scopus,2-s2.0-85123965464
"Humber M., Zubkova M., Giglio L.","6506319257;57203588903;57205779812;","A remote sensing-based approach to estimating the fire spread rate parameter for individual burn patch extraction",2022,"International Journal of Remote Sensing","43","2",,"649","673",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123917705&doi=10.1080%2f01431161.2022.2027544&partnerID=40&md5=d468478fe3e02d0e15e765aceee4898a","For the past two decades, satellite-derived activef fire data have been used in a multitude of operational applications and in a large and growing body of research on the role of fire within the Earth system. More recent work with satellite-based active fire data has been directed toward estimating what are in effect broad-scale fire spread rates that are in turn used as an important temporal parameter for the extraction of individual-fire boundaries from burned area maps. Here we use data mining to identify active fire clusters that serve as an input to a fire spread reconstruction algorithm to derive optimal global fire spread rates suitable for fire-perimeter extraction. The spread rates calculated for the active fire clusters, which are useful for applications beyond perimeter extraction, correlate with the spread rates based on reference fire boundaries (R 2 =.82, NRMSE = 2.6%) and are generally compatible with other studies, despite key differences in data acquisition methods and quantities measured. © 2022 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","active fire; MODIS; remote sensing; spread; wildfire","Data acquisition; Data mining; Extraction; Fires; Parameter estimation; Active fires; Area maps; Burned areas; Earth systems; Fire spread; Growing bodies; Operational applications; Rate parameters; Remote-sensing; Spread; Remote sensing",Article,Scopus,2-s2.0-85123917705
"Guo L., Yin H., Chen T., Zhang X., Zheng K.","56764901200;55007318200;57196727070;9238032200;35250023000;","Hierarchical Hyperedge Embedding-Based Representation Learning for Group Recommendation",2022,"ACM Transactions on Information Systems","40","1","3","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123909143&doi=10.1145%2f3457949&partnerID=40&md5=7b1b63c28527969a3f522775ad5c596a","Group recommendation aims to recommend items to a group of users. In this work, we study group recommendation in a particular scenario, namely occasional group recommendation, where groups are formed ad hoc and users may just constitute a group for the first time-that is, the historical group-item interaction records are highly limited. Most state-of-The-Art works have addressed the challenge by aggregating group members' personal preferences to learn the group representation. However, the representation learning for a group is most complex beyond the aggregation or fusion of group member representation, as the personal preferences and group preferences may be in different spaces and even orthogonal. In addition, the learned user representation is not accurate due to the sparsity of users' interaction data. Moreover, the group similarity in terms of common group members has been overlooked, which, however, has the great potential to improve the group representation learning. In this work, we focus on addressing the aforementioned challenges in the group representation learning task, and devise a hierarchical hyperedge embedding-based group recommender, namely HyperGroup. Specifically, we propose to leverage the user-user interactions to alleviate the sparsity issue of user-item interactions, and design a graph neural network-based representation learning network to enhance the learning of individuals' preferences from their friends' preferences, which provides a solid foundation for learning groups' preferences. To exploit the group similarity (i.e., overlapping relationships among groups) to learn a more accurate group representation from highly limited group-item interactions, we connect all groups as a network of overlapping sets (a.k.a. hypergraph), and treat the task of group preference learning as embedding hyperedges (i.e., user sets/groups) in a hypergraph, where an inductive hyperedge embedding method is proposed. To further enhance the group-level preference modeling, we develop a joint training strategy to learn both user-item and group-item interactions in the same process. We conduct extensive experiments on two real-world datasets, and the experimental results demonstrate the superiority of our proposed HyperGroup in comparison to the state-of-The-Art baselines. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Group recommendation; hyperedge embedding; representation learning","Data mining; Graph neural networks; User profile; Embeddings; Group members; Group recommendations; Group representation; Hyperedge embedding; Hyperedges; Learn+; Personal preferences; Representation learning; State of the art; Embeddings",Article,Scopus,2-s2.0-85123909143
"Zhang Z., Xu J., Xu P., Liu W., He X., Fu K.","57436785300;57436837300;57436735100;57436731300;57436834700;57436705100;","Quetiapine Combined with Sodium Valproate in Patients with Alzheimer's Disease with Mental and Behavioral Symptoms Efficacy Observation",2022,"Journal of Healthcare Engineering","2022",,"1278092","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123905552&doi=10.1155%2f2022%2f1278092&partnerID=40&md5=366215708f1b5451d367451e0d9ac536","Quetiapine combined with sodium valproate is an effective and more suitable drug treatment for Alzheimer's disease. At present, there are relatively few studies on the combined action mechanism of these two drugs. This study has certain practical value. Alzheimer's disease is a multifaceted, highly genetically heterogeneous neurodegenerative disease. The main clinical manifestations are memory loss, abnormal mental behavior, and loss of various cognitive functions. In order to improve the symptoms of patients with Alzheimer's disease, especially those with mental symptoms, this article combines quetiapine and sodium valproate, two commonly used drugs for the treatment of mental illnesses, and applies them to different levels of Alzheimer's and observes the results of the combination's curative effect. This article introduces Alzheimer's disease and its potential mental behaviors in the method section, and it also introduces the mechanism of action of quetiapine and sodium valproate. For the algorithm, this paper introduces a data mining algorithm to understand the effect of drug efficacy. In the experimental part, firstly, it introduces the experimental objects, the proportion of medicines, and the statistical methods. Secondly, this article covers adverse reactions, inflammatory factors and vascular endothelial indicators, Alzheimer's disease performance, MOAS score, treatment effect evaluation, and satisfaction surveys. It can be seen from the experiment that, in mental behavior, the experimental group decreased from 8.2 before treatment to 0.5, and the control group decreased from 7.1 before treatment to 2.6. It can be seen that the scores of the experimental group changed after receiving the treatment of quetiapine combined with sodium valproate. © 2022 Zhihua Zhang et al.",,"Data mining; Patient treatment; Sodium; Action mechanisms; Alzheimers disease; Clinical manifestation; Cognitive functions; Combined actions; Curative effects; Experimental groups; Memory loss; Mental illness; Sodium valproate; Neurodegenerative diseases; quetiapine; valproic acid; aged; Alzheimer disease; Article; behavior disorder; constipation; controlled study; data mining; drug efficacy; drug mechanism; female; human; hypotension; k nearest neighbor; lethargy; logistic regression analysis; major clinical study; male; mental disease; support vector machine; tachycardia; xerostomia",Article,Scopus,2-s2.0-85123905552
"Gonzalez G., Vaculik K., Khalil C., Zektser Y., Arnold C., Almario C.V., Spiegel B., Anger J.","57218372421;57218367379;55651936800;57218368405;18436092400;23134677600;7003350770;8239716000;","Using Large-scale Social Media Analytics to Understand Patient Perspectives about Urinary Tract Infections: Thematic Analysis",2022,"Journal of Medical Internet Research","24","1","e26781","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123879155&doi=10.2196%2f26781&partnerID=40&md5=682d11633eca3dfc1331fa493bd0ba9f","Background: Current qualitative literature about the experiences of women dealing with urinary tract infections (UTIs) is limited to patients recruited from tertiary centers and medical clinics. However, traditional focus groups and interviews may limit what patients share. Using digital ethnography, we analyzed free-range conversations of an online community. Objective: This study aimed to investigate and characterize the patient perspectives of women dealing with UTIs using digital ethnography. Methods: A data-mining service was used to identify online posts. A thematic analysis was conducted on a subset of the identified posts. Additionally, a latent Dirichlet allocation (LDA) probabilistic topic modeling method was applied to review the entire data set using a semiautomatic approach. Each identified topic was generated as a discrete distribution over the words in the collection, which can be thought of as a word cloud. We also performed a thematic analysis of the word cloud topic model results. Results: A total of 83,589 posts by 53,460 users from 859 websites were identified. Our hand-coding inductive analysis yielded the following 7 themes: quality-of-life impact, knowledge acquisition, support of the online community, health care utilization, risk factors and prevention, antibiotic treatment, and alternative therapies. Using the LDA topic model method, 105 themes were identified and consolidated into 9 categories. Of the LDA-derived themes, 25.7% (27/105) were related to online community support, and 22% (23/105) focused on UTI risk factors and prevention strategies. Conclusions: Our large-scale social media analysis supports the importance and reproducibility of using online data to comprehend women’s UTI experience. This inductive thematic analysis highlights patient behavior, self-empowerment, and online media utilization by women to address their health concerns in a safe, anonymous way. ©Gabriela Gonzalez, Kristina Vaculik, Carine Khalil, Yuliya Zektser, Corey Arnold, Christopher V Almario, Brennan Spiegel, Jennifer Anger.","Data mining; Digital ethnography; Female urology; Health services research; Latent dirichlet allocation; Online community; Online forum; Social media; Urinary tract infections","antibiotic agent; alternative medicine; antibiotic prophylaxis; Article; community care; community support; data mining; ethnography; health care utilization; human; infection risk; large scale production; learning; patient attitude; pelvic organ prolapse; pyelonephritis; qualitative analysis; quality of life; quantitative analysis; risk factor; sexual health; social media analysis; thematic analysis; urinary tract infection; vagina atrophy; female; patient attitude; reproducibility; social media; urinary tract infection; Community Support; Female; Humans; Patient Acceptance of Health Care; Reproducibility of Results; Social Media; Urinary Tract Infections",Article,Scopus,2-s2.0-85123879155
"Ohyama K., Tanaka H., Shindo J., Shibayama M., Iwata M., Hori Y.","57190029650;57436025500;57435976600;57435976700;57435932500;57213156641;","Association of gynecomastia with antidiabetic medications in older adults: Data mining from different national pharmacovigilance databases",2022,"International journal of clinical pharmacology and therapeutics","60","1",,"24","31",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123878404&doi=10.5414%2fCP204066&partnerID=40&md5=41519668c54b0270fb9f8ee26cbbb8e4","OBJECTIVE: Gynecomastia is a benign proliferation of the glandular breast tissue in men and is generally caused by a decrease in androgen and an increase in estrogen. Diabetes has been reported to be a risk factor for lowering androgen levels. Moreover, lowered androgen levels are more common in older men. In the present study, we aimed to evaluate the signals for gynecomastia in older men on antidiabetic medications. MATERIALS AND METHODS: A disproportionality analysis was performed to detect the signals for antidiabetic drug-associated gynecomastia in the U.S. Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS) and the Japanese Adverse Drug Event Report database (JADER), using the reporting odds ratio (ROR) and information component (IC). RESULTS: Among 8 classes of medications for diabetes, a signal was detected only for dipeptidyl peptidase-4 (DPP-4) inhibitors (ROR: 1.90, 95% confidence interval (CI): 1.27 - 2.83; IC: 0.84, 95% CI: 0.26 - 1.42) in the FAERS. Regarding individual drugs, ROR and IC signals were detected for sitagliptin (ROR: 2.37, 95% CI: 1.48 - 3.79; IC: 1.12, 95% CI: 0.44 - 1.79) and vildagliptin (ROR: 3.34, 95% CI: 1.39 - 8.08; IC: 1.26, 95% CI: 0.07 - 2.44) in the FAERS and only for sitagliptin (ROR: 4.84, 95% CI: 1.92 - 12.2; IC: 1.48, 95% CI: 0.24 - 2.73) in the JADER. CONCLUSION: This study showed an association between DPP-4 inhibitor use and gynecomastia in older men with diabetes. Further pharmacoepidemiological studies are warranted to verify this finding.",,"antidiabetic agent; aged; data mining; drug surveillance program; factual database; Food and Drug Administration; gynecomastia; human; male; United States; Adverse Drug Reaction Reporting Systems; Aged; Data Mining; Databases, Factual; Gynecomastia; Humans; Hypoglycemic Agents; Male; Pharmacovigilance; United States; United States Food and Drug Administration",Article,Scopus,2-s2.0-85123878404
"Anunciação L., Portugal A.C., Landeira-Fernandez J., Bajcar E.A., Bąbel P.","57194218787;57221224912;6701463902;56532048800;55461131000;","The Lighter Side of Pain: Do Positive Affective States Predict Memory of Pain Induced by Running a Marathon?",2022,"Journal of Pain Research","15",,,"105","113",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123837143&doi=10.2147%2fJPR.S319847&partnerID=40&md5=229336f1f81517a2248db41740da0fba","Background: Memory and in turn, memory of pain is a reconstructive process. This study considers the relationship between time, memory, affective states, and pain induced by running a marathon by investigating the influence of these factors on a participant’s memory of pain experienced after a marathon. The following two hypotheses were formulated: 1) participants’ recalled-pain of marathon experience is underestimated; and 2) the underestimation of recalled pain would be greater for participants experiencing higher positive affect. Methods: A longitudinal design was employed to check pain intensities of marathon participants a) at the finish line and b) 6 months following its completion. The sample size was based on a power analysis, and 108 marathonists rated their pain intensities and positive and negative affects at the finish line. From this sample, 58 participants recalled their pain experience of running the marathon 6 months later. Linear models, including computer-based data-mining algorithms, were used. Results: The experienced pain was higher than their recalled pain (t(55) = 3.412, p < 0.01, d = 0.45), supporting the first hypothesis. The memory of pain faded similarly in all participants, which did not directly support the second hypothesis. Further exploratory analysis suggested that negative and positive affective states were related to participants’ pain memory; positive affective states appeared to be inversely related to the recall (β = −0.289, p = 0.039). Discussion: This study shows that time has a significant effect on memory recall and that emotions may also influence the memory of pain. This is the first study that preliminarily showcased the effect of positive affective states on the memory of pain induced by physical exercise. © 2022 Anunciação et al.","Marathon; Negative affect; Pain; Pain memory; Positive affect","adult; affect; algorithm; Article; computer analysis; controlled study; data mining; exercise; factor analysis; female; human; linear system; longitudinal study; major clinical study; male; marathon runner; marathon running; negative affect; pain; pain intensity; personal experience; positive affect; power analysis; prediction; recall; sample size; statistical model; time factor",Article,Scopus,2-s2.0-85123837143
"Liu K., El-Gohary N.","56927149000;14519146500;","Improved similarity assessment and spectral clustering for unsupervised linking of data extracted from bridge inspection reports",2022,"Advanced Engineering Informatics","51",,"101496","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123825365&doi=10.1016%2fj.aei.2021.101496&partnerID=40&md5=b28a08e01e12ec2b64e6628cb9b92e4c","Textual bridge inspection reports are important data sources for supporting data-driven bridge deterioration prediction and maintenance decision making. Information extraction methods are available to extract data/information from these reports to support data-driven analytics. However, directly using the extracted data/information in data analytics is still challenging because, even within the same report, there exist multiple data records that describe the same entity, which increases the dimensionality of the data and adversely affects the performance of the analytics. The first step to address this problem is to link the multiple records that describe the same entity and same type of instances (e.g., all cracks on a specific bridge deck), so that they can be subsequently fused into a single unified representation for dimensionality reduction without information loss. To address this need, this paper proposes a spectral clustering-based method for unsupervised data linking. The method includes: (1) a concept similarity assessment method, which allows for assessing concept similarity even when corpus or semantic information is not available for the application at hand; (2) a record similarity assessment method, which captures and uses similarity assessment dependencies to reduce the number of falsely-linked records; and (3) an improved spectral clustering method, which uses iterative bi-partitioning to better link records in an unsupervised way and to address the transitive closure problem. The proposed data linking method was evaluated in linking records extracted from ten bridge inspection reports. It achieved an average precision, recall, and F-1 measure of 96.2%, 88.3%, and 92.1%, respectively. © 2021","Bridges; Data linking/linkage; Deterioration prediction; Maintenance decision making; Similarity assessment; Spectral clustering; Unsupervised machine learning","Bridges; Data mining; Decision making; Deterioration; Inspection; Iterative methods; K-means clustering; Machine learning; Semantics; Bridge inspection; Data driven; Data informations; Data linking/linkage; Datalinking; Deterioration prediction; Maintenance decision making; Similarity assessment; Spectral clustering; Unsupervised machine learning; Data Analytics",Article,Scopus,2-s2.0-85123825365
"Dai J., Xu J.","36722540800;57433150000;","Knowledge Graph Construction for Intelligent Media Based on Mobile Internet",2022,"Wireless Communications and Mobile Computing","2022",,"4867220","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123794151&doi=10.1155%2f2022%2f4867220&partnerID=40&md5=d12923151245e8fb95d837335509e513","Mobile Internet-based intelligent media has become a popular academic topic. This study uses the CiteSpace visualisation tool and Scientific Citation Index Expanded database to comb the existing research in the field of intelligent media from a quantitative perspective. A total of 7248 English papers were published on the topic of ""intelligent media""from 2012 to 2021, and 145 highly cited papers refined were analysed. Scientific knowledge graphs were analysed from six dimensions: annual publication quantity, country of publication, institution of publication, author, keywords, and cited references. In the last 10 years, the research literature on intelligent media has been found to increase annually. Presently, the People's Republic of China and the United States of America have a high proportion of documents in this field. Chinese universities and institutions have achieved significantly in terms of the quantity and quality of documents. From the perspective of the whole intelligent media discipline, the high-yield author group has not been formed, and there is minimal cooperation amongst authors. Popular intelligent media topics include film, social media, machine learning, swarming motility, data mining, and artificial intelligence. Subject words of the main research directions are event recognition, fake news, Cable News Network model, reconfigurable intelligent surface, comprehensive survey, microblog message, strain sensor, and traffic event. Combined with popular topics and time zone maps, the future research frontier in the field of smart media is identified. © 2022 Jianhua Dai and Jingxin Xu.",,"Data mining; Fake detection; Publishing; Citation index; Citespace; Graph construction; Highly-cited papers; Internet based; Knowledge graphs; Mobile Internet; Peoples Republic of China; Scientific knowledge; Visualization tools; Knowledge graph",Article,Scopus,2-s2.0-85123794151
"Chen Z., Qian T.","57211393145;57424177500;","Retrieve-and-Edit Domain Adaptation for End2End Aspect Based Sentiment Analysis",2022,"IEEE/ACM Transactions on Audio Speech and Language Processing","30",,,"659","672",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123789361&doi=10.1109%2fTASLP.2022.3146052&partnerID=40&md5=70478edf764a13a975e75ab39ddd6efb","End-to-end aspect based sentiment analysis (E2E-ABSA) aims to jointly extract aspect terms and predict aspect-level sentiment for opinion reviews. Though supervised methods show effectiveness for E2E-ABSA tasks, the annotation cost is extremely high due to the necessity of fine-grained labels. Recent attempts alleviate this problem using the domain adaptation technique to transfer the word-level common knowledge across domains. However, the biggest issue in domain adaptation, i.e., how to transfer the domain-specific words like pizza and delicious in the source 'Restaurant' to the target 'Laptop' domain, has not been resolved. In this paper, we propose a novel domain adaptation method to address this issue by enhancing the transferability of domain-specific source words in a retrieve-and-edit way. Specifically, for all source words, we first retrieve the transferable prototypes from unlabeled target data via their syntactic and semantic roles. We then edit the source words to enhance their transferability by absorbing the knowledge carried in prototypes. Finally, we design an end-to-end framework to jointly accomplish cross-domain aspect term extraction and aspect-level sentiment classification. We conduct extensive experiments on four real-world datasets. The results prove that, by introducing transferable prototypes, our method significantly outperforms the state-of-the-art methods, achieving an absolute 3.95% F1 increase over the best baseline. © 2014 IEEE.","domain adaptation; End-to-end aspect based sentiment analysis; retrieve-and-edit; transferable prototypes","Computational linguistics; Data mining; Job analysis; Semantics; Syntactics; Annotation; Domain adaptation; End to end; End-to-end aspect based sentiment analyse; Prototype; Retrieve-and-edit; Sentiment analysis; Task analysis; Transferable prototype; Sentiment analysis",Article,Scopus,2-s2.0-85123789361
"Wang X., Fan Y.","57433302400;56982162300;","Multiscale Densely Connected Attention Network for Hyperspectral Image Classification",2022,"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15",,,"1617","1628",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123780979&doi=10.1109%2fJSTARS.2022.3145917&partnerID=40&md5=9de1d518305c1c145d7976867b0f33d9","Hyperspectral image classification (HSIC) based on deep learning has always been a research hot spot in the field of remote sensing. However, most of the classification models extract relevant features based on fixed-scales convolution kernels, which ignores the complex features of hyperspectral images (HSIs) at different scales and impairs the classification accuracy. To solve this problem, a multiscale densely connected attention network (MSDAN) is proposed for HSIC. First, the model adopts three different scales modules with dense connection to enhance classification performance, strengthen feature reuse, prevent overfitting and gradient disappearance. Besides, in order to reduce the model parameters and strengthen the extraction of spatial-spectral features, the traditional three-dimensional convolution is replaced by three-dimensional spectral convolution block and three-dimensional spatial convolution block. Furthermore, the spectral-spatial-channel attention is embedded into the end of each scale to enhance the favorable features for classification and further extract the discriminant features of the corresponding scale. Finally, the key feature extraction module is developed to extract multiscale fusion features to further enhance the classification performance of the network. The experimental results carried out on real HSIs show that the proposed MSDAN architecture has significant advantages compared with other most advanced methods. © 2008-2012 IEEE.","3-D spectral convolution; hyperspectral image classification (HSIC); multiscale dense connection; spatial-spectral features; spectral-spatial-channel attention; three-dimensional (3-D) spatial convolution","Classification (of information); Convolution; Deep learning; Extraction; Feature extraction; Image classification; Neural networks; Remote sensing; Spectroscopy; Three dimensional displays; Convolutional neural network; Features extraction; Hyperspectral image classification; License; Multiscale dense connection; Spatial channels; Spatial convolution; Spatial-spectral feature; Spectral convolution; Spectral feature; Spectral-spatial-channel attention; Three-dimensional display; Three-dimensional spatial convolution; Three-dimensional spectral convolution; Data mining; experimental study; image classification; learning; remote sensing; spectral analysis; three-dimensional modeling",Article,Scopus,2-s2.0-85123780979
"Li W.","57432723900;","Big Data Precision Marketing Approach under IoT Cloud Platform Information Mining",2022,"Computational Intelligence and Neuroscience","2022",,"4828108","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123780233&doi=10.1155%2f2022%2f4828108&partnerID=40&md5=c05f80c45eaa306ecc509fdbd62a3fc1","In this article, an in-depth study and analysis of the precision marketing approach are carried out by building an IoT cloud platform and then using the technology of big data information mining. The cloud platform uses the MySQL database combined with the MongoDB database to store the cloud platform data to ensure the correct storage of data as well as to improve the access speed of data. The storage method of IoT temporal data is optimized, and the way of storing data in time slots is used to improve the efficiency of reading large amounts of data. For the scalability of the IoT data storage system, a MongoDB database clustering scheme is designed to ensure the scalability of data storage and disaster recovery capability. The relevant theories of big data marketing are reviewed and analyzed; secondly, based on the relevant theories, combined with the author's work experience and relevant information, a comprehensive analysis and research on the current situation of big data marketing are conducted, focusing on its macro-, micro-, and industry environment. The service model combines the types of user needs, encapsulates the resources obtained by the alliance through data mining for service products, and publishes and delivers them in the form of data products. From the perspective of the development of the telecommunications industry, in terms of technology, the telecommunications industry has seen the development trend of mobile replacing fixed networks and triple play. The development of emerging technologies represented by the Internet of Things and cloud computing has also led to technological changes in the telecommunications industry. Operators are facing new development opportunities and challenges. It also divides the service mode into self-service and consulting service mode according to the different degrees of users' cognition and understanding of the service, as well as proposes standardized data mining service guarantee from two aspects: after-sales service and operation supervision. A customized data mining service is a kind of data mining service for users' personalized needs. And the intelligent data mining service guarantee is proposed from two aspects of multicase experience integration and group intelligence. In the empirical research part, the big data alliance in Big Data Industry Alliance, which provides data mining service as the main business, is selected as the research object, and the data mining service model of the big data alliance proposed in this article is applied to the actual alliance to verify the scientific and rationality of the data mining service model and improve the data mining service model management system. © 2022 Wang Li.",,"Big data; Commerce; Computation theory; Data mining; Database systems; Digital storage; Information management; Information services; Marketing; Scalability; Telecommunication industry; Cloud platforms; Data marketing; Data-mining services; Information mining; Marketing IS; MongoDB; Precision marketings; Service mode; Service modeling; Telecommunications industry; Internet of things; cloud computing; data mining; marketing; technology; Big Data; Cloud Computing; Data Mining; Marketing; Technology",Article,Scopus,2-s2.0-85123780233
"Benkő Z., Stippinger M., Rehus R., Bencze A., Fabó D., Hajnal B., Eröss L.G., Telcs A., Somogyvári Z.","24073202700;35100869300;57219786723;8640809800;14026610800;56226308800;6506096088;6602563857;6603601487;","Manifold-adaptive dimension estimation revisited",2022,"PeerJ Computer Science","8",,"e790","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123747508&doi=10.7717%2fPEERJ-CS.790&partnerID=40&md5=fb1fe36f41c9308df6f284befda216ab","Data dimensionality informs us about data complexity and sets limit on the structure of successful signal processing pipelines. In this work we revisit and improve the manifold adaptive Farahmand-Szepesvari-Audibert (FSA) dimension estimator, making it one of the best nearest neighbor-based dimension estimators available. We compute the probability density function of local FSA estimates, if the local manifold density is uniform. Based on the probability density function, we propose to use the median of local estimates as a basic global measure of intrinsic dimensionality, and we demonstrate the advantages of this a symptotically unbiased estimator over the previously proposed statistics: the mode and the mean. Additionally, from the probability density function, we derive the maximum likelihood formula for global intrinsic dimensionality, if i.i.d. holds. We tackle edge and finite-sample effects with an exponential correction formula, calibrated on hypercube datasets. We compare the performance of the corrected median-FSA estimator with kNN estimators: maximum likelihood (Levina-Bickel), the 2NN and two implementations of DANCo (R and MATLAB). We show that corrected median-FSA estimator beats the maximum likelihood estimator and it is on equal footing with DANCo for standard synthetic benchmarks according to mean percentage error and error rate metrics. With the median-FSA algorithm, we reveal diverse changes in the neural dynamics while resting state and during epileptic seizures. We identify brain areas with lower-dimensional dynamics that are possible causal sources and candidates for being seizure onset zones. © 2022 Benko et al. All Rights Reserved.","Brain-Computer Interface; Causality; DANCo; Data Mining and Machine Learning; Data Science; Dynamical systems; EEG; Epilepsy; Fractal dimension; Intrinsic dimension; Manifold; Maximum likelihood; Takens theorem","Brain computer interface; Dynamical systems; Fractal dimension; Machine learning; Maximum likelihood estimation; Probability density function; Sampling; Signal processing; Statistical tests; Causality; DANCo; Data mining and machine learning; Intrinsic dimensionalities; Intrinsic dimensions; Ma ximum likelihoods; Machine-learning; Manifold; Maximum-likelihood; Taken theorem; Data mining",Article,Scopus,2-s2.0-85123747508
"Shirvani Moghaddam S., Moghaddam K.S.","57449887700;57221564213;","A General Framework for Sorting Large Data Sets Using Independent Subarrays of Approximately Equal Length",2022,"IEEE Access","10",,,"11584","11607",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123731144&doi=10.1109%2fACCESS.2022.3145981&partnerID=40&md5=0fbc8a8814506c9137e1eef3fc973b5f","Designing an efficient data sorting algorithm that requires less time and space complexity is essential for computer science, different engineering disciplines, data mining systems, wireless networks, and the Internet of things. This paper proposes a general low-complex data sorting framework that distinguishes the sorted or similar data, makes independent subarrays approximately in equal length, and sorts the subarrays' data using one of the popular comparison-based sorting algorithms. Two frameworks, one for serial realization and another for parallel realization, are proposed. The time complexity analyses of the proposed framework demonstrate an improvement compared to the conventional Merge and Quick sorting algorithms. Following complexity analysis, the simulation results indicate slight improvements in the elapsed time and the number of swaps of the proposed serial Merge-based and Quick-based frameworks compared to the conventional ones for low/high variance integer/non-integer data sets, in different data sizes and the number of divisions. It is about (1-1.6%) to (3.5-4%) and (0.3-1.8%) to (2-4%) improvements in the elapsed times for 1, 2, 3, and 4 divisions, respectively for small and very large data sets in Merge-based and Quick-based scenarios. Although these improvements in serial realization are minor, making independent low-variance subarrays allows the sorted components to be extracted sequentially and gradually before the end of the sorting process. Also, it proposes a general framework for parallelizing conventional sorting algorithms using non-connected (independent) or connected (dependent) multi-core structures. As the second experiment, the numerical analyses that compare the results of the parallel realization of the proposed framework to the serial one in 1, 2, 3, and 4 divisions, show a speedup factor of (2-4) for small to (2-16) for very large data sets. The third experiment shows the effectiveness of the proposed parallel framework to the parallel sorting based on the random-access machine model. Finally, we prove that the mean-based pivot is as efficient as the median-based and much better than the random pivot for making subarrays of approximately equal length. © 2013 IEEE.","Data sorting; Gaussian; integer; merge; multi-core; non-integer; parallel realization; quick; Rayleigh; time and space complexity; uniform","Complex networks; Computational complexity; Data mining; Merging; Sorting; Array; Complexity theory; Data sorting; Gaussians; Integer; License; Merge; Multi-cores; Non-integer; Parallel realizations; Quick; Rayleigh; Time and space complexity; Time complexity; Uniform; Internet of things",Article,Scopus,2-s2.0-85123731144
"Li H., Huang H., Ye Z., Li H.","57431539600;57430736800;56780856900;57430736900;","Hyperspectral Image Classification Using Adaptive Weighted Quaternion Zernike Moments",2022,"IEEE Transactions on Signal Processing","70",,,"701","713",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123726179&doi=10.1109%2fTSP.2022.3144954&partnerID=40&md5=45983ca4e2ba6aaba559a2cdf4ae21f4","Hyperspectral image classification (HSI) has been widely used in many fields. However, image noise, atmospheric conditions, material distribution and other factors seriously degrade the classification accuracy of HSIs. To alleviate these issues, a new approach, namely adaptive weighted quaternion Zernike moments (AWQZM), is proposed, which extracts effective spatial-spectral features for pixels in HSI classification. The main contributions and novelties of the method are as follows: 1) the AWQZM can adaptively set weights for each pixel in the neighborhood, which not only can flexibly search for homogeneous regions of HSIs, but also can strengthen the similarity of pixels from the same class and the distinctiveness of pixels from different classes; 2) the AWQZM can be constructed in a small subset of bands through a grouping strategy, thereby reducing the computational complexity; and 3) the introduction of quaternions can preserve the spatial correlation among bands and reduce the loss of data information, and the use of quaternion phase information makes the extracted features more informative and discriminative. Moreover, the spectral features and spatial features are combined to achieve better HSI classification results. Experimental results on three benchmark data sets demonstrate that the proposed approach achieves better classification performance than other related approaches. © 1991-2012 IEEE.","Hyperspectral image classification; phase information; quaternions; Zernike moments","Benchmarking; Classification (of information); Data mining; Extraction; Feature extraction; Hyperspectral imaging; Image classification; Spectroscopy; Features extraction; Hyperspectral image classification; Image noise; Phase information; Quaternion; Robustness; Spectral feature; Zernike; Zernike moment; Pixels",Article,Scopus,2-s2.0-85123726179
"Senbel S., Sharma S., Raval M.S., Taber C., Nolan J., Artan N.S., Ezzeddine D., Kaya T.","6603171076;57460608900;6701357003;57406197400;57406131600;57406131700;57406131500;21834230200;","Impact of Sleep and Training on Game Performance and Injury in Division-1 Women's Basketball Amidst the Pandemic",2022,"IEEE Access","10",,,"15516","15527",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123724659&doi=10.1109%2fACCESS.2022.3145368&partnerID=40&md5=01ea98dd62f77fe8722a9b82988b0758","We investigated the impact of sleep and training load of Division-1 women's basketball players on their game performance and injury prediction using machine learning algorithms. The data was collected during a pandemic-condensed season with unpredictable interruptions to the games and athletic training schedules. We collected data from sleep monitoring devices, training data from coaches, injury reports from medical staff, and weekly survey data from athletes for 22 weeks. With proper data imputation, interpretable feature set, data balancing, and classifiers, we showed that we could predict game performance and injuries with more than 90% accuracy. More importantly, our F1 and F2 scores of 0.94 and 0.83 for game performance and injuries, respectively, show that we can use the prediction for informative analysis in the future for coaches to make insightful decisions. Our data analysis also showed that collegiate athletes sleep less than the recommended hours (6-7 instead of 8 hours). This coupled with a long hiatus in games and training increases the risk of injury. Varied training and higher heart rate variability (due to better quality sleep) indicated a better performance, while athletes with poor sleep patterns, were more prone to injuries. © 2013 IEEE.","Basketball; collegiate athletes; data mining; game performance; injury prediction; machine learning; sports analytics","Balancing; Classification (of information); Forecasting; Learning algorithms; Machine learning; Personnel training; Sleep research; Sports; Athletic trainings; Collegiate athlete; Division 1; Game performance; Injury prediction; Machine learning algorithms; Performance; Performance prediction; Sport analytic; Training schedules; Data mining",Article,Scopus,2-s2.0-85123724659
"Huang X., Garg S., Nie J., Lim W.Y.B., Qi Y., Zhang Y., Shamim Hossain M.","56780705800;55601765300;57197710559;57216418969;57193278459;55506039300;57219458636;","Toward Efficient Data Trading in AI Enabled Reconfigurable Wireless Sensor Network Using Contract and Game Theories",2022,"IEEE Transactions on Network Science and Engineering","9","1",,"98","108",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123710499&doi=10.1109%2fTNSE.2020.3013064&partnerID=40&md5=c7c9c0c2fbbf089937f04ae51e8c8b17","Reconfigurable Wireless Sensor Network (RWSN) schedules a set of devices with reconfigurable wireless interface to accomplish different data collection plans in a cost-effective way. AI technologies are applied to optimize decision making for high-level network reconfiguration. Besides, AI based data mining tools are exploited by third parities to extract useful information underlying raw data. This leads to the emergence of AI enabled RWSN. We further study a data trading market to provide the data-centric environment for large-scale applications of AI enabled RWSN. A network operator employs the devices to gather environmental data, and sells the collected data to interested third parities as data consumers. After that, two-level optimizations are performed to ensure efficient data trading. In data collection, a contract based incentive mechanism is presented for the network operator to stimulate the devices and simultaneously achieve the contractor's goal subject to feasible constraints. In data selling, a non-cooperative game is formulated among multiple data consumers. They balance the data demand since the final data price is correlated with the total data demand. Nash equilibrium is analyzed and solved under different conditions. Finally, numerical results are provided to demonstrate the effectiveness of our scheme. © 2013 IEEE.","AI enabled reconfigurable wireless sensor network; Contract theory; Data trading; Non-cooperative game","Commerce; Cost effectiveness; Data acquisition; Data mining; Decision making; Wireless sensor networks; AI enabled reconfigurable wireless sensor network; Contract Theory; Cost effective; Data collection; Data trading; Network operator; Network schedules; Noncooperative game; Reconfigurable; Wireless interfaces; Game theory",Article,Scopus,2-s2.0-85123710499
"Khan M.S., Khan A.W., Khan F., Khan M.A., Whangbo T.K.","57430820700;57460538200;57366667600;57215096761;35617849900;","Critical Challenges to Adopt DevOps Culture in Software Organizations: A Systematic Review",2022,"IEEE Access","10",,,"14339","14349",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123707701&doi=10.1109%2fACCESS.2022.3145970&partnerID=40&md5=ae6d663023e6135b338dd14de1c077c6","DevOps is a set of practices and a cultural movement that aims to break down barriers between development and operation teams to improve collaboration and communication. Different organizations have embraced DevOps principles due to the massive potential, such as a much shorter time to production, increased reliability and stability. However, despite the widespread adoption of DevOps and its infrastructure, there is a lack of understanding and literature on the key concepts, practices, tools, and challenges associated with implementing DevOps strategies. The main goal of this research paper is to explore and discuss challenges related to DevOps culture and practices. Moreover, it describes how DevOps works in an organization, provides a detailed explanation of DevOps, and investigates the cultural challenges that organizations face when implementing DevOps. The proposed paper reveals ten critical challenges that need to be addressed in adopting the DevOps culture. The challenges are further analyzed on the basis of the various continents. According to the findings, the following critical challenges are considered during the implementation of a DevOps culture: lack of collaboration and communication, Lack of skill and knowledge, complicated infrastructure, Lack of management, Lack of DevOps approach, and trust confidence problems. © 2013 IEEE.","challenges; culture; DevOps; systematic literature review (SLR)","Software engineering; Challenge; Collaboration; Critical challenges; Cultural difference; Culture; Software; Software organization; Systematic; Systematic literature review; Data mining",Article,Scopus,2-s2.0-85123707701
"Peng Y., Zhang Y., Tu B., Zhou C., Li Q.","36103447000;57431544600;57193831035;57202774861;56611013200;","Multiview Hierarchical Network for Hyperspectral and LiDAR Data Classification",2022,"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15",,,"1454","1469",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123700172&doi=10.1109%2fJSTARS.2022.3144312&partnerID=40&md5=700f15da7f8821b168dd5b6953fe5b90","In recent times, multisource remote sensing technology [e.g., hyperspectral image (HSI) and light detection and ranging (LiDAR) data] has been widely used in urban land-use recognition owing to its high classification effectiveness compared to using only single-source data. In this study, a multiview hierarchical network (MVHN) technique is developed for HSI and LiDAR data classification, which conducts the following execution procedures. First, based on the a preset band step length, the original HSI is sampled and divided into multiple groups with exactly the same number of bands to obtain spectral features. Then, principal components analysis is performed on the raw HSI to extract the first principal components (PCs) that meet the size of the LiDAR image. The Gabor filters are applied to the PCs and LiDAR to capture spatial details (i.e., textural features) of scenes. Specifically, a stacking mechanism is employed to generate fusion features once the above features are available. Next, a three-dimensional ResNet-like deep CNN is designed to extract the spectral-spatial information of the fusion feature. Finally, majority-voting is introduced into the classification results of the network trained using each fusion feature to achieve high-confidence final results. Experiments on three well-known HSI and LiDAR datasets (i.e., Houston, MUUFL, and Trento datasets) demonstrate the effectiveness of the proposed MVHN method compared to state-of-the-art comparable classification methods. © 2008-2012 IEEE.","Classification; Gabor feature; hyperspectral image (HSI); light detection and ranging (LiDAR); multisource remote sensing; residual network","Classification (of information); Data mining; Gabor filters; Hyperspectral imaging; Image analysis; Land use; Optical radar; Remote sensing; Spectroscopy; Three dimensional displays; Features extraction; Gabor feature; Light detection and ranging; Multi-source remote sensing; Multi-Sources; Principal-component analysis; Remote-sensing; Residual network; Stackings; Three-dimensional display; Principal component analysis",Article,Scopus,2-s2.0-85123700172
"Sobreiro P., Martinho D.D.S., Alonso J.G., Berrocal J.","57188867168;55479501800;55892225800;57430995900;","A SLR on Customer Dropout Prediction",2022,"IEEE Access","10",,,"14529","14547",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123677664&doi=10.1109%2fACCESS.2022.3146397&partnerID=40&md5=8b0d235901cc4e877fb41980bd83ac99","Dropout prediction is a problem that is being addressed with machine learning algorithms; thus, appropriate approaches to address the dropout rate are needed. The selection of an algorithm to predict the dropout rate is only one problem to be addressed. Other aspects should also be considered, such as which features should be selected and how to measure accuracy while considering whether the features are appropriate according to the business context in which they are employed. To solve these questions, the goal of this paper is to develop a systematic literature review to evaluate the development of existing studies and to predict the dropout rate in contractual settings using machine learning to identify current trends and research opportunities. The results of this study identify trends in the use of machine learning algorithms in different business areas and in the adoption of machine learning algorithms, including which metrics are being adopted and what features are being applied. Finally, some research opportunities and gaps that could be explored in future research are presented. © 2013 IEEE.","Customers; dropout prediction; machine learning; systematic review","Artificial intelligence; Data mining; Learning algorithms; Learning systems; 'current; Business contexts; Dropout prediction; Machine learning algorithms; Measure-accuracy; Prediction algorithms; Predictive models; Research opportunities; Systematic literature review; Systematic Review; Forecasting",Article,Scopus,2-s2.0-85123677664
"Mehbodniya A., Khan I.R., Chakraborty S., Karthik M., Mehta K., Ali L., Nuagah S.J.","55954097400;57225907876;57215431328;57206133521;57457991600;57225453189;57219633418;","Data Mining in Employee Healthcare Detection Using Intelligence Techniques for Industry Development",2022,"Journal of Healthcare Engineering","2022",,"6462657","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123673524&doi=10.1155%2f2022%2f6462657&partnerID=40&md5=c0b97acd082eaee3da67e4dbd4fa09df","Background. Even in today's environment, when there is a plethora of information accessible, it may be difficult to make appropriate choices for one's well-being. Data mining, machine learning, and computational statistics are among the most popular arenas of training today, and they are all aimed at secondary empowered person in making good decisions that will maximize the outcome of whatever working area they are involved with. Because the degree of rise in the number of patient roles is directly related to the rate of people growth and lifestyle variations, the healthcare sector has a significant need for data processing services. When it comes to cancer, the prognosis is an expression that relates to the possibility of the patient surviving in general, but it may also be used to describe the severity of the sickness as it will present itself in the patient's future timeline. Methodology. The proposed technique consists of three stages: input data acquisition, preprocessing, and classification. Data acquisition consists of input raw data which is followed by preprocessing to eliminate the missed data and the classification is carried out using ensemble classifier to analyze the stages of cancer. This study explored the combined influence of the prominent labels in conjunction with one another utilizing the multilabel classifier approach, which is successful. Finally, an ensemble classifier model has been constructed and experimentally validated to increase the accuracy of the classifier model, which has been previously shown. The entire performance of the recommended and tested models demonstrates a steady development of 2% to 6% over the baseline presentation on the baseline performance. Results. Providing a good contribution to the general health welfare of noncommercial potential workers in the healthcare sector is an opportunity provided by this recommended job outcome. It is anticipated that alternative solutions to these constraints, as well as automation of the whole process flow of all five phases, will be the key focus of the work to be carried out shortly. Predicting health status of employee in industry or information trends is made easier by these data patterns. The proposed classifier achieves the accuracy rate of 93.265%. © 2022 Abolfazl Mehbodniya et al.",,"Classification (of information); Data handling; Data mining; Diseases; Employment; Health care; Personnel training; Classifier models; Computational statistics; Data classification; Data preprocessing; Ensemble-classifier; Healthcare sectors; Industry development; Input datas; Well being; Working areas; Data acquisition; adult; article; automation; cancer prognosis; cancer staging; cancer survival; case report; classifier; clinical article; data mining; employee; female; health care cost; health status; human; intelligence; lifestyle; male; outcome assessment; prognosis; welfare; worker",Article,Scopus,2-s2.0-85123673524
"Shi A., Ma H., Ma Y.","57430570100;57430428300;57430498600;","Judgment and Prevention of Urinary Tract Injury in Gynecological Surgery Based on Data Mining",2022,"Journal of Healthcare Engineering","2022",,"1270580","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123656726&doi=10.1155%2f2022%2f1270580&partnerID=40&md5=632421715f6de00291daa609f1213047","In this paper, a data mining-enabled model is developed to analyze the case-related data of 39 patients with urinary tract injury who underwent laparoscopic surgery in a certain hospital from 2012 to 2017. Statistics on the history and characteristics of the case data summarized and analyzed the causes of urinary tract injury and the urinary system. The relationship between the occurrence of injury and the type of surgery and the treatment and preventive measures taken for urinary tract injury during and after surgery are summarized. The statistical method with SPSS16.0 statistical software was used to analyze the data of this study, and the X2 test was used to compare the rates. The differences of P≤0.05 and P≤0.01 were statistically significant. Laparoscopic surgery in gynecology is a minimally invasive technique, but it is still accompanied by the possibility of complications. During the experimental setup and implementation, we have observed that among 8742 cases of laparoscopic surgery complicated by urinary tract injury, there were 39 cases with a rate of 0.45%. In the past five years, the incidence of urinary tract injury in gynecological surgery in our country has increased year by year, and the number of cases of urinary tract injury has also increased year by year. Through analysis, it is found that the cause of the injury is related to the level of surgery, pelvic adhesion, and energy equipment. Based on the above problems, according to the clinical data of patients with urinary tract injury complicated by gynecological surgery in the hospital, the relevant factors of gynecological surgery complicated by urinary tract injury are analyzed to improve the awareness of urinary tract protection and prevention of injury during the operation and preventive measures are actively taken to avoid medical treatment. © 2022 Aimei Shi et al.",,"Data mining; Laparoscopy; Software testing; Clinical data; Energy equipments; Laparoscopic surgery; Minimally-invasive technique; Pelvic adhesions; Preventive measures; Statistical software; Treatment measures; Urinary system; Urinary tract; Hospitals; creatinine; abdominal discomfort; accident prevention; anatomical location; Article; backache; bladder injury; bladder surgery; cancer surgery; clinical article; conversion to open surgery; creatinine urine level; cystectomy; cystoscopy; data analysis software; data mining; delayed diagnosis; disease model; disease severity; ectopic pregnancy; endometriosis; endometrium cancer; female; gynecologic surgery; human; incidence; laparoscopic surgery; laparoscopy; laparotomy; micturition disorder; ovary cancer; ovary cyst; pelvis injury; postoperative complication; pregnancy termination; surgical approach; time factor; tissue adhesion; ureter dilatation; ureter injury; urinary tract injury; urogenital tract rupture; uterine cervix cancer; uterine tube disease; uterus surgery; vagina discharge (disease)",Article,Scopus,2-s2.0-85123656726
"Sun Z., Yu Y.","57192667988;55968487800;","Robust multi-class feature selection via l2;0-norm regularization minimization",2022,"Intelligent Data Analysis","26","1",,"57","73",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123643663&doi=10.3233%2fIDA-205724&partnerID=40&md5=915cd2a730afdc7d742a5fe1e7a1695a","Feature selection is an important data preprocessing in data mining and machine learning, that can reduce the number of features without deteriorating model's performance. Recently, sparse regression has received considerable attention in feature selection task due to its good performance. However, because the l2,0-norm regularization term is non-convex, this problem is hard to solve, and most of the existing methods relaxed it by l2,1-norm. Unlike the existing methods, this paper proposes a novel method to solve the l2,0-norm regularized least squares problem directly based on iterative hard thresholding, which can produce exact row-sparsity solution for weights matrix, and features can be selected more precisely. Furthermore, two homotopy strategies are derived to reduce the computational time of the optimization method, which are more practical for real-world applications. The proposed method is verified on eight biological datasets, experimental results show that our method can achieve higher classification accuracy with fewer number of selected features than the approximate convex counterparts and other state-of-the-art feature selection methods. © 2022 - IOS Press. All rights reserved.","0-norm regularization; embedded method; Feature selection; iterative hard thresholding; l2","Classification (of information); Data mining; Iterative methods; Least squares approximations; 0-norm regularization; Data preprocessing; Embedded method; Features selection; Iterative hard thresholding; L2; Minimisation; Modeling performance; Regularisation; Sparse regression; Feature extraction",Article,Scopus,2-s2.0-85123643663
"Cheng L., Ji X., Zhao H., Li J., Xu W.","57429950900;57223051297;24402958200;56075116600;57429064200;","Tensor-based basis function learning for three-dimensional sound speed fields",2022,"Journal of the Acoustical Society of America","151","1",,"269","285",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123641284&doi=10.1121%2f10.0009280&partnerID=40&md5=520d8733f13739ce493e3c80f3f1bbc4","Basis function learning is the stepping stone towards effective three-dimensional (3D) sound speed field (SSF) inversion for various acoustic signal processing tasks, including ocean acoustic tomography, underwater target localization/tracking, and underwater communications. Classical basis functions include the empirical orthogonal functions (EOFs), Fourier basis functions, and their combinations. The unsupervised machine learning method, e.g., the K-singular value decomposition (K-SVD) algorithm, has recently tapped into the basis function design, showing better representation performance than the EOFs. However, existing methods do not consider basis function learning approaches that treat 3D SSF data as a third-order tensor, and, thus, cannot fully utilize the 3D interactions/correlations therein. To circumvent such a drawback, basis function learning is linked to tensor decomposition in this paper, which is the primary drive for recent multi-dimensional data mining. In particular, a tensor-based basis function learning framework is proposed, which can include the classical basis functions (using EOFs and/or Fourier basis functions) as its special cases. This provides a unified tensor perspective for understanding and representing 3D SSFs. Numerical results using the South China Sea 3D SSF data have demonstrated the excellent performance of the tensor-based basis functions. © 2022 Acoustical Society of America.",,"Data mining; Digital storage; Learning systems; Orthogonal functions; Signal processing; Singular value decomposition; Tensors; 3D sound; Base function; Empirical Orthogonal Function; Field data; Fourier basis functions; Function learning; Performance; Sound speed field; Stepping-stones; Three-dimensional sounds; article; data mining; decomposition; learning; sound; South China Sea; velocity",Article,Scopus,2-s2.0-85123641284
"Yuan Y., Dong L., Li X.","57203237779;57216629444;55936260100;","Hyperspectral Unmixing Using Nonlocal Similarity-Regularized Low-Rank Tensor Factorization",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123604863&doi=10.1109%2fTGRS.2021.3095488&partnerID=40&md5=d8d85984fc3594e22115931e94bfb670","Recently, methods based on nonnegative tensor factorization (NTF), which benefits from the tensor representation of hyperspectral imagery (HSI) without any information loss, have attracted increasing attention. However, most existing methods fail to explore the internal spatial structure of data, resulting in low unmixing performance. Moreover, when the algorithm is optimized, the solution is unstable. In this article, a regularizer based on nonlocal tensor similarity is proposed, which can not only fully preserve the global information of HSI but also mine the internal information of data in the spatial domain. HSI is regarded as a 3-D tensor and is directly subjected to endmember extraction and abundance estimation. To fully explore the structural characteristics of data, we simultaneously use the local smoothing and low tensor rank prior of the data to constrain the unmixing model. First, several 4-D tensor groups can be obtained after the nonlocal similarity structure of HSI is learned. Subsequently, a low tensor rank prior is applied to each 4-D tensor, which can fully simulate the nonlocal similarity in the image. In addition, total variation (TV) is also used to explore the local spatial relationship of data, which can generate a smooth abundance map through edge preservation. The optimization is solved by the ADMM algorithm. Experiments on synthetic and real data illustrate the superiority of the proposed method. © 1980-2012 IEEE.","Hyperspectral unmixing; low tensor rank; nonlocal similarity; nonnegative tensor factorization (NTF)","Factorization; Remote sensing; Spectroscopy; Hyperspectral unmixing; Information loss; Low tensor rank; Non-local similarities; Nonnegative tensor factorization; Nonnegative tensor factorizations; Tensor factorization; Tensor ranks; Tensor representation; Unmixing; Tensors; algorithm; data mining; multispectral image; optimization",Article,Scopus,2-s2.0-85123604863
"Ding S., Li J., Li J.","57429701300;57428814000;57429349900;","Application of Data Mining in Effect Evaluation of Lean Management",2022,"Scientific Programming","2022",,"3101614","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123600966&doi=10.1155%2f2022%2f3101614&partnerID=40&md5=dac978e69885aef2b93eb5ab6eaec69f","Quantitative evaluation is an important part of enterprise diagnosis, which promotes the scientific and modern management of enterprises. At present, the existing enterprise management evaluation methods cannot complete the mining of enterprise index data, which leads to large error and low significance coefficient in enterprise management evaluation. Therefore, the application of data mining in enterprise lean management effect evaluation is put forward. The process and main functions of data mining are analyzed; data mining algorithm is used to establish the evaluation index system of lean management effect and calculate the index weight. Using the association rules method in data mining, according to the parameters of enterprise lean management level evaluation index and weight value, through the fuzzy set transformation idea, the fuzzy boundary of each index and factor is described by the membership degree, the fuzzy judgment matrix is constructed, and the final evaluation result is obtained by multilayer compound calculation. Experimental results show that this study has a high significance coefficient, and the proposed evaluation method of enterprise lean management effect has ideal accuracy and short time consumption. In practical application, the cumulative contribution rate is higher and has higher stability. © 2022 Song Ding et al.",,"Information management; Lean production; Linear transformations; Metadata; Data mining algorithm; Effect evaluation; Enterprise diagnosis; Enterprise management; Evaluation indices system; Evaluation methods; Lean management; Modern management; Quantitative evaluation; Scientific management; Data mining",Article,Scopus,2-s2.0-85123600966
"Allahgholi M., Rahmani H., Javdani D., Sadeghi-Adl Z., Bender A., Módos D., Weiss G.","57210211723;57216792350;57210202630;57428648600;8779307500;55240175600;57429530400;","DDREL: From drug-drug relationships to drug repurposing",2022,"Intelligent Data Analysis","26","1",,"221","237",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123586772&doi=10.3233%2fIDA-215745&partnerID=40&md5=d728ca9195426992546a79c1211ddd78","Analyzing the relationships among various drugs is an essential issue in the field of computational biology. Different kinds of informative knowledge, such as drug repurposing, can be extracted from drug-drug relationships. Scientific literature represents a rich source for the retrieval of knowledge about the relationships between biological concepts, mainly drug-drug, disease-disease, and drug-disease relationships. In this paper, we propose DDREL as a general-purpose method that applies deep learning on scientific literature to automatically extract the graph of syntactic and semantic relationships among drugs. DDREL remarkably outperforms the existing human drug network method and a random network respected to average similarities of drugs' anatomical therapeutic chemical (ATC) codes. DDREL is able to shed light on the existing deficiency of the ATC codes in various drug groups. From the DDREL graph, the history of drug discovery became visible. In addition, drugs that had repurposing score 1 (diflunisal, pargyline, fenofibrate, guanfacine, chlorzoxazone, doxazosin, oxymetholone, azathioprine, drotaverine, demecarium, omifensine, yohimbine) were already used in additional indication. The proposed DDREL method justifies the predictive power of textual data in PubMed abstracts. DDREL shows that such data can be used to 1- Predict repurposing drugs with high accuracy, and 2- Reveal existing deficiencies of the ATC codes in various drug groups. © 2022 - IOS Press. All rights reserved.","deep learning; Drug-drug relationships; repurposing drugs; text mining; word embedding","Data mining; Deep learning; Drug products; Computational biology; Deep learning; Drug-drug relationship; Embeddings; Repurposing; Repurposing drug; Scientific literature; Semantic relationships; Text-mining; Word embedding; Semantics",Article,Scopus,2-s2.0-85123586772
"Anandaraj A., Alphonse P.J.A.","57204239607;57200760920;","Tree based Ensemble for Enhanced Prediction (TEEP) of epileptic seizures",2022,"Intelligent Data Analysis","26","1",,"133","151",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123582046&doi=10.3233%2fIDA-205534&partnerID=40&md5=f4b33b829febb396e914070bbb46090e","Accurate and timely prediction of seizures can improve the quality of life of epileptic patients to a huge extent. This work presents a seizure prediction model that performs data extraction and feature engineering to enable effective demarcation of preictal signals from interictal signals. The proposed Tree based Ensemble for Enhanced Prediction (TEEP) model is composed of three major phases; the feature extraction phase, feature selection phase and the prediction phase. The data is preprocessed, and features are extracted based on the nature of the data. This enables the prediction algorithm to perform time-based predictions. Further, statistical features are also extracted, followed by the process of feature aggregation. The resultant data is passed to the feature selection module to identify the attributes that exhibit highest correlation with the prediction variable. Incorporation of these two modules enhances the generalization capability of the TEEP model. The resultant features are passed to the boosted ensemble model for training and prediction. The TEEP model is analyzed using the Epileptic Seizure Recognition Data from University Hospital of Bonn and the NIH Seizure Prediction data from Melbourne University, Australia. Results from both the datasets indicate effective performances. Comparisons with the existing state-of-the-art models in literature exhibits the enhanced prediction levels of the TEEP model. © 2022 - IOS Press. All rights reserved.","boosting; epilepsy; feature extraction; Seizure prediction","Data mining; Extraction; Forecasting; Neurophysiology; Boosting; Data extraction; Data feature; Epileptic patients; Features extraction; Features selection; Prediction modelling; Quality of life; Seizure prediction; Tree-based ensembles; Feature extraction",Article,Scopus,2-s2.0-85123582046
"Aggoune A.","55911520700;","Intelligent data integration from heterogeneous relational databases containing incomplete and uncertain information",2022,"Intelligent Data Analysis","26","1",,"75","99",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123579595&doi=10.3233%2fIDA-205535&partnerID=40&md5=7c2fee1b2f47c6703b490a0e5b35a30f","The integration of incomplete and uncertain information has emerged as a crucial issue in many application domains, including data warehousing, data mining, data analysis, and artificial intelligence. This paper proposes a novel approach of mediation-based integration for integrating these types of information from heterogeneous relational databases. We present in detail the different processes in the layered architecture of the proposed flexible mediator system. The integration process of our mediator is based on the use of fuzzy logic and semantic similarity measures for more effective integration of incomplete and uncertain information. We also define fuzzy views over the mediator's global fuzzy schema to express incomplete and uncertain databases and specify the mappings between this global schema and these sources. Moreover, our approach provides intelligent data integration, enabling efficient generation of cooperative answers from similar ones, retrieved by queried flexible wrappers. These answers contain information that is more detailed and complete than the information contained in the initial answers. A thorough experiment verifies our approach improves the performance of data integration under various configurations. © 2022 - IOS Press. All rights reserved.","Cooperative answers; fuzzy logic; incomplete and uncertain information; intelligent data integration; mediator system","Computer circuits; Data integration; Data mining; Data warehouses; Semantics; Applications domains; Cooperative answer; Fuzzy-Logic; Incomplete information; Integration process; Intelligent data integration; Layered architecture; Mediator systems; Relational Database; Uncertain informations; Fuzzy logic",Article,Scopus,2-s2.0-85123579595
"Sahagun M.A.M.","57191899535;","Machine Learning Based Selection of Incoming Engineering Freshmen in Higher Education Institution",2022,"International Journal of Computing and Digital Systems","11","1",,"325","334",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123542924&doi=10.12785%2fijcds%2f110127&partnerID=40&md5=83f1db0b832ba3723aa048aa9928f5b5","The Accrediting Agency of Chartered Colleges and Universities of the Philippines recommends through the university testing unit, a system to interpret and analyse entrance test results that may help direct and guide students in choosing a Baccalaureate degree to take in the college. While the present system of manually evaluating each of the freshman applicants is used, there is a need to adopt technological tools for faster and accurate analysis. Thus, the study presents machine learning methods of classifying freshmen applicants if they are qualified or not in the college of engineering and architecture. Specifically, determining if a freshmen applicant may succeed in the five engineering program at university. The study used classifiers such as Decision Tree, K-Nearest Neighbor (KNN), Decision Tree, and Support Vector Machine (SVM). A cross-validation of ten-fold model was used better accuracy of classifiers. The predicted models performed well, however, the Decision Tree classifier outputs a higher average accuracy and F1-measure. The result shows that the classifier accurately classifies qualified and non-qualified engineering freshmen for program acceptance. © 2022 University of Bahrain. All rights reserved.",": K-Nearest Neighbor; Data Mining; Decision Tree; Support Vector Machine",,Article,Scopus,2-s2.0-85123542924
"Zhang C., Xu S., Li Z., Liu G., Dai D., Dong C.","57222394768;57222395340;57339916000;57427896100;57340230200;57340438700;","The Evolution and Disparities of Online Attitudes Toward COVID-19 Vaccines: Year-long Longitudinal and Cross-sectional Study",2022,"Journal of Medical Internet Research","24","1","e32394","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123537254&doi=10.2196%2f32394&partnerID=40&md5=e9189b7b78f302059a0a8277b9143432","Background: Due to the urgency caused by the COVID-19 pandemic worldwide, vaccine manufacturers have to shorten and parallel the development steps to accelerate COVID-19 vaccine production. Although all usual safety and efficacy monitoring mechanisms remain in place, varied attitudes toward the new vaccines have arisen among different population groups. Objective: This study aimed to discern the evolution and disparities of attitudes toward COVID-19 vaccines among various population groups through the study of large-scale tweets spanning over a whole year. Methods: We collected over 1.4 billion tweets from June 2020 to July 2021, which cover some critical phases concerning the development and inoculation of COVID-19 vaccines worldwide. We first developed a data mining model that incorporates a series of deep learning algorithms for inferring a range of individual characteristics, both in reality and in cyberspace, as well as sentiments and emotions expressed in tweets. We further conducted an observational study, including an overall analysis, a longitudinal study, and a cross-sectional study, to collectively explore the attitudes of major population groups. Results: Our study derived 3 main findings. First, the whole population’s attentiveness toward vaccines was strongly correlated (Pearson r=0.9512) with official COVID-19 statistics, including confirmed cases and deaths. Such attentiveness was also noticeably influenced by major vaccine-related events. Second, after the beginning of large-scale vaccine inoculation, the sentiments of all population groups stabilized, followed by a considerably pessimistic trend after June 2021. Third, attitude disparities toward vaccines existed among population groups defined by 8 different demographic characteristics. By crossing the 2 dimensions of attitude, we found that among population groups carrying low sentiments, some had high attentiveness ratios, such as males and individuals aged ≥40 years, while some had low attentiveness ratios, such as individuals aged ≤18 years, those with occupations of the 3rd category, those with account age <5 years, and those with follower number <500. These findings can be used as a guide in deciding who should be given more attention and what kinds of help to give to alleviate the concerns about vaccines. Conclusions: This study tracked the year-long evolution of attitudes toward COVID-19 vaccines among various population groups defined by 8 demographic characteristics, through which significant disparities in attitudes along multiple dimensions were revealed. According to these findings, it is suggested that governments and public health organizations should provide targeted interventions to address different concerns, especially among males, older people, and other individuals with low levels of education, low awareness of news, low income, and light use of social media. Moreover, public health authorities may consider cooperating with Twitter users having high levels of social influence to promote the acceptance of COVID-19 vaccines among all population groups. © Chunyan Zhang, Songhua Xu, Zongfang Li, Ge Liu, Duwei Dai, Caixia Dong.","Attitude; COVID-19; Data mining; Disparity; Evolution; Pandemic; Population group; Twitter; Vaccine","SARS-CoV-2 vaccine; adult; age; Article; attention; attitude to health; coronavirus disease 2019; cross-sectional study; data mining; deep learning; female; health disparity; human; longitudinal study; male; observational study; occupation; population group; sex difference; social media; young adult; aged; attitude; pandemic; preschool child; Aged; Attitude; Child, Preschool; COVID-19; COVID-19 Vaccines; Cross-Sectional Studies; Humans; Longitudinal Studies; Male; Pandemics; SARS-CoV-2; Social Media",Article,Scopus,2-s2.0-85123537254
"Yang Z.","57216741168;","Data analysis and personalized recommendation of western music history information using deep learning under Internet of Things",2022,"PLoS ONE","17","1 January","e0262697","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123535634&doi=10.1371%2fjournal.pone.0262697&partnerID=40&md5=8f4b71441436d645245a94a0e80d94ad","To improve the teaching effect of western music history, the curriculum reform of history education needs to be promoted under the background of the Internet of Things (IoT). At first, a discussion is made on the characteristics of history course, which is combined with the characteristics of teaching data easy to collect under the background of IoT. An analysis is conducted on the related theory of educational data mining. Then, the concept of personalized recommendation is proposed based on deep learning (DL) algorithm. Finally, online and offline experiments are designed to verify the performance of the algorithm from review and investigation, smoothness, and participation of difficulty. The research results show that in terms of offline recommendation accuracy, the average record length in Math data set is 24.5, which is much smaller than that in range data set. The research has obvious innovation significance compared with other studies. In the process of target review and investigation, it is found that the research method here involves a wider range of knowledge and higher reliability. In terms of the difficulty of recommending questions, the Deep Reinforcement Exercise (DRE) recommendation algorithm can adaptively adjust the difficulty of recommending questions. It also allows students to set different learning goals through participation goals. But in the experiments on Math data set, Step 10's recommendation results are not very good, and the difficulty level varies greatly. If the goal setting is high, the problem recommended to students is too difficult, students may answer these questions wrongly, forcing the algorithm to adjust the difficulty adaptively. According to the above results, DRE recommendation algorithm can adapt to different learning needs and customize the recommendation results, thus opening up a new path for the teaching of western music history. Besides, the combination of DL algorithm and western music history teaching design can recommend learning materials, which is of great significance in the teaching of history courses. © 2022 Zongye Yang. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"algorithm; article; curriculum; data analysis; data mining; deep learning; education; exercise; human; human experiment; internet of things; learning; music; reinforcement (psychology); reliability; teaching; algorithm; Algorithms; Data Mining; Deep Learning; Humans; Internet of Things; Music",Article,Scopus,2-s2.0-85123535634
"Gu C.","35193886400;","Application of Data Mining Technology in Financial Intervention Based on Data Fusion Information Entropy",2022,"Journal of Sensors","2022",,"2192186","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123524904&doi=10.1155%2f2022%2f2192186&partnerID=40&md5=1b6d358df314f0c8670a83c3c628d571","Finance, as the core of the modern economy, supports sustained economic growth through financing and distribution. With the continuous development of the market economy, finance plays an increasingly important role in economic development. A new economic and financial phenomenon, known as financial intervention, has emerged in recent years, which has created a series of new problems, promoting the rapid increase both in credit and investment and causing many problems on normal operation of financial bodies. In the long run, it will inevitably affect the stability and soundness of the entire economic and financial system. In order to maximize the effect of financial intervention, in response to the above problems, this article uses a series of US practices in financial intervention as the survey content, combined with the loan data provided by the US government financial intervention department, and mines the data of the general C4.5 algorithm of the decision tree algorithm. Generate a decision tree and convert it into classification rules. Next, we will discover the laws hidden behind the loan data, further discover information that may violate relevant financial policies, provide a reliable basis for financial intervention, and improve the efficiency of financial intervention. Experiments show that the method used in this article can effectively solve the above problems and has certain practicability in fiscal intervention. With stratified sampling, the risky accuracy rate increased by 10%, probably because stratified sampling increased the number of high-risk samples. © 2022 Cong Gu.",,"Data fusion; Data mining; Economics; Investments; Continuous development; Data mining technology; Economic development; Economic growths; Economic system; Financial system; Fusion information entropies; Market economies; Normal operations; Stratified sampling; Decision trees",Article,Scopus,2-s2.0-85123524904
"Zhou Y., Shang Y., Cao Y., Li Q., Zhou C., Xu G.","57215653465;55842395900;55431244700;57208660943;56703955100;8987733300;","API-GNN: attribute preserving oriented interactive graph neural network",2022,"World Wide Web","25","1",,"239","258",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123465892&doi=10.1007%2fs11280-021-00987-z&partnerID=40&md5=d7c2dd0a3432dabc2728dc9155ddbc1d","Attributed graph embedding aims to learn node representation based on the graph topology and node attributes. The current mainstream GNN-based methods learn the representation of the target node by aggregating the attributes of its neighbor nodes. These methods still face two challenges: (1) In the neighborhood aggregation procedure, the attributes of each node would be propagated to its neighborhoods which may cause disturbance to the original attributes of the target node and cause over-smoothing in GNN iteration. (2) Because the representation of the target node is derived from the attributes and topology of its neighbors, the attributes and topological information of each neighbor have different effects on the representation of the target node. However, this different contribution has not been considered by the existing GNN-based methods. In this paper, we propose a novel GNN model named API-GNN (Attribute Preserving Oriented Interactive Graph Neural Network). API-GNN can not only reduce the disturbance of neighborhood aggregation to the original attribute of target node, but also explicitly model the different impacts of attribute and topology on node representation. We conduct experiments on six public real-world datasets to validate API-GNN on node classification and link prediction. Experimental results show that our model outperforms several strong baselines over various graph datasets on multiple graph analysis tasks. © 2021, This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply.","Data mining; Graph neural networks; Representation learning; Social analysis","Classification (of information); Graph neural networks; Iterative methods; Topology; Attributed graphs; Graph embeddings; Graph neural networks; Graph topology; Learn+; Neighbourhood; Node attribute; Representation learning; Social analyse; Target nodes; Data mining",Article,Scopus,2-s2.0-85123465892
"Guo Y., Zuo J.","57426320600;57426205400;","Data Μining Αlgorithm for Μoving Οbject Τrajectory",2022,"International Journal of Circuits, Systems and Signal Processing","16",,,"330","337",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123455727&doi=10.46300%2f9106.2022.16.40&partnerID=40&md5=804cbf77670f0837877c213a95875f7a","Aiming at the poor effect and long recognition time of data mining algorithm for moving target trajectory recognition, a data mining algorithm based on improved Hausdorff distance is proposed. The position and angle of abnormal trajectory data are detected by calculating the distance between trajectory classification and sub trajectory line segments, and the trajectory unit is established by using the improved Hausdorff distance algorithm to optimize the similarity matching structure. Experimental results show that the algorithm has low error pruning rate in identifying moving target trajectory, improves the detection efficiency of moving target trajectory recognition data, and ensures the quality of moving target trajectory recognition data mining. © 2022, North Atlantic University Union NAUN. All rights reserved.","Data Mining; Improved Hausdoff Distance; Moving Objects; Running Track","Geometry; Trajectories; Data mining algorithm; Hausdoff distance; Hausdorff distance; Improved hausdoff distance; Moving objects; Moving target trajectory; Recognition time; Running track; Trajectories datum; Trajectory classification; Data mining",Article,Scopus,2-s2.0-85123455727
"Fang M.","57426241500;","Large scale hotel resource retrieval algorithm based on characteristic threshold extraction",2022,"International Journal of Circuits, Systems and Signal Processing","16",,"4","26","31",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123454111&doi=10.46300%2f9106.2022.16.4&partnerID=40&md5=6a6e2ae37c9a70e811537d8e7be27c18","—At present, the hotel resource retrieval algorithm has the problem of low retrieval efficiency, low accuracy, low security and high energy consumption, and this study proposes a large scale hotel resource retrieval algorithm based on characteristic threshold extraction. In the large-scale hotel resource data, the mass sequence is decomposed into periodic component, trend component, random error component and burst component. Different components are extracted, the singular point detection is realized by the extraction results, and the abnormal data in the hotel resource data are obtained. Based on the attribute of hotel resource data, the data similarity is processed with variable window, the total similarity of data is obtained, and the abnormal detection of redundant resource data is realized. The abnormal data detection results and redundant data detection results are substituted into the space-time filter, and the data processing is completed. The retrieval problem is identified, and the data processing results are replaced in the hotel resource retrieval based on the characteristic threshold extraction to achieve the normalization of data source and rule knowledge. The characteristic threshold and retrieval strategy are determined, and data fusion reasoning is carried out. After repeated iteration, effective solutions are obtained. The effective solution is fused to get the best retrieval result. Experimental results showed that the algorithm has higher retrieval accuracy, efficiency and security coefficient, and the average search energy consumption is 56n J/bit. © 2022, North Atlantic University Union NAUN. All rights reserved.","Data processing; Feature threshold extraction; Hotel resources; Retrieval method; —Data sets","Data fusion; Data handling; Data mining; Energy efficiency; Energy utilization; Hotels; Iterative methods; Abnormal data; Data set; Data-detection; Feature threshold extraction; Hotel resource; Large-scales; Resource retrieval; Retrieval algorithms; Retrieval methods; —data set; Extraction",Article,Scopus,2-s2.0-85123454111
"Zou J., Gong W., Huang G., Hu G., Gong W.","57219529583;57280189300;57280312700;57280440200;57280819900;","Research on the Improvement of Big Data Feature Investment Analysis Algorithm for Abnormal Trading in the Financial Securities Market",2022,"International Journal of Circuits, Systems and Signal Processing","16",,"50","406","412",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123454064&doi=10.46300%2f9106.2022.16.50&partnerID=40&md5=a9dbdb24d98237e131eca40a0d4fbeb1","—Traditional investment analysis algorithms usually only analyze the similarity between financial time series and financial data, which leads to inaccurate and inefficient analysis of investment characteristics. In addition, the trading volume of financial securities market is huge, the amount of investment data is also very large, and the detection of abnormal transactions is difficult. The aim of feature extraction is to obtain mathematical features that can be recognized by machine. Different from the traditional methods, this paper studies and improves the big data investment analysis algorithm of abnormal transactions in financial securities market. After processing the captured trading data of financial securities market, the big data feature of abnormal trading is extracted. Combined with the abnormal trading and the financial securities market, the investment strategy is determined. The optimization objective function is set and the genetic algorithm is used to improve the investment analysis algorithm. The simulation experiment verifies the improved investment analysis algorithm, and the average Accuracy of investment analysis is increased by at least 11.24%, the ROI is significantly improved, and the efficiency is higher, which indicates that the proposed algorithm has ideal application performance. © 2022, North Atlantic University Union NAUN. All rights reserved.","Abnormal trading; Algorithm improvement; Big data feature; Investment analysis algorithm; —financial securities market","Big data; Commerce; Data mining; Financial markets; Genetic algorithms; Market Research; Time series analysis; Abnormal trading; Algorithm improvements; Analysis algorithms; Big data feature; Data feature; Financial security; Investment analyse algorithm; Investment analysis; Securities market; —financial security market; Investments",Article,Scopus,2-s2.0-85123454064
"Viveka T., Christopher Columbus C., Senthil Velmurugan N.","57425968700;26423474500;57425822500;","To Control Diabetes Using Machine Learning Algorithm and Calorie Measurement Technique",2022,"Intelligent Automation and Soft Computing","33","1",,"535","547",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123397370&doi=10.32604%2fiasc.2022.022976&partnerID=40&md5=c8c5edba29609fc792901f9e44891071","Because of the increasing workload, people are having several clinical examinations to determine their health status, resulting in limited time. Here, we present a healthful consuming device based on rule mining that can modify your parameter dependency and recommend the varieties of meals that will boost your fitness and assist you to avoid the types of meals that increase your risk for sicknesses. Using the meals database, the data mining technique is useful for gathering meal energy from breakfast, after breakfast, lunch, after lunch, dinner, after dinner, and bedtime for ninety days. The purpose of this study is to determine to mean random plasma glucose levels and h1bc levels using the Nathan, ADAG (A1C-derived average glucose), and DTTC (Dynamic Temporal and Tactile Cueing) methods. This system can identify and recognize food images, as well as keep track of the food items ingested by the user. Deep learning techniques are mostly utilized for picture recognition and categorization. The KNN (k-nearest neighbors algorithm) classification approach is used to determine if diabetes is normal, pre-diabetic, or chronic. This study employs deep learning and a smart camera app called “calorie mom” to track nutrition from meal photographs. In addition, the commonly used measures of divisions such as accuracy, sensitivity, uniqueness, and recalling diabetic dataset using Python 3 Jupyter Notebook were employed to evaluate the performance of a machine learning classifier. © 2022, Tech Science Press. All rights reserved.","Calories; Data mining; Machine learning; Randomplasma glucose (RPM)",,Article,Scopus,2-s2.0-85123397370
"Ryu J., Patil A.K., Chakravarthi B., Balasubramanyam A., Park S., Chai Y.","57211048062;55967728300;57216939205;57211045502;57423577300;7102457214;","Angular Features-Based Human Action Recognition System for a Real Application With Subtle Unit Actions",2022,"IEEE Access","10",,,"9645","9657",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123375467&doi=10.1109%2fACCESS.2022.3144456&partnerID=40&md5=c391f23e977fc013300e095dbaa98700","Human action recognition (HAR) technology is receiving considerable attention in the field of human-computer interaction. We present a HAR system that works stably in real-world applications. In real-world applications, the HAR system needs to identify detailed actions for specific purposes, and the action data includes many variations. Accordingly, we conducted three experiments. First, we tested our recognition system's performance on the UTD-MHAD dataset. We compared our system's accuracy with results from previous research and confirmed that our system achieves a 91% average performance among recognition systems. Furthermore, we hypothesized the use of a HAR system to detect burglary. In the second experiment, we compared the existing benchmark data with our crime detection dataset. We recognized the test scenarios' data by using the recognition system trained by each dataset. The recognition system trained by our dataset achieved higher accuracy than the past benchmark dataset. The results show that the training data should contain detailed actions for a real application. In the third experiment, we tried to find the motion data type that stably recognizes action regardless of data variation. In a real application, the action data are changed by people. Thus, we introduced variations in the action data using the cross-subject protocol and moving area setting. We trained the recognition system using each position and angle data. In addition, we compared the accuracy of each system. We found that the angle format results in better accuracy because the angle data are beneficial for converting the action variation into a consistent pattern. © 2013 IEEE.","ELM classifier; Human action recognition; motion capture; skeleton; surveillance","Classification (of information); Human computer interaction; Motion estimation; Statistical tests; Benchmark testing; ELM classifier; Features extraction; Human-action recognition; Motion capture; Recognition systems; Sensor systems; Skeleton; Training data; Data mining",Article,Scopus,2-s2.0-85123375467
"Li N., Wang Z.","57222042338;57216164120;","Spatial Attention Guided Residual Attention Network for Hyperspectral Image Classification",2022,"IEEE Access","10",,,"9830","9847",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123371086&doi=10.1109%2fACCESS.2022.3144393&partnerID=40&md5=555aece95be6a6a854bf4d86afddf16e","Hyperspectral image (HSI) classification has become a research hotspot. Recently, deep learning-based methods have achieved preferable performances by which the deep spectral-spatial features can be extracted from HSI cubes. However, in complex scenes, due to the diversity of the types of land-cover and the bands in high dimensional, these methods are often hampered by the irrelevant spatial areas and the redundant bands, which results in the indistinguishable features and the restricted performance. In this article, a spatial attention guided residual attention network (SpaAG-RAN) is proposed for HSI classification, which contains a spatial attention module (SpaAM), a spectral attention module (SpeAM), and a spectral-spatial feature extraction module (SSFEM). Based on the spectral similarity, the SpaAM is capable of capturing the relevant spatial areas composed of the pixels of the same category as the center pixel from HSI cube with a novel inverted-shifted-scaled sigmoid activation function. The SpeAM aims to select the bands which are beneficial to the spectral features representation. The SSFEM is exploited to extract the discriminating spectral-spatial features. To facilitate the processes of bands selection and features extraction, two well-designed spatial attention masks generated by the SpaAM are employed to guide the works of the SpeAM and the SSFEM, respectively. Moreover, a spatial consistency loss function is installed to maintain the consistency between the two spatial attention masks so that the network enables the distinction of the relevant features exactly. Experimental results on three HSI data sets show that the proposed SpaAG-RAN model can extract the discriminating spectral-spatial features and outperforms the state-of-the-arts. © 2022 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Computational modeling; Convolutional neural networks; Correlation; Data mining; Feature extraction; Hyperspectral imaging; Training","Data mining; Deep neural networks; Extraction; Hyperspectral imaging; Image classification; Pixels; Spectroscopy; Attention mechanisms; Computational modelling; Convolutional neural network; Correlation; Deep learning; Features extraction; Hyperspectral image classification; Residual network; Spatial attention; Spatial features; Feature extraction",Article,Scopus,2-s2.0-85123371086
"Xu E., Li Y., Han Z., Du J., Yang M., Gao X.","36877197600;57192522183;57193993403;57224897008;57366736500;57366304200;","A method for predicting the remaining life of equipment based on WTTE-CNN-LSTM",2022,"Journal of Advanced Mechanical Design, Systems and Manufacturing","16","1","jamdsm0001","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123370267&doi=10.1299%2fJAMDSM.2022JAMDSM0001&partnerID=40&md5=1fa7d8cd3a57f96b3786e2421e6679e0","At present, the uncertainty and randomness between equipment are not fully considered in the remaining useful life (RUL) prediction. In order to solve this problem, firstly, we use the Weibull distribution to describe the influence of various uncertain factors on the RUL of equipment, and introduce the Weibull Time-To-Event Recurrent Neural Network (WTTE-RNN) framework to transform the RUL of equipment from the prediction of single life value to the prediction of Weibull distribution parameters. Then, in view of the problem that RNN is prone to have low prediction accuracy due to the vanishing of gradient, considering the advantages of Long-Short Term memory (LSTM) in time series modeling, we replace RNN with LSTM to improve the model and construct WTTE- LSTM model. Furthermore, in order to further improve the model's ability to extract data features, Convolutional Neural Network (CNN) is added after the original data is normalized because of its excellent feature extraction ability, and the time series features extracted by CNN are used as the input of LSTM to construct the WTTE-CNN-LSTM model. Finally, the LSTM life prediction model, WTTE-LSTM model and WTTE-CNN-LSTM model are established by taking a data set from a core component of construction machinery as an example. The results demonstrate that the improved WTTE-CNN-LSTM model has the highest prediction accuracy and the smallest error. © 2022 The Japan Society of Mechanical Engineers. This is an open access article under the terms of the Creative Commons Attribution-NonCommercial-NoDerivs license (https://creativecommons.org/licenses/by-nc-nd/4.0/).","Convolutional neural network (CNN); Long-short term memory (LSTM); Remaining useful life prediction(RUL); Weibull time-to-event recurrent neural network (WTTE-RNN)","Brain; Convolution; Convolutional neural networks; Data mining; Forecasting; Machinery; Time series; Uncertainty analysis; Weibull distribution; Convolutional neural network; Long-short term memory; Memory modeling; Prediction accuracy; Remaining life; Remaining useful life predictions; Time to events; Weibull; Weibull time-to-event recurrent neural network; Long short-term memory",Article,Scopus,2-s2.0-85123370267
"Jiang J., Zhang Y.","57424322900;57203829244;","An Improved Action Recognition Network with Temporal Extraction and Feature Enhancement",2022,"IEEE Access","10",,,"13926","13935",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123352746&doi=10.1109%2fACCESS.2022.3144035&partnerID=40&md5=4dd67d5827f2efdee4f310a8ce541516","Image classification and action recognition are both active research topics in the field of computer vision. However, the development of action recognition is rather slow compared with image classification, due to the difficulties in spatial-Temporal information modeling. In this paper, we present TEFE, a deep structure combining temporal extraction with feature enhancement to explore the spatial coherence across temporal dimension. The temporal extraction (TE) module is used to capture the short-Term and long-Term temporal features. Considering the spatial and temporal cues are fine-grained, we believe such cues (if encoded by well-designed bilinear models) could enhance the representation of actions in videos. The feature enhancement (FE) module approximates bilinear pooling operations, which greatly reduce the amount of parameters exist in original bilinear pooling. Extensive experiments have been conducted on mainstream datasets of human action (Something-Something V1 V2, and Jester). Experimental results show that our model achieves competitive performances than some existing methods. © 2013 IEEE.","Action recognition; feature enhancement; spatial-Temporal information modelling","Classification (of information); Data mining; Extraction; Feature extraction; Image classification; Image enhancement; Media streaming; Action recognition; Feature enhancement; Features extraction; Kernel; License; Spatial temporals; Spatial-temporal information modeling; Spatiotemporal phenomenon; Streaming medium; Temporal information models; Optical flows",Article,Scopus,2-s2.0-85123352746
"Lee E., Rustam F., Ashraf I., Washington P.B., Narra M., Shafique R.","57226002782;57211950161;57195478761;57258274300;57219120554;57214225951;","Inquest of Current Situation in Afghanistan Under Taliban Rule Using Sentiment Analysis and Volume Analysis",2022,"IEEE Access","10",,,"10333","10348",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123346769&doi=10.1109%2fACCESS.2022.3144659&partnerID=40&md5=dce463aed6bc8c680f09d9974a0f6c03","Microblogging websites and social media platforms serve as a potential source for mining public opinions and sentiments on a variety of subjects including the prevailing situations in war-afflicted countries. In particular, Twitter has a large number of geotagged tweets that make the analysis of sentiments across time and space possible. This study performs volume analysis and sentiment analysis using LDA (Latent Dirichlet Allocation) and text mining over two datasets collected for different periods. To increase the adequacy and efficacy of the sentiment analysis, a hybrid feature engineering approach is proposed that elevates the performance of machine learning models. Geotagged tweets are used for volume analysis indicating that the highest number of tweets is originated from India, the US, the UK, Pakistan, and Afghanistan. Analysis of positive and negative tweets reveals that negative tweets are mostly originated from India and the US. On the contrary, positive tweets belong to Pakistan and Afghanistan. LDA is used for topic modeling on two datasets containing tweets about the current situation after the Taliban take control of Afghanistan. Topics extracted through LDA suggest that majority of the Afghanistan people seem satisfied with the Taliban’s takeover while the topics from negative tweets reveal that issues discussed in negative tweets are related to the US concerns in Afghanistan. Sentiment analysis over two different datasets indicates that the trend of the sentiments has been shifted positively over three weeks. © 2022 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Analytical models; Blogs; Feature extraction; Machine learning; Sentiment analysis; Social networking (online); Support vector machines","Binary alloys; Data mining; Machine learning; Potassium alloys; Social networking (online); Statistics; Uranium alloys; Afghanistan; Current situation; Latent Dirichlet allocation; Microblogging; Pakistan; Sentiment analysis; Taliban; Taliban regime; Topic Modeling; Volume analysis; Sentiment analysis",Article,Scopus,2-s2.0-85123346769
"Xie J.-C., Pun C.-M., Lam K.-M.","57209272810;7003931852;55839377100;","Implicit and Explicit Feature Purification for Age-Invariant Facial Representation Learning",2022,"IEEE Transactions on Information Forensics and Security","17",,,"399","412",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123345003&doi=10.1109%2fTIFS.2022.3142998&partnerID=40&md5=eb1363bbf9cca749c412db47ff1a208a","This paper presents a new method, named implicit and explicit feature purification (IEFP), for age-invariant face recognition. Facial features extracted from a face image contain the information about the identity, age, and other attributes. For age-invariant face recognition, it is important to remove the irrelevant information, and retain the identity information only, in the facial features. Through the two proposed feature purification mechanisms, our framework can produce facial-feature embeddings that preserve identity information as much as possible and are insensitive to age variations. Specifically, on the one hand, a special network module is devised to implicitly purify the original facial features obtained from a face encoder. On the other hand, to obtain purer facial feature representations for age-invariant face recognition, irrelevant information within the implicitly purified features, such as the age, is further removed. This is realized by using a regularizer, based on information theory, to explicitly minimize the correlation between identity-related features and age-related features. Comprehensive ablation studies show that these two feature purification schemes can work independently, as well as collaboratively, to achieve better performance. Extensive evaluations on several benchmark data sets show that the IEFP method is on par with those competitors learned on far more favorable training samples, and it achieves the best performance in a fair comparison. Furthermore, we provide mathematical interpretation to explain the effectiveness of our approach, and find that it tends to generate low-rank, yet high-dimensional, representations for age-invariant face recognition. © 2022 IEEE.","Data mining; Face recognition; Facial features; Feature extraction; Purification; Task analysis; Training","Benchmarking; Face recognition; Feature extraction; Job analysis; Purification; Age-invariant face recognition; Facial feature; Feature decomposition; Features extraction; Identity information; Implicit and explicit features; Low-rank solution; Mutual informations; Performance; Task analysis; Data mining",Article,Scopus,2-s2.0-85123345003
"Fujioka K., Shirahama K.","57224574186;8886250600;","Generic Itemset Mining Based on Reinforcement Learning",2022,"IEEE Access","10",,,"5824","5841",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123320914&doi=10.1109%2fACCESS.2022.3141806&partnerID=40&md5=8dbc842b32bfd393774f335e2809cfc7","One of the biggest problems in itemset mining is the requirement of developing a data structure or algorithm, every time a user wants to extract a different type of itemsets. To overcome this, we propose a method, called Generic Itemset Mining based on Reinforcement Learning (GIM-RL), that offers a unified framework to train an agent for extracting any type of itemsets. In GIM-RL, the environment formulates iterative steps of extracting a target type of itemsets from a dataset. At each step, an agent performs an action to add or remove an item to or from the current itemset, and then obtains from the environment a reward that represents how relevant the itemset resulting from the action is to the target type. Through numerous trial-and-error steps where various rewards are obtained by diverse actions, the agent is trained to maximise cumulative rewards so that it acquires the optimal action policy for forming as many itemsets of the target type as possible. In this framework, an agent for extracting any type of itemsets can be trained as long as a reward suitable for the type can be defined. The extensive experiments on mining high utility itemsets, frequent itemsets and association rules show the general effectiveness and one remarkable potential (agent transfer) of GIM-RL. We hope that GIM-RL opens a new research direction towards learning-based itemset mining. © 2013 IEEE.","Data mining; Itemset mining; Knowledge discovery; Reinforcement learning","Data structures; Iterative methods; Reinforcement learning; 'current; Itemset; Itemset mining; Metaheuristic; Reinforcement learnings; Runtimes; Target type; Trial and error; Unified framework; Upper Bound; Data mining",Article,Scopus,2-s2.0-85123320914
"Rufo D.D., Debelee T.G., Negera W.G.","57288984800;57202946910;57224076243;","A Hybrid Machine-Learning Model Based on Global and Local Learner Algorithms for Diabetes Mellitus Prediction",2022,"Journal of Biomimetics, Biomaterials and Biomedical Engineering","54",,,"65","88",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123316564&doi=10.4028%2fwww.scientific.net%2fJBBBE.54.65&partnerID=40&md5=ae5d5d46a606def5bbdfb5af3d7d7c10","Health is a critical condition for living things even before the technology exists. Nowadays the healthcare domain provides a lot of scope for research as it has extremely evolved. The most researched areas of health sectors include diabetes mellitus (DM), breast cancer, brain tumor, etc. DM is a severe chronic disease that affects human health and has a high rate throughout the world. Early prediction of DM is important to reduce its risk and even avoid it. In this study, we propose a DM prediction model based on global and local learner algorithms. The proposed global and local learners stacking (GLLS) model; combines the prediction algorithms from two largely different but complementary machine learning paradigms, specifically XGBoost and NB from global learning whereas KNN and SVM (with RBF kernel) from local learning and aggregates them by stacking ensemble technique using LR as meta-learner. The effectiveness of the GLLS model was proved by comparing several performance measures and the results of different contrast experiments. The evaluation results on UCI Pima Indian diabetes data-set (PIDD) indicates the model has achieved the better prediction performance of 99.5%, 99.5%, 99.5%, 99.1%, and 100% in terms of accuracy, AUC, F1 score, sensitivity, and specificity respectively, compared to other research results mentioned in the literature. Moreover, to better validate the GLLS model performance, three additional medical data sets; Messidor, WBC, ILPD, are considered and the model also achieved an accuracy of 82.1%, 98.6%, and 89.3% respectively. Experimental results proved the effectiveness and superiority of our proposed GLLS model. © 2022 Trans Tech Publications Ltd, Switzerland.","Data Mining; Diabetes Prediction; Global Learning; Local Learning; Stacking; Synthetic Minority Oversampling","Data mining; Diseases; Learning algorithms; Learning systems; Diabetes mellitus; Diabetes prediction; Global learning; Hybrid machine learning; Local learning; Model-based OPC; Over sampling; Stacking models; Stackings; Synthetic minority oversampling; Forecasting",Article,Scopus,2-s2.0-85123316564
"Sei Y., Ohsuga A.","56344492700;6603722508;","Private True Data Mining: Differential Privacy Featuring Errors to Manage Internet-of-Things Data",2022,"IEEE Access","10",,,"8738","8757",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123315775&doi=10.1109%2fACCESS.2022.3143813&partnerID=40&md5=ce116bab8d51be1862725cb71f7af08e","Available data may differ from true data in many cases due to sensing errors, especially for the Internet of Things (IoT). Although privacy-preserving data mining has been widely studied during the last decade, little attention has been paid to data values containing errors. Differential privacy, which is the de facto standard privacy metric, can be achieved by adding noise to a target value that must be protected. However, if the target value already contains errors, there is no reason to add extra noise. In this paper, a novel privacy model called true-value-based differential privacy (TDP) is proposed. This model applies traditional differential privacy to the “true value” unknown by the data owner or anonymizer but not to the “measured value” containing errors. Based on TDP, the amount of noise added by differential privacy techniques can be reduced by approximately 20% by our solution. As a result, the error of generated histograms can be reduced by 40.4% and 29.6% on average according to mean square error and Jensen–Shannon divergence, respectively. We validate this result on synthetic and five real data sets. Moreover, we proved that the privacy protection level does not decrease as long as the measurement error is not overestimated. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/","Differential privacy; Internet of Things; Measurement errors; Privacy; Servers; Standards; Thermal sensors","Errors; Internet of things; Mean square error; Privacy-preserving techniques; Data values; De facto standard; Differential privacies; Differentnial privacy; Privacy metric; Privacy models; Privacy-preserving data mining; Sensing errors; Target values; Value-based; Data mining",Article,Scopus,2-s2.0-85123315775
"Senefonte H.C.M., Delgado M.R., Luders R., Silva T.H.","57219487219;57423075800;35757772600;55419845400;","PredicTour: Predicting Mobility Patterns of Tourists Based on Social Media User's Profiles",2022,"IEEE Access","10",,,"9257","9270",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123308024&doi=10.1109%2fACCESS.2022.3143503&partnerID=40&md5=adb11acdc01d9b0f5c4f2df986b0b60a","This paper proposes PredicTour, an approach to process check-ins made by users of location-based social networks (LBSNs), and predict mobility patterns of tourists visiting new countries with or without previous visiting records. PredicTour is composed of three key parts: mobility modeling, profile extraction, and tourist mobility prediction. In the first part, sequences of check-ins within a time interval are associated with other user information to produce a new structure called “mobility descriptor”. In the profile extraction, self-organizing maps and fuzzy C-means work jointly to group users according to their mobility descriptors. PredicTour then identifies tourist profiles and estimates mobility patterns of tourists visiting new countries. When comparing the performance of PredicTour with three well-known machine learning-based models, the results indicate that PredicTour outperforms the baseline approaches. Therefore, it is a good alternative for predicting and understanding international tourists' mobility, which has an economic impact on the tourism industry when services and logistics across international borders should be provided. The proposed approach can be used in different applications, such as in recommender systems for tourists or in decision-making support for urban planners interested in improving tourists' experiences and attractiveness of venues through personalized services. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/","Biological system modeling; Clustering algorithms; Data mining; Data models; Mobile handsets; Predictive models; Social networking (online)","Conformal mapping; Decision making; Extraction; Machine learning; Self organizing maps; Social networking (online); User profile; Descriptors; International tourisms; Key parts; Location-based social networks; Mobility; Mobility pattern; Profile extraction; Social computing; Social media; Urban computing; Forecasting",Article,Scopus,2-s2.0-85123308024
"Feng A., Zhang X., Song X.","57192194319;57219034270;57213456784;","Unrestricted Attention May Not Be All You Need–Masked Attention Mechanism Focuses Better on Relevant Parts in Aspect-Based Sentiment Analysis",2022,"IEEE Access","10",,,"8518","8528",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123307040&doi=10.1109%2fACCESS.2022.3142178&partnerID=40&md5=0615ade78d4ab89fb0ddcacecc782b10","Aspect-Based Sentiment Analysis (ABSA) is one of the highly challenging tasks in natural language processing. It extracts fine-grained sentiment information in user-generated reviews, as it aims at predicting the polarities towards predefined aspect categories or relevant entities in free text. Previous deep learning approaches usually rely on large-scale pre-trained language models and the attention mechanism, which applies the complete computed attention weights and does not place any restriction on the attention assignment. We argue that the original attention mechanism is not the ideal configuration for ABSA, as for most of the time only a small portion of terms are strongly related to the sentiment polarity of an aspect or entity. In this paper, we propose a masked attention mechanism customized for ABSA, with two different approaches to generate the mask. The first method sets an attention weight threshold that is determined by the maximum of all weights, and keeps only attention scores above the threshold. The second selects the top words with the highest weights. Both remove the lower score parts that are assumed to be less relevant to the aspect of focus. By ignoring part of input that is claimed irrelevant, a large proportion of input noise is removed, keeping the downstream model more focused and reducing calculation cost. Experiments on the Multi-Aspect Multi-Sentiment (MAMS) and SemEval-2014 datasets show significant improvements over state-of-the-art pre-trained language models with full attention, which displays the value of the masked attention mechanism. Recent work shows that simple self-attention in Transformer quickly degenerates to a rank-1 matrix, and masked attention may be another cure for that trend. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/","Bit error rate; Data mining; Deep learning; Semantics; Sentiment analysis; Task analysis; Transformers","Bit error rate; Computational linguistics; Deep learning; Job analysis; Semantics; Sentiment analysis; Attention mechanisms; Bit-error rate; Deep learning; Language model; Masked attention; Pre-trained language model; Sentiment analysis; Task analysis; Transformer; Data mining",Article,Scopus,2-s2.0-85123307040
"Sharma R., Kaushik M., Peious S.A., Bazin A., Shah S.A., Fister I., Yahia S.B., Draheim D.","57219159631;57219159577;57406404600;36518222500;57194554620;55545862632;57210079822;57200287738;","A Novel Framework for Unification of Association Rule Mining, Online Analytical Processing and Statistical Reasoning",2022,"IEEE Access","10",,,"12792","12813",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123301116&doi=10.1109%2fACCESS.2022.3142537&partnerID=40&md5=f7b489ddb2e2691810df0f8e478e7d07","Statistical reasoning was one of the earliest methods to draw insights from data. However, over the last three decades, association rule mining and online analytical processing have gained massive ground in practice and theory. Logically, both association rule mining and online analytical processing have some common objectives, but they have been introduced with their own set of mathematical formalizations and have developed their specific terminologies. Therefore, it is difficult to reuse results from one domain in another. Furthermore, it is not easy to unlock the potential of statistical results in their application scenarios. The target of this paper is to bridge the artificial gaps between association rule mining, online analytical processing and statistical reasoning. We first provide an elaboration of the semantic correspondences between their foundations, i.e., itemset apparatus, relational algebra and probability theory. Subsequently, we propose a novel framework for the unification of association rule mining, online analytical processing and statistical reasoning. Additionally, an instance of the proposed framework is developed by implementing a sample decision support tool. The tool is compared with a state-of-the-art decision support tool and evaluated by a series of experiments using two real data sets and one synthetic data set. The results of the tool validate the framework for the unified usage of association rule mining, online analytical processing, and statistical reasoning. The tool clarifies in how far the operations of association rule mining and online analytical processing can complement each other in understanding data, data visualization and decision making. © 2013 IEEE.","Association rule mining; data mining; online analytical processing; statistical reasoning","Algebra; Artificial intelligence; Association rules; Data mining; Data visualization; Decision making; Online systems; Probability; Semantics; Statistics; Application scenario; Association rule mining; Cognition; Formalisation; Itemset; Online analytical processing; Reuse; Rule mining; Spread-spectrum communication; Statistical reasoning; Decision support systems",Article,Scopus,2-s2.0-85123301116
"Esenturk E., Wallace A.G., Khastgir S., Jennings P.","40561100800;57328867600;55773807400;7202925251;","Identification of Traffic Accident Patterns via Cluster Analysis and Test Scenario Development for Autonomous Vehicles",2022,"IEEE Access","10",,,"6660","6675",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123295342&doi=10.1109%2fACCESS.2021.3140052&partnerID=40&md5=433f488fda09321eadb7a4094bb3c29d","Increased safety is one of the main motivations for traffic research and planning. The arduous task has two components: (i) improving the existing traffic policies based on a good understanding of risk factors related to trends in traffic accidents, and (ii) underpinning the emerging technologies that will advance the safety of vehicles. For the latter route, the introduction of connected and automated vehicles (CAVs) is a promising option as CAVs can potentially reduce the number of accidents. However, to reap their benefits, they need to be introduced in a safe manner and tested for their ability to safely deal with risky scenarios. Unfortunately, the identification of such test scenarios remains a key challenge for the industry. This study contributes to increased safety by (i) analyzing UK's STATS19 accident data to identify patterns in past traffic accidents, and (ii) utilizing this information to systematically generate scenarios for CAV testing. For task (i), the patterns in the accidents were identified in terms of static and time-dependent internal and external factors. For this purpose, the study employed a clustering algorithm, COOLCAT, which is particularly suitable for dealing with high-dimensional categorical data. Six different clusters emerged naturally as a result of the algorithm. To interpret the clusters, we applied a frequency analysis to each cluster. The frequency tests showed that in each cluster, certain distinct real-world situations were represented more significantly compared to the non-clustered reference case, which are the markers of each cluster. The second task (ii) complemented the first task by synthesizing the relationships between attributes. This was done by association rule mining using the market basket analysis approach. The method enabled us to develop, drawing from the characteristics of the clusters, non-trivial test scenarios that can be used in the testing of CAVs, especially in virtual testing. © 2013 IEEE.","Accident analysis; Cluster analysis; Market basket analysis; Scenario development","Accidents; Cluster analysis; Clustering algorithms; Commerce; Data mining; Job analysis; Vehicles; Virtual reality; Accident analysis; Automated vehicles; Autonomous Vehicles; Clustering methods; Increased safety; Junction; Market basket analysis; Scenario development; Task analysis; Test scenario; Accident prevention",Article,Scopus,2-s2.0-85123295342
"Liu X., Du W., Wang X., Li R., Sun P., Jing X.","57422616300;57422766300;57422766400;57422766500;57422030300;57422616400;","A mutually-exclusive binary cross tagging framework for joint extraction of entities and relations",2022,"PLoS ONE","17","1 January","e0260426","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123295242&doi=10.1371%2fjournal.pone.0260426&partnerID=40&md5=c047771c0e6eb966db7fd226afa342f3","Joint extraction from unstructured text aims to extract relational triples composed of entity pairs and their relations. However, most existing works fail to process the overlapping issues that occur when the same entities are utilized to generate different relational triples in a sentence. In this work, we propose a mutually exclusive Binary Cross Tagging (BCT) scheme and develop the end-to-end BCT framework to jointly extract overlapping entities and triples. Each token of entities is assigned a mutually exclusive binary tag, and then these tags are cross-matched in all tag sequences to form triples. Our method is compared with other state-of-the-art models in two English public datasets and a large-scale Chinese dataset. Experiments show that our proposed framework achieves encouraging performance in F1 scores for the three datasets investigated. Further detailed analysis demonstrates that our method achieves strong performance overall with three overlapping patterns, especially when the overlapping problem becomes complex. © 2022 Liu et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"article; extraction; data mining; Data Mining",Article,Scopus,2-s2.0-85123295242
"Xie Y., Cai Y., Yu Y., Wang S., Wang W., Song S.","57222569104;57422037300;57422626700;57423078100;57422177500;57423078200;","Endoscopic Ultrasound Image Recognition Based on Data Mining and Deep Learning",2022,"IEEE Access","10",,,"10273","10282",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123293477&doi=10.1109%2fACCESS.2022.3143580&partnerID=40&md5=c7378a68210cab5d485ca22b4c594f9d","The recognition of medical images, especially endoscopic ultrasound images, has the characteristics of changing images and insignificant gray-scale changes, which requires repeated observation and comparison by medical staff. In view of the above-mentioned characteristics of ultrasound imaging, a system scheme suitable for image processing is proposed, which can analyze the biliary tract, gallbladder, abdominal lymph nodes, liver, descending duodenum, duodenal bulb, stomach, pancreas, pancreatic lymph nodes, there are a total of 10 ultrasonic organs, including 21 kinds of sub-categories and 3510 images. The images are preprocessed using binarization, histogram equalization, median filtering and edge enhancement algorithms. The improved YoloV4 convolutional neural network algorithm is used to train the data set and perform high accuracy is detected in real time. Finally, the average accuracy of this algorithm has reached 91.59%. The algorithm proposed in this paper can make up for the shortcomings of manual detection in the original image detection system, improve the efficiency of detection, and at the same time as an auxiliary system can reduce detection misjudgments, and promote the development of automated and intelligent detection in the medical field. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/","Biomedical imaging; Gray-scale; Histograms; Interference; Medical diagnostic imaging; Stomach; Ultrasonic imaging","Convolution; Deep learning; Edge detection; Endoscopy; Image enhancement; Image recognition; Median filters; Medical imaging; Neural networks; Ultrasonic imaging; Abdominal lymph nodes; Convolutional neural network; Endoscopic ultrasonographies; Endoscopic ultrasounds; Gray scale; Images processing; Pancreatic lymph nodes; System scheme; Ultrasound images; Ultrasound imaging; Data mining",Article,Scopus,2-s2.0-85123293477
"Aminifar A., Shokri M., Rabbi F., Pun V.K.I., Lamo Y.","57202959857;57209803055;57062726300;57303011000;8386055000;","Extremely Randomized Trees with Privacy Preservation for Distributed Structured Health Data",2022,"IEEE Access","10",,,"6010","6027",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123286462&doi=10.1109%2fACCESS.2022.3141709&partnerID=40&md5=43cddc74c0596e69822ca4875d64d1ad","Artificial intelligence and machine learning have recently attracted considerable attention in the healthcare domain. The data used by machine learning algorithms in healthcare applications is often distributed over multiple sources, for instance, hospitals or patients' personal devices. One main difficulty lies in analyzing such data without compromising patients' privacy and personal data, which is a primary concern in healthcare applications. Therefore, in these applications, we are interested in running machine learning algorithms over distributed data without disclosing sensitive information about the data subjects. In this paper, we propose a distributed extremely randomized trees algorithm for learning from distributed data with privacy preservation. We present the implementation of our technique (which we refer to as k-PPD-ERT) on a cloud platform and demonstrate its performance based on medical data, including Heart Disease, Breast Cancer, and mental health datasets (Depresjon and Psykose datasets) associated with the Norwegian INTROducing Mental health through Adaptive Technology (INTROMAT) project. © 2013 IEEE.","Distributed learning; Extremely randomized trees; Federated machine learning; Privacy-preserving machine learning; Structured health data","Artificial intelligence; Forestry; Health care; Learning algorithms; Learning systems; Medical computing; Privacy-preserving techniques; Trees (mathematics); Distributed database; Distributed learning; Extremely randomized tree; Federated machine learning; Health data; Machine learning algorithms; Machine-learning; Medical services; Privacy; Privacy preserving; Privacy-preserving machine learning; Randomized trees; Structured health data; Data mining",Article,Scopus,2-s2.0-85123286462
"Abdelkader H.E., Gad A.G., Abohany A.A., Sorour S.E.","57216202546;57219971749;56083043100;56204155300;","An Efficient Data Mining Technique for Assessing Satisfaction Level of Online Learning for Higher Education Students during the COVID-19",2022,"IEEE Access","10",,,"6286","6303",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123285294&doi=10.1109%2fACCESS.2022.3143035&partnerID=40&md5=600566e69eaac3874fec6807850900d2","All the educational organizations mainly aim at elevating the academic performance of students for improving the overall quality of education. In this direction, Educational Data Mining (EDM) is a rapidly trending research area that utilizes the essence of Data Mining (DM) concepts to help academic institutions figure out useful information on the Student Satisfaction Level (SSL) with the Online Learning process (OL) during COVID-19 lock-down. Different practices have been tried with EDM to predict students' behaviors to reach the best educational settings. Therefore, Feature Selection (FS) is typically employed to find the most relevant subset of features with minimum cardinality. As the predictive accuracy of a satisfaction model is significantly influenced by the FS process, the effectiveness of the SSL model is elaborately studied in this paper in connection with FS techniques. In this connection, a dataset was first collected online via a questionnaire of student reviews on OL courses. Using this datatset, the performance of wrapper FS techniques in DM and classification algorithms was analyzed in terms of fitness values. Ultimately, the goodness of subsets with different cardinalities is evaluated in terms of prediction accuracy and number of selected features by measuring the quality of 11 wrapper-based FS algorithms and the k-Nearest Neighbor (k-NN) and Support Vector Machine (SVM) as base-line classifiers. Based on the experiments, the optimal dimensionality of the feature subset was revealed, as well as the best method. The findings of the present study evidently support the well-known conjunction of the existence of minimum number of features and an increase in predictive accuracy. It is remarkable the relevancy of FS for high-accuracy SSL prediction, as the relevant set of features can effectively assist in deriving constructive educational strategies. Our study contributes a feature size reduction of up to 80% along with up to 100% classification accuracy on the adopted real-time dataset. © 2013 IEEE.","Classification; COVID-19; Educational data mining (EDM); Feature selection (FS); Machine learning (ML); Online learning (OL); Student satisfaction level (SSL)","Classification (of information); Data mining; Delta modulation; E-learning; Education computing; Feature extraction; Forecasting; Nearest neighbor search; Students; Classification algorithm; COVID-19; Educational data mining; Feature selection; Features selection; Machine learning; Machine-learning; Online learning; Optimisations; Student satisfaction; Student satisfaction level; Support vectors machine; Support vector machines",Article,Scopus,2-s2.0-85123285294
"Hassanat A.B., Ali H.N., Tarawneh A.S., Alrashidi M., Alghamdi M., Altarawneh G.A., Abbadi M.A.","24343672100;57225688780;57190020156;56405416200;57213626210;56512419600;57194447491;","Magnetic Force Classifier: A Novel Method for Big Data Classification",2022,"IEEE Access","10",,,"12592","12606",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123283345&doi=10.1109%2fACCESS.2022.3142888&partnerID=40&md5=879261452c1ad5a0e0a119368cdfb6f5","There are a plethora of invented classifiers in Machine learning literature, however, there is no optimal classifier in terms of accuracy and time taken to build the trained model, especially with the tremendous development and growth of Big data. Hence, there is still room for improvement. In this paper, we propose a new classification method that is based on the well-known magnetic force. Based on the number of points belonging to a specific class/magnet, the proposed magnetic force (MF) classifier calculates the magnetic force at each discrete point in the feature space. Unknown examples are classified using the magnetic forces recorded in the trained model by various magnets/classes. When compared to existing classifiers, the proposed MF classifier achieves comparable classification accuracy, according to the experimental results utilizing 28 different datasets. More importantly, we found that the proposed MF classifier is significantly faster than all other classifiers tested, particularly when applied to Big datasets and hence could be a viable option for structured Big data classification with some optimization. © 2013 IEEE.","Artificial intelligence; classification algorithms; data mining; machine learning; supervised learning","Big data; Classification (of information); Iron; Magnetism; Supervised learning; Classification algorithm; Classification methods; Data classification; Force; Machine learning literature; Magnetic force; Novel methods; Optimal classifiers; Specific class; Training data; Data mining",Article,Scopus,2-s2.0-85123283345
"Luo S., Lu J.","57422175900;57422320900;","GFNet: A Gradient Information Compensation-Based Face Super-Resolution Network",2022,"IEEE Access","10",,,"8073","8080",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123282923&doi=10.1109%2fACCESS.2022.3143499&partnerID=40&md5=550c6fe5d93f606c81ab7c9ea6114c69","Face super-resolution (FSR) is defined as the generation of high-resolution face images from low-resolution face images. Existing FSR approaches usually improve the performance by combining deep learning with additional tasks such as face parsing and landmark prediction. However, the additional data requires manual labeling, and facial landmark heatmaps and parsing maps cannot represent the intrinsic geometric structure of facial components. In this paper, we introduce a FSR network based on gradient information compensation named GFNet, which consists of feature residual blocks (FRBs) and gradient extraction blocks (GEBs). Specifically, the GEB constructs pixel-level gradient maps directly from the feature maps without requiring data labels and extracts gradient features to compensate for the missing high-frequency components in the face features; the FRB extracts the face features in the network. Furthermore, we introduced a feature fusion mechanism between the GEB and the FRB, which fuses the face features with the gradient features. We evaluate the performance of proposed network on the two public datasets: CelebA-HQ dataset and Helen dataset. Experimental results show that the proposed method is able to reconstruct fine face images, which outperforms the other state-of-the-art methods such as SRResnet, FSRNet, and MSFSR. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/","Data mining; Faces; Feature extraction; Image reconstruction; Licenses; Superresolution; Task analysis","Computer vision; Deep learning; Face features; Face hallucination; Face images; Face super-resolution; Features fusions; Gradient feature; Gradient informations; High resolution; Performance; Superresolution; Optical resolving power",Article,Scopus,2-s2.0-85123282923
"Shafqat W., Byun Y.-C.","57203760538;8897891700;","A Hybrid GAN-Based Approach to Solve Imbalanced Data Problem in Recommendation Systems",2022,"IEEE Access","10",,,"11036","11047",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123274801&doi=10.1109%2fACCESS.2022.3141776&partnerID=40&md5=330e9079712f5162d16675657cf362eb","With the advent of information technology, the amount of online data generation has been massive. Recommendation systems have become an effective tool in filtering information and solving the problem of information overload. Machine learning algorithms to build these recommendation systems require well-balanced data in terms of class distribution, but real-world datasets are mostly imbalanced in nature. Imbalanced data imposes a classifier to focus more on the majority class, neglecting other classes of interests and thus hindering the predictive performance of any classification model. There exist many traditional techniques for oversampling minority classes. Still, generative adversarial networks (GAN) have been showing excellent results in generating realistic synthetic tabular data that keeps the probability distribution of the original data intact. In this paper, we propose a hybrid GAN approach to solve the data imbalance problem to enhance recommendation systems' performance. We implemented conditional Wasserstein GAN with gradient penalty to generate tabular data containing both numerical and categorical values. We also augmented auxiliary classifier loss to enforce the model to explicitly generate data belonging to the minority class. We designed the discriminator architecture with the concept of PacGAN to receive m-packed samples as input instead of a single input. This inclusion of the PacGAN architecture eliminated the mode collapse problem in our proposed model. We did a two-fold evaluation of our model. Firstly based on the quality of the generated data and secondly on how different recommendation models perform using the generated data compared to original data. © 2013 IEEE.","condition GAN; GAN; imbalanced data; oversampling; PacGAN; recommendation systems; synthetic data; WGAN-GP","Data structures; Generative adversarial networks; Internet protocols; Learning algorithms; Network architecture; Online systems; Probability distributions; Recommender systems; Supervised learning; Condition; Condition generative adversarial network; Generator; Imbalanced data; IP-network; Over sampling; PacGAN; Synthetic data; Tabular data; WGAN-GP; Data mining",Article,Scopus,2-s2.0-85123274801
"You Y.","57199499135;","Data Mining of Regional Economic Analysis Based on Mobile Sensor Network Technology",2022,"Journal of Sensors","2022",,"3415055","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123262349&doi=10.1155%2f2022%2f3415055&partnerID=40&md5=d584651b8c13332541725c96a75aaa8a","With the continuous development of regional economy, the difference of regional economy has also aroused the attention of all walks of life. Due to the limitations of the traditional research methods, the research results are relatively simple and unable to conduct a more comprehensive analysis. The traditional methods include the following: (1) analyze the evolution of regional logistics based on the location Gini coefficient and location quotient of GIS, and reflect the situation of industrial agglomeration from the annual change curve of the location Gini coefficient; (2) use SPSS12.0 software to perform multivariate or event factors, and analyze and calculate the factor score to sum up several principal component factors; and (3) the production function analysis method is used to measure the economies of scale and agglomeration. As an extension, the relationship between the estimated total output and the agglomeration index of the factor input to measure the uniform state of the industrial distribution department is an effective measurement method for the agglomeration economy. In order to promote the sustainable development of regional economy, this paper analyzes the regional economy comprehensively based on the emerging mobile sensor network technology and data mining technology. Firstly, this paper analyzes the location technology of mobile sensor networks based on sequential Monte Carlo, selects the C-means clustering method which is suitable for economic large-sample clustering analysis, and constructs a complete data mining model. Then, the model is used to analyze the economic, social, natural, and educational science and technology indicators of a certain region from 2015 to 2019. The results show that the first principal component weight of economic indicators is the highest proportion of fiscal revenue, which is 0.986. This shows that the role of fiscal revenue in economic indicators is greater. The main index of urban consumption is 72.0, which is the highest. This shows that the population growth rate and the average consumption of urban households in social indicators play a greater role. The first principal component of natural index has the highest weight of pollution emission, which is 0.47, while the second principal component has the highest weight of total energy consumption, which is 0.48. This shows that the pollution emissions and total energy consumption in the natural indicators play a greater role. In the educational science and technology index, the first principal component weight is the highest, which is 0.61. This shows that the education funds play an important role in educational science and technology indicators. Therefore, the data mining model based on mobile sensor network technology can comprehensively and accurately analyze various indicators of regional economy. © 2022 Yucong You.",,"Cluster analysis; Data mining; Economic analysis; Economic and social effects; Energy utilization; Engineering education; Environmental technology; Location; Monte Carlo methods; Pollution; Population statistics; Principal component analysis; Sensor networks; Data mining models; Educational science; First principal components; Gini coefficients; Mobile sensors; Paper analysis; Principal Components; Regional economy; Science and technology indicators; Sensor network technology; Regional planning",Article,Scopus,2-s2.0-85123262349
"Guan X., Xin H., Xu M., Ji J., Li J.","57077613100;56865347500;57421025200;57420915500;36910769800;","The Role and Mechanism of SIRT6 in Regulating Phenotype Transformation of Vascular Smooth Muscle Cells in Abdominal Aortic Aneurysm",2022,"Computational and Mathematical Methods in Medicine","2022",,"3200798","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123252104&doi=10.1155%2f2022%2f3200798&partnerID=40&md5=94acdf8dd4b321997d7191d6112481bd","Background. Data mining of current gene expression databases has not been previously performed to determine whether sirtuin 6 (SIRT6) expression participates in the pathological process of abdominal aortic aneurysm (AAA). The present study was aimed at investigating the role and mechanism of SIRT6 in regulating phenotype transformation of vascular smooth muscle cells (VSMC) in AAA. Methods. Three gene expression microarray datasets of AAA patients in the Gene Expression Omnibus (GEO) database and one dataset of SIRT6-knockout (KO) mice were selected, and the differentially expressed genes (DEGs) were identified using GEO2R. Furthermore, Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) analyses of both the AAA-related DEGs and the SIRT6-related DEGs were conducted. Results. GEO2R analysis showed that the expression of SIRT6 was downregulated for three groups and upregulated for one group in the three datasets, and none of them satisfied statistical significance. There were top 5 DEGs (KYNU, NPTX2, SCRG1, GRK5, and RGS5) in both of the human AAA group and SIRT6-KO mouse group. Top 25 ontology of the SIRT6-KO-related DEGs showed that several pathways including tryptophan catabolic process to kynurenine and negative regulation of cell growth were enriched in the tissues of thickness aortic wall biopsies of AAA patients. Conclusions. Although SIRT6 mRNA level itself did not change among AAA patients, SIRT6 may play an important role in regulating several signaling pathways with significant association with AAA, suggesting that SIRT6 mRNA upregulation is a protective factor for VSMC against AAA. © 2022 Xiaomei Guan et al.",,"Amino acids; Biopsy; Blood vessels; Cell proliferation; Data mining; Gene Ontology; Mammals; Muscle; 'current; Abdominal aortic aneurysms; Differentially expressed gene; Gene expression microarray; Genes expression; Knockout mice; Microarray dataset; Pathological process; Sirtuin; Vascular Smooth Muscle Cells; Gene expression",Article,Scopus,2-s2.0-85123252104
"Samuri S.M., Nova T.V., Bahbibirahmatullah, Li W.S., Al-Qaysi Z.T.","56487849100;57421536100;57421429500;57421429600;57202647402;","CLASSIFICATION MODEL FOR BREAST CANCER MAMMOGRAMS",2022,"IIUM Engineering Journal","23","1",,"187","199",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123250299&doi=10.31436%2fIIUMEJ.V23I1.1825&partnerID=40&md5=72762b0f98f15b0530065dfbd15d5f3c","Machine learning has been the topic of interest in research related to early detection of breast cancer based on mammogram images. In this study, we compare the performance results from three (3) types of machine learning techniques: 1) Naïve Bayes (NB), 2) Neural Network (NN) and 3) Support Vector Machine (SVM) with 2000 digital mammogram images to choose the best technique that could model the relationship between the features extracted and the state of the breast (‘Normal’ or ‘Cancer’). Grey Level Co-occurrence Matrix (GLCM) which represents the two dimensions of the level variation gray in the image is used in the feature extraction process. Six (6) attributes consist of contrast, variance, standard deviation, kurtosis, mean and smoothness were computed as feature extracted and used as the inputs for the classification process. The data has been randomized and the experiment has been repeated for ten (10) times to check for the consistencies of the performance of all techniques. 70% of the data were used as the training data and another 30% used as testing data. The result after ten (10) experiments show that, Support Vector Machine (SVM) gives the most consistent results in correctly classifying the state of the breast as ‘Normal’ or ‘Cancer’, with the accuracy of 99.4%, in training and 98.76% in testing. The SVM classification model has outperformed NN and NB model in the study, and it shows that SVM is a good choice for determining the state of the breast at the early stage. © 2022. IIUM Engineering Journal. All Rights Reserved.","Breast cancer detection; Data mining; Data-driven modelling; Machine learning; Mammogram images",,Article,Scopus,2-s2.0-85123250299
"Fu X.","57421418100;","Research on Artificial Intelligence Classification and Statistical Methods of Financial Data in Smart Cities",2022,"Computational Intelligence and Neuroscience","2022",,"9965427","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123247206&doi=10.1155%2f2022%2f9965427&partnerID=40&md5=bfcd2a9a477fcdb3090482d3d6a02724","In order to improve the effect of financial data classification and extract effective information from financial data, this paper improves the data mining algorithm, uses linear combination of principal components to represent missing variables, and performs dimensionality reduction processing on multidimensional data. In order to achieve the standardization of sample data, this paper standardizes the data and combines statistical methods to build an intelligent financial data processing model. In addition, starting from the actual situation, this paper proposes the artificial intelligence classification and statistical methods of financial data in smart cities and designs data simulation experiments to conduct experimental analysis on the methods proposed in this paper. From the experimental results, the artificial intelligence classification and statistical method of financial data in smart cities proposed in this paper can play an important role in the statistical analysis of financial data. © 2022 Xuezhong Fu.",,"Artificial intelligence; Classification (of information); Data handling; Data mining; Finance; Statistics; Data classification; Data mining algorithm; Data processing models; Dimensionality reduction; Financial data; Linear combinations; Multidimensional data; Principal Components; Reduction processing; Sample data; Smart city; algorithm; artificial intelligence; city; computer simulation; intelligence; Algorithms; Artificial Intelligence; Cities; Computer Simulation; Intelligence",Article,Scopus,2-s2.0-85123247206
"Yuan X.","57420274100;","Construction of Moral Education Evaluation Model Based on Quality Cultivation of College Students",2022,"Scientific Programming","2022",,"5641782","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123223072&doi=10.1155%2f2022%2f5641782&partnerID=40&md5=c1608a8306470770e8bc52ebd377c15b","Contemporary young college students are greatly impacted in the aspects of moral cognition and moral choice, which results in the weak moral will of some college students, vague moral concepts, and weak ideals and beliefs, which seriously affect the formation and development of college students' moral quality. Therefore, the moral education evaluation model based on college students' quality cultivation is constructed. Firstly, the present situation and defects of college students' quality training are analyzed. Based on this, association rules in data mining method are constructed and introduced to extract valuable knowledge hidden in the data to assist education managers to make effective decisions and improve management level. Finally, the evaluation index is selected and the weighted principal component TOP-SIS model is constructed to realize the evaluation of moral education based on college students' quality cultivation. The experimental results show that the evaluation results of the model are consistent with the actual situation, high degree of fit and freedom, and good practical performance. © 2022 Xiaolin Yuan.",,"Data mining; Education computing; Quality control; College students; Data mining methods; Evaluation index; Evaluation models; Management level; Model-based OPC; Moral education evaluations; Present situation; Quality training; Student quality; Students",Article,Scopus,2-s2.0-85123223072
"Zou L., Shu S., Lin X., Lin K., Zhu J., Li L.","57419759300;57419581900;57412826000;57420103500;35147395700;57190406237;","Passenger Flow Prediction Using Smart Card Data from Connected Bus System Based on Interpretable XGBoost",2022,"Wireless Communications and Mobile Computing","2022",,"5872225","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123204515&doi=10.1155%2f2022%2f5872225&partnerID=40&md5=cf9b4ac74876c6ec3206e9329d54f80f","Bus passenger flow prediction is a critical component of advanced transportation information system for public traffic management, control, and dispatch. With the development of artificial intelligence, many previous studies attempted to apply machine learning models to extract comprehensive correlations from transit networks to improve passenger flow prediction accuracy, given that the variety and volume of traffic data have been easily obtained. The passenger flow on a station is highly affected by various factors such as the previous time step, peak hours or nonpeak hours, and extracting the key features from the data is essential for a passenger flow prediction model. Although the neural networks, k-nearest neighbor, and some deep learning models have been adopted to mine the temporal correlations of the passenger flow data, the lack of interpretability of the influenced variables is still a big problem. Classical tree-based models can mine the correlations between variables and rank the importance of each variable. In this study, we presented a method to extract passenger flow of different routes on the station and implemented a XGBoost model to find the contributions of variables to the prediction of passenger flow. Comparing to benchmark models, the proposed model can reach state-of-the-art prediction accuracy and computational efficiency on the real-world dataset. Moreover, the XGBoost model can interpret the predicted results. It can be seen that period is the most important variable for the passenger flow prediction, and so the management of buses during peak hours should be improved. © 2022 Liang Zou et al.",,"Buses; Data mining; Deep learning; Forecasting; Information management; Nearest neighbor search; Smart cards; Advanced transportation information systems; Bus systems; Critical component; Machine learning models; Management control; Passenger flow predictions; Passenger flows; Prediction accuracy; Traffic management; Transit networks; Computational efficiency",Article,Scopus,2-s2.0-85123204515
"Mehmood M., Alshammari N., Alanazi S.A., Ahmad F.","57419583700;56901409900;57211018616;57205201893;","Systematic Framework to Predict Early-Stage Liver Carcinoma Using Hybrid of Feature Selection Techniques and Regression Techniques",2022,"Complexity","2022",,"7816200","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123192829&doi=10.1155%2f2022%2f7816200&partnerID=40&md5=511560208ab03058e3016090df7f4740","The liver is the human body's mandatory organ, but detecting liver disease at an early stage is very difficult due to the hiddenness of symptoms. Liver diseases may cause loss of energy or weakness when some irregularities in the working of the liver get visible. Cancer is one of the most common diseases of the liver and also the most fatal of all. Uncontrolled growth of harmful cells is developed inside the liver. If diagnosed late, it may cause death. Treatment of liver diseases at an early stage is, therefore, an important issue as is designing a model to diagnose early disease. Firstly, an appropriate feature should be identified which plays a more significant part in the detection of liver cancer at an early stage. Therefore, it is essential to extract some essential features from thousands of unwanted features. So, these features will be mined using data mining and soft computing techniques. These techniques give optimized results that will be helpful in disease diagnosis at an early stage. In these techniques, we use feature selection methods to reduce the dataset's feature, which include Filter, Wrapper, and Embedded methods. Different Regression algorithms are then applied to these methods individually to evaluate the result. Regression algorithms include Linear Regression, Ridge Regression, LASSO Regression, Support Vector Regression, Decision Tree Regression, Multilayer Perceptron Regression, and Random Forest Regression. Based on the accuracy and error rates generated by these Regression algorithms, we have evaluated our results. The result shows that Random Forest Regression with the Wrapper Method from all the deployed Regression techniques is the best and gives the highest R2-Score of 0.8923 and lowest MSE of 0.0618. © 2022 Marium Mehmood et al.",,"Data mining; Decision trees; Diagnosis; Diseases; Feature extraction; Filtration; Soft computing; Common disease; Features selection; Human bodies; Liver disease; Loss of energy; Random forests; Regression algorithms; Regression techniques; Selection techniques; Systematic framework; Regression analysis",Article,Scopus,2-s2.0-85123192829
"Lai M.","57419925600;","Smart Financial Management System Based on Data Ming and Man-Machine Management",2022,"Wireless Communications and Mobile Computing","2022",,"2717982","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123188368&doi=10.1155%2f2022%2f2717982&partnerID=40&md5=e3ddf8e4bdce5b3602fde4b6e3c8b894","To begin, the architecture of an intelligent financial management system is thoroughly investigated, and a new architecture of an intelligent financial management support system based on data mining is developed. Second, it goes over the definition and structure of a data warehouse and data mining, as well as how to use data mining strategy and technology in financial management. Data mining in relation to technology is being investigated, as is the development of an intelligent data mining algorithm. The flaws of the intelligent data mining algorithm are discovered through an analysis and summary of the algorithm, and an improved algorithm is proposed to address the flaws. Related mining experiments are carried out on the improved algorithm, and the experiment shows that it has certain advantages. Then, using an intelligent forecasting financial management decision as an example, the intelligent financial management based on data mining is thoroughly investigated, the basic design framework for intelligent financial management is established, and the application of a data mining model in decision support system is introduced. © 2022 Maotao Lai.",,"Artificial intelligence; Computer architecture; Data warehouses; Decision making; Decision support systems; Finance; Information management; Data mining algorithm; Financial management systems; Financial managements; Improved * algorithm; Intelligent data minings; Intelligent forecasting; Machine managements; Man machines; Management decisions; Management support systems; Data mining",Article,Scopus,2-s2.0-85123188368
"Wang C., Yang C., Wang X., Zhou G., Chen C., Han G.","57419589200;57419229000;57420283500;57420283600;57188965029;57419589300;","ceRNA Network and Functional Enrichment Analysis of Preeclampsia by Weighted Gene Coexpression Network Analysis",2022,"Computational and Mathematical Methods in Medicine","2022",,"5052354","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123167257&doi=10.1155%2f2022%2f5052354&partnerID=40&md5=58cc41460a1cc8a194cb45b7622b719d","Background. Preeclampsia (PE) is a multisystemic syndrome which has short- and long-term risk to mothers and children and has pluralistic etiology. Objective. This study is aimed at constructing a competitive endogenous RNA (ceRNA) network for pathways most related to PE using a data mining strategy based on weighted gene coexpression network analysis (WGCNA). Methods. We focused on pathways involving hypoxia, angiogenesis, and epithelial mesenchymal transition according to the gene set variation analysis (GSVA) scores. The gene sets of these three pathways were enriched by gene set enrichment analysis (GSEA). WGCNA was used to study the underlying molecular mechanisms of the three pathways in the pathogenesis of PE by analyzing the relationship among pathways and genes. The soft threshold power (β) and topological overlap matrix allowed us to obtain 15 modules, among which the red module was chosen for the downstream analysis. We chose 10 hub genes that satisfied |log2Fold Change|>2 and had a higher degree of connectivity within the module. These candidate genes were subsequently confirmed to have higher gene significance and module membership in the red module. Coexpression networks were established for the hub genes to unfold the connection between the genes in the red module and PE. Finally, ceRNA networks were constructed to further clarify the underlying molecular mechanism involved in the occurrence of PE. 56 circRNAs, 17 lncRNAs, and 20 miRNAs participated in the regulation of the hub genes. Coagulation factor II thrombin receptor (F2R) and lumican (LUM) were considered the most relevant genes, and ceRNA networks of them were constructed. Conclusion. The microarray data mining process based on bioinformatics methods constructed lncRNA and miRNA networks for ten hub genes that were closely related to PE and focused on ceRNAs of F2R and LUM finally. The results of our study may provide insight into the mechanisms underlying PE occurrence. © 2022 Chenxu Wang et al.",,"Data mining; RNA; Angiogenesis; Co-expression networks; Epithelial-mesenchymal transition; Functional enrichment analysis; Gene sets; Lumican; Molecular mechanism; Preeclampsia; Short-term and long-term risks; Variation analysis; Genes",Article,Scopus,2-s2.0-85123167257
"Chang Y., Meng S., Chao H.","57419405500;57419405600;57419761800;","Innovative Research on Teaching Method of Taekwondo in College Elective Courses under the Background of Big Data",2022,"Applied Bionics and Biomechanics","2022",,"2329952","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123162354&doi=10.1155%2f2022%2f2329952&partnerID=40&md5=f4f0f5da1f23e249d09e77eeae086b84","In the new stage of the new century, a new technological revolution is coming quietly. This revolution is represented by ""data.""The application of ""big data (B D A)""technology is causing changes in all walks of life, and the use of ""B D A research methods""in the education field will inevitably become a trend. The purpose of this article is an innovative research on the teaching methods of Taekwondo based on the background of B D A in a college elective course. This paper first introduces the core technology of the database by summarizing the basic theory of the database. Based on the current situation of elective Taekwondo teaching in contemporary universities, analyze the current problems and deficiencies and conduct innovative research on college elective Taekwondo teaching methods combined with Beidou technology. This paper systematically expounds the practical connection, method innovation, and implementation path between BDA technology and college elective Taekwondo teaching methods and compares the traditional Taekwondo teaching methods based on BDA technology. Experimental research shows that compared with traditional Taekwondo teaching methods, the performance of university Taekwondo teaching based on data mining (D M I) in the context of B D A is more than 20% higher, which fully reflects its feasibility and the innovation of traditional Taekwondo teaching methods needs to be solved urgently. © 2022 Yahui Chang et al.",,"Big data; Data mining; Engineering education; Radio navigation; Basic theory; Core technology; Data technologies; Education field; Elective course; Innovative research; Research method; Taekwondo; Teaching methods; Technological revolution; Teaching; article; big data; data mining; feasibility study; human; human experiment; taekwondo; teaching",Article,Scopus,2-s2.0-85123162354
"Wu X.","57419926700;","Performance Appraisal Management System of University Administrators Based on Hybrid Cloud",2022,"Scientific Programming","2022",,"9326563","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123162200&doi=10.1155%2f2022%2f9326563&partnerID=40&md5=9133853a49ce8b3577752f0012666f66","Aiming at the problems of poor functionality, high occupancy, and low real-time performance of the currently designed performance appraisal management system for university administrators, a performance appraisal management system for university administrators based on hybrid cloud is designed. According to the characteristics of hybrid cloud technology, the overall functional requirements and feasibility of the system are analyzed, and the assessment scheme of administrative personnel according to the provisions of relevant documents is analyzed. The performance appraisal index system is designed using an analytic hierarchy process. Using the hybrid cloud architecture and B/S mode based on each component as a service model and J2EE development framework, the basic information management subsystem of administrative personnel, performance appraisal information management subsystem, information analysis and data mining subsystem, and platform system management subsystem are developed. Using XML technology and database technology, the system is integrated with the performance appraisal management system of administrators in colleges and universities. Through the design of data flow diagram and E-R diagram, the design of performance appraisal management system for university administrators based on hybrid cloud is realized. The experimental results show that the proposed method has good functionality, can effectively reduce the system occupancy, and can improve the real-time performance of the system. © 2022 XiuQing Wu.",,"Data flow analysis; Data flow graphs; Data mining; XML; Assessment scheme; Cloud technologies; Functional requirement; High-low; Hybrid clouds; Information Management Sub-Systems; Management systems; Performance appraisal; Real time performance; Relevant documents; Human resource management",Article,Scopus,2-s2.0-85123162200
"Wu F., Liu X., Wang Y., Li X., Zhou M.","57226022254;57226021180;57226032400;57419935800;57419765900;","Research on Evaluation Model of Hospital Informatization Level Based on Decision Tree Algorithm",2022,"Security and Communication Networks","2022",,"3777474","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123161123&doi=10.1155%2f2022%2f3777474&partnerID=40&md5=b979d52b8478ab1fff26c1bc798a62f1","In order to improve the weight calculation accuracy of hospital informatization level evaluation and shorten the evaluation time, a research method of hospital informatization level evaluation model based on the decision tree algorithm is proposed. Using the decision tree algorithm combining fuzzy theory and ID3, the decision tree is constructed to analyze the hospital information data. By means of questionnaire survey, expert experience, mathematical statistics, and in-depth interview, information facilities construction, information resources construction, information scientific research application, management information, and information guarantee are selected as the nodes of the decision tree to evaluate the hospital information level. Construct the structural equation model, standardize the data, extract the weight of each evaluation index, and complete the evaluation of hospital informatization level. The experimental results show that the weight calculation results of this method are basically consistent with the actual results, and the evaluation efficiency is improved. © 2022 Fenglang Wu et al.",,"Data mining; Decision trees; Information management; Statistics; Surveys; Calculation accuracy; Construction information; Decision-tree algorithm; Evaluation models; Fuzzy ID3; Hospital information; Informatization; Model-based OPC; Research method; Weight calculation; Hospitals",Article,Scopus,2-s2.0-85123161123
"Al-Juboori A.M.","57188683757;","Solving Complex Rainfall-Runoff Processes in Semi-Arid Regions Using Hybrid Heuristic Model",2022,"Water Resources Management","36","2",,"717","728",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123102609&doi=10.1007%2fs11269-021-03053-5&partnerID=40&md5=ee06f3c9acc389eafe9620d8ac27b3fc","In the current research, a hybrid model was proposed to solve the complexity of rainfall-runoff models in semi-arid regions. The proposed hybrid model structure consists of linking two data mining models, namely, Group Method of Data Handling (GMDH) and Generalized Linear Model (GLM). The proposed hybrid model structure consists of two phases. The GMDH model was used in the first phase of the hybrid model to predict daily streamflow. The first phase consists of two sections. In the first section a predictive model is developed using the time series of the daily streamflow. In the second section the rainfall-runoff model was developed. The outputs of the first phase of the hybrid model are used as inputs to the second phase of the hybrid model. The second phase of the hybrid model was developed using the GLM model. The Gomel River in Iraq was selected as a case study. The daily rainfall data and daily streamflow data for the period from January 1, 2004 to December 19, 2016 were used to train and validate the model. The results proved the accuracy of the proposed hybrid model in estimating the daily streamflow of the study area, where the value of R2 was 0.92 in the training period and 0.88 in the validation period of the model. © 2021, The Author(s), under exclusive licence to Springer Nature B.V.","And Hybrid model; GLM; GMDH; Rainfall-runoff model","Arid regions; Data handling; Data mining; Rain; Runoff; And hybrid model; Generalized linear model; Group method of data handling; Hybrid heuristics; Hybrid model; Hybrid model structures; Rainfall - Runoff modelling; Rainfall-runoff process; Second phase; Semi-arid region; Stream flow; model validation; rainfall-runoff modeling; semiarid region; streamflow; Iraq",Article,Scopus,2-s2.0-85123102609
"Wang X., Zhang W.","55203956200;56189136000;","Improvement of Students’ Autonomous Learning Behavior by Optimizing Foreign Language Blended Learning Mode",2022,"SAGE Open","12","1",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123078898&doi=10.1177%2f21582440211071108&partnerID=40&md5=1f95bcc286dacfbadafe1a93b4d8aa93","Given the significance of cultivating students’ autonomous learning ability, there is a need to develop an instructional model that can improve students’ awareness and behavior of autonomous learning, as well as to explore the effectiveness and optimization of this model effectively. Taking college English course as a case study, this paper constructs a blended learning mode based on SPOC, which combines advantages of online and offline teaching. 15 types of nonredundant sets resulting from 500 questionnaires has been explored, and the optimal factor combinations have been found out from 15 types with the technology of data mining to optimize the mode constructed previously. Optimized blended learning mode, emphasizing the optimal factors more, has been applied to College English curriculum design and teaching practice in China. Surveys of students’ achievement and autonomous learning behavior have been conducted after experiment. The results of the research indicate that the optimized blended learning mode will stimulate foreign language learners’ learning motivation, cultivate their autonomous learning ability, so as to construct and improve their autonomous learning behavior further. © The Author(s) 2022.","autonomous learning behavior; blended learning mode; data mining; foreign language teaching; teaching optimization",,Article,Scopus,2-s2.0-85123078898
"Pakdaman M., Babaeian I., Javanshiri Z., Falamarzi Y.","35303414900;42660970600;57216629795;56121611000;","European Multi Model Ensemble (EMME): A New Approach for Monthly Forecast of Precipitation",2022,"Water Resources Management","36","2",,"611","623",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123063178&doi=10.1007%2fs11269-021-03042-8&partnerID=40&md5=cab171f479cc93c686fc6f6a6684dc57","Regarding the ability of data mining algorithms for post-processing the output of climate models, and on the other hand, the successful application of multi-model ensemble approaches in climate forecasts, in this paper, some important data mining algorithms are evaluated for the monthly forecast of precipitation over Iran. For this purpose, four European climate models, from DWD, ECMWF, CMCC and Meteo-France, with six lead times, are used to be post-processed by applying four different algorithms including artificial neural networks, support vector regression, decision tree and random forests. Based on the proposed approach, 72 different models are provided for 12 months, each month with six lead times. The approach is applied for the monthly forecast of precipitation over Iran. According to the results, the neural network and random forest methods performed better than the decision tree and the support vector machine. This advantage preserved for all months of the year. Also, the proposed multi-model approach outperformed any of the individual European models. © 2021, The Author(s), under exclusive licence to Springer Nature B.V.","Artificial neural networks; Monthly forecast; Persian Gulf; Post-processing; Random forests; Support vector regression","Climate models; Data mining; Decision trees; Forecasting; Support vector machines; Climate forecasts; Data mining algorithm; Ensemble approaches; Leadtime; Monthly forecast; Multi-model ensemble; New approaches; Persian Gulf; Post-processing; Support vector regressions; Neural networks",Article,Scopus,2-s2.0-85123063178
"Fang J., Li J.","57417098500;57221572671;","Research on Classification of Primary Liver Cancer Syndrome Based on Data Mining Technology",2022,"Journal of Healthcare Engineering","2022",,"2629509","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123052929&doi=10.1155%2f2022%2f2629509&partnerID=40&md5=db5c60732631b72e95aa731d212efe1f","This study is based on the analysis of the status quo of the research on liver cancer syndromes, starting with the clinical objective and true four-diagnosis information of TCM inpatients with primary liver cancer, using computer data mining technology to analyze and summarize the syndrome rules from the bottom to the top. Let the data itself show the essence of liver cancer syndrome. First, with the help of hierarchical cluster analysis, we can understand the general characteristics through the rough preliminary classification of the four-diagnosis information of liver cancer patients. Then, with the help of the emerging and mature hidden structure model analysis in recent years, through data modeling, the classification of common syndromes of liver cancer and the corresponding relationship with the four-diagnosis information are comprehensively analyzed. Finally, considering the inherent shortcomings of implicit structure and hierarchical clustering based on the assumption that there is a unique one-to-one correspondence between the four diagnostic information factors and the class (or hidden class) when classifying, we plan to use factor analysis and joint cluster analysis, as supplementary means to further explore the classification of liver cancer syndromes and the corresponding relationship with the four-diagnosis information. © 2022 Jiwei Fang and Jianfeng Li.",,"Clinical research; Cluster analysis; Computer aided diagnosis; Data mining; Diseases; Hierarchical systems; Cancer patients; Data mining technology; Diagnosis information; Hidden structures; Hierarchical cluster analysis; Liver cancers; Modeling analyzes; Primary liver cancers; Status quo; Structure models; Classification (of information); adult; aged; Article; artificial intelligence; cancer classification; cancer diagnosis; data mining; factor analysis; female; hierarchical clustering; hospital patient; human; liver cancer; major clinical study; male; mathematical model; nuclear magnetic resonance imaging; patient information; positron emission tomography; probability; qi stagnation; single photon emission computed tomography",Article,Scopus,2-s2.0-85123052929
"He P., Zhang B., Shen S.","57417155500;57417382500;57417110300;","Effects of Out-of-Hospital Continuous Nursing on Postoperative Breast Cancer Patients by Medical Big Data",2022,"Journal of Healthcare Engineering","2022",,"9506915","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123045463&doi=10.1155%2f2022%2f9506915&partnerID=40&md5=e96857f6a3c0dd3b277ac4e60f2f1d19","This study aimed to explore the application value of the intelligent medical communication system based on the Apriori algorithm and cloud follow-up platform in out-of-hospital continuous nursing of breast cancer patients. In this study, the Apriori algorithm is optimized by Amazon Web Services (AWS) and graphics processing unit (GPU) to improve its data mining speed. At the same time, a cloud follow-up platform-based intelligent mobile medical communication system is established, which includes the log-in, my workstation, patient records, follow-up center, satisfaction management, propaganda and education center, SMS platform, and appointment management module. The subjects are divided into the control group (routine telephone follow-up, 163) and the intervention group (continuous nursing intervention, 216) according to different nursing methods. The cloud follow-up platform-based intelligent medical communication system is used to analyze patients' compliance, quality of life before and after nursing, function limitation of affected limb, and nursing satisfaction under different nursing methods. The running time of Apriori algorithm is proportional to the data amount and inversely proportional to the number of nodes in the cluster. Compared with the control group, there are statistical differences in the proportion of complete compliance data, the proportion of poor compliance data, and the proportion of total compliance in the intervention group (P<0.05). After the intervention, the scores of the quality of life in the two groups are statistically different from those before treatment (P<0.05), and the scores of the quality of life in the intervention group were higher than those in the control group (P<0.05). The proportion of patients with limited and severely limited functional activity of the affected limb in the intervention group is significantly lower than that in the control group (P<0.05). The satisfaction rate of postoperative nursing in the intervention group is significantly higher than that in the control group (P<0.001), and the proportion of basically satisfied and dissatisfied patients in the control group was higher than that in the intervention group (P<0.05). © 2022 Peijuan He et al.",,"Big data; Computer graphics; Computer graphics equipment; Data mining; Diseases; Hospitals; Learning algorithms; Nursing; Patient treatment; Program processors; Quality control; Web services; Amazon web services; Apriori algorithms; Breast Cancer; Cancer patients; Communications systems; Control groups; Follow up; Follow-up platform; Medical communication; Quality of life; Graphics processing unit; Amazon Web Services; Apriori algorithm; Article; big data; breast cancer; cancer patient; comparative study; data mining; follow up; human; medical record; mobile application; nursing intervention; patient satisfaction; quality of life",Article,Scopus,2-s2.0-85123045463
"Qasim R., Bangyal W.H., Alqarni M.A., Ali Almazroi A.","57370937800;36449048100;56728454300;50860912200;","A Fine-Tuned BERT-Based Transfer Learning Approach for Text Classification",2022,"Journal of Healthcare Engineering","2022",,"3498123","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123044999&doi=10.1155%2f2022%2f3498123&partnerID=40&md5=0e2c27fdbd014f440e93cbe51de01cdc","Text Classification problem has been thoroughly studied in information retrieval problems and data mining tasks. It is beneficial in multiple tasks including medical diagnose health and care department, targeted marketing, entertainment industry, and group filtering processes. A recent innovation in both data mining and natural language processing gained the attention of researchers from all over the world to develop automated systems for text classification. NLP allows categorizing documents containing different texts. A huge amount of data is generated on social media sites through social media users. Three datasets have been used for experimental purposes including the COVID-19 fake news dataset, COVID-19 English tweet dataset, and extremist-non-extremist dataset which contain news blogs, posts, and tweets related to coronavirus and hate speech. Transfer learning approaches do not experiment on COVID-19 fake news and extremist-non-extremist datasets. Therefore, the proposed work applied transfer learning classification models on both these datasets to check the performance of transfer learning models. Models are trained and evaluated on the accuracy, precision, recall, and F1-score. Heat maps are also generated for every model. In the end, future directions are proposed. © 2022 Rukhma Qasim et al.",,"Automation; Classification (of information); Coronavirus; Data mining; Learning systems; Social networking (online); Text processing; Data mining tasks; Entertainment industry; Filtering process; Information retrieval problems; Learning approach; Multiple tasks; Social media; Targeted marketing; Text classification; Transfer learning; Natural language processing systems; albert base-v2 classification algorithm; Article; bart large classification algorithm; bert base classification algorithm; bert large classification algorithm; classification algorithm; comparative study; coronavirus disease 2019; data accuracy; deep learning; disinformation; distilbert classification algorithm; electra small classification algorithm; fine tuned bidirectional encoder representation from transformers; human; machine learning; roberta base classification algorithm; roberta large classification algorithm; social media; text classification; transfer of learning; xgboost neural network; xlm roberta base classification algorithm; natural language processing; COVID-19; Disinformation; Humans; Machine Learning; Natural Language Processing; SARS-CoV-2",Article,Scopus,2-s2.0-85123044999
"Gakii C., Mireji P.O., Rimiru R.","57205392670;16304767000;55027374700;","Graph Based Feature Selection for Reduction of Dimensionality in Next-Generation RNA Sequencing Datasets",2022,"Algorithms","15","1","21","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123044683&doi=10.3390%2fa15010021&partnerID=40&md5=336c8ac0b4733c92cce3b24365b9aa78","Analysis of high-dimensional data, with more features (p) than observations (N) (p > N), places significant demand in cost and memory computational usage attributes. Feature selection can be used to reduce the dimensionality of the data. We used a graph-based approach, principal component analysis (PCA) and recursive feature elimination to select features for classification from RNAseq datasets from two lung cancer datasets. The selected features were discretized for association rule mining where support and lift were used to generate informative rules. Our results show that the graph-based feature selection improved the performance of sequential minimal optimization (SMO) and multilayer perceptron classifiers (MLP) in both datasets. In association rule mining, features selected using the graph-based approach outperformed the other two feature-selection techniques at a support of 0.5 and lift of 2. The non-redundant rules reflect the inherent relationships between features. Biological features are usually related to functions in living systems, a relationship that cannot be deduced by feature selection and classification alone. Therefore, the graph-based featureselection approach combined with rule mining is a suitable way of selecting and finding associations between features in high-dimensional RNAseq data. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Association rule mining; Big data; Classification; Discretization; Feature selection","Association rules; Big data; Clustering algorithms; Data mining; Feature extraction; Graphic methods; Optimization; Principal component analysis; Discretizations; Features selection; Graph-based; Graph-based features; High dimensional data; Lung Cancer; Principal-component analysis; Recursive feature elimination; Reduction of dimensionality; Rule mining; Classification (of information)",Article,Scopus,2-s2.0-85123044683
"Guo Y., Chen S., Fu Y.H., Xiao Y., Wu W., Wang H., De Beurs K.","57206187965;57218847836;36918071700;57416316200;55503021500;56578885300;6506889738;","Comparison of Multi-Methods for Identifying Maize Phenology Using PhenoCams",2022,"Remote Sensing","14","2","244","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123023579&doi=10.3390%2frs14020244&partnerID=40&md5=aaae6e5aba0696eb2bd08b6b5a3e1caa","Accurately identifying the phenology of summer maize is crucial for both cultivar breeding and fertilizer controlling in precision agriculture. In this study, daily RGB images covering the entire growth of summer maize were collected using phenocams at sites in Shangqiu (2018, 2019 and 2020) and Nanpi (2020) in China. Four phenological dates, including six leaves, booting, heading and maturity of summer maize, were pre-defined and extracted from the phenocam-based images. The spectral indices, textural indices and integrated spectral and textural indices were calculated using the improved adaptive feature-weighting method. The double logistic function, harmonic analysis of time series, Savitzky–Golay and spline interpolation were applied to filter these indices and pre-defined phenology was identified and compared with the ground observations. The results show that the DLF achieved the highest accuracy, with the coefficient of determination (R2) and the root-mean-square error (RMSE) being 0.86 and 9.32 days, respectively. The new index performed better than the single usage of spectral and textural indices, of which the R2 and RMSE were 0.92 and 9.38 days, respectively. The phenological extraction using the new index and double logistic function based on the PhenoCam data was effective and convenient, obtaining high accuracy. Therefore, it is recommended the adoption of the new index by integrating the spectral and textural indices for extracting maize phenology using PhenoCam data. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Filtering methods; Maize phenological extraction; Spectral and textural indices","Biology; Data mining; Harmonic analysis; Interpolation; Mean square error; Time series analysis; Filtering method; High-accuracy; Logistics functions; Maize phenological extraction; Multi methods; Precision Agriculture; RGB images; Root mean square errors; Spectral and textural index; Summer maize; Extraction",Article,Scopus,2-s2.0-85123023579
"Klein A.Z., O'Connor K., Gonzalez-Hernandez G.","57201255461;56596185000;57212403040;","Toward Using Twitter Data to Monitor COVID-19 Vaccine Safety in Pregnancy: Proof-of-Concept Study of Cohort Identification",2022,"JMIR Formative Research","6","1","e33792","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123019509&doi=10.2196%2f33792&partnerID=40&md5=117d6f97d7ff5645f2e4fdf2ad1ddfaf","Background: COVID-19 during pregnancy is associated with an increased risk of maternal death, intensive care unit admission, and preterm birth; however, many people who are pregnant refuse to receive COVID-19 vaccination because of a lack of safety data. Objective: The objective of this preliminary study was to assess whether Twitter data could be used to identify a cohort for epidemiologic studies of COVID-19 vaccination in pregnancy. Specifically, we examined whether it is possible to identify users who have reported (1) that they received COVID-19 vaccination during pregnancy or the periconception period, and (2) their pregnancy outcomes. Methods: We developed regular expressions to search for reports of COVID-19 vaccination in a large collection of tweets posted through the beginning of July 2021 by users who have announced their pregnancy on Twitter. To help determine if users were vaccinated during pregnancy, we drew upon a natural language processing (NLP) tool that estimates the timeframe of the prenatal period. For users who posted tweets with a timestamp indicating they were vaccinated during pregnancy, we drew upon additional NLP tools to help identify tweets that reported their pregnancy outcomes. Results: We manually verified the content of tweets detected automatically, identifying 150 users who reported on Twitter that they received at least one dose of COVID-19 vaccination during pregnancy or the periconception period. We manually verified at least one reported outcome for 45 of the 60 (75%) completed pregnancies. Conclusions: Given the limited availability of data on COVID-19 vaccine safety in pregnancy, Twitter can be a complementary resource for potentially increasing the acceptance of COVID-19 vaccination in pregnant populations. The results of this preliminary study justify the development of scalable methods to identify a larger cohort for epidemiologic studies. © Ari Z Klein, Karen O'Connor, Graciela Gonzalez-Hernandez. Originally published in JMIR Formative Research (https://formative.jmir.org), 06.01.2022. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in JMIR Formative Research, is properly cited. The complete bibliographic information, a link to the original publication on https://formative.jmir.org, as well as this copyright and license information must be included.","COVID-19; COVID-19 vaccine; Data mining; Natural language processing; Pregnancy outcomes; Social media",,Article,Scopus,2-s2.0-85123019509
"Luo J., Qiu S., Pan X., Yang K., Tian Y.","9335634000;57416033500;7401930826;57416534500;57415909300;","Exploration of Spa Leisure Consumption Sentiment towards Different Holidays and Different Cities through Online Reviews: Implications for Customer Segmentation",2022,"Sustainability (Switzerland)","14","2","664","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123012542&doi=10.3390%2fsu14020664&partnerID=40&md5=1fcfdbb7e66f8e356fb5d6b03736c1cb","With the improvements in per capita disposable income, and an increase in work-related pressure, demand for leisure consumption such as foot bath spas is constantly increasing. Analysis of leisure consumption sentiment is of great importance for the leisure service industry—to meet customer needs, improve service quality and improve customer relationship management. However, traditional sentiment analysis approaches only aimed to ascertain the overall sentiment of the customer, which is less effective for analyzing customer satisfaction on account of customer size, different customer locations, and different leisure holidays. Sentiment analysis via online reviews can assist different businesses, including foot bath spa services, to better inform the development of customer segmentation strategies and ensure optimal customer relationship management. Hence, the objective of this paper is to explore foot bath spa leisure consumption sentiment towards different holidays and different cities by applying data mining via online reviews, so as to help optimize customer segmentation. A novel general framework and related sentiment analysis methods were proposed and then conducted through a collection of datasets from customers’ textual reviews of foot bath spa merchants in three cities in China on the Meituan social media platform. Findings confirm that the proposed general framework and methods can be used to gain insights into the swing characteristics of sentiment towards different holidays and different cities, to better develop customer segmentation according to the city-holiday emoticon face patterns obtained through sentiment tendency analysis from online social media review data. The study results can help to develop better customer and marketing strategies, thereby creating sustainable competitive advantages, and can be extended to other fields to support sustainable development. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Customer relationship management; Customer segmentation; Online reviews; Sentiment analysis; Social media data; Spa leisure consumption","data mining; marketing; recreational activity; recreational facility; service quality; social media; China",Article,Scopus,2-s2.0-85123012542
"Porzycka-Strzelczyk S., Strzelczyk J., Szostek K., Dwornik M., Leśniak A., Bała J., Franczyk A.","32867980300;35312577300;36614641000;35747847200;6701798178;26633572000;35746227900;","Information Extraction from Satellite-Based Polarimetric SAR Data Using Simulated Annealing and SIRT Methods and GPU Processing",2022,"Energies","15","1","72","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123009040&doi=10.3390%2fen15010072&partnerID=40&md5=67df8df40fe6a62e0a9a9a602199de28","The main goal of this research was to propose a new method of polarimetric SAR data decomposition that will extract additional polarimetric information from the Synthetic Aperture Radar (SAR) images compared to other existing decomposition methods. Most of the current decomposition methods are based on scattering, covariance or coherence matrices describing the radar wave-scattering phenomenon represented in a single pixel of an SAR image. A lot of different decomposition methods have been proposed up to now, but the problem is still open since it has no unique solution. In this research, a new polarimetric decomposition method is proposed that is based on polarimetric signature matrices. Such matrices may be used to reveal hidden information about the image target. Since polarimetric signatures (size 18 × 9) are much larger than scattering (size 2 × 2), covariance (size 3 × 3 or 4 × 4) or coherence (size 3 × 3 or 4 × 4) matrices, it was essential to use appropriate computational tools to calculate the results of the proposed decomposition method within an acceptable time frame. In order to estimate the effectiveness of the presented method, the obtained results were compared with the outcomes of another method of decomposition (Arii decomposition). The conducted research showed that the proposed solution, compared with Arii decomposition, does not overestimate the volume-scattering component in built-up areas and clearly separates objects within the mixed-up areas, where both building, vegetation and surfaces occur. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","GPU; Polarimetric decomposition; Polarimetric signature; Radar polarimetry; Simulated annealing; SIRT","Coherent scattering; Data mining; Graphics processing unit; Polarimeters; Radar imaging; Synthetic aperture radar; Data decomposition; Decomposition methods; GPU processing; matrix; Polarimetric decomposition; Polarimetric signature; Polarimetric synthetic aperture radar data; Radar polarimetry; SIRT; Synthetic aperture radar images; Simulated annealing",Article,Scopus,2-s2.0-85123009040
"Wang X., Huang F., Fan X., Shahabi H., Shirzadi A., Bian H., Ma X., Lei X., Chen W.","57204956687;56096699200;12752112800;57213611082;57209036611;56425315200;56143825300;57215845633;57192207628;","Landslide susceptibility modeling based on remote sensing data and data mining techniques",2022,"Environmental Earth Sciences","81","2","50","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123007164&doi=10.1007%2fs12665-022-10195-1&partnerID=40&md5=63ba0da553fdfe37c7b34320d8826248","The main purpose of this study is to apply three advanced landslide susceptibility models (Fisher’s linear discriminant analysis (FLDA), forest by penalizing attributes (FPA), and functional tree (FT)) to evaluate the landslide susceptibility in Muchuan County, China. Firstly, 12 landslide conditioning factors were prepared based on the local geo-environmental characteristics and relevant studies, including slope angle, slope aspect, elevation, profile curvature, plan curvature, topographic wetness index (TWI), land use, normalized difference vegetation index (NDVI), lithology, soil, distance to roads, and distance to rivers. Secondly, a landslide inventory map consisting of 279 landslides was randomly divided into training and validation datasets with the ratio of 70/30. Then, a certainty factors (CF) model was used to analyze the relationship between landslides and the classes of landslide conditioning factors, and a Chi-Squared statistic was used for contribution analysis. Finally, the receiver operating characteristic (ROC) curve (AUC-value) and some statistical parameters are used to compare and verify the three landslide susceptibility models. After comparison and validation, the FPA model should be used as the best model for landslide susceptibility evaluation in Muchuan County in this study. This study can provide valuable information for local governments or organizations to study slope stability. At the same time, the landslide susceptibility map obtained can be a tool for reference for infrastructure planning, engineering design, and disaster mitigation design in the study area. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Certainty factors; Fisher’s linear discriminant analysis; Forest by penalizing attributes; Functional tree; Landslide susceptibility; Muchuan County","Data mining; Discriminant analysis; Factor analysis; Forestry; Land use; Lithology; Remote sensing; Certainty factors; Data-mining techniques; Fisher’s linear discriminant analyse; Forest by penalizing attribute; Functional tree; Landslide susceptibility; Linear discriminant analyze; Model-based OPC; Muchuan county; Remote sensing data; Landslides; data mining; discriminant analysis; landslide; modeling; satellite data; slope angle; topography; China",Article,Scopus,2-s2.0-85123007164
"Zheng J., Yao W., Lin X., Ma B., Bai L.","57221257392;13408117500;57415772000;57415772100;57416290400;","An Accurate Digital Subsidence Model for Deformation Detection of Coal Mining Areas Using a UAV-Based LiDAR",2022,"Remote Sensing","14","2","421","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122987510&doi=10.3390%2frs14020421&partnerID=40&md5=32d43e906446e09a731a39642abf44d1","Coal mine surface subsidence detection determines the damage degree of coal mining, which is of great importance for the mitigation of hazards and property loss. Therefore, it is very important to detect deformation during coal mining. Currently, there are many methods used to detect deformations in coal mining areas. However, with most of them, the accuracy is difficult to guarantee in mountainous areas, especially for shallow seam mining, which has the characteristics of active, rapid, and high-intensity surface subsidence. In response to these problems, we made a digital subsidence model (DSuM) for deformation detection in coal mining areas based on airborne light detection and ranging (LiDAR). First, the entire point cloud of the study area was obtained by coarse to fine registration. Second, noise points were removed by multi-scale morphological filtering, and the progressive triangulation filtering classification (PTFC) algorithm was used to obtain the ground point cloud. Third, the DEM was generated from the clean ground point cloud, and an accurate DSuM was obtained through multiple periods of DEM difference calculations. Then, data mining was conducted based on the DSuM to obtain parameters such as the maximum surface subsidence value, a subsidence contour map, the subsidence area, and the subsidence boundary angle. Finally, the accuracy of the DSuM was analyzed through a comparison with ground checkpoints (GCPs). The results show that the proposed method can achieve centimeter-level accuracy, which makes the data a good reference for mining safety considerations and subsequent restoration of the ecological environment. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Airborne LiDAR; Coal mine; Deformation detection; Digital subsidence model; Surface subsidence","Aircraft detection; Coal; Coal mines; Damage detection; Data mining; Subsidence; Unmanned aerial vehicles (UAV); Airborne light detection and ranging; Coal mining area; Coal-mining; Deformation detection; Digital subsidence model; Ground point; Light detection and ranging; Point-clouds; Subsidence detection; Surface subsidence; Optical radar",Article,Scopus,2-s2.0-85122987510
"Michael B., Ise A., Kawabata K., Matsubara T.","57222730508;57413515700;57211202032;57406748000;","Task-Relevant Encoding of Domain Knowledge in Dynamics Modeling: Application to Furnace Forecasting from Video",2022,"IEEE Access","10",,,"4615","4627",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122895376&doi=10.1109%2fACCESS.2022.3140758&partnerID=40&md5=fb8e892dd64827bf1ab60f6f45e0f33b","Waste incineration plants are complex dynamical systems that rely on expert human operators to maintain steady combustion, by observing real-time in-chamber video feeds. Real-time plant forecasting provides vital operational support in decision making, and applying machine learning to automatically learn dynamics forecast models from video feeds is an attractive means to realise this. However, learning complex dynamics in systems that requires cost-efficiency remains an open research problem. Specifically, modelling plant dynamics in real-time is challenging due to uncertainties caused by inhomogeneous waste inputs, requiring complex learning that impedes real-time modelling. To address this, this paper presents a real-time data-driven framework for generating video forecasts, by incorporating task-relevant domain-knowledge, during learning. Specifically, this method combines dynamics modelling and forecasting using dynamic mode decomposition, with Fourier transformations informed by expert operator heuristic knowledge for encoding task-relevant frequency information inside the learning process. Experiments in this paper demonstrate that the proposed framework captures intuitive physical aspects of the underlying physiochemical process, with a greatly reduced computational runtime in comparison to standard approaches, allowing for application in real-time domains. Forecasted video predictions are accurate over short time horizons, and capture important system characteristics over longer time periods. © 2021 IEEE.","Dynamic mode decomposition; forecasting; Fourier transforms; machine learning; video signal processing; waste handling","Audio acoustics; Bayesian networks; Data mining; Decision making; Domain Knowledge; Encoding (symbols); Forecasting; Heuristic methods; Interactive computer systems; Inverse problems; Job analysis; Learning systems; Signal encoding; Uncertainty analysis; Video signal processing; Domain knowledge; Dynamic mode decompositions; Dynamics models; Predictive models; Real - Time system; Real- time; Streaming medium; Task analysis; Task relevant; Waste handling; Real time systems",Article,Scopus,2-s2.0-85122895376
"Silva D.H., Maziero E.G., Saadi M., Rosa R.L., Silva J.C., Rodriguez D.Z., Igorevich K.K.","57222469382;26025713700;55638443900;36242302100;57223273783;36242276900;57393192800;","Big data analytics for critical information classification in online social networks using classifier chains",2022,"Peer-to-Peer Networking and Applications","15","1",,"626","641",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122895297&doi=10.1007%2fs12083-021-01269-1&partnerID=40&md5=7cab7b2fc59acf4a8287d28b056adf72","Industrial and academic organizations are using online social network (OSN) for different purposes, such as social and economic aspects. Now, OSN is a new mean of obtaining information from people about their preferences, and interests. Due to the large volume of user-generated content, researchers use various techniques, such as sentiment analysis or data mining to evaluate this information automatically. However, the sentiment analysis of OSN content is performed by different methods, but there are some problems to obtain highly reliable results, mainly because of the lack of user profile information, such as gender and age. In this work, a novel dataset is built, which contains the writing characteristics of 160,000 users of the Twitter OSN. Before creating classification models with Machine Learning (ML) techniques, feature transformation and feature selection methods are applied to determine the most relevant set of characteristics. To create the models, the Classifier Chain (CC) transformation technique and different machine learning algorithms are applied to the training set. Simulation results show that the Random Forest, XGBoost and Decision Tree algorithms obtain the best performance results. In the testing phase, these algorithms reached Hamming Loss values of 0.033, 0.033, and 0.034, respectively, and all of them reached the same F1 micro-average value equal to 0.976. Therefore, our proposal based on a multidimensional learning technique using CC transformation overcomes other similar proposals. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Age-group classifier; Big data; Feature selection; Feature transformation; Gender classifier; Multi-label classification","Big data; Classification (of information); Data Analytics; Data mining; Decision trees; Learning algorithms; Machine learning; Metadata; Sentiment analysis; Social networking (online); User profile; Age groups; Age-group classifier; Classifier chains; Economic aspects; Feature transformations; Features selection; Gender classifier; Information classification; Large volumes; Sentiment analysis; Feature extraction",Article,Scopus,2-s2.0-85122895297
"Sun X., Ding B.","57226804915;57412962300;","Neural Network with Hierarchical Attention Mechanism for Contextual Topic Dialogue Generation",2022,"IEEE Access","10",,,"4628","4639",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122893071&doi=10.1109%2fACCESS.2022.3140820&partnerID=40&md5=aa3815750a8dde812973f65c8a4a43c2","The encoder-decoder model has achieved remarkable results in natural language generation. However, in the dialogue generation work, we often ignore the influence of the dialogue context information and topic information in the generation, resulting in the generated replies not close to the context or lack of topic information leads to general responses. In this work, we study the generation of multi-turn dialogues based on a large corpus and take advantage of the context information and topic information of the conversation in the process of dialogue generation to generate more coherent context-sensitive responses. We improve upon existing models and attention mechanisms and propose a new hierarchical model to better solve the problem of dialogue context (the HAT model). This method enables the model to obtain more contextual information when processing and improves the ability of the model in terms of contextual relevance to produce high-quality responses. In addition, to address the absence of topics in the responses, we pre-train the LDA(Latent Dirichlet Allocation) topic model to extract topic words of the dialogue content and retain as much topic information of dialogue as possible. Our model is extensively tested in several corpora, and the experiments illustrate that our model is superior to most hierarchical and non-hierarchical models with respect to multiple evaluation metrics. © 2021 IEEE.","Dialogue context; dialogue generation; hierarchical attention; topic","Data mining; Decoding; Hierarchical systems; Natural language processing systems; Signal encoding; Statistics; Attention mechanisms; Context information; Context models; Dialog context; Dialogue generations; Encodings; Hierarchical attention; Hierarchical model; Neural-networks; Topic; Semantics",Article,Scopus,2-s2.0-85122893071
"Satter K.B., Tran P.M.H., Tran L.K.H., Ramsey Z., Pinkerton K., Bai S., Savage N.M., Kavuri S., Terris M.K., She J.-X., Purohit S.","57220068408;57213622870;57213335775;57413707500;57413020400;55955151400;26028096200;23980291100;7102651413;7102576807;7005070911;","Oncocytoma-Related Gene Signature to Differentiate Chromophobe Renal Cancer and Oncocytoma Using Machine Learning",2022,"Cells","11","2","287","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122890999&doi=10.3390%2fcells11020287&partnerID=40&md5=cd3e73b67c3094f46aaa2d3bf2a2b5cd","Publicly available gene expression datasets were analyzed to develop a chromophobe and oncocytoma related gene signature (COGS) to distinguish chRCC from RO. The datasets GSE11151, GSE19982, GSE2109, GSE8271 and GSE11024 were combined into a discovery dataset. The tran-scriptomic differences were identified with unsupervised learning in the discovery dataset (97.8% accuracy) with density based UMAP (DBU). The top 30 genes were identified by univariate gene expression analysis and ROC analysis, to create a gene signature called COGS. COGS, combined with DBU, was able to differentiate chRCC from RO in the discovery dataset with an accuracy of 97.8%. The classification accuracy of COGS was validated in an independent meta-dataset consisting of TCGA-KICH and GSE12090, where COGS could differentiate chRCC from RO with 100% accuracy. The differentially expressed genes were involved in carbohydrate metabolism, transcriptomic regulation by TP53, beta-catenin-dependent Wnt signaling, and cytokine (IL-4 and IL-13) signaling highly active in cancer cells. Using multiple datasets and machine learning, we constructed and validated COGS as a tool that can differentiate chRCC from RO and complement histology in routine clinical practice to distinguish these two tumors. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","chromophobe; Classification; Gene signature; Machine learning; Oncocytoma; Transcriptomic","beta catenin; cytokeratin 7; interleukin 13; interleukin 4; parvalbumin; protein p53; accuracy; Article; carbohydrate metabolism; chromophobe renal cell carcinoma; clinical practice; controlled study; data mining; differential gene expression; fractional anisotropy; gene expression; gene expression profiling; gene ontology; glycolysis; glycosaminoglycan metabolism; histology; histopathology; human; human tissue; kidney tumor; lung adenocarcinoma; machine learning; needle biopsy; nuclear magnetic resonance imaging; oncocytoma; oxidative phosphorylation; principal component analysis; receiver operating characteristic; sensitivity and specificity; signal transduction; transcriptomics; tumor gene; upregulation; validation process; Wnt signaling; algorithm; differential diagnosis; gene expression regulation; genetic database; genetics; oncocytoma; renal cell carcinoma; reproducibility; Adenoma, Oxyphilic; Algorithms; Carbohydrate Metabolism; Carcinoma, Renal Cell; Databases, Genetic; Diagnosis, Differential; Gene Expression Profiling; Gene Expression Regulation, Neoplastic; Genes, Neoplasm; Humans; Machine Learning; Reproducibility of Results; ROC Curve; Warburg Effect, Oncologic",Article,Scopus,2-s2.0-85122890999
"Yang Y., Song R., Xue Y., Zhang P., Xu Y., Kang J., Zhao H.","57221525734;57200297259;57109674200;57412943400;57226441005;16506731900;24577889100;","A Detection Method for Group Fixed Ratio Electricity Thieves Based on Correlation Analysis of Non-Technical Loss",2022,"IEEE Access","10",,,"5608","5619",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122885161&doi=10.1109%2fACCESS.2022.3141610&partnerID=40&md5=f3c40bb22b3688a3c09d11043ac3ae3a","Owing to the contagiousness of theft behaviors among customers, collaborative energy theft, such as village fraud, has become particularly common. In this study, a bunch of electricity thieves that steal energy at a constant ratio were considered. Conventional correlation-sorting-based methods may have some trouble handling these electricity thieves when they exist in the same area. To overcome such limitation, we firstly establish the mathematical model of non-technical loss (NTL) and the load data of fixed ratio electricity thieves (FRETs). Subsequently, an interesting correlation trend, which can be exploited to locate FRETs, was observed and analyzed. Based on this trend, we propose a correlation analysis-based detection method. It adopts a standardized covariance to measure the correlation between the NTL and user data. The detection of FRETs is realized by solving a combinatorial optimization problem. A corresponding framework in practice was also designed. Finally, numerical experiments based on a realistic dataset and an electricity theft dataset from an electricity theft emulator (ETE) are conducted to validate the effectiveness and superiority of the proposed method in terms of accuracy, stability, and scalability. © 2013 IEEE.","Covariance analysis; Data mining; Electricity theft detection; Fixed ratio electricity theft","Combinatorial optimization; Correlation methods; Crime; Numerical methods; Constant ratio; Correlation analysis; Covariance analysis; Detection methods; Electricity theft; Electricity theft detection; Energy; Fixed ratio electricity theft; Load data; Non-technical loss; Data mining",Article,Scopus,2-s2.0-85122885161
"Kontogiannis S., Kastellos A., Kokkonis G., Gkamas T., Pikridas C.","36602849600;57413625400;54397103100;51461286900;6504648147;","Driving Speed Estimation and Trapped Drivers’ Detection inside Tunnels Using Distributed MIMO Bluetooth Devices",2022,"Electronics (Switzerland)","11","2","265","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122870651&doi=10.3390%2felectronics11020265&partnerID=40&md5=bff45b4d48dc909798cc3dc14127fbe1","Accidents in highway tunnels involving trucks carrying flammable cargoes can be dangerous, needing immediate confrontation to detect and safely evacuate the trapped people to lead them to the safety exits. Unfortunately, existing sensing technologies fail to detect and track trapped persons or moving vehicles inside tunnels in such an environment. This paper presents a distributed Bluetooth system architecture that uses detection equipment following a MIMO approach. The proposed equipment uses two long-range Bluetooth and one BLE transponder to locate vehicles and trapped people in motorway tunnels. Moreover, the detector’s parts and distributed architecture are analytically described, along with interfacing with the authors’ resources management system implementation. Furthermore, the authors also propose a speed detection process, based on classifier training, using RSSI input and speed calculations from the tunnel inductive loops as output, instead of the Friis equation with Kalman filtering steps. The proposed detector was experimentally placed at the Votonosi tunnel of the EGNATIA motorway in Greece, and its detection functionality was validated. Finally, the detector classification process accuracy is evaluated using feedback from the existing tunnel inductive loop detectors. According to the evaluation process, classifiers based on decision trees or random forests achieve the highest accuracy. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Bluetooth and BLE detection systems; Bluetooth sniffing; Data mining; Distributed systems; IoT; Speed detection and classification",,Article,Scopus,2-s2.0-85122870651
"Chen W., Ouyang S., Tong W., Li X., Zheng X., Wang L.","55822747800;57413242300;57218501791;41261664800;57413797000;23029267900;","GCSANet: A Global Context Spatial Attention Deep Learning Network for Remote Sensing Scene Classification",2022,"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15",,,"1150","1162",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122866086&doi=10.1109%2fJSTARS.2022.3141826&partnerID=40&md5=86f54a3e9e5d7a70eb69beb963219cbe","Deep convolutional neural networks have become an indispensable method in remote sensing image scene classification because of their powerful feature extraction capabilities. However, the ability of the models to extract multiscale features and global features on surface objects of complex scenes is currently insufficient. We propose a framework based on global context spatial attention (GCSA) and densely connected convolutional networks to extract multiscale global scene features, called GCSANet. The mixup operation is used to enhance the spatial mixed data of remote sensing images, and the discrete sample space is rendered continuous to improve the smoothness in the neighborhood of the data space. The characteristics of multiscale surface objects are extracted, and their internal dense connection is strengthened by the densely connected backbone network. GCSA is introduced into the densely connected backbone network to encode the context information of the remote sensing scene image into the local features. Experiments were performed on four remote sensing scene datasets to evaluate the performance of GCSANet. The GCSANet achieved the highest classification precision on AID and NWPU datasets and the second-best performance on the UC Merced dataset, indicating the GCSANet can effectively extract the global features of remote sensing images. In addition, the GCSANet presents the highest classification accuracy on the constructed mountain image scene dataset. These results reveal that the GCSANet can effectively extract multiscale global scene features on complex remote sensing scenes. The source codes of this method can be foundin <uri>https://github.com/ShubingOuyangcug/GCSANet</uri>. © 2022 Institute of Electrical and Electronics Engineers. All rights reserved.","Convolutional neural networks; Data mining; Feature extraction; Image analysis; Manuals; Remote sensing; Training","Classification (of information); Complex networks; Convolution; Data mining; Deep neural networks; Extraction; Image enhancement; Remote sensing; Semantics; Space optics; Attention mechanisms; Context information; Convolutional neural network; Feature channel; Features extraction; Global context; Global context information; Image-analysis; Manual; Remote-sensing; Scene classification; Feature extraction",Article,Scopus,2-s2.0-85122866086
"Baralic K., Živancevic K., Božic D., Jennen D., Djordjevic A.B., Miljakovic E.A., Dukic-Cosic D.","57213621761;57214136709;57343183700;8250623100;57216286846;56910295300;26639264900;","Potential genomic biomarkers of obesity and its comorbidities for phthalates and bisphenol A mixture: In silico toxicogenomic approach",2022,"Biocell","46","2",,"519","533",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122861485&doi=10.32604%2fbiocell.2022.018271&partnerID=40&md5=d316ba66ff99eb0ef6ed485dda83b9a1","This in silico toxicogenomic study aims to explore the relationship between phthalates and bisphenol A (BPA) co-exposure and obesity, as well as its comorbid conditions, in order to construct a possible set of genomic biomarkers. The Comparative Toxicogenomics Database (CTD; http://ctd.mdibl.org) was used as the main data mining tool, along with GeneMania (https://genemania.org), ToppGene Suite (https://toppgene.cchmc.org) and DisGeNET (http://www. disgenet.org). Among the phthalates, bis(2-ethylhexyl) phthalate (DEHP) and dibutyl phthalate (DBP) were chosen as the most frequently curated phthalates in CTD, which also share similar mechanisms of toxicity. DEHP, DBP and BPA interacted with 84, 90 and 194 obesity-related genes/proteins, involved in 67, 65 and 116 pathways, respectively. Among these, 53 genes/proteins and 42 pathways were common to all three substances. 31 genes/proteins had matching interactions for all three investigated substances, while more than half of these genes/proteins (56.49%) were in co-expression. 7 of the common genes/proteins (6 relevant to humans: CCL2, IL6, LPL, PPARG, SERPINE1, and TNF) were identified in all the investigated obesity comorbidities, while PPARG and LPL were most closely linked to obesity. These genes/proteins could serve as a target for further in vitro and in vivo studies of molecular mechanisms of DEHP, DBP and BPA mixture obesogenic properties. Analysis reported here should be applicable to any mixture of environmental chemicals and any disease present in CTD. © 2022 Centro Regional de Invest. Cientif. y Tecn.. All rights reserved.","Bioinformatics; Data mining; Endocrine disruptors; Toxicology",,Article,Scopus,2-s2.0-85122861485
"John J., Soangra R.","57222813661;37085613200;","Visualization-Driven Time-Series Extraction from Wearable Systems Can Facilitate Differentiation of Passive ADL Characteristics among Stroke and Healthy Older Adults",2022,"Sensors","22","2","598","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122859099&doi=10.3390%2fs22020598&partnerID=40&md5=f5c31a1850cc15e90123e2fa629e3bc2","Wearable technologies allow the measurement of unhindered activities of daily living (ADL) among patients who had a stroke in their natural settings. However, methods to extract meaningful information from large multi-day datasets are limited. This study investigated new visualization-driven time-series extraction methods for distinguishing activities from stroke and healthy adults. Fourteen stroke and fourteen healthy adults wore a wearable sensor at the L5/S1 position for three consecutive days and collected accelerometer data passively in the participant’s naturalistic environment. Data from visualization facilitated selecting information-rich time series, which resulted in classification accuracy of 97.3% using recurrent neural networks (RNNs). Individuals with stroke showed a negative correlation between their body mass index (BMI) and higher-acceleration fraction produced during ADL. We also found individuals with stroke made lower activity amplitudes than healthy counterparts in all three activity bands (low, medium, and high). Our findings show that visualization-driven time series can accurately classify movements among stroke and healthy groups using a deep recurrent neural network. This novel visualization-based time-series extraction from naturalistic data provides a physical basis for analyzing passive ADL monitoring data from real-world environments. This time-series extraction method using unit sphere projections of acceleration can be used by a slew of analysis algorithms to remotely track progress among stroke survivors in their rehabilitation program and their ADL abilities. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Activities of daily living (ADL); Body mass index (BMI); Long short-term memory (LSTM); Recurrent neural network (RNN); Stroke; Time-series extraction","Classification (of information); Data mining; Data visualization; Extraction; Large dataset; Long short-term memory; Time series analysis; Visualization; Wearable sensors; Activities of Daily Living; Activity of daily living; Body mass; Body mass index; Long short-term memory; Mass index; Recurrent neural network; Stroke; Time-series extraction; Times series; Time series; aged; cerebrovascular accident; daily life activity; electronic device; human; stroke rehabilitation; time factor; Activities of Daily Living; Aged; Humans; Stroke; Stroke Rehabilitation; Time Factors; Wearable Electronic Devices",Article,Scopus,2-s2.0-85122859099
"Zauner R., Wimmer M., Dorfer S., Ablinger M., Koller U., Hofbauer J.P., Guttmann-Gruber C., Bauer J.W., Wally V.","56728387500;56727659600;56203317400;56993892700;23502026900;57210139470;57201093522;35482951900;23502125400;","Transcriptome-Guided Drug Repurposing for Aggressive SCCs",2022,"International Journal of Molecular Sciences","23","2","1007","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122833331&doi=10.3390%2fijms23021007&partnerID=40&md5=c7cd0f591df703a5fd3edb77db3ee99d","Despite a significant rise in the incidence of cutaneous squamous cell carcinoma (SCC) in recent years, most SCCs are well treatable. However, against the background of pre-existing risk factors such as immunosuppression upon organ transplantation, or conditions such as recessive dystrophic epidermolysis bullosa (RDEB), SCCs arise more frequently and follow a particularly aggressive course. Notably, such SCC types display molecular similarities, despite their differing etiologies. We leveraged the similarities in transcriptomes between tumors from organ transplant recipients and RDEB-patients, augmented with data from more common head and neck (HN)-SCCs, to identify drugs that can be repurposed to treat these SCCs. The in silico approach used is based on the assumption that SCC-derived transcriptome profiles reflect critical tumor pathways that, if reversed towards healthy tissue, will attenuate the malignant phenotype. We determined tumor-specific signatures based on differentially expressed genes, which were then used to mine drug-perturbation data. By leveraging recent efforts in the systematic profiling and cataloguing of thousands of small molecule compounds, we identified drugs including selumetinib that specifically target key molecules within the MEK signaling cascade, representing candidates with the potential to be effective in the treatment of these rare and aggressive SCCs. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Drug repurposing; Epidermolysis bullosa; Organ transplant recipients; Squamous cell carcinoma","antineoplastic agent; adverse event; biology; comparative study; complication; data mining; drug effect; drug repositioning; epidermolysis bullosa dystrophica; gene expression profiling; gene expression regulation; gene regulatory network; genetics; human; organ transplantation; procedures; skin tumor; squamous cell carcinoma; Antineoplastic Agents; Carcinoma, Squamous Cell; Computational Biology; Data Mining; Drug Repositioning; Epidermolysis Bullosa Dystrophica; Gene Expression Profiling; Gene Expression Regulation, Neoplastic; Gene Regulatory Networks; Humans; Organ Transplantation; RNA-Seq; Skin Neoplasms",Article,Scopus,2-s2.0-85122833331
"Fan C.-L.","57202836794;","Data mining model for predicting the quality level and classification of construction projects",2022,"Journal of Intelligent and Fuzzy Systems","42","1",,"139","153",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122830063&doi=10.3233%2fJIFS-219182&partnerID=40&md5=ed51da237c85d4749aaef0bba5b5bef7","Project managers supervise projects to ensure their smooth completion within a stipulated time frame and budget while guaranteeing construction quality. The relationships of various attributes with quality can be quantified and classified to facilitate such supervision. Therefore, this study used a data mining algorithm to analyze the relationships between defects, quality levels, contract sums, project categories, and progress in 1,015 inspection projects. In the first part, association rule mining (ARM), an unsupervised data mining approach, was used to obtain 11 rules relating two defect types (i.e., quality management system and construction quality) and determine the relationships between the four attributes (i.e., quality level, contract sum, project category, and progress). The resulting association rule may be beneficial for construction management because project managers can use it to determine the correlations between defects and attributes. In the second part, supervised data mining techniques, namely neural network (NN), support vector machine (SVM), and decision tree (C5.0 and QUEST) algorithms, were applied to develop a classification model for quality prediction. The target variable was quality, which was divided into four levels, and the decision variables comprised 499 defects, 3 contract sums, 7 project categories, and 2 progress variables. The results indicated that five defects were important. Finally, the four indicators of gain chart, break-even point (BEP), accuracy, and area under the curve (AUC) were calculated to evaluate the model. For the SVM model, the actual value predicted by the gain chart was 96.04%, the BEP was 0.95, and the AUC was 0.935. The SVM yielded optimal classification efficiency and effectively predicted the quality level. The data mining model developed in this study can serve as a reference for effective construction management. © 2022 - IOS Press. All rights reserved.","association rule; classification; Data mining; defect; quality level","Association rules; Budget control; Classification (of information); Data mining; Decision trees; Information management; Managers; Project management; Support vector machines; Areas under the curves; Break-even point; Construction management; Construction projects; Construction quality; Data mining models; Project managers; Quality classification; Quality levels; Support vectors machine; Defects",Article,Scopus,2-s2.0-85122830063
"Wang Y.","57411444300;","Big Data Mining Method of Marketing Management Based on Deep Trust Network Model",2022,"International Journal of Circuits, Systems and Signal Processing","16",,,"578","584",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122827508&doi=10.46300%2f9106.2022.16.72&partnerID=40&md5=5a07385ad91fbcf83f734e991bc379d9","Through big data mining, enterprises can deeply understand the consumer preferences, behavior characteristics, market demand and other derived data of customers, so as to provide the basis for formulating accurate marketing strategies. Therefore, this paper proposes a marketing management big date mining method based on deep trust network model. This method first preprocesses the big data of marketing management, including data cleaning, data integration, data transformation and data reduction, and then establishes a big data mining model by using deep trust network to realize the research on the classification of marketing management data. Experimental results show that the proposed method has 99.08% accuracy, the capture rate reaches 88.11%, and the harmonic average between the accuracy and the recall rate is 89.27%, allowing for accurate marketing strategies. © 2022, North Atlantic University Union NAUN. All rights reserved.","Big date mining method; Deep trust network model; Marketing management","Commerce; Consumer behavior; Data integration; Data mining; Information management; Metadata; Strategic planning; Big date mining method; Consumers' preferences; Data mining methods; Deep trust network model; Marketing management; Marketing strategy; Mining enterprise; Mining methods; Network models; Trust networks; Big data",Article,Scopus,2-s2.0-85122827508
"Tuncali Yaman T., Bilgiç E., Fevzi Esen M.","57219840468;57210356840;57412167700;","Analysis of traffic accidents with fuzzy and crisp data mining techniques to identify factors affecting injury severity",2022,"Journal of Intelligent and Fuzzy Systems","42","1",,"575","592",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122816427&doi=10.3233%2fJIFS-2191213&partnerID=40&md5=715b7b49a9e25f53c087a24457f07ad4","Injury severity in motor vehicle traffic accidents is determined by a number of factors including driver, vehicle, and environment. Airbag deployment, vehicle speed, manner of collusion, atmospheric and light conditions, degree of ejection of occupant's body from the crash, the use of equipment or other forces to re-move occupants from the vehicle, model and type of vehicle have been considered as important risk factors affecting accident severity as well as driver-related conditions such as age, gender, seatbelt use, alcohol and drug involvement. In this study, we aim to identify important variables that contribute to injury severity in the traffic crashes. A contemporary dataset is obtained from National Highway Traffic Safety Administration's (NHTSA) Fatality Analysis Reporting System (FARS). To identify accident severity groups, we performed different clustering algorithms including fuzzy clustering. We then assessed the important factors affecting injury severity by using classification and regression trees (CRT). The results which would guide car manufacturers, policy makers and insurance companies indicate that the most important factor in defining injury severity is deployment of air-bag, followed by extrication, ejection occurrences, and travel speed and alcohol involvement. © 2022 - IOS Press. All rights reserved.","clustering; CRT; data mining; fuzzy clustering; injury severity; Traffic accidents","Automobile manufacture; Clustering algorithms; Fuzzy clustering; Highway accidents; Insurance; Vehicles; Accident severity; Classification trees; Clusterings; Crisp data; Data-mining techniques; Fuzzy data; Injury severity; Motor vehicle; Regression trees; Vehicle traffic; Data mining",Article,Scopus,2-s2.0-85122816427
"Zdrodowska M., Dardzińska-Głbocka A.","57198358540;6505804771;","Classification and action rules in identification and self-care assessment problems",2022,"Technology and Health Care","30","1",,"257","269",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122801688&doi=10.3233%2fTHC-219008&partnerID=40&md5=e61813b7765402c2a3fa224969926e4b","Disability, especially in children, is a very important and current problem. Lack of proper diagnosis and care increases the difficulty for children to adapt to disabilities. Disabled children have many problems with basic activities of daily living. Therefore, it is very important to support diagnosticians and physiotherapists in recognizing self-care problems in children. OBJECTIVE: The aim of this paper is to extract classification and action rules, useful for those who work with children with disabilities. METHODS: First, features and their impact on the accuracy of classification are determined. Then, two models are built: one with all features and one with selected ones. For these models the classification rules are extracted. Finally, action rules are mined and the next step in treatment process is predicted. RESULTS: Seventeen features with the greatest impact on classifying a child into a particular group of self-care problems were identified. Based on the implemented algorithms, decision and action rules were obtained. CONCLUSIONS: The obtained model, selected attributes and extracted classification and action rules can support the work of therapists and direct their work to those areas of disability where even a minimal reduction of features would be of great benefit to the children. © 2022 - IOS Press. All rights reserved.","action rules; classification; classification rules; data mining; Disability; feature selection; ICF-CY; self-care problem","child; daily life activity; disability; handicapped child; human; self care; Activities of Daily Living; Child; Disability Evaluation; Disabled Children; Humans; Self Care",Article,Scopus,2-s2.0-85122801688
"Gayathri T., Lalitha Bhaskari D.","57210696216;55383673800;","Oppositional Cuckoo Search Optimization based Clustering with Classification Model for Big Data Analytics in Healthcare Environment",2022,"Journal of Applied Science and Engineering (Taiwan)","25","4",,"743","751",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122767944&doi=10.6180%2fjase.202208_25%284%29.0019&partnerID=40&md5=f6a3c05b820721afbb8e328d3d62aba9","Big data in healthcare defines a massive quantity of healthcare data accumulated from massive sources like electronic health records (EHR), medical imaging, genomic sequence, pharmacological research, wearable, and medical gadgets, etc. One of the data mining approaches commonly employed to classify big data is the MapReduce model. Data clustering, a significant data mining technique has been extensively investigated in the recent years in handling the diversity in data and various sets of application necessities. In this view, this paper develops an enhanced metaheuristic algorithm based clustering and classification model with MapReduce (EMACC-MR) framework for big data environment. The presented EMACC-MR model involves an oppositional cuckoo search optimization algorithm (OCSOA) with Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and wavelet kernel extreme learning machine (WKELM) model based on the classification model. The inclusion of oppositional based learning (OBL) concept helps to improve the convergence rate of the CSOA. For handling big data, Hadoop MapReduce environment is employed. The proposed OCSOA model improves the clustering quality and MapReduce architecture to cope with the large-scale dataset. For validating the experimental analysis of the proposed model, two benchmark datasets namely Activity recognition and diabetes datasets are used. The simulation outcomes confirmed that the presented model outperforms the compared methods in terms of several evaluation parameters. © The Author(’s).","Big data analytics; Classification process; Clustering; MapReduce; Metaheuristics; Neural network","Classification (of information); Clustering algorithms; Data Analytics; Data mining; Health care; Large dataset; Machine learning; Medical imaging; Optimization; Based clustering; Classification models; Classification process; Clusterings; Cuckoo searches; Map-reduce; MapReduce models; Meta-heuristics algorithms; Metaheuristic; Neural-networks; MapReduce",Article,Scopus,2-s2.0-85122767944
"Mehmood S., Ahmad I., Khan M.A., Khan F., Whangbo T.","57226140474;57206747933;57215096761;57353121900;35617849900;","Sentiment Analysis in Social Media for Competitive Environment Using Content Analysis",2022,"Computers, Materials and Continua","71","2",,"5603","5618",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122763782&doi=10.32604%2fcmc.2022.023785&partnerID=40&md5=34d4820a4f72de404e999b61930cf2de","Education sector has witnessed several changes in the recent past. These changes have forced private universities into fierce competition with each other to get more students enrolled. This competition has resulted in the adoption of marketing practices by private universities similar to commercial brands. To get competitive gain, universities must observe and examine the students' feedback on their own social media sites along with the social media sites of their competitors. This study presents a novel framework which integrates numerous analytical approaches including statistical analysis, sentiment analysis, and text mining to accomplish a competitive analysis of social media sites of the universities. These techniques enable local universities to utilize social media for the identification of the most-discussed topics by students as well as based on the most unfavorable comments received, major areas for improvement. A comprehensive case study was conducted utilizing the proposed framework for competitive analysis of few top ranked international universities as well as local private universities in Lahore Pakistan. Experimental results show that diversity of shared content, frequency of posts, and schedule of updates, are the key areas for improvement for the local universities. Based on the competitive intelligence gained several recommendations are included in this paper that would enable local universities generally and Riphah international university (RIU) Lahore specifically to promote their brand and increase their attractiveness for potential students using social media and launch successful marketing campaigns targeting a large number of audiences at significantly reduced cost resulting in an increased number of enrolments. © 2022 Tech Science Press. All rights reserved.","Competitive analysis; Content analysis; Higher education; Sentiment analysis; Social media; Text mining","Commerce; Competition; Competitive intelligence; Data mining; Marketing; Social networking (online); Students; Analytical approach; Competitive analysis; Competitive environment; Content analysis; Education sectors; High educations; Sentiment analysis; Social media; Student feedback; Text-mining; Sentiment analysis",Article,Scopus,2-s2.0-85122763782
"Saeed F., Al-Sarem M., Al-Mohaimeed M., Emara A., Boulila W., Alasli M., Ghabban F.","54892561900;43461087100;57220026782;57410281100;37088273900;55820343900;57192309432;","Enhancing Parkinson's Disease Prediction Using Machine Learning and Feature Selection Methods",2022,"Computers, Materials and Continua","71","2",,"5639","5657",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122754355&doi=10.32604%2fcmc.2022.023124&partnerID=40&md5=93caad98f0c15b45e660b7c78bd8825e","Several millions of people suffer from Parkinson's disease globally. Parkinson's affects about 1% of people over 60 and its symptoms increase with age. The voice may be affected and patients experience abnormalities in speech that might not be noticed by listeners, but which could be analyzed using recorded speech signals. With the huge advancements of technology, the medical data has increased dramatically, and therefore, there is a need to apply data mining and machine learning methods to extract new knowledge from this data. Several classification methods were used to analyze medical data sets and diagnostic problems, such as Parkinson's Disease (PD). In addition, to improve the performance of classification, feature selection methods have been extensively used in many fields. This paper aims to propose a comprehensive approach to enhance the prediction of PD using several machine learning methods with different feature selection methods such as filter-based and wrapper-based. The dataset includes 240 recodes with 46 acoustic features extracted from 3 voice recording replications for 80 patients. The experimental results showed improvements when wrapper-based features selection method was used with K-NN classifier with accuracy of 88.33%. The best obtained results were compared with other studies and it was found that this study provides comparable and superior results. © 2022 Tech Science Press. All rights reserved.","Filter-based feature selection methods; Machine learning; Parkinson's disease; Wrapper-based feature selection methods","Biomedical signal processing; Classification (of information); Data mining; Diagnosis; Feature extraction; Filtration; Medical computing; Nearest neighbor search; Neurodegenerative diseases; Feature selection methods; Filter-based; Filter-based feature selection method; Learning selection; Machine learning methods; Machine-learning; Parkinson's disease; Wrapper-based feature selection; Wrapper-based feature selection method; Machine learning",Article,Scopus,2-s2.0-85122754355
"Ma K., Chen Z., Fu L., Tian W., Jiang F., Yi J., Du Z., Sun H.","57223046918;57410064900;54397020700;57410796400;57217145322;57409920700;55237402900;55729314700;","Performance and Sensitivity of Individual Tree Segmentation Methods for UAV-LiDAR in Multiple Forest Types",2022,"Remote Sensing","14","2","298","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122737252&doi=10.3390%2frs14020298&partnerID=40&md5=a89fd25177fe771cabb08a96ab352e05","Using unmanned aerial vehicles (UAV) as platforms for light detection and ranging (LiDAR) sensors offers the efficient operation and advantages of active remote sensing; hence, UAV-LiDAR plays an important role in forest resource investigations. However, high-precision individual tree segmentation, in which the most appropriate individual tree segmentation method and the optimal algorithm parameter settings must be determined, remains highly challenging when applied to multiple forest types. This article compared the applicability of methods based on a canopy height model (CHM) and a normalized point cloud (NPC) obtained from UAV-LiDAR point cloud data. The watershed algorithm, local maximum method, point cloud-based cluster segmentation, and layer stacking were used to segment individual trees and extract the tree height parameters from nine plots of three forest types. The individual tree segmentation results were evaluated based on experimental field data, and the sensitivity of the parameter settings in the segmentation methods was analyzed. Among all plots, the overall accuracy F of individual tree segmentation was between 0.621 and 1, the average RMSE of tree height extraction was 1.175 m, and the RMSE% was 12.54%. The results indicated that compared with the CHM-based methods, the NPC-based methods exhibited better performance in individual tree segmentation; additionally, the type and complexity of a forest influence the accuracy of individual tree segmentation, and point cloud-based cluster segmentation is the preferred scheme for individual tree segmentation, while layer stacking should be used as a supplement in multilayer forests and extremely complex heterogeneous forests. This research provides important guidance for the use of UAV-LiDAR to accurately obtain forest structure parameters and perform forest resource investigations. In addition, the methods compared in this paper can be employed to extract vegetation indices, such as the canopy height, leaf area index, and vegetation coverage. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Forest investigation; Individual tree segmentation; LiDAR; Tree detection; Tree height extraction","Aircraft detection; Antennas; Data mining; Extraction; Forestry; Image segmentation; Parameter estimation; Remote sensing; Unmanned aerial vehicles (UAV); Vegetation; Forest investigation; Height extractions; Individual tree; Individual tree segmentation; Light detection and ranging; Tree detections; Tree height; Tree height extraction; Tree segmentation; Vehicle light detections; Optical radar",Article,Scopus,2-s2.0-85122737252
"Schneider B., Hassan J., Sung G.","55051404100;57222736691;57365373400;","Augmenting Social Science Research with Multimodal Data Collection: The EZ-MMLA Toolkit",2022,"Sensors","22","2","568","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122687553&doi=10.3390%2fs22020568&partnerID=40&md5=eb692f22cc839f691119cc75c6e95298","While the majority of social scientists still rely on traditional research instruments (e.g., surveys, self-reports, qualitative observations), multimodal sensing is becoming an emerging method-ology for capturing human behaviors. Sensing technology has the potential to complement and enrich traditional measures by providing high frequency data on people’s behavior, cognition and affects. However, there is currently no easy-to-use toolkit for recording multimodal data streams. Existing methodologies rely on the use of physical sensors and custom-written code for accessing sensor data. In this paper, we present the EZ-MMLA toolkit. This toolkit was implemented as a website and provides easy access to multimodal data collection algorithms. One can collect a variety of data modalities: data on users’ attention (eye-tracking), physiological states (heart rate), body posture (skeletal data), gestures (from hand motion), emotions (from facial expressions and speech) and lower-level computer vision algorithms (e.g., fiducial/color tracking). This toolkit can run from any browser and does not require dedicated hardware or programming experience. We compare this toolkit with traditional methods and describe a case study where the EZ-MMLA toolkit was used by aspiring educational researchers in a classroom context. We conclude by discussing future work and other applications of this toolkit, potential limitations and implications. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Computer vision; Data mining; Sensor applications and deployments","Behavioral research; Computer vision; Data mining; Eye tracking; Data collection; Human behaviors; Multi-modal data; Multimodal sensing; Qualitative observations; Research instruments; Sensor applications; Sensors deployments; Social science research; Social scientists; Data acquisition; facial expression; gesture; human; questionnaire; sociology; speech; Facial Expression; Gestures; Humans; Social Sciences; Speech; Surveys and Questionnaires",Article,Scopus,2-s2.0-85122687553
"Duniam G., Kitaeff V.V., Wicenec A.","57408103100;55315270700;6603397670;","Data modelling approaches to astronomical data: Mapping large spectral line data cubes to dimensional data models",2022,"Astronomy and Computing","38",,"100539","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122644923&doi=10.1016%2fj.ascom.2021.100539&partnerID=40&md5=db2713e7ffa2478ff953d224dc9d5e27","As a new generation of large-scale telescopes are expected to produce single data products in the range of hundreds of GBs to multiple TBs, different approaches to I/O efficient data interaction and extraction need to be investigated and made available to researchers. This will become increasingly important as the downloading and distribution of TB scale data products will become unsustainable, and researchers will have to take their processing analysis to the data. We present a methodology to extract 3 dimensional spatial–spectral data from dimensionally modelled tables in Parquet format on a Hadoop system. The data is loaded into the Parquet tables from FITS cube files using a dedicated process. We compare the performance of extracting data using the Apache Spark parallel compute framework on top of the Parquet-Hadoop ecosystem with data extraction from the original source files on a shared file system. We have found that the Spark-Parquet-Hadoop solution provides significant performance benefits, particularly in a multi user environment. We present a detailed analysis of the single and multi-user experiments conducted and also discuss the benefits and limitations of the platform used for this study. © 2021 Elsevier B.V.","Distributed databases; Distributed processing","Data handling; Distributed database systems; Extraction; Photomapping; Astronomical data; Data cube; Data extraction; Data mappings; Data products; Distributed database; Distributed processing; Line data; Modeling approach; Spectral line; Data mining",Article,Scopus,2-s2.0-85122644923
"He C., Li J., Liu W., Peng J., Wang Z.J.","36627069600;57221160007;56310827500;9246837000;57406978900;","A Low-Complexity Quantum Principal Component Analysis Algorithm",2022,"IEEE Transactions on Quantum Engineering","3",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122596404&doi=10.1109%2fTQE.2021.3140152&partnerID=40&md5=e42e224ba008da2e78366a82c25d0123","In this article, we propose a low-complexity quantum principal component analysis (qPCA) algorithm. Similar to the state-of-the-art qPCA, it achieves dimension reduction by extracting principal components of the data matrix, rather than all components of the data matrix, to quantum registers, so that the samples of measurement required can be reduced considerably. Both our qPCA and Lin's qPCA are based on quantum singular-value thresholding (QSVT). The key of Lin's qPCA is to combine QSVT, and modified QSVT is to obtain the superposition of the principal components. The key of our algorithm, however, is to modify QSVT by replacing the rotation-controlled operation of QSVT with the controlled-not operation to obtain the superposition of the principal components. As a result, this small trick makes the circuit much simpler. Particularly, the proposed qPCA requires three phase estimations, while the state-of-the-art qPCA requires five phase estimations. Since the runtime of qPCA mainly comes from phase estimations, the proposed qPCA achieves a runtime of roughly 3/5 of that of the state of the art. We simulate the proposed qPCA on the IBM quantum computing platform, and the simulation result verifies that the proposed qPCA yields the expected quantum state. © 2020 IEEE.","Quantum computing; Quantum principal component analysis (qPCA); Quantum singular-value threshold","Data mining; Matrix algebra; Principal component analysis; Quantum computers; Quantum optics; Timing circuits; Eigenvalue and eigenfunctions; Phase-estimation; Principal-component analysis; Quantum circuit; Quantum Computing; Quantum principal component analyse; Quantum singular value threshold; Quantum state; Register; Runtimes; Singular values; Eigenvalues and eigenfunctions",Article,Scopus,2-s2.0-85122596404
"He J., Hu H.","57221314758;55740743700;","MF-BERT: Multimodal Fusion in Pre-Trained BERT for Sentiment Analysis",2022,"IEEE Signal Processing Letters","29",,,"454","458",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122587217&doi=10.1109%2fLSP.2021.3139856&partnerID=40&md5=2d8306e50edff74f033b23846367017a","Multimodal sentiment analysis mainly concentrates on language, acoustic and visual information. Previous work based on BERT utilizes only text (language) representation to fine-tune BERT, while ignoring the importance of nonverbal information. Due to the fact that features extracted from a single modality may contain uncertainty, it is challenging for BERT to perform well in real-world applications. In this paper, we propose a multimodal fusion BERT that can explore the time-dependent interactions among different modalities. Additionally, prior BERT-based methods tend to train the models with only one optimizer to update the parameters. However, we argue that BERT has been pre-trained with a lot of corpora so it needs to be fine-tuned slightly. Therefore, an internal updating mechanism is introduced to avoid the overfitting of the model in the training process. We set two optimizers for multimodal fusion BERT and other components of the model with different learning rates, which enables the model to attain optimal parameters. The results of experiments on public datasets demonstrate that our model is superior to the baselines and achieves the state-of-the-art. 1070-9908 © 2021 IEEE.","Acoustics; Analytical models; Bit error rate; Convolution; Fuses; Sentiment analysis; Visualization","Bit error rate; Data mining; Modal analysis; Visual languages; Bit-error rate; Internal updating; Language informations; Multi-modal; Multi-modal fusion; Multimodal fusion BERT; Multimodal sentiment analyse; Optimizers; Sentiment analysis; Sentiment analysis",Article,Scopus,2-s2.0-85122587217
"Imran H.A.","57214669982;","UltaNet: An Antithesis Neural Network for Recognizing Human Activity Using Inertial Sensors Signals",2022,"IEEE Sensors Letters","6","1",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122570873&doi=10.1109%2fLSENS.2021.3139947&partnerID=40&md5=0886430296e8e82b0ec3a8552f0c6ed0","Human activity recognition (HAR) is an essential component of ambient assistive living. HAR has traditionally relied on computer vision techniques. However, it has several drawbacks, including lack of privacy, higher operational costs, and being constrained by the number of spaces available for cameras, so it cannot be used for applications that require long-term monitoring of people. The use of initial sensors is proving to be a vital solution for HAR. Smartphones and smartwatches have embedded accelerometers and gyroscope sensors that help deep neural networks for reliable activity recognition and classification problems. In this letter, we have presented a novel deep neural network model, which is opposite to the traditional models. It has a gated recurrent unit layer followed by different kernel-sized convolutional layers. Our model has been evaluated on an openly available dataset by the wireless sensor data mining lab, and comparison with previously existing models proves that it outperforms them. © 2017 IEEE.","Ambient assistive living; Antithesis model; Human activity recognition (HAR); Inertial signal processing; Sensor signal processing","Data mining; Deep neural networks; Inertial navigation systems; Pattern recognition; Signal processing; Activity recognition; Ambient assistive living; Ambients; Antithesis model; Assistive living; Human activity recognition; Inertial sensor; Inertial signal processing; Kernel; Signal-processing; Accelerometers",Article,Scopus,2-s2.0-85122570873
"Boumhidi A., Benlahbib A., Nfaoui E.H.","57216688499;57207618788;55793822300;","Cross-Platform Reputation Generation System Based on Aspect-Based Sentiment Analysis",2022,"IEEE Access","10",,,"2515","2531",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122568207&doi=10.1109%2fACCESS.2021.3139956&partnerID=40&md5=fd64cf9dc46f87cec2320b47ad4555ac","The active growth of Internet-based applications such as social networks and e-commerce websites leads people to generate a tremendous amount of opinions and reviews about products and services. Thus, it becomes very crucial to automatically process them. Over the last ten years, many systems have been proposed to generate and visualize reputation by mining textual and numerical reviews. However, they have neglected the fact that online reviews could be posted by malicious users that intend to affect the reputation of the target product. Besides, these systems provide an overall reputation value toward the entity and disregard generating reputation scores toward each aspect of the product. Therefore, we developed a system that incorporates spam filtering, review popularity, review posting time, and aspect-based sentiment analysis to generate accurate and reliable reputation values. The proposed model computes numerical reputation values for an entity and its aspects based on opinions collected from various platforms. Our proposed system also offers an advanced visualization tool that displays detailed information about its output. Experiment results conducted on multiple datasets collected from various platforms (Twitter, Facebook, Amazon..) show the efficacy of the proposed system compared with state-of-the-art reputation generation systems. © 2013 IEEE.","Aspect-based sentiment analysis; decision-making; e-commerce; opinion mining; reputation generation","Behavioral research; Data mining; Electronic commerce; Sentiment analysis; Social networking (online); Aspect-based sentiment analyse; Cross-platform; Decisions makings; E-commerce websites; Generation systems; Internet based application; Online reviews; Product and services; Reputation generation; Sentiment analysis; Decision making",Article,Scopus,2-s2.0-85122568207
"Liu J., Yu M., Chen Y., Xu J.","57202269682;57406704600;57114607100;54796280200;","Cross-Domain Slot Filling as Machine Reading Comprehension: A New Perspective",2022,"IEEE/ACM Transactions on Audio Speech and Language Processing","30",,,"673","685",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122563658&doi=10.1109%2fTASLP.2022.3140559&partnerID=40&md5=6abf17b9b9c9722d9a430a2503f9c40c","With intelligent dialogue systems becoming more and more important in our daily lives, slot filling, one of the most important components of an intelligent dialogue system, has gotten a lot of attention from academia and industry. Despite many advancements in the single-domain learning paradigm for slot filling, leveraging resources from different domains to boost learning for a target domain remains a challenge. In contrast to prior methods that supplemented a sequence labeling model with slot meta-information, we address cross-domain slot filling as a machine reading comprehension (MRC) problem for the first time, where the extraction of slot values is viewed as a question answering process. In the framework above, we present both static and dynamic question generating mechanisms, which have complimentary effects in diverse cross-domain contexts. Furthermore, we devise a dynamic question generation approach that can generate numerous values for a slot at the same time. Finally, we construct a pre-training and fine-tuning training approach that enables us to improve learning by utilizing MRC's resources. We conducted extensive experiments on four datasets to evaluate our approach, and the experimental results clearly justified the advantages of our approach in various cross-domain settings. © 2014 IEEE.","Cross-domain slot filling; machine reading comprehension; slot filling","Data mining; Hidden Markov models; Speech processing; Cross-domain; Cross-domain slot filling; Daily lives; Hidden-Markov models; Intelligent dialogue systems; Labelings; Machine reading comprehension; Reading comprehension; Slot filling; Task analysis; Filling",Article,Scopus,2-s2.0-85122563658
"Sun C., Hou Z., Shen D., Nie T.","55484791300;15622832600;57221404147;8655865200;","Progressive Entity Matching via Cost Benefit Analysis",2022,"IEEE Access","10",,,"3979","3989",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122561072&doi=10.1109%2fACCESS.2021.3139987&partnerID=40&md5=066af15b9d3fb28c15a0c97e59b52f4d","Entity matching (EM) is a fundamental problem in data preprocessing, and is a long running topic in big data analytics and mining communities. In big data era, (nearly) real-time data applications become popular, and call for progressive EM, which produces as many match pairs as possible in very limited time. Previous progressive EM focus on memory based solutions, but disk based solutions are necessary when dirty datasets cannot be fully loaded into memory. To this end, we propose a cost benefit analysis based progressive EM approach, which partitions data according to coarse clustering results and then iteratively schedules data partitions in a greedy way for high progressive resolution. At first, based on estimated record pair similarities, records are fast coarsely clustered; then, record clusters with near average similarities are greedily distributed to the same partitions, and data partitions are cached. After that, cost model is defined with time and space constrains, and benefit model is defined with expected resolution results. On the basis of the cost benefit model, a greedy approximate method is proposed to effectively schedule data for high progressiveness of EM. Finally, we implement extensive experiments over several datasets to evaluate our approach, and show its advantages over existing works. © 2013 IEEE.","Cost benefit model; Data integration; Data partitioning; Entity matching; Progressive","Big data; Clustering algorithms; Costs; Data integration; Data mining; Iterative methods; Cost-benefit models; Cost-benefits analysis; Data application; Data partition; Data partitioning; Data preprocessing; Entity matching; Mining communities; Progressive; Real-time data; Cost benefit analysis",Article,Scopus,2-s2.0-85122561072
"Shao J., Ma X.","56418813500;57396443600;","Hierarchical Pseudo Labeling Based Embranchment Learning for One-Shot Person Re-Identification",2022,"IEEE Signal Processing Letters","29",,,"434","438",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122558489&doi=10.1109%2fLSP.2021.3139571&partnerID=40&md5=1fe9633677cbc06bbded30a41c3dfabd","In this paper, one-shot person re-identification (person re-ID) task is brought into focus, where there is only one labeled sample of each pedestrian. Existing one-shot person re-ID researches usually utilize data insufficiently or treat both labeled and pseudo-labeled data in the same way. To solve these problems, we propose a Hierarchical Pseudo Labeling strategy based on Density and Distance and an Embranchment Learning framework. The pseudo labeling strategy can fully exploit the unlabeled data information and generate accurate pseudo labels by multiple clustering based on pairwise feature distances and distribution densities hierarchically. Our Embranchment Learning framework learns data in an individualized way by feeding data with labels from different sources into distinct network branches paired with personalized loss functions. Besides, Batch Distance Loss and Global Center Loss are designed to target the characteristics of distinct input data. They are capable of distinguishing diverse categories and reducing the intra-class distance respectively. Our method outperforms the existing state-of-the-art algorithm by 36.1, 28.7 of mAP and shortens the training time 6 and 10 epochs on Market-1501 and DukeMTMC-reID respectively. 1070-9908 © 2021 IEEE.","Artificial neural networks; Data mining; Feature extraction; Labeling; Reliability; Task analysis; Training","Data mining; Job analysis; Reliability analysis; Center loss; Embranchment learning; Features extraction; Hierarchical pseudo-label assignment; Labeling strategy; Labelings; Learning frameworks; One-shot person re-ID; Person re identifications; Task analysis; Neural networks",Article,Scopus,2-s2.0-85122558489
"Hassanat A.B., Tarawneh A.S., Abed S.S., Altarawneh G.A., Alrashidi M., Alghamdi M.","24343672100;57190020156;57405306600;56512419600;56405416200;57213626210;","RDPVR: Random Data Partitioning with Voting Rule for Machine Learning from Class-Imbalanced Datasets",2022,"Electronics (Switzerland)","11","2","228","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122537207&doi=10.3390%2felectronics11020228&partnerID=40&md5=e12ac6bd8971688ca2e44b55a227b98d","Since most classifiers are biased toward the dominant class, class imbalance is a challenging problem in machine learning. The most popular approaches to solving this problem include oversampling minority examples and undersampling majority examples. Oversampling may increase the probability of overfitting, whereas undersampling eliminates examples that may be crucial to the learning process. We present a linear time resampling method based on random data partitioning and a majority voting rule to address both concerns, where an imbalanced dataset is partitioned into a number of small subdatasets, each of which must be class balanced. After that, a specific classifier is trained for each subdataset, and the final classification result is established by applying the majority voting rule to the results of all of the trained models. We compared the performance of the proposed method to some of the most well-known oversampling and undersampling methods, employing a range of classifiers, on 33 benchmark machine learning class-imbalanced datasets. The classification results produced by the classifiers employed on the generated data by the proposed method were comparable to most of the resampling methods tested, with the exception of SMOTEFUNA, which is an oversampling method that increases the probability of overfitting. The proposed method produced results that were comparable to the Easy Ensemble (EE) undersampling method. As a result, for solving the challenge of machine learning from class-imbalanced datasets, we advocate using either EE or our method. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","CART; Classification; Data mining; KNN; SMOTE; SVM",,Article,Scopus,2-s2.0-85122537207
"Alqaheri H., Panda M.","57202387986;24829805900;","An Education Process Mining Framework: Unveiling Meaningful Information for Understanding Students’ Learning Behavior and Improving Teaching Quality",2022,"Information (Switzerland)","13","1","29","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122528443&doi=10.3390%2finfo13010029&partnerID=40&md5=8f724bde58e6656f53933ba106178fb8","This paper focuses on the study of automated process discovery using the Inductive visual Miner (IvM) and Directly Follows visual Miner (DFvM) algorithms to produce a valid process model for educational process mining in order to understand and predict the learning behavior of students. These models were evaluated on the publicly available xAPI (Experience API or Experience Application Programming Interface) dataset, which is an education dataset intended for tracking students’ classroom activities, participation in online communities, and performance. Experimental results with several performance measures show the effectiveness of the developed process models in helping experts to better understand students’ learning behavioral patterns. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Directly follows visual miner; Educational process mining (EPM); Inductive visual miner; Learning performance prediction; Model discovery algorithms","Application programming interfaces (API); Automation; Education computing; Learning algorithms; Learning systems; Miners; Students; Directly follow visual miner; Discovery algorithm; Educational process; Educational process mining; Inductive visual miner; Learning performance; Learning performance prediction; Model discoveries; Model discovery algorithm; Performance prediction; Process mining; Data mining",Article,Scopus,2-s2.0-85122528443
"Suat-Rojas N., Gutierrez-Osorio C., Pedraza C.","57405661400;57209267130;56825517500;","Extraction and Analysis of Social Networks Data to Detect Traffic Accidents",2022,"Information (Switzerland)","13","1","26","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122519154&doi=10.3390%2finfo13010026&partnerID=40&md5=11b12ca9b80803d9f839eed316a60e1e","Traffic accident detection is an important strategy governments can use to implement policies intended to reduce accidents. They usually use techniques such as image processing, RFID devices, among others. Social network mining has emerged as a low-cost alternative. However, social networks come with several challenges such as informal language and misspellings. This paper proposes a method to extract traffic accident data from Twitter in Spanish. The method consists of four phases. The first phase establishes the data collection mechanisms. The second consists of vectorially representing the messages and classifying them as accidents or non-accidents. The third phase uses named entity recognition techniques to detect the location. In the fourth phase, locations pass through a geocoder that returns their geographic coordinates. This method was applied to Bogota city and the data on Twitter were compared with the official traffic information source; comparisons showed some influence of Twitter on the commercial and industrial area of the city. The results reveal how effective the information on accidents reported on Twitter can be. It should therefore be considered as a source of information that may complement existing detection methods. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Classification; Intelligent transportation system; Machine learning; Named entity recognition; Natural language processing; Social media; Social sensors; Text mining; Traffic accident","Accidents; Character recognition; Data mining; Image processing; Learning algorithms; Machine learning; Natural language processing systems; Social networking (online); Text processing; Accident data; Accident detections; Images processing; Intelligent transportation systems; Low-costs; Named entity recognition; Network data; Social media; Social networks minings; Social sensors; Intelligent systems",Article,Scopus,2-s2.0-85122519154
"Kuki Á., Róth G., Nagy A., Zsuga M., Kéki S., Nagy T.","6701729781;57221604591;57404940600;35551623000;7003364321;7102141664;","A short-cut data mining method for the mass spectrometric characterization of block copolymers",2022,"Processes","10","1","42","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122516481&doi=10.3390%2fpr10010042&partnerID=40&md5=5fab87955532053db5f504c106598094","A new data mining approach as a short cut method is given for the determination of the copolymer composition from mass spectra. Our method simplifies the copolymer mass spectra by reduction of the number of mass peaks. The proposed procedure, namely the selection of the mass peaks, which is based on the most abundant peak of the mass spectrum, can be performed manually or more efficiently using our recently invented Mass-remainder analysis (MARA). The considerable reduction of the MS spectra also simplifies the calculation of the copolymer quantities for instance the number-and weight-average molecular weights (Mn and Mw, respectively), polydispersity index (Ð = Mw/Mn), average molar fraction (cA) and weight fraction (wA) of the comonomer A and so on. These copolymer properties are in line with those calculated by a reference method taking into account all the mass peaks of the copolymer distribution. We also suggest a highly efficient method and template for the determination of the composition drift by processing the reduced mass spectra. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Copolymers; Data mining; Mass spectrometry; Mass-remainder analysis",,Article,Scopus,2-s2.0-85122516481
"Lombardo G., Tomaiuolo M., Mordonini M., Codeluppi G., Poggi A.","57201158248;6507142395;6603120253;57212269027;7101947836;","Mobility in Unsupervised Word Embeddings for Knowledge Extraction—The Scholars’ Trajectories across Research Topics",2022,"Future Internet","14","1","25","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122509024&doi=10.3390%2ffi14010025&partnerID=40&md5=5dea335b1f358e57abf1576fafa38901","In the knowledge discovery field of the Big Data domain the analysis of geographic positioning and mobility information plays a key role. At the same time, in the Natural Language Processing (NLP) domain pre-trained models such as BERT and word embedding algorithms such as Word2Vec enabled a rich encoding of words that allows mapping textual data into points of an arbitrary multi-dimensional space, in which the notion of proximity reflects an association among terms or topics. The main contribution of this paper is to show how analytical tools, traditionally adopted to deal with geographic data to measure the mobility of an agent in a time interval, can also be effectively applied to extract knowledge in a semantic realm, such as a semantic space of words and topics, looking for latent trajectories that can benefit the properties of neural network latent representations. As a case study, the Scopus database was queried about works of highly cited researchers in recent years. On this basis, we performed a dynamic analysis, for measuring the Radius of Gyration as an index of the mobility of researchers across scientific topics. The semantic space is built from the automatic analysis of the paper abstracts of each author. In particular, we evaluated two different methodologies to build the semantic space and we found that Word2Vec embeddings perform better than the BERT ones for this task. Finally, The scholars’ trajectories show some latent properties of this model, which also represent new scientific contributions of this work. These properties include (i) the correlation between the scientific mobility and the achievement of scientific results, measured through the H-index; (ii) differences in the behavior of researchers working in different countries and subjects; and (iii) some interesting similarities between mobility patterns in this semantic realm and those typically observed in the case of human mobility. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Bert; Human mobility; Knowledge discovery; Radius of Gyration; Semantic space; Word embedding; Word2vec","Behavioral research; Data mining; Natural language processing systems; Semantics; Trajectories; Bert; Embeddings; Human mobility; Knowledge extraction; Property; Radius of gyration; Research topics; Semantic Space; Word embedding; Word2vec; Embeddings",Article,Scopus,2-s2.0-85122509024
"Yang W., Guo J.","57405052600;57404897100;","Consumers’ Purchase Behavior Preference in E-Commerce Platform Based on Data Mining Algorithm",2022,"International Journal of Circuits, Systems and Signal Processing","16",,,"603","609",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122495367&doi=10.46300%2f9106.2022.16.75&partnerID=40&md5=6d080bdd54ec2f91386384c88c24bb71","E-commerce platform can recommend products to users by analyzing consumers’ purchase behavior preference. In the clustering process, the existing methods of purchasing behavior preference analysis are easy to fall into the local optimal problem, which makes the results of preference analysis inaccurate. Therefore, this paper proposes a method of consumer purchasing behavior preference analysis on e-commerce platform based on data mining algorithm. Create e-commerce platform user portrait template with consumer data records, select attribute variables and set value range. This paper uses data mining algorithm to extract the purchase behavior characteristics of user portrait template, takes the characteristics as the clustering analysis object, designs the clustering algorithm of consumer purchase behavior, and grasps the common points of group behavior. On this basis, the model of consumer purchase behavior preference is established to predict and evaluate the behavior preference. The experimental results show that the accuracy rate of this method is 91.74%, the recall rate is 88.67%, and the F1 value is 90.17%, which are higher than the existing methods, and can provide consumers with more satisfactory product information push. © 2022, North Atlantic University Union NAUN. All rights reserved.","Behavioral preference; Consumer purchase behavior; Data mining algorithm; E-commerce platform","Clustering algorithms; Consumer behavior; Data mining; Purchasing; Sales; Behavioural preferences; Clustering process; Commerce platforms; Consumer purchase; Consumer purchase behavior; Data mining algorithm; E- commerces; E-commerce platform; Preference analysis; Purchasing behaviors; Electronic commerce",Article,Scopus,2-s2.0-85122495367
"Revathy G., Preethi T., Benazir Begum A., Priyadarshini S., Subhalakshmi R.T.","57194002956;57224172329;57201722900;57404159200;55823953400;","MACHINE LEARNING ALGORITHMS FOR PREDICTION OF DISEASES",2022,"International Journal of Mechanical Engineering","7","1",,"2672","2676",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122444316&partnerID=40&md5=b1196e0e836955afbbba5d25737daf8e","The development and application of several well-known data mining techniques in a variety of real-world application areas (e.g., industry, healthcare, and bio science) has led to their use in machine learning environments to extract useful information from specified data in healthcare communities, biomedical fields, and other fields. Early illness prediction, patient treatment, and community services all benefit from precise medical database analysis. Machine learning techniques have been effectively used in a variety of applications, including disease prediction.The goal of constructing a classifier system utilising machine learning algorithms is to greatly assist physicians in predicting and diagnosing diseases at an early stage, which will greatly aid in the resolution of health-related difficulties. For study, a sample of 4920 patient records diagnosed with 41 disorders was chosen. There were 41 diseases in the dependent variable. We chose 95 out of 132 independent variables (symptoms) that are strongly associated to diseases and improved them.The illness prediction system constructed utilising Machine learning techniques such as Decision Tree classifier, Random forest classifier, Nave Bayes classifier, and K-NN classifier is demonstrated in this study work. The project gives a comparison of the results of the algorithms mentioned above. © Kalahari Journals.","Data mining; Decision tree classifier; K-NN classifier; Machine learning; Naïve bayes classifier; Random forest classifier",,Article,Scopus,2-s2.0-85122444316
"Fink M.A., Mayer V.L., Schneider T., Seibold C., Stiefelhagen R., Kleesiek J., Weber T.F., Kauczor H.-U.","55978793200;57403331700;57225832793;57217523260;6602180348;9743752700;57404197500;57402985600;","CT Angiography Clot Burden Score from Data Mining of Structured Reports for Pulmonary Embolism",2022,"Radiology","302","1",,"175","184",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122442191&doi=10.1148%2fradiol.2021211013&partnerID=40&md5=9748047247c688646b5131d785a978ac","Background: Many studies emphasize the role of structured reports (SRs) because they are readily accessible for further automated analyses. However, using SR data obtained in clinical routine for research purposes is not yet well represented in literature. Purpose: To compare the performance of the Qanadli scoring system with a clot burden score mined from structured pulmonary embolism (PE) reports from CT angiography. Materials and Methods: In this retrospective study, a rule-based text mining pipeline was developed to extract descriptors of PE and right heart strain from SR of patients with suspected PE between March 2017 and February 2020. From standardized PE reporting, a pulmonary artery obstruction index (PAOI) clot burden score (PAOICBS) was derived and compared with the Qanadli score (PAOIQ). Scoring time and confidence from two independent readings were compared. Interobserver and interscore agreement was tested by using the intraclass correlation coefficient (ICC) and Bland-Altman analysis. To assess conformity and diagnostic performance of both scores, areas under the receiver operating characteristic curve (AUCs) were calculated to predict right heart strain incidence, as were optimal cutoff values for maximum sensitivity and specificity. Results: SR content authored by 67 residents and signed off by 32 consultants from 1248 patients (mean age, 63 years ± 17 [standard deviation]; 639 men) was extracted accurately and allowed for PAOICBS calculation in 304 of 357 (85.2%) PE-positive reports. The PAOICBS strongly correlated with the PAOIQ (r = 0.94; P < .001). Use of PAOICBS yielded overall time savings (1.3 minutes ± 0.5 vs 3.0 minutes ± 1.7), higher confidence levels (4.2 ± 0.6 vs 3.6 ± 1.0), and a higher ICC (ICC, 0.99 vs 0.95), respectively, compared with PAOIQ (each, P < .001). AUCs were similar for PAOICBS (AUC, 0.75; 95% CI: 0.70, 0.81) and PAOIQ (AUC, 0.77; 95% CI: 0.72, 0.83; P = .68), with cutoff values of 27.5% for both scores. Conclusion: Data mining of structured reports enabled the development of a CT angiography scoring system that simplified the Qanadli score as a semiquantitative estimate of thrombus burden in patients with pulmonary embolism. © RSNA, 2021.",,"computed tomographic angiography; data mining; diagnostic imaging; female; human; lung embolism; male; middle aged; pathology; procedures; pulmonary artery; retrospective study; sensitivity and specificity; thrombosis; Computed Tomography Angiography; Data Mining; Female; Humans; Male; Middle Aged; Pulmonary Artery; Pulmonary Embolism; Retrospective Studies; Sensitivity and Specificity; Thrombosis",Article,Scopus,2-s2.0-85122442191
"Sun C., Yang Z., Wang L., Zhang Y., Lin H., Wang J.","57201741324;16029941200;57191903778;57404002200;24468572400;55934279300;","MRC4BioER: Joint extraction of biomedical entities and relations in the machine reading comprehension framework",2022,"Journal of Biomedical Informatics","125",,"103956","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122436444&doi=10.1016%2fj.jbi.2021.103956&partnerID=40&md5=418518a4137d88a3f7713ec2d4fa23f4","Extracting entities and their relations from unstructured literature to form structured triplets is essential for biomedical knowledge extraction. Because sentences in biomedical datasets usually have many special overlapping triplets, it is difficult to use previous work to extract these triplets effectively. In this work, we propose a novel tagging strategy to achieve joint extraction in the machine reading comprehension framework. On the one hand, our method uses Query in the machine reading comprehension framework to introduce the information of the specific relation. On the other hand, our method introduces a tagging strategy for overlapping triplets in the biomedical domain. We use CHEMPROT and DDIExtraction2013 datasets to evaluate our method. The experimental results demonstrate that our proposed method can enhance the model's ability to deal with overlapping triplets, improving extraction performance. © 2021 Elsevier Inc.","Joint extraction; Machine reading comprehension; MRC; Text mining; Triplet extraction","Data mining; Biomedical domain; Joint extraction; Knowledge extraction; Machine reading comprehension; Modeling abilities; MRC; Performance; Reading comprehension; Triplet extraction; Extraction; article; mining; reading; comprehension; data mining; language; methodology; publication; Comprehension; Data Mining; Language; Publications; Research Design",Article,Scopus,2-s2.0-85122436444
"Bui-Ngoc D., Nguyen-Tran H., Nguyen-Ngoc L., Tran-Ngoc H., Bui-Tien T., Tran-Viet H.","57403577400;57221116226;57204858205;57204859146;57204859112;57404094900;","Damage detection in structural health monitoring using hybrid convolution neural network and recurrent neural network",2022,"Frattura ed Integrita Strutturale","16","59",,"461","470",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122427041&doi=10.3221%2fIGF-ESIS.59.30&partnerID=40&md5=69de50235c69a70bf64b812d698bd3fe","The process of damage identification in Structural Health Monitoring (SHM) gives us a lot of practical information about the current status of the inspected structure. The target of the process is to detect damage status by processing data collected from sensors, followed by identifying the difference between the damaged and the undamaged states. Different machine learning techniques have been applied to attempt to extract features or knowledge from vibration data, however, they need to learn prior knowledge about the factors affecting the structure. In this paper, a novel method of structural damage detection is proposed using a hybrid convolution neural network and recurrent neural network. A convolution neural network is used to extract deep features while recurrent neural network is trained to learn the long-term historical dependency in time series data. This proposed method which combines two types of features-spatial and temporal-helps to increase discrimination ability when being compared with the one that contains deep features only. Finally, the neural network is applied to categorize the time series into two states-undamaged and damaged. The accuracy of the proposed method was tested on a benchmark dataset of Z24-bridge (Switzerland). The result shows that the hybrid method provides a high level of accuracy in damage identification of the tested structure. © 2022.","Convolution Neural Network; Damage Detection; Recurrent Neural Network","Convolution; Damage detection; Data handling; Data mining; Structural health monitoring; Time series; Convolution neural network; Current status; Damage Identification; Learn+; Machine learning techniques; Novel methods; Prior-knowledge; Structural damage detection; Time-series data; Vibration data; Recurrent neural networks",Article,Scopus,2-s2.0-85122427041
"Miao F., Xie X., Wu Y., Zhao F.","57102827200;57226714025;55635536600;57401866500;","Data Mining and Deep Learning for Predicting the Displacement of “Step-Like” Landslides",2022,"Sensors","22","2","481","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122381092&doi=10.3390%2fs22020481&partnerID=40&md5=2e3ca0eab5a94c7dcdbd0d2a5d536f22","Landslide displacement prediction is one of the unsolved challenges in the field of geological hazards, especially in reservoir areas. Affected by rainfall and cyclic fluctuations in reservoir water levels, a large number of landslide disasters have developed in the Three Gorges Reservoir Area. In this article, the Baishuihe landslide was taken as the research object. Firstly, based on time series theory, the landslide displacement was decomposed into three parts (trend term, periodic term, and random term) by Variational Mode Decomposition (VMD). Next, the landslide was divided into three deformation states according to the deformation rate. A data mining algorithm was introduced for selecting the triggering factors of periodic displacement, and the Fruit Fly Optimization Algorithm–Back Propagation Neural Network (FOA-BPNN) was applied to the training and prediction of periodic and random displacements. The results show that the displacement monitoring curve of the Baishuihe landslide has a “step-like” trend. Using VMD to decompose the displacement of a landslide can indicate the triggering factors, which has clear physical significance. In the proposed model, the R2 values between the measured and predicted displacements of ZG118 and XD01 were 0.977 and 0.978 respectively. Compared with previous studies, the prediction model proposed in this article not only ensures the calculation efficiency but also further improves the accuracy of the prediction results, which could provide guidance for the prediction and prevention of geological disasters. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Baishuihe landslide; Data mining; Displacement prediction; Three Gorges Reservoir; VMD-FOA-BPNN","Deep learning; Deformation; Disaster prevention; Disasters; Forecasting; Geology; Landslides; Neural networks; Reservoirs (water); Water levels; Back-propagation neural networks; Baishuihe landslide; Displacement prediction; Fly optimization algorithms; Geological hazards; Mode decomposition; Reservoir area; Three Gorge reservoir; Triggering factors; Variational mode decomposition-fly optimization algorithm–back propagation neural network; Data mining; article; back propagation neural network; calculation; data mining; decomposition; deep learning; disaster; Drosophila; landslide; nonhuman; prediction; time series analysis",Article,Scopus,2-s2.0-85122381092
"Du S., Hong J., Wang Y., Xing K., Qiu T.","57215834558;35778190300;36761751800;57311101800;57310381600;","Physical-Related Feature Extraction from Simulated SAR Image Based on the Adversarial Encoding Network for Data Augmentation",2022,"IEEE Geoscience and Remote Sensing Letters","19",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122377918&doi=10.1109%2fLGRS.2021.3100642&partnerID=40&md5=9254b7a299d47a6f4d5d83854024b78c","The synthetic aperture radar automatic target recognition (SAR ATR) application based on the deep convolutional network often faces data scarcity. SAR image simulation based on electromagnetic and geometric calculations can provide a large amount of data that contains interpretable physical features, such as shadow and contour. However, there is a big difference between the simulated SAR image and the real image, and it is difficult to directly use it for data augmentation. This letter proposes the adversarial encoding network to extract the physical-related features, which can be understood as the common features between the simulated and real data. By designing the adversarial learning between an encoder and a discriminator, the encoder can extract real features from the simulated images. The encoded features are sent to a classifier to ensure the correct category information. A decoder is used to reconstruct the encoded features into the input image so that the encoded feature retains the image information as much as possible. Ablation experiments and comparative experiments are used to verify the ability of each module and the performance of the proposed method. The results show that the proposed model can achieve 98.55% accuracy, especially when the real data are insufficient for classification, which verifies that the proposed method is effective for data augmentation. © 2004-2012 IEEE.","Automatic target recognition (ATR); data augmentation; generative adversarial network (GAN)","Automatic target recognition; Classification (of information); Data mining; Encoding (symbols); Generative adversarial networks; Network coding; Radar imaging; Radar target recognition; Automatic target recognition; Convolutional networks; Data augmentation; Data scarcity; Face data; Features extraction; Generative adversarial network; Image-based; SAR Images; Synthetic aperture radar automatic target recognition; Synthetic aperture radar; algorithm; data set; detection method; satellite imagery; simulation; synthetic aperture radar",Article,Scopus,2-s2.0-85122377918
"Vincent D., Bui A., Ram D., Ezernieks V., Bedon F., Panozzo J., Maharjan P., Rochfort S., Daetwyler H., Hayden M.","13409758500;57402041500;57210442942;25647277000;25631700700;6603689693;57202007913;6603353411;24586822900;55762825800;","Mining the Wheat Grain Proteome",2022,"International Journal of Molecular Sciences","23","2","713","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122375031&doi=10.3390%2fijms23020713&partnerID=40&md5=ccef487be9eada71a42487570e1057b2","Bread wheat is the most widely cultivated crop worldwide, used in the production of food products and a feed source for animals. Selection tools that can be applied early in the breeding cycle are needed to accelerate genetic gain for increased wheat production while maintaining or improving grain quality if demand from human population growth is to be fulfilled. Proteomics screening assays of wheat flour can assist breeders to select the best performing breeding lines and discard the worst lines. In this study, we optimised a robust LC–MS shotgun quantitative proteomics method to screen thousands of wheat genotypes. Using 6 cultivars and 4 replicates, we tested 3 resuspension ratios (50, 25, and 17 µL/mg), 2 extraction buffers (with urea or guanidine-hydrochloride), 3 sets of proteases (chymotrypsin, Glu-C, and trypsin/Lys-C), and multiple LC settings. Protein identifications by LC–MS/MS were used to select the best parameters. A total 8738 wheat proteins were identified. The best method was validated on an independent set of 96 cultivars and peptides quantities were normalised using sample weights, an internal standard, and quality controls. Data mining tools found particularly useful to explore the flour proteome are presented (UniProt Retrieve/ID mapping tool, KEGG, AgriGO, REVIGO, and Pathway Tools). © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Data mining; LC–MS/MS; Normalisation; Protease; Shotgun proteomics; Triticum aestivum","plant protein; proteome; flour; food grain; gene expression regulation; genetics; human; liquid chromatography; metabolism; procedures; proteomics; reproducibility; tandem mass spectrometry; wheat; Chromatography, Liquid; Edible Grain; Flour; Gene Expression Regulation, Plant; Humans; Plant Proteins; Proteome; Proteomics; Reproducibility of Results; Tandem Mass Spectrometry; Triticum",Article,Scopus,2-s2.0-85122375031
"Wang Y., Gao X., Jiang P., Guo X., Wang R., Guan Z., Chen L., Xu C.","57209057445;35073791800;57209868589;57191738574;57209053069;50760988700;57196309761;57402333700;","An extreme gradient boosting technique to estimate TBM penetration rate and prediction platform",2022,"Bulletin of Engineering Geology and the Environment","81","1","58","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122366176&doi=10.1007%2fs10064-021-02527-5&partnerID=40&md5=fc6b968b9600515b211c3002ca38e537","An accurate prediction of the penetration rate (PR) of a tunnel boring machine (TBM) is essential for the schedule and cost estimation of tunnel excavation. To better meet the needs of modern information construction, more computer technologies are being used to integrate the analysis and management of construction data. Herein, an online prediction platform based on a data mining algorithm using ensemble learning (extreme gradient boosting (XGBoost)) is developed for TBM performance prediction. The platform establishes the model and displays the prediction results, while storing a considerable amount of machine data, and providing services for TBMs of multiple projects simultaneously. In establishing the prediction model, users can change the algorithm parameters according to the engineering situation. The prediction capabilities of the platform are demonstrated by 200 field samples obtained from the Songhua River water conveyance project in Jilin. The mean absolute percentage error, coefficient of determination, root mean squared error, variance account for (VAF), and a20-index of the PR are 6.07%, 0.8651, 3.5862, 87.06%, and 0.925, respectively. The results show that the prediction model has a reliable prediction accuracy, which is higher than that of the gradient boosting decision tree, and these results can be displayed on the online platform. It provides effective help for TBM intelligent tunneling. © 2021, Springer-Verlag GmbH Germany, part of Springer Nature.","Ensemble learning; Online prediction platform; TBM performance prediction; XGBoost","Construction equipment; Cost estimating; Data mining; Decision trees; E-learning; Forecasting; Mean square error; Tunneling (excavation); Ensemble learning; Gradient boosting; Machine performance; Online prediction; Online prediction platform; Penetration rates; Performance prediction; Prediction modelling; Tunnel boring machine performance prediction; Xgboost; Boring machines (machine tools); algorithm; ensemble forecasting; estimation method; prediction; TBM; tunnel; China; Songhua River",Article,Scopus,2-s2.0-85122366176
"Landowska A., Zawadzka T., Zawadzki M.","8967776800;15770532200;8961538700;","Mining Inconsistent Emotion Recognition Results with the Multidimensional Model",2022,"IEEE Access","10",,,"6737","6759",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122331024&doi=10.1109%2fACCESS.2021.3139078&partnerID=40&md5=c5429f16f36bae1d6a06e9da7fc00c3a","The paper deals with the challenge of inconsistency in multichannel emotion recognition. The focus of the paper is to explore factors that might influence the inconsistency. The paper reports an experiment that used multi-camera facial expression analysis with multiple recognition systems. The data were analyzed using a multidimensional approach and data mining techniques. The study allowed us to explore camera location, occlusions and algorithm factors in the late fusion of emotion recognition results. We proposed to use a multidimensional data model for mining the various interdependencies between the factors of inconsistency. The study allowed the exploration of challenges in multichannel emotion recognition. It was achieved by comparing the consistency of obtained emotions and identification of rules determining conditions when the obtained emotions are consistent. However, the main novelty of the paper is the method of mining the inconsistencies. The study might be interesting both for researchers dealing with integration in emotion recognition, as well as for practitioners who use automatic emotion analysis software and expect to get valid results. © 2013 IEEE.","Data mining; Emotion recognition; Facial expression analysis; Inconsistency; Late fusion; Multidimensional model","Cameras; Data mining; Job analysis; Speech recognition; Adaptation models; Emotion recognition; Facial expressions analysis; Inconsistency; Late fusion; Multi channel; Multi-cameras; Multi-dimensional model; Task analysis; Face recognition",Article,Scopus,2-s2.0-85122331024
"Ma Z.","55279243800;","Analysis of the complete genome sequence of a rhizosphere-derived Pseudomonas sp. HN3-2 leads to the characterization of a cyclic lipopeptide-type antibiotic bananamide C",2022,"3 Biotech","12","1","35","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122328224&doi=10.1007%2fs13205-021-03100-3&partnerID=40&md5=bec88ee004c6f03c2e6bbd9e0efc8659","A fluorescence and biosurfactant-producing strain HN3-2 was isolated from a rhizosphere soil sample of wheat plants and the chromosome of the strain HN3-2 was sequenced and was analyzed by multiple bioinformatics tools in this study. The genome size of the strain HN3-2 is 6,441,476 bp, with a GC content of 60.54%. 16Sr RNA-based phylogeny analysis showed that the strain HN3-2 belongs to Pseudomonas koreensis subgroup in Pseudomonas species. Preliminary data from genome mining have showed that the strain Pseudomonas sp. HN3-2 is capable of producing a peptide-type metabolite. Solid-phase extraction, reversed-phase high performance liquid chromatography (RP-HPLC) together with liquid chromatography–mass spectrometry, high-resolution mass spectrometry and tandem mass spectrometry analysis have led to the purification and identification of a cyclic lipopeptide (CLP) bananamide C (1) from the fermentative broth of the strain Pseudomonas sp. HN3-2. Moreover, the biological activity tests showed that banananmide 3 displays moderate antagonistic activity against Staphylococcus aureus and Escherichia coli. Collectively, these results provide the possibility of developing the CLP bananamide C as a drug leads for medical applications. © 2022, King Abdulaziz City for Science and Technology.","Antimicrobial activity; Cyclic lipopeptide; Nonribosomal peptide synthetase; Pseudomonas genome","bananamide C; polypeptide antibiotic agent; RNA 16S; unclassified drug; antibacterial activity; Article; bacterial chromosome; bacterial genome; bacterial strain; bacterium isolation; bioinformatics; controlled study; data mining; DNA base composition; drug identification; drug purification; Escherichia coli; fluorescence; genome size; liquid chromatography-mass spectrometry; nonhuman; phylogenetic tree; phylogeny; Pseudomonas; Pseudomonas HN3 2; Pseudomonas koreensis; reversed phase high performance liquid chromatography; rhizosphere bacterium; soil microflora; solid phase extraction; Staphylococcus aureus; tandem mass spectrometry; wheat; whole genome sequencing",Article,Scopus,2-s2.0-85122328224
"Ayesha S., Hanif M.K., Talib R.","57214070419;57194325074;57194393379;","Performance Enhancement of Predictive Analytics for Health Informatics Using Dimensionality Reduction Techniques and Fusion Frameworks",2022,"IEEE Access","10",,,"753","769",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122325079&doi=10.1109%2fACCESS.2021.3139123&partnerID=40&md5=95aa598aa02cf0bd324dc11db376d8c8","Predictive analytics has become an essential area of research in health informatics. The availability of multi-source and multi-modal data in healthcare has made the disease prediction, diagnosis, and medication process more effective and reliable. However, the analysis and decision making have become challenging task, particularly when data is in multiple formats and from different sources. In this study, different frameworks have been proposed to handle multi-nature data at different levels for predictive analytics. Dimensionality reduction techniques have been applied to extract relevant features to enhance the analysis. To improve the performance of predictive analytics at different fusion levels, the potential benefits of multi-modal data have been discussed. Moreover, notable improvement in prediction accuracy has been observed through experimental evaluation of the proposed frameworks. Furthermore, the issues which have been found during dimension reduction and fusion approaches have also been highlighted. © 2013 IEEE.","data fusion; Dimensionality reduction; feature extraction; feature fusion; high dimensional data; machine learning","Clustering algorithms; Data fusion; Data integration; Data mining; Data reduction; Decision making; Diagnosis; Extraction; Learning systems; Modal analysis; Predictive analytics; Dimensionality reduction; Dimensionality reduction techniques; Features extraction; Features fusions; Health informatics; High dimensional data; Machine-learning; Medical services; Feature extraction",Article,Scopus,2-s2.0-85122325079
"Fouad M.A., Hussein W., Rady S., Yu P.S., Gharib T.F.","57398394000;55984628300;34870497700;7402366049;6602973637;","An Efficient Approach for Mining Reliable High Utility Patterns",2022,"IEEE Access","10",,,"1419","1431",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122321798&doi=10.1109%2fACCESS.2021.3139028&partnerID=40&md5=fc087d08f0267b177f77f3d42b9c64b8","Utility mining is one of the most thriving research topics with a wide range of real-world applications. High utility pattern mining uses a utility function to extract all desired patterns that exceed a minimum utility threshold. However, a significant number of patterns will be generated if this threshold is set too low, which is an inherent limitation of these algorithms. This may cause the mining process to be inefficient as it would be difficult to analyze the patterns found. Furthermore, most of these patterns are unreliable and hard to be employed in making decisions. This paper proposed a novel problem of mining reliable high utility patterns by adapting the concept of reliability to mine a significant type of pattern called reliable high utility patterns. To address this issue, an efficient approach named RUPM (Reliable Utility-based Pattern Mining) is presented. RUPM introduces three novel measurements for estimating the reliability of utility-based patterns and proposes several strategies to efficiently handle reliable patterns with high utility values. Experimental results suggest that up to 99% of the patterns discovered by existing traditional high utility pattern mining algorithms were, in fact, unreliable. In contrast, the average reliability proportion in the resultant patterns obtained from the RUPM approach is at least 47.6% higher. Moreover, the proposed pruning strategies provide a reduction in both the runtime and memory usage. © 2013 IEEE.","Data mining; pruning strategy; reliable high-utility itemset; utility mining","Reliability; Weighing; Correlation; High utility itemsets; Itemset; Pattern mining; Prediction algorithms; Pruning strategy; Real-world; Reliable high-utility itemset; Research topics; Utility mining; Data mining",Article,Scopus,2-s2.0-85122321798
"Talebi H., Peeters L.J.M., Otto A., Tolosana-Delgado R.","55623471800;24773909400;57398432900;23390787200;","A Truly Spatial Random Forests Algorithm for Geoscience Data Analysis and Modelling",2022,"Mathematical Geosciences","54","1",,"","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122315210&doi=10.1007%2fs11004-021-09946-w&partnerID=40&md5=7cf7dbf4824cad82cac02bf04db72700","Spatial data mining helps to find hidden but potentially informative patterns from large and high-dimensional geoscience data. Non-spatial learners generally look at the observations based on their relationships in the feature space, which means that they cannot consider spatial relationships between regionalised variables. This study introduces a novel spatial random forests technique based on higher-order spatial statistics for analysis and modelling of spatial data. Unlike the classical random forests algorithm that uses pixelwise spectral information as predictors, the proposed spatial random forests algorithm uses the local spatial-spectral information (i.e., vectorised spatial patterns) to learn intrinsic heterogeneity, spatial dependencies, and complex spatial patterns. Algorithms for supervised (i.e., regression and classification) and unsupervised (i.e., dimension reduction and clustering) learning are presented. Approaches to deal with big data, multi-resolution data, and missing values are discussed. The superior performance and usefulness of the proposed algorithm over the classical random forests method are illustrated via synthetic and real cases, where the remotely sensed geophysical covariates in North West Minerals Province of Queensland, Australia, are used as input spatial data for geology mapping, geochemical prediction, and process discovery analysis. © 2021, Crown.","Geostatistical learning; Higher-order spatial statistics; Random forests; Spatial correlation; Spatial data","Clustering algorithms; Data mining; Geology; Higher order statistics; Learning algorithms; Spatial variables measurement; Geoscience data; Geostatistical; Geostatistical learning; High-order; High-order spatial statistic; Higher-order; Random forest algorithm; Spatial correlations; Spatial data; Spatial statistics; Decision trees; algorithm; computer simulation; data assimilation; image analysis; image resolution; numerical model; pixel; spatial cognition; Australia; Queensland",Article,Scopus,2-s2.0-85122315210
"Frisoni G., Moro G., Carlassare G., Carbonaro A.","57219265807;8702087400;57397768300;57211468869;","Unsupervised event graph representation and similarity learning on biomedical literature",2022,"Sensors","22","1","3","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122315152&doi=10.3390%2fs22010003&partnerID=40&md5=4340054322032374f176f7f6533af7aa","The automatic extraction of biomedical events from the scientific literature has drawn keen interest in the last several years, recognizing complex and semantically rich graphical interactions otherwise buried in texts. However, very few works revolve around learning embeddings or similarity metrics for event graphs. This gap leaves biological relations unlinked and prevents the application of machine learning techniques to promote discoveries. Taking advantage of recent deep graph kernel solutions and pre-trained language models, we propose Deep Divergence Event Graph Kernels (DDEGK), an unsupervised inductive method to map events into low-dimensional vectors, preserving their structural and semantic similarities. Unlike most other systems, DDEGK operates at a graph level and does not require task-specific labels, feature engineering, or known correspondences between nodes. To this end, our solution compares events against a small set of anchor ones, trains cross-graph attention networks for drawing pairwise alignments (bolstering interpretability), and employs transformer-based models to encode continuous attributes. Extensive experiments have been done on nine biomedical datasets. We show that our learned event representations can be effectively employed in tasks such as graph classification, clustering, and visualization, also facilitating downstream semantic textual similarity. Empirical results demonstrate that DDEGK significantly outperforms other state-of-the-art methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Biomedical text mining; Event embedding; Event extraction; Graph kernels; Graph neural networks; Graph representation learning; Graph similarity learning; Metric learning","Data mining; Extraction; Graph neural networks; Semantics; Biomedical text minings; Embeddings; Event embedding; Events extractions; Graph kernels; Graph neural networks; Graph representation; Graph representation learning; Graph similarity; Graph similarity learning; Metric learning; Similarity learning; Embeddings; cluster analysis; machine learning; publication; semantics; Cluster Analysis; Machine Learning; Publications; Semantics",Article,Scopus,2-s2.0-85122315152
"Nguyen Huu P., Nguyen Thi N., Ngoc T.P.","57413895800;57214758043;57398391100;","Proposing Posture Recognition System Combining MobilenetV2 and LSTM for Medical Surveillance",2022,"IEEE Access","10",,,"1839","1849",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122305149&doi=10.1109%2fACCESS.2021.3138778&partnerID=40&md5=051634e5fb82f0e9ba15d74c96e8a30f","This paper proposes a posture recognition system that can be applied for medical surveillance. The proposed method estimates human posture using mobilenetV2 and long short-term memory (LSTM) to extract the important features of an image. The output of the system was a fully estimated skeleton. We used seven human indoor postures, including lying, sitting, crouching, standing, walking, fighting, and falling, and classified them. The output results are the extraction of the human skeleton and the corresponding labels for the poses. We first experiment with classification using machine learning. The system only achieves approximately 88% accuracy because it is not able to classify similar postures, such as standing and walking. This difference can be caused by the extraction of features for static images, and the machine learning classification algorithm has not reached accuracy with training data. Therefore, we proposed the integration of the LSTM model into the proposed system. LSTM learns the features of the skeleton and provides classification results for postures. As a result, our system improved the accuracy by up to 99%. Similar postures, such as standing and walking, have improved accuracy by up to 7%. In addition, we performed the system on the Jetson Nano hardware. The results show that it can run on a low-profile (44% CPU and 2.1 frames per second) that is capable of applications for remote patient monitoring devices. © 2013 IEEE.","intelligent healthcare; long short-term memory; Openpose; posture detection; skeleton model","Brain; Classification (of information); Data mining; Extraction; Long short-term memory; Medical imaging; Musculoskeletal system; Patient monitoring; Security systems; Features extraction; Intelligent healthcare; Legged locomotion; Openpose; Posture detection; Posture recognition; Recognition systems; Skeleton; Skeleton models; Video; Feature extraction",Article,Scopus,2-s2.0-85122305149
"Oliveira E.E., Migueis V.L., Borges J.L.","57222010670;47661262500;55513362000;","Understanding Overlap in Automatic Root Cause Analysis in Manufacturing Using Causal Inference",2022,"IEEE Access","10",,,"191","201",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122296581&doi=10.1109%2fACCESS.2021.3139199&partnerID=40&md5=40e20418f053ff9d20de49983d230f3a","Overlap has been identified in previous works as a significant obstacle to automated diagnosis using data mining algorithms, since it makes it impossible to discern how each machine influences product quality. Several solutions that handle overlap have been proposed, but the final result is a list of potential overlapped root causes. The goal of this paper is to develop a solution resilient to overlap that can determine the true root cause from a list of possible root causes, when possible, and determine the conditions in which it is possible to identify the root causes. This allows for a better understanding of overlap, and enables the development of a fully automatic root cause analysis for manufacturing. To do so, we propose an automatic root cause analysis approach that uses causal inference and do calculus to determine the true root cause. The proposed approach was validated on simulated and real case-study data, and allowed for an estimation of the effect of a product passing through a certain machine while disregarding the effect of overlap, in certain conditions. The results were on par with the state-of-the-art solutions capable of handling overlap. The contributions of this paper are a graphical definition of overlap, the identification of the conditions in which is possible to overcome the effect of overlap, and a solution that can present a single true root cause when such conditions are met. © 2013 IEEE.","Causal inference; fault diagnosis; manufacturing; root cause analysis","Calculations; Classification (of information); Computer aided diagnosis; Fault detection; Inference engines; Object recognition; Quality control; Causal inferences; Classification algorithm; Condition; Faults diagnosis; Manufacturing process; Objects recognition; Quality assessment; Root cause; Root cause analysis; Data mining",Article,Scopus,2-s2.0-85122296581
"Chen S., Shi W., Zhou M., Zhang M., Xuan Z.","57216964265;57221530932;56308820500;57207106820;57397471200;","CGSANet: A Contour-Guided and Local Structure-Aware Encoder-Decoder Network for Accurate Building Extraction from Very High-Resolution Remote Sensing Imagery",2022,"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15",,,"1526","1542",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122291478&doi=10.1109%2fJSTARS.2021.3139017&partnerID=40&md5=0c0e0347bf7f9c2353b431b93ffda8c1","Extracting buildings accurately from very high-resolution (VHR) remote sensing imagery is challenging due to diverse building appearances, spectral variability, and complex background in VHR remote sensing images. Recent studies mainly adopt a variant of the fully convolutional network (FCN) with an encoder-decoder architecture to extract buildings, which has shown promising improvement over conventional methods. However, FCN-based encoder-decoder models still fail to fully utilize the implicit characteristics of building shapes. This adversely affects the accurate localization of building boundaries, which is particularly relevant in building mapping. A contour-guided and local structure-aware encoder-decoder network (CGSANet) is proposed to extract buildings with more accurate boundaries. CGSANet is a multitask network composed of a contour-guided (CG) and a multiregion-guided (MRG) module. The CG module is supervised by a building contour that effectively learns building contour-related spatial features to retain the shape pattern of buildings. The MRG module is deeply supervised by four building regions that further capture multiscale and contextual features of buildings. In addition, a hybrid loss function was designed to improve the structure learning ability of CGSANet. These three improvements benefit each other synergistically to produce high-quality building extraction results. Experimental results on the WHU and NZ32km2 building datasets demonstrate that compared with the tested algorithms, CGSANet can produce more accurate building extraction results and achieve the best intersection over union value 91.55% and 90.02%, respectively. Experiments on the INRIA building dataset further demonstrate the ability for generalization of the proposed framework, indicating great practical potential. © 2008-2012 IEEE.","Building extraction; fully convolutional network (FCN); hybrid loss function; multitask learning; very high resolution (VHR) remote sensing imagery","Buildings; Convolution; Data mining; Decoding; Extraction; Network coding; Neural networks; Remote sensing; Building extraction; Convolutional networks; Convolutional neural network; Features extraction; Fully convolutional network; High resolution remote sensing imagery; Hybrid loss function; Loss functions; Remote-sensing; Shape; Very high resolution; Very high resolution remote sensing imagery; Semantics",Article,Scopus,2-s2.0-85122291478
"Li Q., Peng H., Li J., Wu J., Ning Y., Wang L., Yu P.S., Wang Z.","57221243818;57190014707;55720560100;23971568900;57211751494;57007714500;7402366049;35111811300;","Reinforcement Learning-Based Dialogue Guided Event Extraction to Exploit Argument Relations",2022,"IEEE/ACM Transactions on Audio Speech and Language Processing","30",,,"520","533",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122289846&doi=10.1109%2fTASLP.2021.3138670&partnerID=40&md5=bcd44c1fa1608547409ac121e0420d64","Event extraction is a ftask for natural language processing. Finding the roles of event arguments like event participants is essential for event extraction. However, doing so for real-life event descriptions is challenging because an argument’s role often varies in different contexts. While the relationship and interactions between multiple arguments are useful for settling the argument roles, such information is largely ignored by existing approaches. This paper presents a better approach for event extraction by explicitly utilizing the relationships of event arguments. We achieve this through a carefully designed task-oriented dialogue system. To model the argument relation, we employ reinforcement learning and incremental learning to extract multiple arguments via a multi-turned, iterative process. Our approach leverages knowledge of the already extracted arguments of the same sentence to determine the role of arguments that would be difficult to decide individually. It then uses the newly obtained information to improve the decisions of previously extracted arguments. This two-way feedback process allows us to exploit the argument relations to effectively settle argument roles, leading to better sentence understanding and event extraction. Experimental results show that our approach consistently outperforms seven state-of-the-art event extraction methods for the classification of events and argument role and argument identification. 2329-9290 © 2021 IEEE.","Data mining; Explosives; Generators; Instruments; Speech processing; Task analysis; Weapons","Extraction; Iterative methods; Job analysis; Natural language processing systems; Reinforcement learning; Speech processing; Event description; Events extractions; Generator; Incremental learning; Life events; Multi-turned; Reinforcement learnings; Task analysis; Task-oriented; Weapon; Data mining",Article,Scopus,2-s2.0-85122289846
"Varzaneh Z.A., Orooji A., Erfannia L., Shanbehzadeh M.","57225125899;56664521300;55986093000;57216971321;","A new COVID-19 intubation prediction strategy using an intelligent feature selection and K-NN method",2022,"Informatics in Medicine Unlocked","28",,"100825","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122283222&doi=10.1016%2fj.imu.2021.100825&partnerID=40&md5=8e59e6056a3b4e65ea6d59334da02d9c","Background: Predicting severe respiratory failure due to COVID-19 can help triage patients to higher levels of care, resource allocation and decrease morbidity and mortality. The need for this research derives from the increasing demand for innovative technologies to overcome complex data analysis and decision-making tasks in critical care units. Hence the aim of our paper is to present a new algorithm for selecting the best features from the dataset and developing Machine Learning(ML) based models to predict the intubation risk of hospitalized COVID-19 patients. Methods: In this retrospective single-center study, the data of 1225 COVID-19 patients from February 9, 2020, to July 20, 2021, were analyzed by several ML algorithms which included, Decision Tree(DT), Support Vector Machine (SVM), Multilayer perceptron (MLP), and K-Nearest Neighbors(K-NN). First, the most important predictors were identified using the Horse herd Optimization Algorithm (HOA). Then, by comparing the ML algorithms' performance using some evaluation criteria, the best performing one was identified. Results: Predictive models were trained using 12 validated features. Also, it found that proposed DT-based predictive model enables a reasonable level of accuracy (=93%) in predicting the risk of intubation among hospitalized COVID-19 patients. Conclusions: The experimental results demonstrate the effectiveness of the proposed meta-heuristic feature selection technique in combining with DT model in predicting intubation risk for hospitalized patients with COVID-19. The proposed model have the potential to inform frontline clinicians with quantitative and non-invasive tool to assess illness severity and to identify high risk patients. © 2021 The Authors","Artificial intelligent; Coronavirus; COVID-19; Data mining; Intubation; Machine learning; Mechanical ventilator",,Article,Scopus,2-s2.0-85122283222
"Sokhangoee Z.F., Rezapour A.","57395982100;55881467800;","A novel approach for spam detection based on association rule mining and genetic algorithm",2022,"Computers and Electrical Engineering","97",,"107655","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122259739&doi=10.1016%2fj.compeleceng.2021.107655&partnerID=40&md5=dbce502e532c3b173432cfa988167c56","Spam detection is considered to be one of the most challenging issues in Online Social Networks (OSNs). In this paper, a supervised method is used to detect spam on these platforms. The accuracy of supervised methods depended on two factors: (i) the desired feature selection and (ii) the use of an appropriate classifier. An innovative method is also used for the first factor. This method is a combination of association rule mining and genetic algorithm with the aim of the desired feature selection from a variety of features. On the other hand, the second factor uses a large number of popular classifiers. The proposed method is assessed on three datasets, and the results show the effectiveness of the proposed feature selection method on the accuracy of the classifiers. The average accuracy for both approaches compared to the basic methods is 87.99% and 95.24%, respectively. © 2021","Association Rule Mining; Feature Selection; Genetic Algorithm; Online Social Network; Spam Detection","Association rules; Classification (of information); Data mining; Genetic algorithms; Social networking (online); Supervised learning; Association rule mining; Feature selection methods; Features selection; Innovative method; Rule mining; Spam detection; Supervised methods; Feature extraction",Article,Scopus,2-s2.0-85122259739
"Abou-Kreisha M.T., Yaseen H.K., Fathy K.A., Ebeid E.A., Eldahshan K.A.","57226598325;57221399837;54398398700;57211269238;36967841100;","Multisource Smart Computer-Aided System for Mining COVID-19 Infection Data",2022,"Healthcare (Switzerland)","10","1","109","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122249321&doi=10.3390%2fhealthcare10010109&partnerID=40&md5=7cafa91cdcc134fbf9fa43d081dc6fa7","In this paper, we approach the problem of detecting and diagnosing COVID-19 infections using multisource scan images including CT and X-ray scans to assist the healthcare system during the COVID-19 pandemic. Here, a computer-aided diagnosis (CAD) system is proposed that utilizes analysis of the CT or X-ray to diagnose the impact of damage in the respiratory system per infected case. The CAD was utilized and optimized by hyper-parameters for shallow learning, e.g., SVM and deep learning. For the deep learning, mini-batch stochastic gradient descent was used to overcome fitting problems during transfer learning. The optimal parameter list values were found using the naïve Bayes technique. Our contributions are (i) a comparison among the detection rates of pre-trained CNN models, (ii) a suggested hybrid deep learning with shallow machine learning, (iii) an extensive analysis of the results of COVID-19 transition and informative conclusions through developing various transfer techniques, and (iv) a comparison of the accuracy of the previous models with the systems of the present study. The effectiveness of the proposed CAD is demonstrated using three datasets, either using an intense learning model as a fully end-to-end solution or using a hybrid deep learning model. Six experiments were designed to illustrate the superior performance of our suggested CAD when compared to other similar approaches. Our system achieves 99.94, 99.6, 100, 97.41, 99.23, and 98.94 accuracy for binary and three-class labels for the CT and two CXR datasets. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Computer-aided diagnosis (CAD); COVID-19; Data mining; Deep learning; Diagnosis; Machine learning; Medical information system; Transfer learning",,Article,Scopus,2-s2.0-85122249321
"Xu L., Li C.","57339752600;57339281500;","Single-Cell Transcriptome Analysis Reveals the M2 Macrophages and Exhausted T Cells and Intratumoral Heterogeneity in Triple-Negative Breast Cancer",2022,"Anti-Cancer Agents in Medicinal Chemistry","22","2",,"294","312",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122245047&doi=10.2174%2f1871520621666210618100857&partnerID=40&md5=a022a20041baf82a68e0bc3d3ffba409","Background: Triple-Negative Breast Cancer (TNBC) is a highly heterogeneous and invasive malignancy that is characterized by high recurrence and mortality rates as well as extremely poor prognosis. Objective: The objective of this study is to analyze T cells and Macrophages in the tumor microenvironment with the aim of identifying targets with therapeutic potential. Methods: Single-cell sequencing data of TNBC patients from the GSE118389 dataset were analyzed to examine the immune environment and intratumoral heterogeneity of TNBC patients. Results: Polarized alternatively activated macrophages (M2) and exhausted CD8+ T cells were identified in TNBC patients. Immunosuppressive checkpoint analysis revealed that levels of lymphocyte-activation gene 3 (LAG3) and T cell immunoglobulin and mucin domain-containing protein 3 (TIM-3) of exhausted T cells were significantly higher than levels of programmed cell death protein 1 (PD-1) and cytotoxic T-lymphocyte-associated protein 4 (CTLA-4). This indicates that these markers are potential immunotherapy targets. Furthermore, analysis of significantly altered immune cell markers showed that several markers were associated with the prognosis of TNBC. Conclusion: Overall, these findings demonstrate inter-tissue heterogeneity of TNBC, and provides novel therapeutic targets for the treatment of TNBC. © 2022 Bentham Science Publishers.","Activated macrophages; Exhausted CD8+ T cells; Immune ecosystem; Immunosuppressive checkpoint; Single-cell transcriptome; Triple-negative breast cancer","CD163 antigen; CD68 antigen; cell marker; cytotoxic T lymphocyte antigen 4; granzyme B; hepatitis A virus cellular receptor 2; initiation factor 5A; interferon; interferon induced protein with tetratricopeptide repeats 3; peptides and proteins; programmed death 1 receptor; prolylleucylglycinamide; radical S adenosyl methionine domain containing 2; regulator of G protein signaling 1; transcriptome; unclassified drug; Akt signaling; angiogenesis; antineoplastic activity; Article; B lymphocyte; cancer prognosis; CD8+ T lymphocyte; cell communication; cell heterogeneity; cell interaction; cluster analysis; data mining; down regulation; endothelium cell; functional enrichment analysis; gene set variation analysis; human; human cell; human tissue; IL 6 signaling; immunocompetent cell; immunofluorescence assay; immunotherapy; JAK-STAT signaling; lymphocyte activation; M2 macrophage; marker gene; oxidative phosphorylation; protein expression; single cell analysis; single cell RNA seq; transcriptomics; triple negative breast cancer; tumor growth; tumor immunity; tumor microenvironment; upregulation",Article,Scopus,2-s2.0-85122245047
"Kanetaki Z., Stergiou C., Bekas G., Troussas C., Sgouropoulou C.","57220815739;6602298177;56426615700;49964556400;6507133031;","Evaluating Remote Task Assignment of an Online Engineering Module through Data Mining in a Virtual Communication Platform Environment",2022,"Electronics (Switzerland)","11","1","158","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122204352&doi=10.3390%2felectronics11010158&partnerID=40&md5=5da521c0b71bfd6b41a64b83f23832f1","E-learning has traditionally emphasised educational resources, web access, student participation, and social interaction. Novel virtual spaces, e-lectures, and digital laboratories have been developed with synchronous or asynchronous practices throughout the migration from face-to-face teaching modes to remote teaching during the pandemic restrictions. This research paper presents a case study concerning the evaluation of the online task assignment of students, using MS Teams as an electronic platform. MS Teams was evaluated to determine whether this communication platform for online lecture delivery and tasks’ assessments could be used to avoid potential problems caused during the teaching process. Students’ data were collected, and after filtering out significant information from the online questionnaires, a statistical analysis, containing a correlation and a reliability analysis, was conducted. The substantial impact of 37 variables was revealed. Cronbach’s alpha coefficient calculation revealed that 89% of the survey questions represented internally consistent and reliable variables, and for the sampling adequacy measure, Bartlett’s test was calculated at 0.816. On the basis of students’ diligence, interaction abilities, and knowledge embedding, two groups of learners were differentiated. The findings of this study shed light on the special features of fully online teaching specifically in terms of improving assessment through digital tools and merit further investigation in virtual and blended teaching spaces, with the goal of extracting outputs that will benefit the educational community. © 2022 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/).","CAD; COVID-19; Data analysis; Data mining; Engineering education; MS Teams; Online learning",,Article,Scopus,2-s2.0-85122204352
"Li B., Fan G., Zhao T., Deng Z., Yu Y.","57211579274;57202284001;55453818600;57395682800;57395473000;","Retrieval of DTM under Complex Forest Stand Based on Spaceborne LiDAR Fusion Photon Correction",2022,"Remote Sensing","14","1","218","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122192406&doi=10.3390%2frs14010218&partnerID=40&md5=dfcfe3ac72fd7f7426f7b9e90fd2285f","The new generation of satellite-borne laser radar Ice, Cloud, and land Elevation Satellite-2 (ICESat-2) data has been successfully used for ground information acquisition. However, when dealing with complex terrain and dense vegetation cover, the accuracy of the extracted understory Digital Terrain Model (DTM) is limited. Therefore, this paper proposes a photon correction data processing method based on ICESat-2 to improve the DTM inversion accuracy in complex terrain and high forest coverage areas. The correction value is first extracted based on the ALOS PALSAR DEM reference data to correct the cross-track photon data of ICESat-2. The slope filter threshold is then selected from the reference data, and the extracted possible ground photons are slope filtered to obtain accurate ground photons. Finally, the impacts of cross-track photon and slope filtering on fine ground extraction from the ICESat-2 data are discussed. The results show that the proposed photon correction and slope filtering algorithms help to improve the extraction accuracy of forest DTM in complex terrain areas. Compared with the forest DTM extracted without the photon correction and slope filtering methods, the MAE (Mean Absolute Error) and RMSE (Root Mean Square Error) are reduced by 51.90~57.82% and 49.37~53.55%, respectively. To the best of our knowledge, this is the first study demonstrating that photon correction can improve the terrain inversion ability of ICESat-2, while providing a novel method for ground extraction based on ICESat-2 data. It provides a theoretical basis for the accurate inversion of canopy parameters for ICESat-2. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Digital ground model; Ground photon; ICESat-2/ATLAS; Photon correction; Slope filter","Data handling; Data mining; Extraction; Forestry; Landforms; Mean square error; Optical radar; Complex terrains; Digital ground model; Digital terrain model; Ground models; Ground photon; Ice clouds; Ice, cloud, and land elevation satellite-2/ATLAS; Land elevation satellites; Photon correction; Slope filter; Photons",Article,Scopus,2-s2.0-85122192406
"Xu W., Zhao Q., Zhan Y., Wang B., Hu Y.","57192831352;57394653800;57214439395;55991487900;55656841000;","Privacy-preserving association rule mining based on electronic medical system",2022,"Wireless Networks","28","1",,"303","317",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122156236&doi=10.1007%2fs11276-021-02846-1&partnerID=40&md5=fb553ebf08240f66d441577013f0a8f5","Privacy protection during collaborative distributed association rule mining is an important research, which has been widely used in market prediction, medical research and other fields. In medical research, Domadiya et al. (Sadhana 43(8):127, 2018) focused on mining association rules from horizontally distributed healthcare data to diagnose heart disease. They claimed they proposed a more effective privacy-preserving distributed association rule mining (PPDARM) scheme. However, a serious security scrutiny of the scheme is performed, and we find it vulnerable to protect the support of the itemsets from any electronic health record (EHR) system, which is the most important parameter Domadiya et al. tried to protect. In this paper, we first present the cryptanalysis of the PPDARM scheme proposed by Domadiya et al. as well as some revised performance analyses. Then a new PPDARM scheme with less interactions is proposed to avert the shortcomings of Domadiya et al., using the homomorphic properties of the distributed Paillier cryptosystem to accomplish the cooperative computation. Our scheme allows the directed authority (miner) to obtain the final results rather than all cooperative EHR systems, in case of semi-honest but pseudo EHR systems. Moreover, security analysis and performance evaluation demonstrate our proposal is efficient and feasible. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Association rule mining; Cooperative computation; Homomorphic encryption; Privacy-preserving","Data mining; Privacy-preserving techniques; Cooperative computation; Distributed association rule minings; Ho-momorphic encryptions; Homomorphic-encryptions; Market prediction; Medical research; Medical systems; Privacy preserving; Privacy protection; Rule mining; Association rules",Article,Scopus,2-s2.0-85122156236
"Leng H.-S., Tsai C.-J., Wu T.-J.","55807863800;14038429000;57393211200;","A Multilayer Steganographic Method Using Improved Exploiting Modification Directions Scheme",2022,"IEEE Access","10",,,"468","485",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122098757&doi=10.1109%2fACCESS.2021.3136883&partnerID=40&md5=89be2c810ce657370551569495738552","The exploiting modification direction scheme is a well-known irreversible steganographic method because of its high embedding efficiency and imperceptibility. The shortcomings of the exploiting modification direction scheme are the use of a ( $2n +1$ )-ary notational system secret digit and low payload. Some studies transform secret data into a sequence of secret digits in the 8-ary notational system to overcome the disadvantage of using a non-binary secret data problem, which also increases the payload. Additionally, some studies have proposed a two-layer embedding method to enhance data security, which also increases the payload. Besides, most of the EMD-based scheme has the fall of boundary problems when the pixel group is located in the boundary area, which increase the distortion of the stego-image. In this study, we proposed a multilayer steganographic method using an improved exploiting modification direction scheme, which includes the abovementioned advantages, using binary secret data, increasing the payload, enhance data security and preventing the fall of boundary problem. In addition, we demostrate a three-layer EMD-based data hiding example. The experimental results show that the proposed method achieves a high payload (approximately 4.5 bpp) and the quality of the stego-image satisfying the human vision system sensitivity (PSNR value is greater than 30 dB). © 2013 IEEE.","8-ary notational system; Exploiting modification direction scheme; multilayer embedding","Computer vision; Data mining; Multilayers; 8-ary notational system; Exploiting modification direction scheme; Exploiting modification directions; Machine-vision; Multi-layer embedding; Nonhomogeneous medium; Notational systems; Payload; Secret data; Sensitivity; Steganography",Article,Scopus,2-s2.0-85122098757
"Xue F., Tan F., Ye Z., Chen J., Wei Y.","57393197000;57393197100;56780856900;36975184000;23500419300;","Spectral-Spatial Classification of Hyperspectral Image Using Improved Functional Principal Component Analysis",2022,"IEEE Geoscience and Remote Sensing Letters","19",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122092979&doi=10.1109%2fLGRS.2021.3089278&partnerID=40&md5=32647a9f9ceb8c5dfbaee338278cbc16","The functional principal component analysis (FPCA) method can effectively solve the problems of the high dimensionality of data, large information redundancy, and noise interference in hyperspectral image (HSI) classification. However, this unsupervised FPCA cannot make full use of the label information of training samples or spatial information, so that it is impossible to obtain satisfactory classification results. In this letter, a set of improved FPCA methods for HSI classification are proposed. First, the B-spline basis system is used to establish the functional data fitting model, which can convert discrete spectral information into continuous spectral curves and lay the foundation for functional feature extraction. Second, a supervised FPCA (SFPCA) method is built for extracting more effective functional features by making full use of the label information of training samples. Furthermore, to overcome the lack of training samples, two semisupervised FPCA (SSFPCA) methods are proposed for extracting more discriminative functional features and improving the classification accuracies. Finally, we perform the local mean filtering method on the HSI in order to extract the spatial information for each pixel, and then design spectral-spatial classification frameworks based on improved FPCA. Experiments on the commonly used HSI dataset show that improved FPCA can achieve higher classification accuracies than FPCA, and the proposed functional spectral-spatial classification frameworks can greatly improve classification accuracies. © 2004-2012 IEEE.","B-spline basis function; functional feature extraction; hyperspectral image (HSI); semisupervised functional principal component analysis (SSFPCA); spectral-spatial classification","Classification (of information); Curve fitting; Data mining; Extraction; Image analysis; Image classification; Image enhancement; Information filtering; Information use; Interpolation; Least squares approximations; Principal component analysis; Sampling; Spectroscopy; B-spline base function; Features extraction; Functional feature extraction; Functional features; Functional principal component analysis; Hyperspectral image; Optimisations; Principal-component analysis; Semi-supervised; Semisupervised functional principal component analyse; Spectral-spatial classification; Spectral-spatial classification.; Spline (mathematic); Splines (mathematics); Feature extraction; accuracy assessment; experimental study; image classification; principal component analysis; sampling; training; unsupervised classification",Article,Scopus,2-s2.0-85122092979
"Piri J., Mohapatra P., Pradhan M.R., Acharya B., Patra T.K.","57216224759;56692286400;57217311543;57216616739;22035521300;","A Binary Multi-Objective Chimp Optimizer with Dual Archive for Feature Selection in the Healthcare Domain",2022,"IEEE Access","10",,,"1756","1774",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122091155&doi=10.1109%2fACCESS.2021.3138403&partnerID=40&md5=b5fc10c23a04d91bd0d1b4ad23ae40cc","Medical datasets frequently include vast feature sets with numerous features that are related to one another. As a result, the curse of dimensionality affects learning from a medical dataset to discover significant characteristics, making it necessary to minimize the feature set. Feature selection (FS) is a major step in classification and also in reducing the dimension. This study attempts a novel Binary Multi-objective Chimp Optimization Algorithm (BMOChOA) with dual archive and k-nearest neighbors (KNN) classifier for mining relevant aspects from medical data. In this research, 12 versions of BMOChOA are implemented based on the group information and types of chaotic functions used. The best Pareto front obtained from suggested BMOChOA variations is compared with three benchmark multi-objective FS methods by taking 14 popular medical datasets of variable dimensions. By analyzing the experimental outputs using four multi-objective performance evaluators, it is found that the proposed FS method is superior in finding the best trade-off between the two objective functions: the number of features and classification performance. © 2013 IEEE.","Chimp optimization; classification; data mining; feature selection; healthcare; multi-objective","Classification (of information); Economic and social effects; Feature extraction; Health care; Medical computing; Multiobjective optimization; Nearest neighbor search; Chimp optimization; Feature selection methods; Features selection; Features sets; Healthcare domains; Medical data sets; Multi objective; Optimisations; Optimization algorithms; Optimizers; Data mining",Article,Scopus,2-s2.0-85122091155
"Chen Y., Zhou L., Zhou Y., Chen Y., Hu S., Dong Z.","57196055574;56923973400;57393189200;57196459195;57226650602;57392584300;","Multiple Histograms Shifting-Based Video Data Hiding Using Compression Sensing",2022,"IEEE Access","10",,,"699","707",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122081293&doi=10.1109%2fACCESS.2021.3137398&partnerID=40&md5=7675ef3e64c9d3b80255cf87e8e4b9af","With the development of multimedia editing technologies, the copyright protection has attacked more attentions. Reversible data hiding (RDH), in which the cover can be recovered losslessly, is an effect method to eliminate embedding distortions. As a typical RDH method, histogram shifting (HS) is used widely. Most existing RDH schemes based on HS usually build sharp histograms by predicting and sorting techniques. To make use of spatial correlations of multimedia, several RDH schemes based on multiple HS (MHS) are proposed to protect copyright, in which some rigid rules are used to build multiple histograms. Against images, videos have more spatial and temple correlations and it is easier to acquire sharper histograms. In this paper, a video MHS scheme based on compression sensing (CS) is proposed. As a linear sensing algorithm, CS can measure macroblock residuals by reducing corrections among pixels to acquire distinguishable macroblock features, while keeping their statistical characteristics immutable. By employing CS, macroblocks with similar characteristics cluster together to formulate multiple histograms. For each of these histograms, data embedding is implemented to reduce shifting distortions by expanding the outermost bins while other bins are unchanged. Experimental results show that the quality of most test videos in our scheme are higher than that in the state-of-art schemes. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/","Correlation; Data mining; Distortion; Feature extraction; Histograms; Sensors; Videos","Copyrights; Data mining; Error correction; Steganography; Compression sensing; Correlation; Features extraction; Histogram shifting; Macro block; Macroblocks; Multiple histogram shifting; Reversible data hiding; Video; Video data; Graphic methods",Article,Scopus,2-s2.0-85122081293
"Dong S., Chen Z.","57221908711;7409481437;","Block Multi-Dimensional Attention for Road Segmentation in Remote Sensing Imagery",2022,"IEEE Geoscience and Remote Sensing Letters","19",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122074513&doi=10.1109%2fLGRS.2021.3137551&partnerID=40&md5=3ec36625c996e890198be0b0e8196240","High-resolution remote sensing image (RSI) segmentation is a relatively mature application in various deep learning projects. In this study, aiming at slender objects in road RSIs, BMDANet combines cross-layer information exchange and block multi-dimensional attention (BMDA) module and optimizes road feature extraction by using multi-dimensional information to construct a global attention module. The experimental results based on the Ottawa road dataset show that our algorithm improved the recognition results of the road in RSI, and excelled the existing RSI road segmentation algorithm and reached the state-of-the-art. In addition, based on comparative experiments, the addition of the BMDA module to different algorithms can effectively improve the accuracy of the algorithm. It has proven the effectiveness and embedding of our BMDA module in RSI road segmentation algorithms. © 2004-2012 IEEE.","Block multi-dimensional attention (BMDA); cross-layer information exchange; remote sensing image (RSI); road semantic segmentation","Data mining; Deep learning; Extraction; Feature extraction; Information dissemination; Remote sensing; Roads and streets; Semantic Segmentation; Block multi-dimensional attention; Cross layer; Cross-layer information exchange; Features extraction; Images segmentations; Information exchanges; Multi dimensional; Remote sensing images; Remote-sensing; Road semantic segmentation; Semantic segmentation; Semantics; accuracy assessment; algorithm; experimental study; remote sensing; satellite imagery; segmentation",Article,Scopus,2-s2.0-85122074513
"Essam A., Abdel-Fattah M.A., Abdelhamid L.","57223259870;55545556200;57393189300;","Towards Enhancing the Performance of Parallel FP-Growth on Spark",2022,"IEEE Access","10",,,"286","296",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122066878&doi=10.1109%2fACCESS.2021.3137789&partnerID=40&md5=001858d2da3089115bfa6abd0c26f261","Frequent itemset mining (FIM) is a crucial tool for identifying hidden patterns in information. FP-Growth is an FIM algorithm used to find associations. When the data size increases, the execution of FIM algorithms on a single machine suffers from computational problems, such as memory and time consumption. For these reasons, parallel and distributed processing on platforms such as Spark is essential. The parallel frequent pattern (PFP) is the implementation of FP-Growth in Spark. The main problem with PFP is that it does not consider the load balancing between cluster units. This research proposes an enhanced balanced parallel frequent pattern 'EBPFP' algorithm to enhance and balance the PFP. The proposed algorithm (EBPFP) proposes two ideas. First, a strategy for load balancing between groups is proposed to ensure that the items are evenly divided between the nodes, and the cluster resources are used more effectively. Second, the improved conditional pattern base (ICPB) method aims to remove infrequent items from the conditional pattern base before constructing local FP-Trees. The experimental results show that the proposed EBPFP algorithm outperforms PFP, and the difference in running time between EBPFP and PFP was 21.56% and 39.72%, respectively. © 2013 IEEE.","association rule analysis; Big data; data mining; frequent pattern growth algorithm; load balancing; spark","Big data; Clustering algorithms; Job analysis; Scheduling algorithms; Association rule analysis; FP growths; Frequent itemset mining; Frequent pattern growth; Frequent pattern growth algorithm; Growth algorithms; Itemset; Load-Balancing; Partitioning algorithms; Task analysis; Data mining",Article,Scopus,2-s2.0-85122066878
"Huan H., Liu Y., Xie Y., Wang C., Xu D., Zhang Y.","56122187900;56342912900;25931789100;57211834282;56299205100;57393500800;","MAENet: Multiple Attention Encoder-Decoder Network for Farmland Segmentation of Remote Sensing Images",2022,"IEEE Geoscience and Remote Sensing Letters","19",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122058761&doi=10.1109%2fLGRS.2021.3137522&partnerID=40&md5=91d4ce0b3c036da8443bedd3438f8084","With the rapid development of computer vision, semantic segmentation as an important part of the technology has made some achievements in different applications. However, in the farmland segmentation scenario of remote sensing images, the capability of common semantic segmentation methods in restoring the farmland edge and identifying narrow farmland ridges needs to be improved. Therefore, in this letter a semantic segmentation method-multiple attention encoder-decoder network (MAENet)-for farmland segmentation is proposed. The design of a dual-pooling efficient channel attention (DPECA) module and its embedment in the backbone to improve the efficiency of feature extraction is described; secondly, a dual-feature attention (DFA) module is proposed to extract contextual information of high-level features; finally, a global-guidance information upsample (GIU) module is added to the decoder to reduce the influence of redundant information on feature fusion. We use three self-made farmland image datasets representing UAV data to train MAENet and compare them with other methods. The results show that the performances of segmentation and generalization of MAENet are improved compared with other methods. The MIoU and Kappa coefficient in the farmland multi-classification test set can reach 93.74% and 96.74%. © 2004-2012 IEEE.","Attention module; farmland segmentation; feature fusion; pyramid; UAV images","Classification (of information); Data mining; Decoding; Extraction; Farms; Feature extraction; Image enhancement; Image fusion; Network coding; Remote sensing; Semantics; Unmanned aerial vehicles (UAV); Attention module; Encoder-decoder; Farmland segmentation; Features extraction; Features fusions; Images segmentations; Pyramid; Remote-sensing; Semantic segmentation; UAV image; Image segmentation; agricultural land; artificial neural network; computer vision; remote sensing; segmentation",Article,Scopus,2-s2.0-85122058761
"Ottinger M., Bachofer F., Huth J., Kuenzer C.","55821701700;36960527400;36542522100;55927784300;","Mapping aquaculture ponds for the coastal zone of asia with sentinel-1 and sentinel-2 time series",2022,"Remote Sensing","14","1","153","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122044019&doi=10.3390%2frs14010153&partnerID=40&md5=b90e9adcced4ef3cce73f7ab48b9b689","Asia dominates the world’s aquaculture sector, generating almost 90 percent of its total annual global production. Fish, shrimp, and mollusks are mainly farmed in land-based pond aquaculture systems and serve as a primary protein source for millions of people. The total production and area occupied for pond aquaculture has expanded rapidly in coastal regions in Asia since the early 1990s. The growth of aquaculture was mainly boosted by an increasing demand for fish and seafood from a growing world population. The aquaculture sector generates income and employment, contributes to food security, and has become a billion-dollar industry with high socio-economic value, but has also led to severe environmental degradation. In this regard, geospatial information on aquaculture can support the management of this growing food sector for the sustainable development of coastal ecosystems, resources, and human health. With free and open access to the rapidly growing volume of data from the Copernicus Sentinel missions as well as machine learning algorithms and cloud computing services, we extracted coastal aquaculture at a continental scale. We present a multi-sensor approach that utilizes Earth observation time series data for the mapping of pond aquaculture within the entire Asian coastal zone, defined as the onshore area up to 200 km from the coastline. In this research, we developed an object-based framework to detect and extract aquaculture at a single-pond level based on temporal features derived from high-spatial-resolution SAR and optical satellite data acquired from the Sentinel-1 and Sentinel-2 satellites. In a second step, we performed spatial and statistical data analyses of the Earth-observation-derived aquaculture dataset to investigate spatial distribution and identify production hotspots at various administrative units at regional, national, and sub-national scale. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Aquaculture; Asia; Coastal zone; Earth observation; Ponds; SAR; Sentinel-1; Time series","Coastal zones; Data mining; Economics; Ecosystems; Fish; Fisheries; Food supply; Information management; Lakes; Learning algorithms; Machine learning; Object detection; Observatories; Population statistics; Sustainable development; Aquaculture ponds; Aquaculture systems; Asia; Earth observations; Global production; Pond aquacultures; Protein sources; SAR; Sentinel-1; Times series; Time series",Article,Scopus,2-s2.0-85122044019
"Qin Y., Zhang X., Zhao Z., Li Z., Yang C., Huang Q.","57391918400;57391534600;55555287100;57392048700;57391660200;55467448100;","Coupling Relationship Analysis of Gold Content Using Gaofen-5 (GF-5) Satellite Hyperspectral Remote Sensing Data: A Potential Method in Chahuazhai Gold Mining Area, Qiubei County, SW China",2022,"Remote Sensing","14","1","109","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122035128&doi=10.3390%2frs14010109&partnerID=40&md5=7ce6cc8b3d9d0a6022ba7c9b163320da","The gold (Au) geochemical anomaly is an important indicator of gold mineralization. While the traditional field geochemical exploration method is time-consuming and expensive, the hyperspectral remote sensing technique serves as a robust technique for the delineation and mapping of hydrothermally altered and weathered mineral deposits. Nonetheless, mineralization element anomaly detection was still seldomly used in previous hyperspectral remote sensing applications in mineralization. This study explored the coupling relationship between Gaofen-5 (GF-5) hyperspectral data and Au geochemical anomalies through several models. The Au geochemical anomalies in the Chahuazhai mining area, Qiubei County, Yunnan Province, SW China, was studied in detail. First, several noise reduction methods including radiometric calibration, Fast Line-of-sight Atmospheric Analysis of Spectral Hypercubes (FLAASH), Savitzky–Golay filter, and endmember choosing methods including Minimum Noise Fraction (MNF) transformation, matched filtering, and Fast Fourier Transform (FFT) transformation were applied to the Gaofen-5 (GF-5) hyperspectral data processing. The Spectrum-Area (S-A) method was introduced to build an FFT filter to highlight the spectral abnormal characteristics associated with Au geochemical anomaly information. Specifically, the Matched Filtering (MF) technique was applied to the dataset to find the Au geochemical anomaly abundances of endmembers with innovative large-sample learning. Then, Multiple Linear Regression (MLR), Partial Least Squares (PLS) regression, a Back Propagation (BP) network, and Geographically Weighted Regression (GWR) were used to reveal the coupling relationship between the spectra of the processed hyperspectral data and the Au geochemical anomalies. The results show that the GWR analysis has a much higher coefficient of determination, which implies that the Au geochemical anomalies and the spectral information are highly related to spatial locations. GWR works especially well for showing the regional Au geochemical anomaly trend and simulating the Au concentrated areas. The GWR model with application of the S-A method is applicable to the detection of Au geochemical anomalies, which could provide a potential method for Au deposit exploration using GF-5 hyperspectral data. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Au geochemical anomaly; Chahuazhai Carlin-type Gold Deposit; Coupling relationship analysis; Geographically weighted regression (GWR); GF-5 hyperspectral data; Large-sample-learning; Spectrum-area (S-A); SW China","Backpropagation; Convolution; Data handling; Data mining; Deposits; Factorization; Fast Fourier transforms; Filtration; Gold deposits; Image enhancement; Large dataset; Least squares approximations; Linear regression; Mean square error; Metadata; Mineral exploration; Mineralogy; Remote sensing; Spectroscopy; Au geochemical anomaly; Chahuazhai carlin-type gold deposit; Coupling relationship analyse; Coupling relationships; Gaofen-5 hyperspectral data; Geochemical anomaly; Geographically weighted regression; Hyperspectral Data; Large-sample-learning; Relationship analysis; Sample learning; Spectra's; Spectrum-area; SW china; Matched filters",Article,Scopus,2-s2.0-85122035128
"Desloires J., Ienco D., Botrel A., Ranc N.","57392020900;25027558600;57391890000;53881835600;","Positive unlabelled learning for satellite images’time series analysis: An application to cereal and forest mapping",2022,"Remote Sensing","14","1","140","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122019412&doi=10.3390%2frs14010140&partnerID=40&md5=551fd2cf9f9ddb5f118ed9010a2a45b1","Applications in which researchers aim to extract a single land type from remotely sensed data are quite common in practical scenarios: extract the urban footprint to make connections with socio-economic factors; map the forest extent to subsequently retrieve biophysical variables and detect a particular crop type to successively calibrate and deploy yield prediction models. In this scenario, the (positive) targeted class is well defined, while the negative class is difficult to describe. This one-class classification setting is also referred to as positive unlabelled learning (PUL) in the general field of machine learning. To deal with this challenging setting, when satellite image time series data are available, we propose a new framework named positive and unlabelled learning of satellite image time series (PUL-SITS). PUL-SITS involves two different stages: In the first one, a recurrent neural network autoencoder is trained to reconstruct only positive samples with the aim to higight reliable negative ones. In the second stage, both labelled and unlabelled samples are exploited in a semi-supervised manner to build the final binary classification model. To assess the quality of our approach, experiments were carried out on a real-world benchmark, namely Haute-Garonne, located in the southwest area of France. From this study site, we considered two different scenarios: a first one in which the process has the objective to map Cereals/Oilseeds cover versus the rest of the land cover classes and a second one in which the class of interest is the Forest land cover. The evaluation was carried out by comparing the proposed approach with recent competitors to deal with the considered positive and unlabelled learning scenarios. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Land cover mapping; Positive unlabelled learning; Satellite image time series","Benchmarking; Data mining; Forestry; Mapping; Recurrent neural networks; Satellites; Deep learning; Forest mapping; Image time-series; Land cover; Land cover mapping; Positive and unlabeled learning; Positive unlabeled learning; Satellite image time series; Satellite images; Time-series analysis; Time series analysis",Article,Scopus,2-s2.0-85122019412
"Ince T.","36782431900;","Superpixel-Based Graph Laplacian Regularization for Sparse Hyperspectral Unmixing",2022,"IEEE Geoscience and Remote Sensing Letters","19",,,"","",,4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121956492&doi=10.1109%2fLGRS.2020.3027055&partnerID=40&md5=f691aa069aae0a40c35e81a5cc04d0ed","An efficient spatial regularization method using superpixel segmentation and graph Laplacian regularization is proposed for the sparse hyperspectral unmixing method. Since it is likely to find spectrally similar pixels in a homogeneous region, we use a superpixel segmentation algorithm to extract the homogeneous regions by considering the image boundaries. We first extract the homogeneous regions, which are called superpixels, and then, a weighted graph in each superpixel is constructed by selecting $K$ -nearest pixels in each superpixel. Each node in the graph represents the spectrum of a pixel, and edges connect the similar pixels inside the superpixel. The spatial similarity is investigated using the graph Laplacian regularization. Sparsity regularization for an abundance matrix is provided using a weighted sparsity promoting norm. Experimental results on simulated and real data sets show the superiority of the proposed algorithm over the well-known algorithms in the literature. © 2004-2012 IEEE.","Abundance estimation; graph Laplacian; sparse unmixing (SU); superpixel","Graph theory; Hyperspectral imaging; Image segmentation; Laplace transforms; Matrix algebra; Spectroscopy; Superpixels; Abundance estimation; Graph Laplacian; Images segmentations; Laplace equation; Laplacian regularizations; Sparse matrices; Sparse unmixing; Super pixels; Superpixel.; Unmixing; Data mining; graphical method; Laplace transform; pixel; spectral analysis",Article,Scopus,2-s2.0-85121956492
"Shao J., Mao Y., Zhang J.","57218794768;56542388200;36659981100;","Learning Task-Oriented Communication for Edge Inference: An Information Bottleneck Approach",2022,"IEEE Journal on Selected Areas in Communications","40","1",,"197","211",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121880572&doi=10.1109%2fJSAC.2021.3126087&partnerID=40&md5=556dd9a9ba778f5f4feb522d5a3826e8","This paper investigates task-oriented communication for edge inference, where a low-end edge device transmits the extracted feature vector of a local data sample to a powerful edge server for processing. It is critical to encode the data into an informative and compact representation for low-latency inference given the limited bandwidth. We propose a learning-based communication scheme that jointly optimizes feature extraction, source coding, and channel coding in a task-oriented manner, i.e., targeting the downstream inference task rather than data reconstruction. Specifically, we leverage an information bottleneck (IB) framework to formalize a rate-distortion tradeoff between the informativeness of the encoded feature and the inference performance. As the IB optimization is computationally prohibitive for the high-dimensional data, we adopt a variational approximation, namely the variational information bottleneck (VIB), to build a tractable upper bound. To reduce the communication overhead, we leverage a sparsity-inducing distribution as the variational prior for the VIB framework to sparsify the encoded feature vector. Furthermore, considering dynamic channel conditions in practical communication systems, we propose a variable-length feature encoding scheme based on dynamic neural networks to adaptively adjust the activated dimensions of the encoded feature to different channel conditions. Extensive experiments evidence that the proposed task-oriented communication system achieves a better rate-distortion tradeoff than baseline methods and significantly reduces the feature transmission latency in dynamic channel conditions. © 1983-2012 IEEE.","Edge inference; Information bottleneck; Task-oriented communication; Variational inference","Clustering algorithms; Data mining; Electric distortion; Encoding (symbols); Extraction; Image coding; Neural networks; Channel conditions; Communications systems; Dynamic channels; Edge inference; Features vector; Information bottleneck; Learning tasks; Task-oriented; Task-oriented communication; Variational inference; Signal distortion",Article,Scopus,2-s2.0-85121880572
"Lei D., Chen L., Tang J.","55788200700;57386605700;57387545400;","Mining of Weak Fault Information Adaptively Based on DNN Inversion Estimation for Fault Diagnosis of Rotating Machinery",2022,"IEEE Access","10",,,"6147","6164",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121838961&doi=10.1109%2fACCESS.2021.3136235&partnerID=40&md5=f306a3c62e5285fc75add9b8efc919c5","Fault feature extraction plays a significant role in bearing fault diagnosis, especially in incipient fault period. Recently, deep neural network has been favored by many researchers due to its excellent hierarchical feature extraction capabilities. The existing diagnosis methods based on deep neural network mostly take original condition monitoring data as input and further convert fault diagnosis into pattern recognition issues. Although it improves the level of intelligent diagnosis, it is still confronted with practical problems. Since deep neural network includes many non-linear mapping layers, the extracted fault-related features show a high degree of abstraction, which reduces the practical level of this technology. Aimed at this problem, a mining method of understandably weak fault information is proposed based on deep neural network inversion estimation. From perspective of neuron response degree, this method mines the most sensitive input pattern that maximizes the neuron's activation value of network output layer in the original input feature space. It can intuitively mine the weak fault information from the original signal. Two bearing experiments verify the effectiveness and reliability of the proposed method. © 2013 IEEE.","Deep neural network inversion estimation; fault feature extraction; mining of weak fault information","Condition monitoring; Data mining; Deep neural networks; Estimation; Extraction; Failure analysis; Fault detection; Machinery; Network layers; Deep learning; Deep neural network inversion estimation; Fault diagnosis of rotating machineries; Faults diagnosis; Faults feature extractions; Faults information; Features extraction; Mining of weak fault information; Neural network inversion; Weak faults; Feature extraction",Article,Scopus,2-s2.0-85121838961
"Kyeong S., Kim D., Shin J.","57387465100;57387465200;57386901800;","Can system log data enhance the performance of credit scoring?—evidence from an internet bank in Korea",2022,"Sustainability (Switzerland)","14","1","130","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121816730&doi=10.3390%2fsu14010130&partnerID=40&md5=3c767d425459fcebdaeec2c64e03e734","The credit scoring model is one of the most important decision-making tools for the sustainability of banking systems. This study is the first to examine whether it can be improved by using system log data that are stoed extensively for system operation. We used the log data recorded by the mobile application system of KakaoBank, a leading internet bank used by more than 14 million people in Korea. After generating candidate variables from KakaoBank’s log data, we created a credit scoring model by utilizing variables with high information values and logistic regression, the most common method for developing credit scoring models in financial institutions. To prove our hypothesis on the improvement of credit scoring model performance, we performed an independent sample t-test using the simulation results of repeated model development and performance measurement based on randomly sampled data. Consequently, the discrimination power of the proposed model using logistic regression (neural network) compared to the credit bureau-based model significantly improved by 1.84 (2.22) percentage points based on the Kolmogorov–Smirnov statistics. The results of this study suggest that a bank can utilize the accumulated log data inside the bank to improve decision-making systems, including credit scoring, at a low cost. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Credit scoring model; Data mining; Fintech; Logistic regression; Machine learning; System log data","banking; credit provision; decision making; Internet; regression; simulation; sustainability; Korea",Article,Scopus,2-s2.0-85121816730
"Wu Z., Zhang H., Liu W., Li Z., Zheng W.","57207937374;57386888400;56106147600;57216860950;57216869389;","Machine identification of potential manufacturing process failure modes based on process constituent elements",2022,"Advanced Engineering Informatics","51",,"101491","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121799871&doi=10.1016%2fj.aei.2021.101491&partnerID=40&md5=309e46a744d212021ff7084cfce8f4cc","It is urgent to solve the problems of low efficiency, high cost and lack of system integrity when PFMEA identifies potential process failure modes in the process of multi-variety and small batch production. Current study innovatively proposes a machine identification method of potential process failure modes based on the combination of process constituent elements (PCE) model and feature case-based reasoning represented by extension. Firstly, the specific description of the process content is developed with the PCE and the way of expression is standardized through the method of data mining. Then, the PCE of the normative knowledge representation of extension feature cases were constructed, and natural language processing (NLP) technology is employed to solve the feature similarity between the same PCE after the specification of the sentence and chunk features respectively, and case-based reasoning and extension operator are applied to carry out analogical reasoning and calculation for the feature cases with high similarity degree of feature attributes, so as to realize the machine identification of the potential process failure modes in the manufacturing process. Finally, the proposed method is specifically applied to the three parts assembly process of an aircraft assembly to verify the effectiveness and applicability of the proposed method. © 2021 Elsevier Ltd","Extension operator; PFMEA; Process constituent elements; Process failure modes; Similarity","Case based reasoning; Data mining; Knowledge representation; Natural language processing systems; Constituent elements; Extension operators; Machine identification; Manufacturing process; Mode-based; PFMEA; Process constituent element; Process failure; Process failure mode; Similarity; Failure modes",Article,Scopus,2-s2.0-85121799871
"Tavakoli-Zaniani M., Gholamian M.R.","57222815259;8353851200;","Improving Heuristic Process Discovery Methods Through Determining the Optimal Split/Join Patterns of Dependency Graphs",2022,"IEEE Access","10",,,"1116","1131",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121786118&doi=10.1109%2fACCESS.2021.3135298&partnerID=40&md5=fc6d44197705e63684dd84e0a84cbd56","Identifying the split and join patterns of dependency graphs is an essential step in Heuristics Mining process discovery methods. The existing methods determine the split/join patterns (consisting of AND and XOR relations) according to the event log information about the activities involved in the splits and joins. Hence, they neglect the event log information available for the other activities on the paths from split points to join points. On the other hand, the current methods determine the patterns of each split/join separately and do not aim to select the best set of patterns. Therefore, the outputs of the existing methods can be non-optimal. Furthermore, the current methods cannot guarantee that there is a matching And-join for each AND-split, and vice versa. This can make some split/join patterns incapable of being activated. To handle these issues, this paper, for the first time, presents an integer linear programming model which identifies the optimal patterns of splits/joins with regard to all succession information that is available in the event log; simultaneously, it ensures that for each AND-split there is at least one matching AND-join, and vice versa. The objective function of the proposed model is inspired by replay fitness and precision dimensions of process model quality. According to the assessments, the process models obtained by the proposed method are superior to the results of the most prominent methods of determining split/join patterns in terms of replay fitness, precision, simplicity, and matching AND-splits with AND-joins. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/","Concurrent computing; Integer linear programming; Licenses; Linear programming; Mathematical programming; Pattern matching; Volume measurement","Data mining; Heuristic methods; Integer programming; Concurrent computing; Heuristic miner; Integer Linear Programming; Join-patterns; License; Linear-programming; Mining split/join pattern; Optimisations; Pattern-matching; Process Discovery; Process mining; Pattern matching",Article,Scopus,2-s2.0-85121786118
"Mishra R.B., Jiang H.","57316255400;55577310400;","Management and organizational research: Structural topic modeling for a better understanding of theory application",2022,"Sustainability (Switzerland)","14","1","159","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121777377&doi=10.3390%2fsu14010159&partnerID=40&md5=c427f6ff3ab07f5b51a19c3fc942cd2b","In management and organization research, theory development is often linked with developing a new theory. However, regardless of the number of existing theories, most theories remain empirically untested, and the progress in understanding the application of theories has been scarce. This article discusses how theories are applied in existing management and organization research studies. This study applies the Structural Topic Model to 4636 research papers from the S2ORC dataset. The results reveal twelve research themes, establish correlations, and document the evolution of themes over time. The findings of this study reveal that the theoretical application is not consistent across research themes, theories are primarily used for descriptive and communicative properties, and most research themes in management and organization research are more concerned with discovering phenomena rather than with understanding and forecasting them. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Data and text analysis; Management and organization theory; Management science; Structural topic modeling; Theory research","data mining; management practice; modeling",Article,Scopus,2-s2.0-85121777377
"Belostecinic G., Mogoș R.I., Popescu M.L., Burlacu S., Rădulescu C.V., Bodislav D.A., Bran F., Oancea-Negescu M.D.","57218206827;35299826300;56104844300;55876518100;26432154800;37560991400;26431730000;57384507700;","Teleworking—an economic and social impact during covid-19 pandemic: A data mining analysis",2022,"International Journal of Environmental Research and Public Health","19","1","298","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121755678&doi=10.3390%2fijerph19010298&partnerID=40&md5=5a6bed6942bfcb48c15f68b8d019e5d2","The health crisis generated by the COVID-19 pandemic has induced, among other things, an increase in the importance of remote work or teleworking (TL) in the current period. The objective of this research is to identify the economic and social impact of telework in changing the behavior of employees in Romania. The research was conducted approximately one year after the onset of the pandemic until the beginning of the vaccination period in Romania. The research proposed includes three main directions of analysis of the extracted data, which are related to telework efficiency, this being considered one of the most important indicators for a company. In order to obtain conclusive results, we used a mixed methodology, combining results obtained through a survey based on a self-administered electronic questionnaire, with a data mining analysis. Detailed analysis of the groups identified based on work efficiency allowed us to highlight the most common employee profiles. This analysis was doubled by a second classification experiment, which provided us a more detailed analysis of the groups identified based on job satisfaction and highlighted the most common employee profiles. The expansion of telework in various economic areas is a result of adaptation to the new economic and social conditions caused by the COVID-19 pandemic. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Data mining analysis; Economic and social factors; Teleworking","adaptive management; COVID-19; data mining; economic impact; social impact; Article; clinical effectiveness; coronavirus disease 2019; economics; employee; health survey; human; pandemic; questionnaire; Romania; social aspect; telecommuting; vaccination; data mining; pandemic; social change; telecommuting; COVID-19; Data Mining; Humans; Pandemics; SARS-CoV-2; Social Change; Teleworking",Article,Scopus,2-s2.0-85121755678
"Łydżba-Kopczyńska B.I., Szwabiński J.","24176411200;6507995948;","Attribution markers and data mining in art authentication",2022,"Molecules","27","1","70","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121729448&doi=10.3390%2fmolecules27010070&partnerID=40&md5=abd60acebabe5fb85a3ddc6054b1f39f","Today’s global art market is a billion-dollar business, attracting not only investors but also forgers. The high number of forged works requires reliable authentication procedures to mitigate the risk of investments. However, with the developments in the methodology, continuous time pressure and the threat of litigation, authenticating artwork is becoming increasingly complex. In this paper, we examined whether the decision process involved in the authenticity examination may be supported by machine learning algorithms. The idea is motivated by existing clinical decision support systems. We used a set of 55 artworks (including 12 forged ones) with determined attribution markers to train a decision tree model. From our preliminary results, it follows that it is a very promising technique able to support art experts. Decision trees are able to summarize the existing knowledge about all investigations and may also be used as a classifier for new paintings with known markers. However, larger datasets with artworks of known provenance are needed to build robust classification models. The method can also utilize the most important markers and, consequently, reduce the costs of investigations. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Analytical procedures; Authentications; Data mining; Forensic analysis; Paintings","algorithm; article; classifier; clinical decision support system; criminalistics; data mining; decision making; decision tree; human; painting; preliminary data",Article,Scopus,2-s2.0-85121729448
"Mendes P.S.F., Siradze S., Pirro L., Thybaut J.W.","56585480300;57220163476;57204692287;6603468995;","Extracting kinetic information in catalysis: An automated tool for the exploration of small data",2022,"Reaction Chemistry and Engineering","7","1",,"142","155",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121719940&doi=10.1039%2fd1re00215e&partnerID=40&md5=e288593f9486533db8be8609f178db11","For numerous reactions in catalysis, the lack of (big) data in kinetics is compensated for by the availability of numerous small, scattered datasets as typically found in the literature. To exploit the potential of such peculiar, small data, incorporation of fundamental knowledge into data-driven approaches is essential. In this work, a novel tool was developed to automatically extract kinetically relevant information from small datasets of steady-state kinetic data for heterogeneously catalysed reactions. The developed tool, based on the principles of qualitative trend analysis, was tailored to the needs of catalysis and enriched with chemical knowledge, balancing thereby the limited amount of data and ensuring that meaningful information is extracted. A detailed account of the development steps discloses how the chemical knowledge was incorporated, such that this approach can inspire new tools and applications. As demonstrated for a hydrodeoxygenation case study, such a tool is the first step into automatic construction of kinetic models, which will ultimately lead to a more rational design of novel catalysts. © 2022 The Royal Society of Chemistry.",,"Chemical analysis; Data mining; Kinetics; Automated tools; Data-driven approach; Heterogeneously catalyzed reactions; Kinetic data; Kinetic information; Qualitative trend analysis; Small data; Small data set; Steady-state kinetics; Tools and applications; Catalysis",Article,Scopus,2-s2.0-85121719940
"Usmankhujaev S., Ibrokhimov B., Baydadaev S., Kwon J.","57209746620;57216817924;57216416401;55646002200;","Time series classification with inceptionFCN",2022,"Sensors","22","1","157","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121708600&doi=10.3390%2fs22010157&partnerID=40&md5=47ac5af101bf674c9d498051848a3ab8","Deep neural networks (DNN) have proven to be efficient in computer vision and data classification with an increasing number of successful applications. Time series classification (TSC) has been one of the challenging problems in data mining in the last decade, and significant research has been proposed with various solutions, including algorithm‐based approaches as well as machine and deep learning approaches. This paper focuses on combining the two well‐known deep learning techniques, namely the Inception module and the Fully Convolutional Network. The proposed method proved to be more efficient than the previous state‐of‐the‐art InceptionTime method. We tested our model on the univariate TSC benchmark (the UCR/UEA archive), which includes 85 time‐series datasets, and proved that our network outperforms the InceptionTime in terms of the training time and overall accuracy on the UCR archive. © 2021 by the authors. Li-censee MDPI, Basel, Switzerland.","Deep neural networks (DNN); Fully convolutional network (FCN); Inception; Optimization; Time‐series classification (TSC)","Convolution; Convolutional neural networks; Data mining; Time series; Convolutional networks; Data classification; Deep neural network; Fully convolutional network; Inception; Learning approach; Learning techniques; Optimisations; Time series classifications; Time‐series classification; Deep neural networks; algorithm; benchmarking; data mining; time factor; Algorithms; Benchmarking; Data Mining; Neural Networks, Computer; Time Factors",Article,Scopus,2-s2.0-85121708600
"Machova K., Mach M., Vasilko M.","17435396000;22735085000;57385520400;","Comparison of machine learning and sentiment analysis in detection of suspicious online reviewers on different type of data",2022,"Sensors","22","1","155","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121691677&doi=10.3390%2fs22010155&partnerID=40&md5=bc2aeef589a7d59bb26e26cd91723cd2","The article focuses on solving an important problem of detecting suspicious reviewers in online discussions on social networks. We have concentrated on a special type of suspicious authors, on trolls. We have used methods of machine learning for generation of detection models to discrimi-nate a troll reviewer from a common reviewer, but also methods of sentiment analysis to recognize the sentiment typical for troll’s comments. The sentiment analysis can be provided also using machine learning or lexicon‐based approach. We have used lexicon‐based sentiment analysis for its better abil-ity to detect a dictionary typical for troll authors. We have achieved Accuracy = 0.95 and F1 = 0.80 using sentiment analysis. The best results using machine learning methods were achieved by support vector machine, Accuracy = 0.986 and F1 = 0.988, using a dataset with the set of all selected attributes. We can conclude that detection model based on machine learning is more successful than lexicon‐based sentiment analysis, but the difference in accuracy is not so large as in F1 measure. © 2021 by the authors. Li-censee MDPI, Basel, Switzerland.","Detection of a troll; Machine learning; Sentiment analysis; Text data processing; Web mining","Data handling; Data mining; E-learning; Support vector machines; Detection models; Detection of a troll; Lexicon-based; Machine accuracy; Machine learning methods; Model-based OPC; Online discussions; Sentiment analysis; Support vectors machine; Web Mining; Sentiment analysis",Article,Scopus,2-s2.0-85121691677
"Ye C., Wang H., Zheng K., Kong Y., Zhu R., Gao J., Li J.","56245048100;14061534000;57210822192;57226844787;57188809346;55702655600;57211142666;","Constrained Truth Discovery",2022,"IEEE Transactions on Knowledge and Data Engineering","34","1",,"205","218",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121686422&doi=10.1109%2fTKDE.2020.2982393&partnerID=40&md5=d2cab937d93ada1b8f1ee76f997c33f5","To aggregate useful information among diversified sources, a hotspot research topic called truth discovery has emerged in recent years. Existing truth discovery methods attempt to infer the true attribute values for the entities by identifying and trusting reliable data sources. That is, the values provided by reliable sources are more likely to be the true values. However, all these methods neglect the relations among different entities, which play important roles in truth discovery task. When reliable data sources cannot provide sufficient information of entities, the true attribute values of these entities can still be inferred by propagating trustworthy information from related entities. Motivated by this, in this paper, we introduce the constrained truth discovery problem. We incorporate denial constraints, a universally quantified first-order logic formalism which can express a large number of effective and widely existing relations among entities, into the process of truth discovery. We formulate it as a constrained optimization problem and analyze its hardness. To address the problem, we propose algorithms to partition the entities into disjoint groups, and generate arithmetic constraints for each disjoint group separately. Then, the true attribute values of the entities in each disjoint group are derived by minimizing the objective function under the corresponding arithmetic constraints. Experimental results on both real-world and synthetic datasets demonstrate that the proposed approach achieves good performance even with very few constraints and reliable sources. © 1989-2012 IEEE.","Arithmetic constraints; Denial constraints; Iterative process; Source weights; Truth discovery","Constrained optimization; Data mining; Formal logic; Arithmetic constraints; Attribute values; Data-source; Denial constraint; Hotspots; Iterative process; Related entities; Research topics; Source weight; Truth discovery; Iterative methods",Article,Scopus,2-s2.0-85121686422
"Bourouis S., Pawar Y., Bouguila N.","24474162900;57221599711;6603545988;","Entropy-based variational scheme with component splitting for the efficient learning of gamma mixtures",2022,"Sensors","22","1","186","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121680949&doi=10.3390%2fs22010186&partnerID=40&md5=3fc3a219036e2dc51b1a608dbfe5dd2d","Finite Gamma mixture models have proved to be flexible and can take prior information into account to improve generalization capability, which make them interesting for several machine learning and data mining applications. In this study, an efficient Gamma mixture model-based approach for proportional vector clustering is proposed. In particular, a sophisticated entropy-based variational algorithm is developed to learn the model and optimize its complexity simultaneously. Moreover, a component-splitting principle is investigated, here, to handle the problem of model selection and to prevent over-fitting, which is an added advantage, as it is done within the variational framework. The performance and merits of the proposed framework are evaluated on multiple, realchallenging applications including dynamic textures clustering, objects categorization and human gesture recognition. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Component splitting; Entropy; Gamma mixtures; Gesture recognition; Objects categorization; Texture clustering; Variational Bayes","Data mining; Gesture recognition; Mixtures; Textures; Component splitting; Efficient learning; Entropy-based; Gamma mixtures; Gestures recognition; Mixture modeling; Object categorization; Splittings; Texture clustering; Variational bayes; Entropy; algorithm; artificial intelligence; cluster analysis; entropy; human; machine learning; Algorithms; Artificial Intelligence; Cluster Analysis; Entropy; Humans; Machine Learning",Article,Scopus,2-s2.0-85121680949
"Rahman M.K., Sujon M.H., Azad A.","36009058400;55258487200;36611394200;","Scalable force-directed graph representation learning and visualization",2022,"Knowledge and Information Systems","64","1",,"207","233",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121666477&doi=10.1007%2fs10115-021-01634-9&partnerID=40&md5=77308fa7b26d9bb296e7002eb68154a8","A graph embedding algorithm embeds a graph into a low-dimensional space such that the embedding preserves the inherent properties of the graph. While graph embedding is fundamentally related to graph visualization, prior work did not exploit this connection explicitly. We develop Force2Vec that uses force-directed graph layout models in a graph embedding setting with an aim to excel in both machine learning (ML) and visualization tasks. We make Force2Vec highly parallel by mapping its core computations to linear algebra and utilizing multiple levels of parallelism available in modern processors. The resultant algorithm is an order of magnitude faster than existing methods (43× faster than DeepWalk, on average) and can generate embeddings from graphs with billions of edges in a few hours. In comparison to existing methods, Force2Vec is better in graph visualization and performs comparably or better in ML tasks such as link prediction, node classification, and clustering. Source code is available at https://github.com/HipGraph/Force2Vec.This paper is an extension of a conference paper by Rahman et al. (in: 20th IEEE international conference on data mining, IEEE ICDM, 2020b) published in IEEE ICDM 2020. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Clustering; Force-directed model; Graph embedding; Link prediction; Node classification; Visualization","Data mining; Directed graphs; Embeddings; Linear algebra; Clusterings; Embedding algorithms; Force-Directed; Force-directed models; Graph embeddings; Graph representation; Graph visualization; Link prediction; Node classification; Visualization",Article,Scopus,2-s2.0-85121666477
"Wang J., Li X., Wang P., Liu Q., Deng Z., Wang J.","57383244500;35755945000;57219040613;55963540000;57383244600;57383894900;","Research trend of the unified theory of acceptance and use of technology theory: A bibliometric analysis",2022,"Sustainability (Switzerland)","14","1","10","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121656629&doi=10.3390%2fsu14010010&partnerID=40&md5=e2a482c28ce025f2c9550818114b2bb5","Information technology-acceptance research has always been a research hotspot. In 2003, Venkatesh established the unified theory of acceptance and use of technology (UTAUT), which pushed information technology-acceptance research to a new climax. This study uses bibliometrics, Bibliometrix, and CiteSpace software to conduct data mining and quantitative analysis on 1694 research papers in the UTAUT in the Web of Science core collection database from 2003 to 2021 (the data update time is 13 August 2021). Combined with a visual bibliometric analysis, this paper makes an in-depth discussion on the UTAUT model from the aspects of research trends, research fields, main research journals, authors/institutions, national or regional cooperation networks, etc. This study comprehensively and systematically shows the evolution track and characteristics of the UTAUT. On this basis, the future development trend of the UTAUT is put forward. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Bibliometrics; Bibliometrix; CiteSpace; Knowledge map; The unified theory of acceptance and use of technology (UTAUT)","data mining; database; future prospect; quantitative analysis; software; technological development",Article,Scopus,2-s2.0-85121656629
"Gao H., Zhao Q., Ning C., Guo D., Wu J., Li L.","57226724906;57364192500;57383979800;57226705373;57226724361;57226727942;","Does the covid-19 vaccine still work that “most of the confirmed cases had been vaccinated”? A content analysis of vaccine effectiveness discussion on sina weibo during the outbreak of covid-19 in Nanjing",2022,"International Journal of Environmental Research and Public Health","19","1","241","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121622113&doi=10.3390%2fijerph19010241&partnerID=40&md5=0878ef347cb5b48d062ddbe55187f644","In July 2021, breakthrough cases were reported in the outbreak of COVID-19 in Nanjing, sparking concern and discussion about the vaccine’s effectiveness and becoming a trending topic on Sina Weibo. In order to explore public attitudes towards the COVID-19 vaccine and their emotional orientations, we collected 1542 posts under the trending topic through data mining. We set up four categories of attitudes towards COVID-19 vaccines, and used a big data analysis tool to code and manually checked the coding results to complete the content analysis. The results showed that 45.14% of the Weibo posts (n = 1542) supported the COVID-19 vaccine, 12.97% were neutral, and 7.26% were doubtful, which indicated that the public did not question the vaccine’s effectiveness due to the breakthrough cases in Nanjing. There were 66.47% posts that reflected significant negative emotions. Among these, 50.44% of posts with negative emotions were directed towards the media, 25.07% towards the posting users, and 11.51% towards the public, which indicated that the negative emotions were not directed towards the COVID-19 vaccine. External sources outside the vaccine might cause vaccine hesitancy. Public opinions expressed in online media reflect the public’s cognition and attitude towards vaccines and their core needs in terms of information. Therefore, online public opinion monitoring could be an essential way to understand the opinions and attitudes towards public health issues. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Breakthrough cases; COVID-19 vaccine; Sentiment orientation; Social media","SARS-CoV-2 vaccine; vaccine; COVID-19; public attitude; social media; vaccine; World Wide Web; Article; attitude to illness; China; content analysis; coronavirus disease 2019; data mining; drug efficacy; emotion; infection prevention; negative emotion; pandemic; public health problem; public opinion; social media; vaccine hesitancy; epidemic; human; social media; China; Jiangsu; Nanjing [Jiangsu]; COVID-19; COVID-19 Vaccines; Disease Outbreaks; Humans; SARS-CoV-2; Social Media; Vaccination Hesitancy; Vaccine Efficacy",Article,Scopus,2-s2.0-85121622113
"Vathy-Fogarassy Á., Vassányi I., Kósa I.","14030821000;55902458500;6602733250;","Multi-level process mining methodology for exploring disease-specific care processes",2022,"Journal of Biomedical Informatics","125",,"103979","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121596378&doi=10.1016%2fj.jbi.2021.103979&partnerID=40&md5=93a98f0a9977d669313f262e8e9c99c2","Background: Public healthcare is a complex domain with many actors and highly variable protocols, which makes traditional process mining tools less effective and calls for specialized methods. Aim: The objective of the work was to develop a generally applicable process mining methodology to explore care processes related to diseases. Methods: The proposed methodology called Process Mining Methodology for Exploring Disease-specific Care Processes (MEDCP) is based on a systematic, step-wise refinement of the raw event logs by using such a multi-level expert taxonomy of events that encapsulates the professional concepts of the analysis. A treatment process is defined according to domain-specific rules to identify the starting (index) and closing events. Concepts from various levels of the taxonomy support the final process definition for an analysis that can deliver meaningful conclusions for domain experts. Results: The applicability of the methodology was demonstrated on two case studies in the cardiological and oncological care domains, in the public health care system in Hungary over a period of ten years. Thanks to the multi-level taxonomy, these studies successfully identified the most important high-level event sequence patterns and some key anomalies in the national care system, such as the significantly different behavior of low-volume vs. high volume care providers in the oncology study or the geographically connected, homogeneous clusters of providers with similar care spectra in the cardiology study. Discussion: As the case studies showed, the proposed methodology can improve the efficiency of standard process mining methods, and deliver high level conclusions that are easy to interpret by domain experts. System-level insight into health care processes can serve as a basis for the optimisation and long-term planning of the whole care system. © 2021 The Author(s)","Cardiology; Clinical pathway; Healthcare treatment sequence; Multi-level analysis; Oncology; Process mining","Cardiology; Health care; Oncology; Taxonomies; Care process; Case-studies; Clinical pathways; Domain experts; Healthcare treatment sequence; Multi-level analysis; Multi-level process; Multilevels; Process mining; Public healthcares; Data mining; cardiology; documentation; health care delivery; Abstracting and Indexing; Cardiology; Delivery of Health Care",Article,Scopus,2-s2.0-85121596378
"Čok V., Vlah D., Povh J.","54891956500;57214994850;23568747900;","Methodology for mapping form design elements with user preferences using Kansei engineering and VDI",2022,"Journal of Engineering Design","33","2",,"144","170",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121512605&doi=10.1080%2f09544828.2021.2012133&partnerID=40&md5=dc3c90dcfee40b6b1b353c64e75c7755","In product development, decisions about the appearance of the product are risky and difficult to make. Engineers and designers are aware that adding new design features or form design elements can degrade the visual appearance. Therefore, it is important to understand how future users perceive different design configurations. In this paper, an adapted Kansei Engineering (KE) methodology focusing on the extraction of affective attributes in product design is presented. The methodology is demonstrated using a case study in which we investigated the influence of e-bike form design elements on user perception. The study was conducted using 15 pairwise adjectives to describe feelings and a set of collected e-bike image samples with different product designs, converted to silhouettes. In addition to methodological refinement, a space of properties, specifically form design elements were categorised based on VDI 2223 guidelines. Semantic space was defined using predefined affective attributes and later reduced using factor analysis, while e-bike image similarity was exploited using the Agglomerative Hierarchical Clustering (AHC) method. Influential form design elements were extracted using the decision tree method for classification based on a C4.5 algorithm. Using this methodology, we succeeded in discovering key form design elements that determine user perception. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","data mining; Kansei engineering; Product design; product development; VDI 2223","Bicycles; Data mining; Decision trees; Product design; Semantics; Design elements; Design features; Design forms; Features design; Form design; Kansei Engineering; User perceptions; User's preferences; VDI 2223; Visual appearance; Product development",Article,Scopus,2-s2.0-85121512605
"Ghasemzadeh S., Maghsoudi A., Yousefi M., Mihalasky M.J.","57200209556;55485721800;54880470900;25025873500;","Information value-based geochemical anomaly modeling: A statistical index to generate enhanced geochemical signatures for mineral exploration targeting",2022,"Applied Geochemistry","136",,"105177","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121424353&doi=10.1016%2fj.apgeochem.2021.105177&partnerID=40&md5=31b00dd78fc45f6374e6f3c86d7076e5","Converting geochemical exploration data into information relevant to descriptive statistics of indicator elements facilitates gaining the knowledge of spatial and genetic relationships between mineralization and elemental dispersion patterns. This allows obtaining insights into alternatives to exploration strategies aiming at vectoring toward undiscovered mineral deposit sites. In this regard, shaping geochemical data for better understanding of their underlying patterns and extraction of information about ore deposition-related anomalies are challenging issues. To better extraction of the information from the geochemical data, we adapted the “information value” concept to quantify the significance of geochemical anomaly classes as spatial proxies of mineral deposits. To illustrate and evaluate the procedure proposed, a dataset of porphyry-Cu exploration was used to delineate exploration targets in the Baft district, Kerman province, Iran. Through this, geochemical models of information values were created for ore deposition-related elements. Then, the geochemical models were combined to produce an information value-based multi-element geochemical signature aiming at utilizing the prediction ability of every indicator element in a single model. Through the combination, spatial patterns of geochemical signals around mineral deposits are revealed that makes it easier to explore undiscovered deposit sites. We also applied fuzzy operators, as common and alternative practices, to integrate values of the same indicator elements for comparison purpose. The results demonstrated that the model of information value-based multi-element geochemical signature is an efficient geochemical proxy, over those geochemical models generated by the application of fuzzy operators, for exploration targeting. © 2021 Elsevier Ltd","Geometric average integration function; Information extraction; Mineral exploration targeting; Multi-element geochemical signature","Data mining; Deposits; Geochemistry; Minerals; Ore deposits; Exploration targeting; Geochemical models; Geochemical signatures; Geometric average integration function; Information extraction; Information value; Mineral exploration targeting; Multi-element geochemical signature; Multielements; Value-based; Mineral exploration; extraction method; geochemistry; geometry; index method; mineral deposit; mineral exploration; numerical model; statistical analysis",Article,Scopus,2-s2.0-85121424353
"Ma B., Sun H., Wang J., Qi Q., Liao J.","57191853544;57190948696;57285766000;57226881061;57226839870;","Extractive Dialogue Summarization without Annotation Based on Distantly Supervised Machine Reading Comprehension in Customer Service",2022,"IEEE/ACM Transactions on Audio Speech and Language Processing","30",,,"87","97",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121381339&doi=10.1109%2fTASLP.2021.3133206&partnerID=40&md5=2a1439f532e1c95ef58d08261417e6de","Given a long dialogue, the dialogue summarization system aims to obtain a shorter highlight which retains the important information in the original text. For the customer service scenarios, the summaries of most dialogues between an agent and a user focus on several fixed key points, such as users' question, users' purpose, the agent's solution, and so on. Traditional extractive methods are difficult to extract all predefined key points exactly. Furthermore, there is a lack of large-scale and high-quality extractive summarization datasets containing the annotation for key points. Moreover, the speaker's role information is ignored or not fully utilized in previous work. In order to solve the above challenges, we propose a Distant Supervision based Machine Reading Comprehension model for extractive Summarization (DSMRC-S). DSMRC-S transforms the summarization task into the machine reading comprehension problem, to fetch key points from the original text exactly according to the predefined questions. In addition, a distant supervision method is proposed to alleviate the lack of eligible extractive summarization datasets. What's more, a speaker's role token and the solver classification task are proposed to make full use of speaker's role information. We conduct experiments on a real-world summarization dataset collected in customer service scenarios, and the results show that the proposed method outperforms the strong baseline methods by 6 percentage points on ROUGE$_L$. © 2014 IEEE.","customer service; distant supervision; Extractive summarization; machine reading comprehension","Classification (of information); Data mining; Job analysis; Sales; Semantics; Context models; Customer-service; Distant supervision; Extractive summarizations; Keypoints; Machine reading comprehension; Reading comprehension; Summarization systems; Task analysis; Speech processing",Article,Scopus,2-s2.0-85121381339
"Santo G.C.M., Garcia C., Chaves C.R.","57373935700;7401486028;57210389700;","System identification with historical data: A multivariable approach applied to a 50MW petrochemical furnace",2022,"Computers and Chemical Engineering","157",,"107623","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121267924&doi=10.1016%2fj.compchemeng.2021.107623&partnerID=40&md5=f024b4515f959702ec699eac0a0f0a57","This paper addresses the complex problem of scanning historical data from industrial processes with the purpose of finding useful intervals for system identification. More specifically, it considers the problem in its multivariable version, for a system under closed-loop control. A detailed review of the literature on the subject is presented and some of the studied methodologies are condensed into an algorithm. The resulting algorithm is applied to seven months of historical data from a petrochemical furnace of the Brazilian company Petrobras. The objective of the algorithm is to obtain process models that represent the dynamics of the system between the set-points and the output variables with the goal of designing predictive controllers. A comparison of different techniques is presented, elucidating their impacts on the results in the furnace dataset. In addition, a viable way of choosing parameters is presented, allowing the use of the algorithm in massive data. © 2021 Elsevier Ltd","Automatic system identification; Condition number; Data mining; Data quality; Data segmentation; Effective rank; Historical data; Multivariable system identification","Data mining; Multivariable systems; Petrochemicals; Religious buildings; Automatic system identification; Automatic systems; Condition numbers; Data quality; Data segmentation; Effective rank; Historical data; Multivariable approach; Multivariable system identification; System-identification; Number theory",Article,Scopus,2-s2.0-85121267924
"Orwat-Kapola J.K., Bird A.J., Hill A.B., Altamirano D., Huppenkothen D.","57221835205;57373160300;57353661400;15821780400;35731349000;","Light-curve fingerprints: An automated approach to the extraction of X-ray variability patterns with feature aggregation - An example application to GRS 1915+105",2022,"Monthly Notices of the Royal Astronomical Society","509","1",,"1269","1290",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121211431&doi=10.1093%2fmnras%2fstab3043&partnerID=40&md5=cb1278e0c085de889224c3ad22b55858","Time series data mining is an important field of research in the era of 'Big Data'. Next generation astronomical surveys will generate data at unprecedented rates, creating the need for automated methods of data analysis. We propose a method of light-curve characterization that employs a pipeline consisting of a neural network with a long-short te memory variational autoencoder architecture and a Gaussian mixture model. The pipeline perfos extraction and aggregation of features from light-curve segments into feature vectors of fixed length that we refer to as light-curve 'fingerprints'. This representation can be readily used as input of down-stream machine learning algorithms. We demonstrate the proposed method on a data set of Rossi X-ray Timing Explorer observations of the Galactic black hole X-ray binary GRS 1915+105, which was chosen because of its observed complex X-ray variability. We find that the proposed method can generate a representation that characterizes the observations and reflects the presence of distinct classes of GRS 1915+105 X-ray flux variability. We find that this representation can be used to perfo efficient classification of light curves. We also present how the representation can be used to quantify the similarity of different light curves, highlighting the problem of the popular classification system of GRS 1915+105 observations, which does not account for inteediate class behaviour. © 2021 The Author(s) Published by Oxford University Press on behalf of Royal Astronomical Society.","binaries; data analysis; methods; X-rays","Data handling; Data mining; Extraction; Gaussian distribution; Learning algorithms; Machine learning; Astronomical surveys; Automated approach; Binary; Feature aggregation; GRS 1915+105; Light curves; Method; Time series data mining; Variability patterns; X-ray variability; Pipelines",Article,Scopus,2-s2.0-85121211431
"Buchaiah S., Shakya P.","57219547411;56269865500;","Bearing fault diagnosis and prognosis using data fusion based feature extraction and feature selection",2022,"Measurement: Journal of the International Measurement Confederation","188",,"110506","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121153579&doi=10.1016%2fj.measurement.2021.110506&partnerID=40&md5=c89c5122ee004357ce7cbd6bf7be7c68","The extraction of significant features is essential for efficient fault diagnosis and prognosis of rolling element bearing. Data fusion is the predominant technology for extracting significant features by fusing several original features. In this paper, seventy-two original features are extracted from bearing vibration data using various signal processing techniques. The relevant features subset is selected from the extracted features using the Random Forest method. The selected features are fused by fourteen dimensionality reduction techniques to extract 2D fault features and health indicators, and a comparison is made between the techniques to identify the most efficient technique. The Bhattacharyya distance and Support vector machine are used to verify fault diagnosis accuracy. A new index is computed for selecting the suitable prognosis health indicator, and the Long short-term memory technique is used to predict the remaining useful life of bearing. Two real-world bearing datasets are utilized to validate the proposed methodology. © 2021 Elsevier Ltd","Data fusion; Fault diagnosis; Long short-term memory; Prognosis; Rolling element bearing; Support vector machine","Brain; Data fusion; Data mining; Decision trees; Extraction; Failure analysis; Fault detection; Feature extraction; Long short-term memory; Roller bearings; Signal processing; Bearing fault diagnosis; Diagnosis and prognosis; Fault prognosis; Faults diagnosis; Feature extraction/selection; Features selection; Health indicators; Prognose; Rolling Element Bearing; Support vectors machine; Support vector machines",Article,Scopus,2-s2.0-85121153579
"de Freitas K.L.F., da Silva P.N., Faria B.M., Gonçalves E.C., Rios E.H., Nobre-Lopes J., Rabe C., Plastino A., de Vasconcelos Azeredo R.B.","57371704700;56358177800;57372006500;26651871000;54682314100;56902645900;57197490356;56187378700;7801605141;","A data mining approach for automatic classification of rock permeability",2022,"Journal of Applied Geophysics","196",,"104514","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121125219&doi=10.1016%2fj.jappgeo.2021.104514&partnerID=40&md5=c3b6b3b38c6c28f53fe08e6fa65a159c","Reliable estimates of porosity can be obtained from different types of geophysical well logs. However, obtaining in situ permeability estimates is still a major challenge in the geosciences. This work aims to evaluate the application of data mining techniques to NMR logs for rock permeability classification, thus far tested only on laboratory data. For this study, we used a petrophysical database from two Brazilian Pre-salt wells located in the Santos Basin, a formation notoriously difficult to characterize, mainly due to its diversity and complexity. Six classification algorithms were evaluated (k-NN, NB, C4.5, RF, SMO, and MLP) according to their ability to estimate the permeability of rocks in four distinct classes (low: <1 mD, intermediate: 1–10 mD, high: 10–100 mD, and excellent: >100 mD). The predictive performance of the algorithms was compared to the behavior of two traditional permeability estimators. With an accuracy of 66%, the Naïve Bayes algorithm, combined with two preprocessing steps – unsupervised discretization and attribute selection – achieved the highest predictive performance. That mark surpassed the accuracy obtained by Kenyon and Timur-Coates estimators by 154% and 106%, respectively, providing evidence for the superiority of the data mining technique to recognize permeability classes based on NMR logs. Classification experiments employing NMR logs in conjunction with conventional logs were also conducted, but this log combination was not able to best the predictive result based solely on the NMR log data. © 2021 Elsevier B.V.","Carbonates of the pre-salt; Data mining; NMR log; Permeability","Classification (of information); Nearest neighbor search; Automatic classification; Carbonate of the pre-salt; Data-mining techniques; Geosciences; NMR log; Permeability; Pre salts; Predictive performance; Reliable estimates; Rock permeability; Data mining; carbonate rock; classification; data mining; nuclear magnetic resonance; permeability; well logging; Atlantic Ocean; Santos Basin",Article,Scopus,2-s2.0-85121125219
"Yang D., Hou N., Lu J., Ji D.","57216826077;57372116800;57216481791;57327351000;","Novel leakage detection by ensemble 1DCNN-VAPSO-SVM in oil and gas pipeline systems",2022,"Applied Soft Computing","115",,"108212","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121101796&doi=10.1016%2fj.asoc.2021.108212&partnerID=40&md5=55f0c88b79099d04f49c898d31e4d63d","In this paper, a novel ensemble model of one-dimensional convolution neural network (1DCNN) and support vector machine (SVM) is proposed to improve the detection accuracy in the process of pipeline leakage detection. Firstly, 1DCNN is constructed by experiments on different network structures and parameters, and it is used to extract data features adaptively. Then, an improved particle swarm optimization (PSO) algorithm is put forward, called variable amplitude PSO (VAPSO), with the adjustment strategy of parameter variable amplitude vibration to optimize the parameter combination in SVM and decrease the risk of trapping into local optimum in the training process. Finally, the data features extracted adaptively from the network are input into the improved VAPSO-SVM to classify. It is demonstrated by the experimental results that, compared with the existing models, the developed ensemble model has the capacity to extract the features of pipeline data more quickly and accurately with effective improvement in the classification accuracy, and has better robustness in the process of pipeline leakage detection. © 2021","Deep learning; One-dimensional convolution neural network; Pipeline leakage detection; Support vector machine; Variable amplitude particle swarm optimization","Convolution; Data mining; Deep learning; Fiber optic sensors; Leakage (fluid); Particle swarm optimization (PSO); Pipelines; Convolution neural network; Data feature; Deep learning; Ensemble models; One-dimensional; One-dimensional convolution neural network; Pipeline leakage detection; Support vectors machine; Variable amplitude particle swarm optimization; Variable amplitudes; Support vector machines",Article,Scopus,2-s2.0-85121101796
"Eshmawi A.A., Alhumyani H., Khalek S.A., Saeed R.A., Ragab M., Mansour R.F.","35112877500;55586872500;57370909500;16022855100;55932216500;36960713000;","Design of automated opinion mining model using optimized fuzzy neural network",2022,"Computers, Materials and Continua","71","2",,"2543","2557",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121024445&doi=10.32604%2fcmc.2022.021833&partnerID=40&md5=24304d2438316304d00906ebcd6c81db","Sentiment analysis or Opinion Mining (OM) has gained significant interest among research communities and entrepreneurs in the recent years. Likewise, Machine Learning (ML) approaches is one of the interesting research domains that are highly helpful and are increasingly applied in several business domains. In this background, the current research paper focuses on the design of automated opinion mining model using Deer Hunting Optimization Algorithm (DHOA) with Fuzzy Neural Network (FNN) abbreviated as DHOA-FNN model. The proposed DHOA-FNN technique involves four different stages namely, preprocessing, feature extraction, classification, and parameter tuning. In addition to the above, the proposed DHOA-FNN model has two stages of feature extraction namely, Glove and N-gram approach. Moreover, FNN model is utilized as a classification model whereas GTOA is used for the optimization of parameters. The novelty of current work is that the GTOA is designed to tune the parameters of FNN model. An extensive range of simulations was carried out on the benchmark dataset and the results were examined under diverse measures. The experimental results highlighted the promising performance of DHOA-FNN model over recent state-of-the-art techniques with a maximum accuracy of 0.9928. © 2022 Tech Science Press. All rights reserved.","Classification; Feature extraction; Fuzzy neural network; Metaheuristics; Opinion mining; Sentiment analysis","Classification (of information); Data mining; Extraction; Fuzzy inference; Fuzzy neural networks; Optimization; Sentiment analysis; 'current; Deer hunting; Features extraction; Fuzzy neural network model; Fuzzy-neural-networks; Metaheuristic; Opinion mining; Optimization algorithms; Research communities; Sentiment analysis; Feature extraction",Article,Scopus,2-s2.0-85121024445
"Jimoh R.G., Abisoye O.A., Uthman M.M.B.","57192954046;57221871133;24345162300;","Ensemble Feed-Forward Neural Network and Support Vector Machine for Prediction of Multiclass Malaria Infection",2022,"Journal of Information and Communication Technology","21","1",,"117","148",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120979710&doi=10.32890%2fjict2022.21.1.6&partnerID=40&md5=c05260107eb17c1124fab55d201c7398","Globally, recent research are focused on developing appropriate and robust algorithms to provide a robust healthcare system that is versatile and accurate. Existing malaria models are plagued with low rate of convergence, overfitting, limited generalization due to restriction to binary cases prediction, and proneness to local minimum errors in finding reliable testing output due to complexity of features in the feature space, which is a black box in nature. This study adopted a stacking method of heterogeneous ensemble learning of Artificial Neural Network (ANN) and Support Vector Machine (SVM) algorithms to predict multiclass, symptomatic, and climatic malaria infection. ANN produced 48.33 percent accuracy, 60.61 percent sensitivity, and 45.58 percent specificity. SVM with Gaussian kernel function gave better performance results of 85.60 percent accuracy, 84.06 percent sensitivity, and 86.09 percent specificity. Consequently, to improve prediction performance, a stacking method was introduced to ensemble SVM with ANN. The proposed ensemble malaria model was tuned on different thresholds at a threshold value of 0.60, theensemble model gave an optimum accuracy of 99.86 percent, sensitivity 100 percent, specificity 98.68 percent, and mean square error 0.14. The ensemble model experimental results indicated that stacked multiple classifiers produced better results than a single model.This research demonstrated the efficiency of heterogeneous stacking ensemble model on effects of climatic variations on multiclass malaria infection classification. Furthermore, the model reduced complexity, overfitting, low rate of convergence, and proneness to local minimum error problems of multiclass malaria infection in comparison to previous related models © 2022, Journal Of Information And Communication Technology. All Rights Reserved.","Artificial neural network; data mining; ensemble; malaria infection; support vector machine",,Article,Scopus,2-s2.0-85120979710
"Du X., Jia L., Ul Haq I.","35239583700;57224202043;57426334700;","Fault diagnosis based on SPBO-SDAE and transformer neural network for rotating machinery",2022,"Measurement: Journal of the International Measurement Confederation","188",,"110545","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120972751&doi=10.1016%2fj.measurement.2021.110545&partnerID=40&md5=35f18a9904f85022a83cbb1de16873c0","Fault diagnosis for rotating machinery requires both high diagnosis accuracy and time efficiency. A rotating machinery fault diagnosis method based on intelligent feature self-extraction and transformer neural network is proposed. Firstly, the proposed method employs the student psychology based optimization (SPBO) algorithm to adaptively select hyper parameters, including the number of hidden layer nodes, sparsity coefficient and input data zeroing ratio, of the denoising auto encoder (DAE) network to determine the optimal structure of the stacked denoising auto encoders (SDAE) network. Secondly, the optimized SPBO-SDAE network is used to extract features from high-dimensional original data layer by layer. On this basis, the weight parameters of self-extracted features of SPBO-SDAE network are optimized through the self-attention mechanism of transformer deep neural network. The target features are retained, and the redundant features are filtered. Finally, in order to further validate the performance of the proposed model in the complex conditions, by adding Gaussian noise to the original data, the diagnosis performance of the proposed method is verified through four open data sets. The simulation results indicate that compared with the existing common shallow learning and deep learning methods, the proposed method has great advantages in generalization performance, fault diagnosis accuracy and time efficiency. © 2021 Elsevier Ltd","Fault diagnosis; Feature self-extraction; Hyper parameter optimization; Rotating machinery; Self attention mechanism; Transformer neural network","Data mining; Deep neural networks; Efficiency; Extraction; Fault detection; Gaussian noise (electronic); Rotating machinery; Signal encoding; Structural optimization; Attention mechanisms; Auto encoders; De-noising; Faults diagnosis; Feature self-extraction; Hyper-parameter optimizations; Neural-networks; Self attention mechanism; Self extractions; Transformer neural network; Failure analysis",Article,Scopus,2-s2.0-85120972751
"Martínez-deMiguel C., Segura-Bedmar I., Chacón-Solano E., Guerrero-Aspizua S.","57226950100;35303400800;57200920080;36677301900;","The RareDis corpus: A corpus annotated with rare diseases, their signs and symptoms",2022,"Journal of Biomedical Informatics","125",,"103961","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120964120&doi=10.1016%2fj.jbi.2021.103961&partnerID=40&md5=05a071f4ecaff437b659337051039493","Rare diseases affect a small number of people compared to the general population. However, more than 6,000 different rare diseases exist and, in total, they affect more than 300 million people worldwide. Rare diseases share as part of their main problem, the delay in diagnosis and the sparse information available for researchers, clinicians, and patients. Finding a diagnostic can be a very long and frustrating experience for patients and their families. The average diagnostic delay is between 6–8 years. Many of these diseases result in different manifestations among patients, which hampers even more their detection and the correct treatment choice. Therefore, there is an urgent need to increase the scientific and medical knowledge about rare diseases. Natural Language Processing (NLP) can help to extract relevant information about rare diseases to facilitate their diagnosis and treatments, but most NLP techniques require manually annotated corpora. Therefore, our goal is to create a gold standard corpus annotated with rare diseases and their clinical manifestations. It could be used to train and test NLP approaches and the information extracted through NLP could enrich the knowledge of rare diseases, and thereby, help to reduce the diagnostic delay and improve the treatment of rare diseases. The paper describes the selection of 1,041 texts to be included in the corpus, the annotation process and the annotation guidelines. The entities (disease, rare disease, symptom, sign and anaphor) and the relationships (produces, is a, is acron, is synon, increases risk of, anaphora) were annotated. The RareDis corpus contains more than 5,000 rare diseases and almost 6,000 clinical manifestations are annotated. Moreover, the Inter Annotator Agreement evaluation shows a relatively high agreement (F1-measure equal to 83.5% under exact match criteria for the entities and equal to 81.3% for the relations). Based on these results, this corpus is of high quality, supposing a significant step for the field since there is a scarcity of available corpus annotated with rare diseases. This could open the door to further NLP applications, which would facilitate the diagnosis and treatment of these rare diseases and, therefore, would improve dramatically the quality of life of these patients. © 2021 The Author(s)","Gold-standard corpus; Named Entity Recognition; Rare Diseases; Relation Extraction","Data mining; Diagnosis; Natural language processing systems; Patient treatment; Clinical manifestation; General population; Gold standards; Gold-standard corpus; Medical knowledge; Named entity recognition; Number of peoples; Rare disease; Relation extraction; Scientific knowledge; Diseases; Article; clinical feature; delayed diagnosis; human; information processing; medical informatics; natural language processing; quality of life; rare disease; symptomatology; quality of life; Delayed Diagnosis; Humans; Natural Language Processing; Quality of Life; Rare Diseases",Article,Scopus,2-s2.0-85120964120
"Zabin A., González V.A., Zou Y., Amor R.","57216672500;55854039700;57072337800;6603399556;","Applications of machine learning to BIM: A systematic literature review",2022,"Advanced Engineering Informatics","51",,"101474","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120960468&doi=10.1016%2fj.aei.2021.101474&partnerID=40&md5=af2651f20294beb23c072f23c526f559","As Building Information Modeling (BIM) workflows are becoming very relevant for the different stages of the project's lifecycle, more data is produced and managed across it. The information and data accumulated in BIM-based projects present an opportunity for analysis and extraction of project knowledge from the inception to the operation phase. In other industries, Machine Learning (ML) has been demonstrated to be an effective approach to automate processes and extract useful insights from different types and sources of data. The rapid development of ML applications, the growing generation of BIM-related data in projects, and the different needs for use of this data present serious challenges to adopt and effectively apply ML techniques to BIM-based projects in the Architecture, Engineering, Construction and Operations (AECO) industry. While research on the use of BIM data through ML has increased in the past decade, it is still in a nascent stage. In order to asses where the industry stands today, this paper carries out a systematic literature review (SLR) identifying and summarizing common emerging areas of application and utilization of ML within the context of BIM-generated data. Moreover, the paper identifies research gaps and trends. Based on the observed limitations, prominent future research directions are suggested, focusing on information architecture and data, applications scalability, and human information interactions. © 2021 Elsevier Ltd","Artificial intelligence; Building information modeling; Data mining; Machine learning; Systematic literature review","Architectural design; Information theory; Life cycle; Machine learning; Building Information Modelling; Different stages; Effective approaches; In-buildings; Machine-learning; Model-based OPC; Operation phasis; Project lifecycle; Systematic literature review; Work-flows; Data mining",Article,Scopus,2-s2.0-85120960468
"Sabbagh R., Živković S., Gawlik B., Sreenivasan S.V., Stothert A., Majstorovic V., Djurdjanovic D.","57193714420;56368189800;57156203900;57293522200;6602435575;6603288002;6603566263;","Organization of big metrology data within the Cyber-Physical Manufacturing Metrology Model (CPM3)",2022,"CIRP Journal of Manufacturing Science and Technology","36",,,"90","99",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120904981&doi=10.1016%2fj.cirpj.2021.10.009&partnerID=40&md5=96eef6b6d9c30ec3d08157c7ec011a58","In this paper, we propose a novel data curation concept that enables data mining and analytics within the recently described Cyber-Physical Manufacturing Metrology Model (CPM3). The newly proposed methodology is based on organizing the metrology data into tree-based database structures using distance-based unsupervised clustering of the raw metrology data. Compared to traditionally utilized temporally organized lists, the new tree-based database organization of metrology data enables logarithmic acceleration of searches within the data and thus provides dramatic advantages for data mining. The newly proposed data curation methodology was evaluated in case studies involving hyper-spectral metrology of nanopatterned surfaces, coordinate measurement machine (CMM) inspection of aircraft engine turbines and imaging-based metrology of nano-volume droplets in the jet and fill stage of imprint lithography processes. Significant improvements in search speeds with minimal or no losses in search precision and recall were observed in all case-studies, with benefits of tree-based data organization growing with the size of the data. © 2021","Big data curation; Big data management; Cyber-Physical Manufacturing Systems; Industrial internet of things; Industry 4.0; Metrology","Big data; Coordinate measuring machines; Data mining; Industry 4.0; Big data curation; Big data management; Case-studies; Cybe-physical manufacturing system; Cyber physicals; Data curation; Database structures; Manufacturing metrologies; Metrology data; Tree-based; Information management",Article,Scopus,2-s2.0-85120904981
"Yepmo V., Smits G., Pivert O.","57367227000;35185786000;6701395936;","Anomaly explanation: A review",2022,"Data and Knowledge Engineering","137",,"101946","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120895837&doi=10.1016%2fj.datak.2021.101946&partnerID=40&md5=777b057307eb7977c636a07b1ddf96bd","Anomaly detection has been studied intensively by the data mining community for several years. As a result, many methods to detect anomalies have emerged, and others are still under development. But during the recent years, anomaly detection, just like a lot of machine learning tasks, is facing a wall. This wall, erected by the lack of trust of the final users, has slowed down the usage of these algorithms in the real-world situations for which they are designed. Having the best empirical accuracy is not enough anymore; there is a need for algorithms to explain their outputs to the users in order to increase their trust. Consequently, a new expression has emerged recently: eXplainable Artificial Intelligence (XAI). This expression, which gathers all the methods that provide explanations to the output of algorithms has gained popularity, especially with the outbreak of deep learning. A lot of work has been devoted to anomaly detection in the literature, but not as much to anomaly explanation. There is so much work on anomaly detection that several reviews can be found on the topic. In contrast, we were not able to find a survey on anomaly explanation in particular, while there are a lot of surveys on XAI in general or on XAI for neural networks for example. With this paper, we want to provide a comprehensive review of the anomaly explanation field. After a brief recall of some important anomaly detection algorithms, the anomaly explanation methods that we discovered in the literature will be classified according to a taxonomy that we define. This taxonomy stems from an analysis of what is really important when trying to explain anomalies. © 2021 Elsevier B.V.","Anomaly detection; Anomaly explanation; Explainable Artificial Intelligence (XAI); Interpretability; Outlier interpretation","Data mining; Deep learning; Surveys; Taxonomies; Anomaly detection; Anomaly explanation; Anomaly-detection algorithms; Classifieds; Data mining community; Explainable artificial intelligence (XAI); Interpretability; Neural-networks; Outlier interpretation; Real world situations; Anomaly detection",Article,Scopus,2-s2.0-85120895837
"Ros-Roca X., Montero L., Barceló J., Nökel K., Gentile G.","57200147057;7003966532;55574123051;6505570322;55890379500;","A practical approach to assignment-free Dynamic Origin–Destination Matrix Estimation problem",2022,"Transportation Research Part C: Emerging Technologies","134",,"103477","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120893735&doi=10.1016%2fj.trc.2021.103477&partnerID=40&md5=ecd0a9c77a6825eda6dba73fc5b8cd2a","Dynamic traffic models require dynamic inputs, one of the main ones being the Dynamic Origin–Destinations (OD) matrices describing the variability over time of the trip patterns across the network. The Dynamic OD Matrix Estimation (DODME) is a challenging problem since no direct observations are available, and therefore one should resort to indirect estimation approaches. Among the most efficient approaches, the one that formulates the problem in terms of a bi-level optimization problem has been widely used. This formulation solves at the upper level a nonlinear optimization problem that minimizes some distance measures between observed and estimated link flow counts at certain counting stations located in a subset of links in the network, and at the lower level a traffic assignment that estimates these link flow counts assigning the current estimated matrix. The variants of this formulation differ in the analytical approaches that estimate the link flows in terms of the traffic assignment and their time dependencies. Since these estimations are based on a traffic assignment at the lower level, these analytical approaches, although numerically efficient, imply a high computational cost. The advent of ICT applications has made available new sets of traffic-related measurements enabling new approaches; under certain conditions, the data collected allows to estimate the most likely used paths, from which a de facto assignment matrix can be computed. This allows extracting empirically similar information to that provided by the dynamic traffic assignment that is used in the analytical approaches. This paper explores how to extract such information from the recorded commercial data, proposes a new constrained non-linear optimization model to solve the DODME problem, with a reduced number of variables linearly depending on network size instead of quadratically. Moreover, the bilevel iterative process and the traffic assignment need are avoided. Validation and computational results on its performance are presented. © 2021 The Author(s)","Dynamic origin–destination matrices; Dynamic traffic assignment; ICT traffic data; Nonlinear optimization","Constrained optimization; Data mining; Nonlinear programming; Traffic control; Analytical approach; Dynamic origin/destination matrixes; Dynamic traffic; Dynamic traffic assignment; ICT traffic data; Link-flow; Non-linear optimization; Origin-destination matrix estimation; Traffic assignment; Traffic data; Matrix algebra; computer simulation; estimation method; information and communication technology; model validation; modeling; nonlinearity; numerical model; optimization; performance assessment; spatial analysis; travel behavior",Article,Scopus,2-s2.0-85120893735
"Rostami M., Forouzandeh S., Berahmand K., Soltani M., Shahsavari M., Oussalah M.","56538047900;56328020700;57199714126;57218321554;57367325000;55587723600;","Gene selection for microarray data classification via multi-objective graph theoretic-based method",2022,"Artificial Intelligence in Medicine","123",,"102228","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120845860&doi=10.1016%2fj.artmed.2021.102228&partnerID=40&md5=f8b116652ab9a75dd42559810f2fb8ed","In recent decades, the improvement of computer technology has increased the growth of high-dimensional microarray data. Thus, data mining methods for DNA microarray data classification usually involve samples consisting of thousands of genes. One of the efficient strategies to solve this problem is gene selection, which improves the accuracy of microarray data classification and also decreases computational complexity. In this paper, a novel social network analysis-based gene selection approach is proposed. The proposed method has two main objectives of the relevance maximization and redundancy minimization of the selected genes. In this method, on each iteration, a maximum community is selected repetitively. Then among the existing genes in this community, the appropriate genes are selected by using the node centrality-based criterion. The reported results indicate that the developed gene selection algorithm while increasing the classification accuracy of microarray data, will also decrease the time complexity. © 2021 The Authors","Community detection; Feature selection; Gene selection; Microarray data classification; Multi-objective; Node centrality","Classification (of information); Complex networks; Data mining; Feature extraction; Graph theory; Iterative methods; Community detection; Computer technology; Data classification; Features selection; Gene selection; Graph theoretics; Microarray data classification; Microarrays data; Multi objective; Node centrality; Genes; algorithm; article; data classification; DNA microarray; feature selection; human; social network analysis",Article,Scopus,2-s2.0-85120845860
"Yuvaraj D., Dinesh M., Sivaram M., Nageswari S.","56597897900;46161442400;55220262500;57211237308;","An efficient data mining process on temporal data using relevance feedback method",2022,"World Review of Science, Technology and Sustainable Development","18","1",,"20","29",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120811329&doi=10.1504%2fWRSTSD.2022.119299&partnerID=40&md5=82013e2d0edb818af1895988f5a7c0a1","Some data is historical such as the judicial or medical which the factor of time is very important. Temporal databases can deal with these data because it provides a systematic way of dealing with historical data. The temporal data mining deals with these types of data, that has time stamping and influences by the factor of time after mining. The relevance feedback technique is included in the individual's revisitation habits and memory strength. We have additionally executed and assessed the performance by utilising a trace driven methodology dependent on the online real behaviour dataset. We also presented the large level datasets introducing a profile that encode the user subjective notation of similarity in domain. These profiles can be gained ceaselessly from connection with client. We further show how the client profile might be embedded in a system that utilisation relevance feedback mechanism. At long last, we compute the scalability of our system by utilising various datasets from the various domains. Copyright © 2022 Inderscience Enterprises Ltd.","Behaviour dataset; Data mining; Pattern; Relevance feedback; Scalable framework; Scientific strategies; Skyline pyramid; Temporal data; Time-stamp data; Uniform resource locator; Web revisitation",,Article,Scopus,2-s2.0-85120811329
"Shukla A.K., Das S.","57211267360;56298977200;","Deep neural network and pseudo relevance feedback based query expansion",2022,"Computers, Materials and Continua","71","2",,"3557","3570",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120808280&doi=10.32604%2fcmc.2022.022411&partnerID=40&md5=570a1f63f35a7f58aef78b1bfd921ca1","The neural network has attracted researchers immensely in the last couple of years due to its wide applications in various areas such as Data mining, Natural language processing, Image processing, and Information retrieval etc. Word embedding has been applied by many researchers for Information retrieval tasks. In this paper word embedding-based skip-gram model has been developed for the query expansion task. Vocabulary terms are obtained from the top ""k"" initially retrieved documents using the Pseudo relevance feedback model and then they are trained using the skip-grammodel to find the expansion terms for the user query. The performance of the model based on mean average precision is 0.3176. The proposed model compares with other existing models. An improvement of 6.61%, 6.93%, and 9.07% on MAP value is observed compare to the Original query, BM25 model, and query expansion with the Chi-Square model respectively. The proposed model also retrieves 84, 25, and 81 additional relevant documents compare to the original query, query expansion with Chi-Square model, and BM25 model respectively and thus improves the recall value also. The per query analysis reveals that the proposed model performs well in 30, 36, and 30 queries compare to the original query, query expansion with Chi-square model, and BM25 model respectively. © 2022 Tech Science Press. All rights reserved.","Deep neural network; Information retrieval; Neural network; Query expansion; Word embedding","Data mining; Deep neural networks; Embeddings; Expansion; Image processing; Natural language processing systems; Embeddings; Feed-back based; Gram models; Image information; Images processing; Neural-networks; Pseudo-relevance feedbacks; Query expansion; Retrieved documents; Word embedding; Information retrieval",Article,Scopus,2-s2.0-85120808280
"Song K., Zhao Z., Ma Y., Wang J., Wu W., Qiang Y., Zhao J., Chaudhary S.","57211684200;57211845259;57223052606;57211026994;57207082700;26639724500;35097898900;57225365162;","A multitask dual-stream attention network for the identification of KRAS mutation in colorectal cancer",2022,"Medical Physics","49","1",,"254","270",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120806260&doi=10.1002%2fmp.15361&partnerID=40&md5=2482b6488475739c3726b6b57a2d219f","Purpose: It is of great significance to accurately identify the KRAS gene mutation status for patients in tumor prognosis and personalized treatment. Although the computer-aided diagnosis system based on deep learning has gotten all-round development, its performance still cannot meet the current clinical application requirements due to the inherent limitations of small-scale medical image data set and inaccurate lesion feature extraction. Therefore, our aim is to propose a deep learning model based on T2 MRI of colorectal cancer (CRC) patients to identify whether KRAS gene is mutated. Methods: In this research, a multitask attentive model is proposed to identify KRAS gene mutations in patients, which is mainly composed of a segmentation subnetwork and an identification subnetwork. Specifically, at first, the features extracted by the encoder of segmentation model are used as guidance information to guide the two attention modules in the identification network for precise activation of the lesion area. Then the original image of the lesion and the segmentation result are concatenated for feature extraction. Finally, features extracted from the second step are combined with features activated by the attention modules to identify the gene mutation status. In this process, we introduce the interlayer loss function to encourage the similarity of the two subnetwork parameters and ensure that the key features are fully extracted to alleviate the overfitting problem caused by small data set to some extent. Results: The proposed identification model is benchmarked primarily using 15-fold cross validation. Three hundred and eighty-two images from 36 clinical cases were used to test the model. For the identification of KRAS mutation status, the average accuracy is 89.95 (Formula presented.) 1.23%, the average sensitivity is 89.29 (Formula presented.) 1.79%, the average specificity is 90.53 (Formula presented.) 2.45%, and the average area under the curve (AUC) is 95.73 (Formula presented.) 0.52%. For segmentation of lesions, the average dice is 88.11 (Formula presented.) 0.86%. Conclusions: We developed a novel deep learning–based model to identify the KRAS status in CRC. We demonstrated the excellent properties of the proposed identification through comparison with ground truth gene mutation status of 36 clinical cases. And all these results show that the novel method has great potential for clinical application. © 2021 American Association of Physicists in Medicine",,"Computer aided diagnosis; Computer aided instruction; Data mining; Deep learning; Extraction; Feature extraction; Genes; Image segmentation; Medical computing; Medical imaging; Patient treatment; Attention mechanisms; Clinical application; Computer aided diagnosis systems; Deep learning; Features extraction; Genes mutation; KRAS; Rectal cancer; Small data set; Subnetworks; Diseases; K ras protein; KRAS protein, human; protein p21; adult; Article; attention network; colorectal cancer; cross validation; deep learning; feature extraction; female; gene mutation; human; image segmentation; major clinical study; male; measurement accuracy; middle aged; nuclear magnetic resonance imaging; area under the curve; colorectal tumor; diagnostic imaging; genetics; image processing; mutation; nuclear magnetic resonance imaging; Area Under Curve; Colorectal Neoplasms; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Mutation; Proto-Oncogene Proteins p21(ras)",Article,Scopus,2-s2.0-85120806260
"He D., Chen T., Huang H., Qiu W., Tang Y., Jiang J.","57366156500;57366448300;57366448400;57365869900;57365870000;57366301200;","Dynamic fault diagnosis means of the power message system based on big data",2022,"International Journal of Information and Communication Technology","20","1",,"83","96",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120774277&doi=10.1504%2fIJICT.2022.119320&partnerID=40&md5=1de391725e93220ece086c6c456f468c","Aiming at the poor fault diagnosis ability of traditional power information system, a dynamic fault diagnosis method based on big data for power information system is proposed. Firstly, the original fault information of power information system is sampled, and the collected fault characteristic data are reconstructed by multi feature and information fitting. Then, the attribute distribution detection and big data mining are carried out for the fault dynamic characteristics of power information system. According to the high-order spectrum feature distribution of the extracted power information system fault signals, the dynamic fault diagnosis and fuzzy clustering analysis are carried out for the power information system, and the fault diagnosis is optimised according to the classification results. The simulation results show that the dynamic fault diagnosis accuracy of power information system is high, the fault sample detection results are accurate and reliable, and the dynamic fault detection ability is improved. Copyright © 2022 Inderscience Enterprises Ltd.","Big data; Detection; Dynamic diagnosis; Electricity message system; Fault","Big data; Classification (of information); Computer aided diagnosis; Data mining; Fault detection; Information systems; Information use; Spectrum analysis; Detection; Dynamic diagnosis; Dynamic faults; Electricity message system; Fault; Fault diagnosis method; Faults diagnosis; Message systems; Power; Power information systems; Failure analysis",Article,Scopus,2-s2.0-85120774277
"Karim A., Azhari A., Shahroz M., Belhaouri S.B., Mustofa K.","57220672494;57201433962;57216637396;35109905700;14827384600;","LDSVM: Leukemia cancer classification using machine learning",2022,"Computers, Materials and Continua","71","2",,"3887","3903",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120771683&doi=10.32604%2fcmc.2022.021218&partnerID=40&md5=588b95ea3f9cce247c3e1f96bc250692","Leukemia is blood cancer, including bone marrow and lymphatic tissues, typically involving white blood cells. Leukemia produces an abnormal amount of white blood cells compared to normal blood. Deoxyribonucleic acid (DNA) microarrays provide reliable medical diagnostic services to help more patients find the proposed treatment for infections. DNA microarrays are also known as biochips that consist of microscopic DNA spots attached to a solid glass surface. Currently, it is difficult to classify cancers using microarray data. Nearly many data mining techniques have failed because of the small sample size, which has become more critical for organizations. However, they are not highly effective in improving results and are frequently employed by doctors for cancer diagnosis. This study proposes a novel method using machine learning algorithms based on microarrays of leukemia GSE9476 cells. The main aim was to predict the initial leukemia disease. Machine learning algorithms such as decision tree (DT), naive bayes (NB), random forest (RF), gradient boosting machine (GBM), linear regression (LinR), support vector machine (SVM), and novel approach based on the combination of Logistic Regression (LR), DT and SVM named as ensemble LDSVM model. The k-fold cross-validation and grid search optimization methods were used with the LDSVM model to classify leukemia in patients and comparatively analyze their impacts. The proposed approach evaluated better accuracy, precision, recall, and f1 scores than the other algorithms. Furthermore, the results were relatively assessed, which showed LDSVM performance. This study aims to successfully predict leukemia in patients and enhance prediction accuracy in minimum time. Moreover, a Synthetic minority oversampling technique (SMOTE) and Principal compenent analysis (PCA) approaches were implemented. This makes the records generalized and evaluates the outcomeswell.PCAreduces the feature count without losing any information and deals with class imbalanced datasets, as well as faster model execution along with less computation cost. In this study, a novel process was used to reduce the column results to develop a faster and more rapid experiment execution. © 2022 Tech Science Press. All rights reserved.","Cancer; Classification; Ensemble LDSVM classifier; Genes; GSE9476; Leukemia; Machine learning","Adaptive boosting; Biochips; Blood; Data mining; Decision trees; Diagnosis; DNA; Forecasting; Logistic regression; Patient treatment; Support vector machines; Blood cancer; Bone marrow; Cancer classification; Deoxyribonucleic acid microarray; Diagnostic services; Ensemble LDSVM classifier; Gse9476; Machine learning algorithms; Medical diagnostics; White blood cells; Diseases",Article,Scopus,2-s2.0-85120771683
"Barbon Junior S., Pinto A., Barroso J.V., Caetano F.G., Moura F.A., Cunha S.A., Torres R.S.","57364455000;55557663800;57363943700;56387021100;16417087000;16416879600;56385629000;","Sport action mining: Dribbling recognition in soccer",2022,"Multimedia Tools and Applications","81","3",,"4341","4364",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120719548&doi=10.1007%2fs11042-021-11784-1&partnerID=40&md5=86e0f6dd8962fe28a0948cd96b554b2e","Recent advances in Computer Vision and Machine Learning empowered the use of image and positional data in several high-level analyses in Sports Science, such as player action classification, recognition of complex human movements, and tactical analysis of team sports. In the context of sports action analysis, the use of positional data allows new developments and opportunities by taking into account players’ positions over time. Exploiting the positional data and its sequence in a systematic way, we proposed a framework that bridges association rule mining and action recognition. The proposed Sports Action Mining (SAM) framework is grounded on the usage of positional data for recognising actions, e.g., dribbling. We hypothesise that different sports actions could be modelled using a sequence of confidence levels computed from previous players’ locations. The proposed method takes advantage of an association rule mining algorithm (e.g., FPGrowth) to generate displacement sequences for modelling actions in soccer. In this context, transactions are sequences of traces representing player displacements, while itemsets are players’ coordinates on the pitch. The experimental results pointed out the Random Forest classifier achieved a balanced accuracy value of 93.3% for detecting dribbling actions, which are considered complex events in soccer. Additionally, the proposed framework provides insights on players’ skills and player’s roles based on a small amount of positional data. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Association rules; Dribbling action detection; Machine learning; Soccer analysis","Association rules; Data mining; Decision trees; Machine learning; Action classifications; Dribbling action detection; High-level analysis; Human movements; Machine-learning; Player action; Soccer analysis; Sport science; Tacticals; Vision learning; Sports",Article,Scopus,2-s2.0-85120719548
"Papageorgiou L., Alkenaris H., Zervou M.I., Vlachakis D., Matalliotakis I., Spandidos D.A., Bertsias G., Goulielmos G.N., Eliopoulos E.","56421847900;57362755100;35486512400;23096470700;7005784456;57353877000;6602377310;6701555078;6701692141;","Epione application: An integrated web‑toolkit of clinical genomics and personalized medicine in systemic lupus erythematosus",2022,"International Journal of Molecular Medicine","49","1","8","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120655950&doi=10.3892%2fijmm.2021.5063&partnerID=40&md5=424de82fab4258e4b06727f256e1d2bd","Genome wide association studies (GWAS) have identified autoimmune disease‑associated loci, a number of which are involved in numerous disease‑associated pathways. However, much of the underlying genetic and pathophysi‑ ological mechanisms remain to be elucidated. Systemic lupus erythematosus (SLE) is a chronic, highly heterogeneous auto‑ immune disease, characterized by differences in autoantibody profile, serum cytokines and a multi‑system involvement. This study presents the Epione application, an integrated bioinformatics web‑toolkit, designed to assist medical experts and researchers in more accurately diagnosing SLE. The application aims to identify the most credible gene variants and single nucleotide polymorphisms (SNPs) associated with SLE susceptibility, by using patient's genomic data to aid the medical expert in SLE diagnosis. The application contains useful knowledge of >70,000 SLE‑related publications that have been analyzed, using data mining and semantic tech‑ niques, towards extracting the SLE‑related genes and the corresponding SNPs. Probable genes associated with the patient's genomic profile are visualized with several graphs, including chromosome ideograms, statistic bars and regulatory networks through data mining studies with relative publica‑ tions, to obtain a representative number of the most credible candidate genes and biological pathways associated with the SLE. Furthermore, an evaluation study was performed on a patient diagnosed with SLE and is presented herein. Epione has also been expanded in family‑related candidate patients to evaluate its predictive power. All the recognized gene variants that were previously considered to be associated with SLE were accurately identified in the output profile of the patient, and by comparing the results, novel findings have emerged. The Epione application may assist and facilitate in early stage diagnosis by using the patients' genomic profile to compare against the list of the most predictable candidate gene variants related to SLE. Its diagnosis‑oriented output presents the user with a structured set of results on variant association, posi‑ tion in genome and links to specific bibliography and gene network associations. The overall aim of the present study was to provide a reliable tool for the most effective study of SLE. This novel and accessible webserver tool of SLE is available at http://geneticslab.aua.gr/epione/. © 2022 Spandidos Publications. All rights reserved.","Bioinformatics; Clinical informatics; Data mining; Genomics; Systemic lupus erythematosus; Variant analysis; Whole exome sequencing; Whole genome sequencing",,Article,Scopus,2-s2.0-85120655950
"Lykkeboe S., Andersen S.L., Nielsen C.G., Vestergaard P., Christensen P.A.","55388295500;55643504800;57199326121;7102207630;11339511200;","Blood sampling frequency as a proxy for comorbidity indices when identifying patient samples for review of reference intervals",2022,"Clinical Chemistry and Laboratory Medicine","60","2",,"252","260",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120653518&doi=10.1515%2fcclm-2021-0987&partnerID=40&md5=2dbeb5403046f605ef3d18b3293dc239","Objectives: Indirect data mining methods have been proposed for review of published reference intervals (RIs), but methods for identifying patients with a low likelihood of disease are needed. Many indirect methods extract test results on patients with a low frequency blood sampling history to identify putative healthy individuals. Although it is implied there has been no attempt to validate if patients with a low frequency blood sampling history are healthy and if test results from these patients are suitable for RI review. Methods: Danish nationwide health registers were linked with a blood sample database, recording a population of 316,337 adults over a ten-year period. Comorbidity indexes were defined from registrations of hospital diagnoses and redeemed prescriptions of drugs. Test results from patients identified as having a low disease burden were used for review of RIs from the Nordic Reference Interval Project (NORIP). Results: Blood sampling frequency correlated with comorbidity Indexes and the proportion of patients without disease conditions were enriched among patients with a low number of blood samples. RIs based on test results from patients with only 1-3 blood samples per decade were for many analytes identical compared to NORIP RIs. Some analytes showed expected incongruences and gave conclusive insights into how well RIs from a more than 10 years old multi-center study (NORIP) performed on current pre-analytical and analytical methods. Conclusions: Blood sampling frequency enhance the selection of healthy individuals for reviewing reference intervals, providing a simple method solely based on laboratory data without the addition of clinical information. © 2021 Walter de Gruyter GmbH, Berlin/Boston.","data mining; indirect reference interval; Nordic reference interval project (NORIP)",,Article,Scopus,2-s2.0-85120653518
"Li X., Zhang F., Sun Z., Li D., Kong X., Zhang Y.","57363393700;57363100600;57363683500;57224319069;57213736410;55386585500;","Automatic heartbeat classification using S-shaped reconstruction and a squeeze-and-excitation residual network",2022,"Computers in Biology and Medicine","140",,"105108","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120644734&doi=10.1016%2fj.compbiomed.2021.105108&partnerID=40&md5=e5fd3794593cc930a959bc6b8a87c159","To facilitate the identification of arrhythmia, in this study, an S-shaped reconstruction method was proposed, and a two-dimensional (2-D) 19-layer deep squeeze-and-excitation residual network (SE-ResNet) was used to classify heartbeats. The proposed method has three steps. The first step involves data preprocessing, which includes denoising of the original electrocardiogram (ECG) data, removing of baseline drift, heartbeat extraction, and data balancing using a synthetic minority oversampling technique algorithm. Subsequently, the extracted one-dimensional heartbeat series is transformed into a 2-D matrix by employing the novel S-shaped reconstruction method for determining the relationship between distant points in an ECG series. Finally, the 2-D 19-layer SE-ResNet is used to divide the 2-D heartbeat matrix into five heartbeat categories, namely normal, supraventricular ectopic, ventricular ectopic, fusion, and unknown beats, in accordance with the American National Standards Institute/Advancement of Medical Instrumentation standard, and 10-fold cross-validation is employed to train the 2-D 19-layer SE-ResNet. The accuracy, positive prediction rate, sensitivity, and specificity of the proposed method reached 99.61%, 93.87%, 93.78%, and 99.27%, respectively. The results indicated that the S-shaped reconstruction method can be helpful for acquiring additional information from ECG heartbeat data. © 2021 Elsevier Ltd","Arrhythmia heartbeat; S- shaped reconstruction; SE-ResNet","Data mining; Diseases; Matrix algebra; Arrhythmia heartbeat; Baseline drift; Data preprocessing; De-noising; Heartbeat classifications; Reconstruction method; S- shaped reconstruction; S-shaped; Squeeze-and-excitation residual network; Two Dimensional (2 D); Electrocardiography",Article,Scopus,2-s2.0-85120644734
"Sieranoja S., Fränti P.","57192091727;35562071600;","Adapting k-means for graph clustering",2022,"Knowledge and Information Systems","64","1",,"115","142",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120642689&doi=10.1007%2fs10115-021-01623-y&partnerID=40&md5=dad878dcab851f958fb9479de3fe2dd5","We propose two new algorithms for clustering graphs and networks. The first, called K‑algorithm, is derived directly from the k-means algorithm. It applies similar iterative local optimization but without the need to calculate the means. It inherits the properties of k-means clustering in terms of both good local optimization capability and the tendency to get stuck at a local optimum. The second algorithm, called the M-algorithm, gradually improves on the results of the K-algorithm to find new and potentially better local optima. It repeatedly merges and splits random clusters and tunes the results with the K-algorithm. Both algorithms are general in the sense that they can be used with different cost functions. We consider the conductance cost function and also introduce two new cost functions, called inverse internal weight and mean internal weight. According to our experiments, the M-algorithm outperforms eight other state-of-the-art methods. We also perform a case study by analyzing clustering results of a disease co-occurrence network, which demonstrate the usefulness of the algorithms in an important real-life application. © 2021, The Author(s).","Cluster analysis; Community detection; Graph clustering; Graph mining; k-means","Cluster analysis; Data mining; Inverse problems; Iterative methods; K-means clustering; Community detection; Cost-function; Graph clustering; Graph mining; Internal weights; K algorithm; K-means; Local optima; Local optimizations; M-algorithms; Cost functions",Article,Scopus,2-s2.0-85120642689
"Zhao J., Juntao C., Huilin X., Zhao Z., Xinguo Z.","56337915700;35320814000;57362603700;57363338100;56830548700;","Dynamic Mechanical Response and Movement Evolution Characteristics of Fault Systems in the Coal Mining Process",2022,"Pure and Applied Geophysics","179","1",,"233","246",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120627443&doi=10.1007%2fs00024-021-02905-w&partnerID=40&md5=125cd13da73b9d1841aa04d30a2ffb0b","In the mining process of a coal seam containing a fault structure, due to the influence of mining stress, the rock mass inside the fault zone and between the fault and the coal pillar that is preserved to prevent fault activation is prone to producing damage and activation. This study is focused on this problem. Based on the analysis of the failure, activation and movement of the rock mass in front of the fault body and the coal wall, extended finite element numerical simulation software is adopted to study the stress evolution and deformation failure rules of a coal seam containing a fault floor. In particular, the dynamic response process (including slip velocity, slip distance and friction force) of the fault structure under mining conditions is monitored, and then the turning and failure process of the fault structure is analysed. However, an analogue modelling is reformed. A similar material analysis model is established to monitor the displacement field changes near the fault during the mining process, obtain the movement rules of the fault zone and surrounding rock mass during the mining process, and then verify the reliability of the numerical simulation results. The research results can provide references for exploring the coupling movement rules between the fault and the work area for production in the mining process of coal seam stopes, as well as prevention before water inrush under the influence of the fault and treatment after water inrush. © 2021, The Author(s), under exclusive licence to Springer Nature Switzerland AG.","dynamic response; fault surface friction; Mining-induced fault; nonlinear slip; rotatory motion","Activation analysis; Chemical activation; Coal; Coal deposits; Coal mines; Computer software; Data mining; Dynamic response; Friction; Mining; Numerical models; Reliability analysis; Rocks; Coal seams; Fault structure; Fault surface friction; Fault surfaces; Mining process; Mining-induced fault; Nonlinear slip; Rock-mass; Rotatory motion; Surface friction; Rock mechanics",Article,Scopus,2-s2.0-85120627443
"Schouten R.M., Bueno M.L.P., Duivesteijn W., Pechenizkiy M.","57363308600;57213506752;25644448300;55900254600;","Mining sequences with exceptional transition behaviour of varying order using quality measures based on information-theoretic scoring functions",2022,"Data Mining and Knowledge Discovery","36","1",,"379","413",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120614947&doi=10.1007%2fs10618-021-00808-x&partnerID=40&md5=cabce92f17c1910e3cf4cf9ff991f3d7","Discrete Markov chains are frequently used to analyse transition behaviour in sequential data. Here, the transition probabilities can be estimated using varying order Markov chains, where order k specifies the length of the sequence history that is used to model these probabilities. Generally, such a model is fitted to the entire dataset, but in practice it is likely that some heterogeneity in the data exists and that some sequences would be better modelled with alternative parameter values, or with a Markov chain of a different order. We use the framework of Exceptional Model Mining (EMM) to discover these exceptionally behaving sequences. In particular, we propose an EMM model class that allows for discovering subgroups with transition behaviour of varying order. To that end, we propose three new quality measures based on information-theoretic scoring functions. Our findings from controlled experiments show that all three quality measures find exceptional transition behaviour of varying order and are reasonably sensitive. The quality measure based on Akaike’s Information Criterion is most robust for the number of observations. We furthermore add to existing work by seeking for subgroups of sequences, as opposite to subgroups of transitions. Since we use sequence-level descriptive attributes, we form subgroups of entire sequences, which is practically relevant in situations where you want to identify the originators of exceptional sequences, such as patients. We show this relevance by analysing sequences of blood glucose values of adult persons with diabetes type 2. In the experiments, we find subgroups of patients based on age and glycated haemoglobin (HbA1c), a measure known to correlate with average blood glucose values. Clinicians and domain experts confirmed the transition behaviour as estimated by the fitted Markov chain models. © 2021, The Author(s).","Exceptional Model Mining; Information-theoretic scoring functions; Markov chains; Sequential medical data","Blood; Glucose; Information theory; Markov processes; Medical computing; Blood glucose; Exceptional model minings; Information-theoretic scoring function; Medical data; Mining sequences; Quality measures; Scoring functions; Sequential data; Sequential medical data; Transition behavior; Data mining",Article,Scopus,2-s2.0-85120614947
"Upadhyay N.M., Singh R.S., Dwivedi S.P.","57210951392;56125721800;56152164700;","Prediction of multicore CPU performance through parallel data mining on public datasets",2022,"Displays","71",,"102112","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120612426&doi=10.1016%2fj.displa.2021.102112&partnerID=40&md5=a9227f9898ec680c0aca6d4f2026c16a","In the present scenario, high-performance computing needs more attention towards multicore computing. While designing the CPU, we need to consider hardware for processing speed, cache bandwidth, minimum memory requirements, etc. So for the selection of the best combination of CPU data mining tools may play an important role. In this era, data mining attracts more interest on parallel computing to enhance the performance of multicores. This paper demonstrates a parallel strategy similar to the traditional parallel programming paradigm to improve the performance of multicores by using a data mining approach. We have selected one approach EM with Gaussian and analyze the impact of its parallel execution on selected multicore clusters obtained through data mining. We have also evaluated our finding with a virtual environment of having 32 different families of CPUs, showing a speedup of up to ≈1.02x. This paper considers data clusters, cache mapping techniques and ranking of MPI programming techniques. © 2021 Elsevier B.V.","Cache mapping techniques; Data clustering techniques; Data mining; MPI programming; Multi-cores; Parallel computing","Cluster analysis; Clustering algorithms; Mapping; Multicore programming; Parallel programming; Program processors; Virtual reality; Cache mapping technique; Clustering techniques; Data clustering; Data clustering technique; Mapping techniques; MPI programming; Multi-cores; Parallel com- puting; Parallel data mining; Performance; Data mining",Article,Scopus,2-s2.0-85120612426
"Kebede S.D., Tiwari B., Tiwari V., Chandravanshi K.","57362005100;36934359400;56496391800;57194184644;","Predictive machine learning-based integrated approach for DDoS detection and prevention",2022,"Multimedia Tools and Applications","81","3",,"4185","4211",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120578130&doi=10.1007%2fs11042-021-11740-z&partnerID=40&md5=d3df2f4afa3062227bc6a29977f40c3e","Distributed Denial of Service attack has been a huge threat to the Internet and may carry extreme losses to systems, companies, and national security. The invader can disseminate Distributed denial of service (DDoS) attacks easily, and it ends up being significantly harder to recognize and forestall DDoS attacks. In recent years, many IT-based companies are attacked by DDoS attacks. In this view, the primary concern of this work is to detect and prevent DDoS attacks. To fulfill the objective, various data mining techniques such that Jrip, J48, and k-NN have been employed for DDoS attacks detection. These algorithms are implemented and thoroughly evaluated individually to validate their performance in this domain. The presented work has been evaluated using the latest dataset CICIDS2017. The dataset characterizes different DDoS attacks viz. brute force SSH, brute force FTP, Heartbleed, infiltration, botnet TCP, UDP, and HTTP with port scan attack. Further, the prevention method takes place in progress to block the malicious nodes participates in any of the said attacks. The proposed DDoS prevention works in a proactive mode to defend all these attack types and gets evaluated concerning various parameters such as Throughput, PDR, End-to-End Delay, and NRL. This study claimed that the proposed technique outperforms with respect to the AODV routing algorithm. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","CICIDS2017; Classification algorithm; DDoS Attack; DDoS Detection; DDoS Prevention; Machine learning","Data mining; HTTP; Machine learning; National security; Nearest neighbor search; Network security; Brute force; CICIDS2017; Classification algorithm; Denialof- service attacks; Distributed denial of service; Distributed denial of service attack; Distributed denial of service detection; Distributed denial of service prevention; Denial-of-service attack",Article,Scopus,2-s2.0-85120578130
"Huan R., Jiang C., Ge L., Shu J., Zhan Z., Chen P., Chi K., Liang R.","23992354500;57361729400;57204392920;57219669693;57240656400;57362463300;23476774100;55622054000;","Human Complex Activity Recognition with Sensor Data Using Multiple Features",2022,"IEEE Sensors Journal","22","1",,"757","775",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120560665&doi=10.1109%2fJSEN.2021.3130913&partnerID=40&md5=580b5d037cfc55d5cdfd499a771b4642","In order to achieve better results of human complex activity recognition, the optimal feature representation of human complex activities from sensor data are studied. Features from multiple sets that are obtained by the learned networks, the manual extraction, and the location information are fused to generate mixed features to achieve better results of human complex activity recognition. A method extracting multi-layer features from the hybrid CNN and BLSTM network is proposed. The output feature vector of the second layer of the CNN and that of the BLSTM are combined to generate multi-layer features. Meanwhile, a new feature selection method based on SFS and network weight analysis is proposed. The method is to select a series of features extracted from the segmented sensor data using manual methods as dominant features. The location information of complex activities after one-hot encoding is also used as feature, which is then fused with multi-layer features and manual dominant features to generate mixed features. To further improve the recognition performance, the sensor data collected at different body positions are separated and used independently to train the hybrid CNN and BLSTM network to obtain their respective state information. The PAMAP2 and UT-Data datasets are used in our experiments to verify the proposed method. The results show that the method based on multiple features proposed in this paper dramatically outperforms the existing state-of-the-art methods for human complex activity recognition using sensor data. © 2001-2012 IEEE.","Deep learning; Feature extraction; Human complex activity recognition; Location; Sensor data","Complex networks; Data mining; Deep learning; Extraction; Location; Activity recognition; Complex activity; Deep learning; Features extraction; Human complex activity recognition; Manual; Sensor phenomenon and characterizations; Sensors data; Feature extraction",Article,Scopus,2-s2.0-85120560665
"Jin L., Wang X., Chu J., He M.","56437132800;18038686800;57361834700;57200641625;","Human Activity Recognition Machine with an Anchor-Based Loss Function",2022,"IEEE Sensors Journal","22","1",,"741","756",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120551127&doi=10.1109%2fJSEN.2021.3130761&partnerID=40&md5=9f0ca3bbb2115aa2497da980c83ebcd6","More recently, Human Activity Recognition (HAR) based on sensors has become a hot topic due to its wide application. Researchers significantly reduce the cost of feature extraction and improve the accuracy of recognition by introducing deep learning networks. However, human activity data are greatly affected by the inter-personal variability which brings the interclass similarity and the intraclass diversity. They not only increase the difficulty of the closed-set classification, but also affect the performance on the open-set problem. To solve the problem, we design a framework using a loss function of Euclidean distance and a high-dimensional embedding layer to enhance the ability of deep learning networks to mine discriminative features. Furthermore, we define two kinds of open-set problems in HAR: the pseudo open-set problem and the completely open-set problem. We propose a new clustering method based on the Euclidean distance for the pseudo open-set problem, which reduces the computation cost and improves the accuracy. For the completely open-set problem ignored by other researches, we introduce the rejection score to evaluate the distance score between samples and all known classes, and realize the completely open-set classification. We conduct experiments using four common deep learning networks on three public datasets: OPPORTUNITY, PAMAP2 and UniMiB-SHAR. The results show that the performances of the model modified by our method are much better than those of the original model. © 2001-2012 IEEE.","Deep learning; Human activity recognition; Metric learning; Open-set classification","Classification (of information); Cluster analysis; Cost reduction; Data mining; Deep neural networks; Extraction; Face recognition; Problem solving; Convolutional neural network; Deep learning; Features extraction; Human activity recognition; Learning network; Loss functions; Metric learning; Open-set classification; Set problems; Feature extraction",Article,Scopus,2-s2.0-85120551127
"Druskin V., Mamonov A.V., Zaslavsky M.","6603892431;25958215400;6602293349;","Distance Preserving Model Order Reduction of Graph-Laplacians and Cluster Analysis",2022,"Journal of Scientific Computing","90","1","32","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120528118&doi=10.1007%2fs10915-021-01660-3&partnerID=40&md5=3d1e175f05c2cc8bef96712821225f33","Graph-Laplacians and their spectral embeddings play an important role in multiple areas of machine learning. This paper is focused on graph-Laplacian dimension reduction for the spectral clustering of data as a primary application, however, it can also be applied in data mining, data manifold learning, etc. Spectral embedding provides a low-dimensional parametrization of the data manifold which makes the subsequent task (e.g., clustering with k-means or any of its approximations) much easier. However, despite reducing the dimensionality of data, the overall computational cost may still be prohibitive for large data sets due to two factors. First, computing the partial eigendecomposition of the graph-Laplacian typically requires a large Krylov subspace. Second, after the spectral embedding is complete, one still has to operate with the same number of data points, which may ruin the efficiency of the approach. For example, clustering of the embedded data is typically performed with various relaxations of k-means which computational cost scales poorly with respect to the size of data set. Also, they become prone to getting stuck in local minima, so their robustness depends on the choice of initial guess. In this work, we switch the focus from the entire data set to a subset of graph vertices (target subset). We develop two novel algorithms for such low-dimensional representation of the original graph that preserves important global distances between the nodes of the target subset. In particular, it allows to ensure that target subset clustering is consistent with the spectral clustering of the full data set if one would perform such. That is achieved by a properly parametrized reduced-order model (ROM) of the graph-Laplacian that approximates accurately the diffusion transfer function of the original graph for inputs and outputs restricted to the target subset. Working with a small target subset reduces greatly the required dimension of Krylov subspace and allows to exploit the conventional algorithms (like approximations of k-means) in the regimes when they are most robust and efficient. This was verified in the numerical clustering experiments with both synthetic and real data. We also note that our ROM approach can be applied in a purely transfer-function-data-driven way, so it becomes the only feasible option for extremely large graphs that are not directly accessible. There are several uses for our algorithms. First, they can be employed on their own for representative subset clustering in cases when handling the full graph is either infeasible or simply not required. Second, they may be used for quality control. Third, as they drastically reduce the problem size, they enable the application of more sophisticated algorithms for the task under consideration (like more powerful approximations of k-means based on semi-definite programming (SDP) instead of the conventional Lloyd’s algorithm). Finally, they can be used as building blocks of a multi-level divide-and-conquer type algorithm to handle the full graph. The latter will be reported in a separate article. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",,"Approximation algorithms; Cluster analysis; Cost reduction; Data mining; Embeddings; Graph theory; K-means clustering; Laplace transforms; Large dataset; Reduction; Set theory; Clusterings; Computational costs; Data manifolds; Data set; Graph Laplacian; Graph laplacians; K-means; Krylov subspace; Spectral clustering; Spectral embedding; Transfer functions",Article,Scopus,2-s2.0-85120528118
"Zhang Z., Cui F., Cao C., Wang Q., Zou Q.","57222409030;57219935350;57360066000;57360497700;23391564900;","Single-cell RNA analysis reveals the potential risk of organ-specific cell types vulnerable to SARS-CoV-2 infections",2022,"Computers in Biology and Medicine","140",,"105092","","",,4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120495786&doi=10.1016%2fj.compbiomed.2021.105092&partnerID=40&md5=2ac3e2525e0b7bb185c4de2f1f0b502a","Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has caused a global pandemic of coronavirus disease 2019 (COVID-19) since December 2019 that has led to more than 160 million confirmed cases, including 3.3 million deaths. To understand the mechanism by which SARS-CoV-2 invades human cells and reveal organ-specific susceptible cell types for COVID-19, we conducted comprehensive bioinformatic analysis using public single-cell RNA sequencing datasets. Utilizing the expression information of six confirmed COVID-19 receptors (ACE2, TMPRSS2, NRP1, AXL, FURIN and CTSL), we demonstrated that macrophages are the most likely cells that may be associated with SARS-CoV-2 pathogenesis in lung. Besides the widely reported ‘chemokine storm’, we identified ribosome related pathways that may also be potential therapeutic target for COVID-19 lung infection patients. Moreover, cell-cell communication analysis and trajectory analysis revealed that M1-like macrophages showed the highest relation to severe COVID-19 patients. And we also demonstrated that up-regulation of chemokine pathways generally lead to severe symptoms, while down-regulation of ribosome and RNA activity related pathways are more likely to be mild. Other organ-specific susceptible cell type analyses could also provide potential targets for COVID-19 therapy. This work can provide clues for understanding the pathogenesis of COVID-19 and contribute to understanding the mechanism by which SARS-CoV-2 invades human cells. © 2021 Elsevier Ltd","Bioinformatics; COVID-19; Data mining; Macrophage; Single-cell RNA sequencing","Bioinformatics; Biological organs; Cytology; Data mining; Macrophages; Risk assessment; RNA; Cell types; Chemokines; Coronavirus disease 2019; Coronaviruses; Human cells; Potential risks; RNA analysis; Severe acute respiratory syndrome coronavirus; Single cells; Single-cell RNA sequencing; Coronavirus",Article,Scopus,2-s2.0-85120495786
"Zhao X., Liu Y., Xu Y., Yang Y., Luo X., Miao C.","56609503100;55954393600;55537371300;57209223903;57155505700;57225204880;","Heterogeneous star graph attention network for product attributes prediction",2022,"Advanced Engineering Informatics","51",,"101447","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120477379&doi=10.1016%2fj.aei.2021.101447&partnerID=40&md5=9c855e51a45bd54c5e2dd236d421d746","The target of product attributes prediction is to complete the characteristics set for defining a particular product. Most of the existing methods treat the product attributes prediction as a Named-Entity Recognition (NER) problem from the products’ affiliated data, such as product title and introduction. However, in a large number of industrial applications of Alibaba, we found that the existing methods are good at concrete attributes extraction (e.g., color, size) but short of abstract attributes extraction (e.g., applicable event). Moreover, these abstract attributes are usually not easy to extract from the products’ affiliated data. In this paper, we propose a novel heterogeneous star graph attention network called “SAN”, which incorporates the advantages of multiple information in the e-commerce scene to predict the abstract attributes of products. Specifically, we model the customer interactive behaviors, product title and concrete attributes of a product as a star graph. Then, we extract the node features, node types and graph structure information from the heterogeneous star graph network which consists of n star graphs. By leveraging the parallel multiple attention mechanism, SAN can aggregate features and learn weights of nodes for product representation and abstract attribute prediction. Extensive experimental results of a real-world e-commerce dataset have demonstrated that SAN outperforms state-of-the-art methods significantly for product attributes prediction. These series of solutions are already planned for use in the applications of Alibaba. © 2021 Elsevier Ltd","E-commerce; Heterogeneous graph attention network; Parallel attention; Product attributes prediction","Automata theory; Concretes; Data mining; Electronic commerce; Extraction; Attributes extractions; Characteristic set; E- commerces; Heterogeneous graph; Heterogeneous graph attention network; Named entity recognition; Parallel attention; Product attribute prediction; Product attributes; Star graphs; Forecasting",Article,Scopus,2-s2.0-85120477379
"Poon K.H., Wong P.K.-Y., Cheng J.C.P.","57360515700;57210803584;57204665849;","Long-time gap crowd prediction using time series deep learning models with two-dimensional single attribute inputs",2022,"Advanced Engineering Informatics","51",,"101482","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120468860&doi=10.1016%2fj.aei.2021.101482&partnerID=40&md5=217adad6affd45dfdc04d0c84c4336e3","Crowd prediction is a crucial aspect of modern life with innumerable applications. By predicting future human occupancy in advance, crowd prediction can support the decision-making processes of facility stakeholders, e.g., the campus operator can schedule facility maintenance during the period of lowest pedestrian flow to eliminate any disturbance. Conventional crowd prediction utilizes statistical models and rule-based data mining techniques, which are tedious in data processing and error-prone. Hence, this study formulates crowd prediction into a time-series analysis based on deep learning. Despite its wide adaptability in various research fields, deep learning-based time series analysis is seldom adopted in crowd prediction. There are two major limitations in previous studies: firstly, the prediction accuracy notably degrades with increased prediction length, and secondly only the temporal pattern along a single time dimension is exploited, i.e., the consecutive time steps in the most recent input data. Therefore, a Long-Time Gap Two-Dimensional method, entitled LT2D-method, is proposed to increase the crowd prediction length of with high accuracy. The LT2D-method is composed of two parts, (1) long-time gap prediction, which extends the prediction length to 240 time steps (1 day) with high accuracy, and (2) 2D inputs method, which exploits the prior knowledge from different time dimensions to further improve the prediction accuracy of long-time gap prediction. The proposed LT2D-method can be generally adapted to deep learning models, such as LSTM, BiLSTM, and GRU, to improve the prediction accuracy. By incorporating the proposed LT2D-method into different baseline models, the accuracy is generally improved by around 22%, demonstrating the robustness and generalizability of our method. © 2021 Elsevier Ltd","Crowd prediction; Deep learning; Long-time gap prediction; Multivariate inputs; Time series analysis","Data handling; Data mining; Decision making; Harmonic analysis; Long short-term memory; Time series analysis; Crowd prediction; Deep learning; Learning models; Long-time gap prediction; Multivariate input; Prediction accuracy; Time dimension; Time gap; Time-series analysis; Two-dimensional; Forecasting",Article,Scopus,2-s2.0-85120468860
"Zhou H., Yip W.S., Ren J., To S.","57210996333;56712432200;35220394000;23486913200;","Thematic analysis of sustainable ultra-precision machining by using text mining and unsupervised learning method",2022,"Journal of Manufacturing Systems","62",,,"218","233",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120438364&doi=10.1016%2fj.jmsy.2021.11.013&partnerID=40&md5=4d3b24e931dba266e38ba5e49b3fb768","Sustainable manufacturing is one key research area to reduce environmental damages and resource waste nowadays. As a cutting-edge manufacturing method, ultra-precision machining (UPM) plays an increasingly significant role to achieve sustainable manufacturing because of its rapidly increasing demand. The purpose of this paper is to discover and evaluate the main themes of current works about sustainable UPM. By utilizing the latent Dirichlet allocation (LDA) method to analyze the abstracts of the relevant publications, four main themes of sustainable UPM were identified. The percentage of each documents’ content contributing to these four themes was also extracted. According to the documents’ contribution data, the publications can be classified into four groups by using the K-means algorithm. It shows that the machining process is the most focused theme in this field and the majority of works about surface structure involved multiple topics. And the social aspect of sustainable UPM needs extensive investigation in the future. In this paper, the thematic analysis was conducted for the first time in the area of sustainable UPM. And the LDA-unpreserved learning approach was also proposed in this work originally. This work provides an overall map of sustainable UPM literature to help researchers select the topics which have not been discussed. © 2021 The Society of Manufacturing Engineers","Latent Dirichlet allocation; Sustainable development; Text mining; Thematic analysis; Ultra-precision machining; Unsupervised learning","Data mining; Industrial research; K-means clustering; Machining; Manufacture; Precision engineering; Statistics; Sustainable development; Environmental damage; Environmental resources; Latent Dirichlet allocation; Manufacturing IS; Research areas; Sustainable manufacturing; Text-mining; Thematic analysis; Ultraprecision machining; Unsupervised learning method; Unsupervised learning",Article,Scopus,2-s2.0-85120438364
"Xiong L., Ning J., Dong Y.","57224968440;57224975017;57224980865;","Pollution reduction effect of the digital transformation of heavy metal enterprises under the agglomeration effect",2022,"Journal of Cleaner Production","330",,"129864","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120433816&doi=10.1016%2fj.jclepro.2021.129864&partnerID=40&md5=c172a1a094c1309897d263c937beaab3","There are few studies on the reduction of heavy metal pollution from the perspectives of digital transformation and the agglomeration effect of heavy metal enterprises. In this study, 430 heavy metal enterprises in the middle reaches of the Yangtze River were selected as research samples based on the data learning method. The content analysis method based on data mining was used to measure the degree of digital transformation of heavy metal enterprises, and spatial dynamic analysis methods such as kernel density analysis and inverse distance weight were used to build an integrated framework of quantitative analysis based on linear regression and qualitative analysis based on visualization. First, the pollution emission behavior, industrial agglomeration degree and enterprise pollution reduction performance of heavy metal enterprises in the urban agglomeration of the middle reaches of the Yangtze River were visualized by GIS. Then, the systematic GMM method was used to analyze the pollution reduction performance of enterprises in digital transformation, as well as the interaction effect and threshold characteristics under the effect of enterprise agglomeration. The results were as follows. (1) The average concentrations of Cd, Pb and Cr in the middle reaches of the Yangtze River in 2020 were 1.111 μg/L, 1.145 μg/L and 1.163 μg/L, respectively, which were lower than the standards of heavy metal pollution. (2) The geographical coupling degrees of digital transformation and pollution reduction of heavy metal enterprises in the urban agglomeration of the middle reaches of the Yangtze River is high, and the coupling degree of high value areas of digital transformation degree and high value areas of emission reduction degree is as high as 84.3%. (3) Under the agglomeration effect, there is a U-shaped curve between the digital transformation of enterprises and pollution reduction performance. When the agglomeration degree of enterprises is above the threshold of 2.303, the digital transformation of enterprises has a significant positive impact on pollution reduction. © 2021 Elsevier Ltd","Digital transformation; Enterprises agglomeration; Heavy metal; Machine learning method; Pollution reduction effect","Data mining; E-learning; Inverse problems; Linear transformations; Machine learning; Metadata; River pollution; Rivers; Agglomeration effects; Digital transformation; Enterprise agglomeration; Heavy metals pollution; Machine learning methods; Performance; Pollution reduction; Pollution reduction effect; Reduction effects; Yangtze River; Heavy metals",Article,Scopus,2-s2.0-85120433816
"Lin L., Rao Y., Xie H., Lau R.Y.K., Yin J., Wang F.L., Li Q.","57217174366;55753260800;57219619828;12794272900;35316639800;7501312845;57199178903;","Copula Guided Parallel Gibbs Sampling for Nonparametric and Coherent Topic Discovery",2022,"IEEE Transactions on Knowledge and Data Engineering","34","1",,"219","235",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120373073&doi=10.1109%2fTKDE.2020.2976945&partnerID=40&md5=672dd6f8362bc590b85fb6177a4e1e90","Hierarchical Dirichlet Process (HDP) has attracted much attention in the research community of natural language processing. Given a corpus, HDP is able to determine the number of topics automatically, possessing an important feature dubbed nonparametric that overcomes the challenging issue of manually specifying a suitable topic number in parametric topic models, such as Latent Dirichlet Allocation (LDA). Nevertheless, HDP requires a much higher computational cost than LDA for parameter estimation. By taking the advantage of multi-threading, a parallel Gibbs sampling algorithm is proposed to estimate parameters for HDP based on the equivalence between HDP and Gamma-Gamma Poisson Process (G2PP) in terms of the generative process. Unfortunately, the above parallel Gibbs sampling algorithm requires to apply the finite approximation on the number of topics manually (i.e., predefine the topic number), thus can not retain the nonparametric feature of HDP. Another drawback of the above models is the lack of capturing the semantic dependencies between words, because the topic assignment of words is independent with each other. Although some works have been done in phrase-based topic modelling, these existing methods are still limited by either enforcing the entire phrase to share a common topic or requiring much complex and time-consuming phrase mining methods. In this paper, we aim to develop a copula guided parallel Gibbs sampling algorithm for HDP which can adjust the number of topics dynamically and capture the latent semantic dependencies between words that compose a coherent segment. Extensive experiments on real-world datasets indicate that our method achieves low perplexities and high topic coherence scores with a small time cost. In addition, we validate the effectiveness of our method on the modelling of word semantic dependencies by comparing the extracted topical phrases with those learned by state-of-the-art phrase-based baselines. © 1989-2012 IEEE.","Copulas; Parallel gibbs sampling; Topic modelling","Approximation algorithms; Data mining; Mining; Natural language processing systems; Parameter estimation; Semantics; Statistics; Copula; Gibbs sampling; Hierarchical Dirichlet process; Latent Dirichlet allocation; Nonparametrics; Parallel gibbs sampling; Sampling algorithm; Semantic dependency; Topic Discovery; Topic Modeling; Learning algorithms",Article,Scopus,2-s2.0-85120373073
"Pal Singh S., Adhikari A., Majumdar A., Bisi A.","57358479800;57190621872;56645709400;6701874335;","Does service quality influence operational and financial performance of third party logistics service providers? A mixed multi criteria decision making -text mining-based investigation",2022,"Transportation Research Part E: Logistics and Transportation Review","157",,"102558","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120371098&doi=10.1016%2fj.tre.2021.102558&partnerID=40&md5=abdcca80e7315f01f6ef64f7aafb096e","Since its inception, the third party logistics (3PL) industry has remained an area of interest for academicians and practitioners. The existing literature mostly focuses on single multi criteria decision making (MCDM) method-based holistic performance evaluations of 3PL service providers, whereas distinct operational and financial performance measurements have not received enough attention. Several real-life examples of organizations, such as Hub Group and DSV, indicate that the reliance on financial performance improvement solely does not ensure better operational performance and integrated performance, and vice versa. Additionally, there is an absence of works that focus on designing an integrated MCDM methodology that applies multiple MCDM methods to increase the robustness of the methodology and consider distinct operational, financial, and integrated performance measurements of the 3PL service providers. Additionally, the application of emerging ratio analysis-based MCDM methods such as multi objective optimization based on ratio analysis (MOORA) and complex proportional assessment (COPRA) for performance evaluation has been ignored. Furthermore, the assessment of the service quality of 3PL service providers through their customers’ feedback and the association of this service quality with the abovementioned performance measures have not received enough attention. This motivates us to design a criteria importance through intercriteria correlation (CRITIC) weighting-based integrated MOORA-COPRA MCDM methodology for the performance evaluation of 3PL service providers. We apply our proposed methodology to evaluate the performance of 21 leading 3PL service providers in North America. Additionally, we incorporate text mining methods such as sentiment analysis and topic modeling to analyze the effect of these service providers’ service quality captured through their customers’ reviews on distinct operational, financial, and integrated performance. The insights obtained from the study indicate that service quality (as captured from the consumer reviews) has a positive association with the operational and financial performance of 3PL service providers. © 2021 Elsevier Ltd","COPRA; MOORA; Performance evaluation; Service quality; Text mining; Third-party logistics","data mining; multicriteria analysis; performance assessment; service quality; third sector; North America",Article,Scopus,2-s2.0-85120371098
"Wan Z.","57358163000;","The application of big data in the legal improvement of agricultural product quality and safety governance",2022,"Acta Agriculturae Scandinavica Section B: Soil and Plant Science","72","1",,"200","213",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120341826&doi=10.1080%2f09064710.2021.2007994&partnerID=40&md5=244760350bc5353dbfe7da1f04adb616","The quality and safety of agricultural products is not only related to the health of consumers, but also to the sustainable and stable development of the economy, and even to the harmony and stability of the society. The application of the theory model of multiple co-governance of agricultural product quality and safety has certain theoretical support. This paper applies big data technology to agricultural product quality and safety governance, and uses big data methods to study the key control points in the process of agricultural product traceability. Moreover, based on the selected key control points, this paper studies the key traceability indicators corresponding to the key control points of each link. In addition, this paper combines multi-disciplinary knowledge to carry out a systematic study on the legal issues of multivariate co-governance of agricultural product quality and safety in my country from the perspective of law. From the experimental research and the final decision-making suggestions, we can see that the method proposed in this paper is feasible. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","agricultural product quality and safety; Big data; data mining; legal improvement",,Article,Scopus,2-s2.0-85120341826
"Nolde J.M., Lugo-Gavidia L.M., Carnagarin R., Azzam O., Kiuchi M.G., Mian A., Schlaich M.P.","57205561339;55575076200;57193401190;57196287296;57357572800;57358209300;7003349164;","K-means panning – Developing a new standard in automated MSNA signal recognition with a weakly supervised learning approach",2022,"Computers in Biology and Medicine","140",,"105087","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120316818&doi=10.1016%2fj.compbiomed.2021.105087&partnerID=40&md5=3d01569deca327dc45e58011a101beef","Background: Accessibility of labelled datasets is often a key limitation for the application of Machine Learning in clinical research. A novel semi-automated weak-labelling approach based on unsupervised clustering was developed to classify a large dataset of microneurography signals and subsequently used to train a Neural Network to reproduce the labelling process. Methods: Clusters of microneurography signals were created with k-means and then labelled in terms of the validity of the signals contained in each cluster. Only purely positive or negative clusters were labelled, whereas clusters with mixed content were passed on to the next iteration of the algorithm to undergo another cycle of unsupervised clustering and labelling of the clusters. After several iterations of this process, only pure labelled clusters remained which were used to train a Deep Neural Network. Results: Overall, 334,548 individual signal peaks form the integrated data were extracted and more than 99.99% of the data was labelled in six iterations of this novel application of weak labelling with the help of a domain expert. A Deep Neural Network trained based on this dataset achieved consistent accuracies above 95%. Discussion: Data extraction and the novel iterative approach of labelling unsupervised clusters enabled creation of a large, labelled dataset combining unsupervised learning and expert ratings of signal-peaks on cluster basis in a time effective manner. Further research is needed to validate the methodology and employ it on other types of physiologic data for which it may enable efficient generation of large labelled datasets. © 2021 Elsevier Ltd","Cardiovascular risk; Machine learning; Microneurography; Sympathetic nervous system; Unsupervised learning","Automation; Classification (of information); Clinical research; Data mining; Deep neural networks; Electrophysiology; Iterative methods; K-means clustering; Large dataset; Learning algorithms; Neurons; Cardiovascular risk; K-means; Labeled dataset; Labelings; Microneurography; Signal recognition; Supervised learning approaches; Sympathetic nervous systems; Unsupervised clustering; Weakly supervised learning; Unsupervised learning; adrenergic system; algorithm; article; cardiovascular risk; data extraction; deep neural network; human; muscle sympathetic nerve activity; unsupervised machine learning; validity",Article,Scopus,2-s2.0-85120316818
"Hua Z., Li X., Feng Y., Zhao L.","57217827921;7501707862;57215651087;7404455148;","Dual Branch Autoencoder Network for Spectral-Spatial Hyperspectral Unmixing",2022,"IEEE Geoscience and Remote Sensing Letters","19",,,"","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120315498&doi=10.1109%2fLGRS.2021.3091858&partnerID=40&md5=1c8a1191f5ea5b3261e6f4d533887ee2","Spatial information can play a supporting role in spectral unmixing. In this letter, we propose a dual branch autoencoder network to incorporate spatial-contextual information for spectral-spatial unmixing. The two branches leverage different architectures to efficiently extract spatial information and spectral information. In the first branch, we use fully connected layers to extract spectral information, where the neuron in each layer can capture all spectral features. In the second branch, 2-D convolution is adopted to exploit spatial features, which does not require hand-crafted assumptions compared with conventional methods. Then the extracted features are concatenated and propagated to generate the abundance and reconstruct the pixel. Moreover, to solve the drawbacks of the existing reconstruction functions, we propose a new function termed squared sine distance to improve the convergence quality of the proposed network. Experimental results reveal the effectiveness of our proposed method on both synthetic data and real-world data. © 2004-2012 IEEE.","Dual branch autoencoder (DBA); hyperspectral image (HSI); spectral unmixing (SU); spectral-spatial model; squared sine distance (SSD)","Convolution; Data mining; Feature extraction; Hyperspectral imaging; Learning systems; Spectroscopy; Auto encoders; Convergence; Dual branch autoencoder; Features extraction; Hyperspectral image; Images reconstruction; Spatial modelling; Spectral unmixing; Spectral-spatial model; Squared sine distance .; Vein; Image reconstruction; pixel; remote sensing; spatial data; spectral analysis",Article,Scopus,2-s2.0-85120315498
"Li B., Peng X., Xiang Q., Wang H., Xie T., Sun J., Liu X.","57356530200;53865467700;57210931805;57188742982;55574210063;57230981500;57356821100;","Enjoy your observability: an industrial survey of microservice tracing and analysis",2022,"Empirical Software Engineering","27","1","25","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120162918&doi=10.1007%2fs10664-021-10063-9&partnerID=40&md5=158868acf416ddc1bd319922bd0891c7","Microservice systems are often deployed in complex cloud-based environments and may involve a large number of service instances being dynamically created and destroyed. It is thus essential to ensure observability to understand these microservice systems’ behaviors and troubleshoot their problems. As an important means to achieve the observability, distributed tracing and analysis is known to be challenging. While many companies have started implementing distributed tracing and analysis for microservice systems, it is not clear whether existing approaches fulfill the required observability. In this article, we present our industrial survey on microservice tracing and analysis through interviewing developers and operation engineers of microservice systems from ten companies. Our survey results offer a number of findings. For example, large microservice systems commonly adopt a tracing and analysis pipeline, and the implementations of the pipeline in different companies reflect different tradeoffs among a variety of concerns. Visualization and statistic-based metrics are the most common means for trace analysis, while more advanced analysis techniques such as machine learning and data mining are seldom used. Microservice tracing and analysis is a new big data problem for software engineering, and its practices breed new challenges and opportunities. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Industrial survey; Logging; Microservice; Tracing","Data mining; Data visualization; Pipelines; Software engineering; Surveys; Cloud-based; Distributed analysis; Distributed tracing; Industrial surveys; Microservice; Number of services; Service instances; System behaviors; Tracing; Troubleshoots; Observability",Article,Scopus,2-s2.0-85120162918
"De Angeli K., Gao S., Danciu I., Durbin E.B., Wu X.-C., Stroup A., Doherty J., Schwartz S., Wiggins C., Damesyn M., Coyle L., Penberthy L., Tourassi G.D., Yoon H.-J.","57219721766;57201077784;22940242900;22633969400;7407064425;6602112934;57356863500;57357009800;57356933300;6508143408;6701910617;7004055226;7003845683;34874205500;","Class imbalance in out-of-distribution datasets: Improving the robustness of the TextCNN for the classification of rare cancer types",2022,"Journal of Biomedical Informatics","125",,"103957","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120156672&doi=10.1016%2fj.jbi.2021.103957&partnerID=40&md5=8ca81874bd9ee8ad8af28a6cf3f5e10a","In the last decade, the widespread adoption of electronic health record documentation has created huge opportunities for information mining. Natural language processing (NLP) techniques using machine and deep learning are becoming increasingly widespread for information extraction tasks from unstructured clinical notes. Disparities in performance when deploying machine learning models in the real world have recently received considerable attention. In the clinical NLP domain, the robustness of convolutional neural networks (CNNs) for classifying cancer pathology reports under natural distribution shifts remains understudied. In this research, we aim to quantify and improve the performance of the CNN for text classification on out-of-distribution (OOD) datasets resulting from the natural evolution of clinical text in pathology reports. We identified class imbalance due to different prevalence of cancer types as one of the sources of performance drop and analyzed the impact of previous methods for addressing class imbalance when deploying models in real-world domains. Our results show that our novel class-specialized ensemble technique outperforms other methods for the classification of rare cancer types in terms of macro F1 scores. We also found that traditional ensemble methods perform better in top classes, leading to higher micro F1 scores. Based on our findings, we formulate a series of recommendations for other ML practitioners on how to build robust models with extremely imbalanced datasets in biomedical NLP applications. © 2021","Class Imbalance; CNN; Deep Learning; Ensemble; NLP; Text Classification","Classification (of information); Clinical research; Convolutional neural networks; Data mining; Deep learning; Learning algorithms; Natural language processing systems; Pathology; Text processing; Class imbalance; Clinical notes; Convolutional neural network; Deep learning; Ensemble; F1 scores; Information mining; Language processing techniques; Performance; Text classification; Diseases; Article; cancer classification; cancer registry; convolutional neural network; evolution; intermethod comparison; machine learning; natural language processing; out of distribution; prevalence; statistical analysis; statistical distribution; text classification; electronic health record; human; neoplasm; Electronic Health Records; Humans; Machine Learning; Natural Language Processing; Neoplasms; Neural Networks, Computer",Article,Scopus,2-s2.0-85120156672
"Narang P.K., Dey J., Mahapatra S.R., Roy R., Kushwaha G.S., Misra N., Suar M., Raina V.","57216797882;57224086955;57216322575;57220764002;36015364100;55037762300;6507346118;23109597300;","Genome-based identification and comparative analysis of enzymes for carotenoid biosynthesis in microalgae",2022,"World Journal of Microbiology and Biotechnology","38","1","8","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120037868&doi=10.1007%2fs11274-021-03188-y&partnerID=40&md5=496582525649a78d0d436fe2294d631b","Abstract: Microalgae are potential feedstocks for the commercial production of carotenoids, however, the metabolic pathways for carotenoid biosynthesis across algal lineage are largely unexplored. This work is the first to provide a comprehensive survey of genes and enzymes associated with the less studied methylerythritol 4-phosphate/1-deoxy-d-xylulose 5-phosphate pathway as well as the carotenoid biosynthetic pathway in microalgae through bioinformatics and comparative genomics approach. Candidate genes/enzymes were subsequently analyzed across 22 microalgae species of lineages Chlorophyta, Rhodophyta, Heterokonta, Haptophyta, Cryptophyta, and known Arabidopsis homologs in order to study the evolutional divergence in terms of sequence-structure properties. A total of 403 enzymes playing a vital role in carotene, lutein, zeaxanthin, violaxanthin, canthaxanthin, and astaxanthin were unraveled. Of these, 85 were hypothetical proteins whose biological roles are not yet experimentally characterized. Putative functions to these hypothetical proteins were successfully assigned through a comprehensive investigation of the protein family, motifs, intrinsic physicochemical features, subcellular localization, pathway analysis, etc. Furthermore, these enzymes were categorized into major classes as per the conserved domain and gene ontology. Functional signature sequences were also identified which were observed conserved across microalgal genomes. Additionally, the structural modeling and active site architecture of three vital enzymes, DXR, PSY, and ZDS catalyzing the vital rate-limiting steps in Dunaliella salina were achieved. The enzymes were confirmed to be stereochemically reliable and stable as revealed during molecular dynamics simulation of 100 ns. The detailed functional information about individual vital enzymes will certainly help to design genetically modified algal strains with enhanced carotenoid contents. Graphical abstract: [Figure not available: see fulltext.] © 2021, The Author(s), under exclusive licence to Springer Nature B.V.","Bioinformatics; Carotenoid; Hypothetical proteins; MEP/DOXP pathway; Microalgae","Biochemistry; Bioinformatics; Biosynthesis; Enzymes; Gene Ontology; Genes; Microorganisms; Molecular dynamics; Pigments; Biosynthetic pathway; Carotenoid; Carotenoid biosynthesis; Commercial productions; Comparative analyzes; d-Xylulose 5-phosphate; Hypothetical protein; MEP/DOXP pathway; Metabolic pathways; Potential feedstock; Microalgae; carotenoid; protein; biology; biosynthesis; chemistry; classification; comparative study; data mining; enzyme active site; enzymology; gene ontology; genetics; genomics; metabolism; microalga; molecular evolution; molecular model; procedures; protein conformation; protein domain; Biosynthetic Pathways; Carotenoids; Catalytic Domain; Computational Biology; Data Mining; Evolution, Molecular; Gene Ontology; Genomics; Microalgae; Models, Molecular; Protein Conformation; Protein Domains; Proteins",Article,Scopus,2-s2.0-85120037868
"Liu Y.-F., Li H., Liang S.","57216416556;57060906400;56403823900;","Any reputation is a good reputation: influence of investor-perceived reputation in restructuring on hospitality firm performance",2022,"Annals of Tourism Research","92",,"103327","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119985201&doi=10.1016%2fj.annals.2021.103327&partnerID=40&md5=314001ea2df71580567ef2f83ef863e3","This study examines how the performance of restructured hospitality firms is impacted by investor-perceived reputation in restructuring (measured through investors' positive, negative and neutral online reviews), as well as by ‘reputation heterogeneity’ and ‘strategy heterogeneity’. The results indicate that: 1) hospitality firm performance after restructuring increases as investor-perceived reputation increases; 2) a positive reputation consistently increases hospitality firm performance after restructuring, while a negative reputation does not decrease performance; 3) time is needed for negative and neutral reputations to positively impact performance; 4) performance is better for hospitality firms undergoing expansion if they have a positive reputation, for firms undergoing shrinkage if they have a negative reputation, and for firms seeking stabilization if they have a neutral reputation. © 2021 Elsevier Ltd","Event study in asset restructuring; Hospitality firm performance; Investor-perceived reputation; Merger and acquisition; Online review; Text mining with support vector machine","data mining; industrial performance; investment; merger; support vector machine",Article,Scopus,2-s2.0-85119985201
"Han X., Jiang F., Needleman J., Zhou H., Yao C., Tang Y.-L.","57203112896;57203118311;7004438782;56555446100;56449979100;7404592408;","Comorbidity combinations in schizophrenia inpatients and their associations with service utilization: A medical record-based analysis using association rule mining",2022,"Asian Journal of Psychiatry","67",,"102927","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119954070&doi=10.1016%2fj.ajp.2021.102927&partnerID=40&md5=e13bc33be10ee8b74b923809bd49b9c5","Background: Comorbidities are common among patients with schizophrenia yet the prevalence of comorbidity combinations and their associations with inpatient service utilization and readmission have been scarcely explored. Methods: Data were extracted from discharge summaries of patients whose primary diagnosis was schizophrenia spectrum disorders (ICD-10: F20-F29). We identified 30 most frequent comorbidities in patients’ secondary diagnoses and then used the association rule mining (ARM) method to derive comorbidity combinations associated with length of stay (LOS), daily expense and one-year readmission. Results: The study included data from 8252 patients. The top five most common comorbidities were extrapyramidal syndrome (EPS, 44.58%), constipation (31.63%), common cold (21.80%), hyperlipidemia (20.99%) and tachycardia (19.13%). Most comorbidity combinations identified by ARM were significantly associated with longer LOS (≥70 days), few were associated with higher daily expenses, and fewer with readmission. The 3-way combination of common cold, hyperlipidemia and fatty liver had the strongest association with longer LOS (adjusted OR (aOR): 3.38, 95% CI: 2.12–5.38). The combination of EPS and mild cognitive disorder was associated with higher daily expense (≥700 RMB) (aOR: 1.67, 95% CI: 1.20–2.31). The combination of constipation, tachycardia and fatty liver were associated with higher 1-year readmission (aOR: 2.05, 95% CI: 1.03–4.09). Conclusion: EPS, constipation, and tachycardia were among the most commonly reported comorbidities in schizophrenia patients in Beijing, China. Specific groups of comorbidities may contribute to higher inpatient psychiatric service utilization and readmission. The mechanism behind the associations and potential interventions to optimize service use warrant further investigation. © 2021 Elsevier B.V.","Association rule mining (ARM); Comorbidity patterns; Multimorbidity; Psychiatric inpatient; Schizotypal disorders","adult; algorithm; anemia; Article; association rule mining; China; common cold; comorbidity; constipation; controlled study; extrapyramidal syndrome; fatty liver; female; health care cost; health care utilization; hospital patient; hospital readmission; human; hyperlipidemia; hyperprolactinemia; hypertension; length of stay; major clinical study; male; medical record; mental patient; middle aged; mild cognitive impairment; multiple chronic conditions; non insulin dependent diabetes mellitus; osteoporosis; prevalence; schizophrenia; tachycardia; comorbidity; data mining; medical record; retrospective study; schizophrenia; Comorbidity; Data Mining; Humans; Inpatients; Length of Stay; Medical Records; Retrospective Studies; Schizophrenia",Article,Scopus,2-s2.0-85119954070
"Wang B., Zhao J.","57350463400;35489746600;","Automatic frequency estimation of contributory factors for confined space accidents",2022,"Process Safety and Environmental Protection","157",,,"193","207",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119703645&doi=10.1016%2fj.psep.2021.11.004&partnerID=40&md5=24eed872d1313dcbf6924f981f15a0db","Although the dangers of working in confined spaces have been known for many years, fatal accidents related to working in confined spaces still frequently occur. Considerable research has been conducted to identify potential contributory factors of confined space incidents through analyzing accident reports. However, accident databases are usually read and interpreted manually by human experts. The process of analyzing confined space accident reports can be time-consuming and labor-intensive. As the number of accident records increases, it is difficult for the experts to manually review all the reports. Moreover, different individuals may reach various conclusions from the same accident report. Some analysts may fail to capture all the meaningful and relevant causal factors. Automatic information extraction using special rules and ontology-based approaches can be used to mine reports of confined space accidents. However, such approaches tend to suffer from the problem of weak generalization. To overcome this limitation and improve the performance of contributory factors analysis, an improved deep learning based framework is proposed in this paper to automatically extract and classify contributory factors from confined space accident reports using BERT-BiLSTM-CRF and CNN models. Research results suggested that the proposed framework can be used as a feasible method to qualitatively and quantitatively explore the contributory factors of confined space accidents. By analyzing a large quantity of confined space accident reports, the frequency of contributory factors can be estimated automatically. This outcome is helpful to significantly improve the risk assessment quality of confined space works. © 2021 Institution of Chemical Engineers","Accident reports; BERT-BiLSTM-CRF; CNN; Confined space; Text-mining","Data mining; Deep learning; Frequency estimation; Ontology; Risk assessment; Accident report; Automatic frequency; Automatic information extraction; BERT-BiLSTM-CRF; CNN; Confined space; Contributory factors; Fatal accidents; Human expert; Labour-intensive; Accidents",Article,Scopus,2-s2.0-85119703645
"Wang Y., Liu M., Yang Y., Li Z., Du Q., Chen Y., Li F., Yang H.","57207002123;57222007940;57349445100;36986930100;7202060063;15059992600;56304289900;57350009100;","Heterogeneous Few-Shot Learning for Hyperspectral Image Classification",2022,"IEEE Geoscience and Remote Sensing Letters","19",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119624611&doi=10.1109%2fLGRS.2021.3117577&partnerID=40&md5=76fda4dbd09d354f946dad2ecc9e0b80","Deep learning has achieved great success in hyperspectral image (HSI) classification. However, its success relies on the availability of sufficient training samples. Unfortunately, the collection of training samples is expensive, time-consuming, and even impossible in some cases. Natural image datasets that are different from HSI, such as Image Net and mini-ImageNet, have abundant texture and structure information. Effective knowledge transfer between two heterogeneous datasets can significantly improve the accuracy of HSI classification. In this letter, heterogeneous few-shot learning (HFSL) for HSI classification is proposed with only a few labeled samples per class. First, few-shot learning is performed on the mini-ImageNet datasets to learn the transferable knowledge. Then, to make full use of the spatial and spectral information, a spectral-spatial fusion network is devised. Spectral information is obtained by the residual network with pure 1-D operators. Spatial information is extracted by a convolution network with pure 2-D operators, and the weights of the spatial network are initialized by the weights of the model trained on the mini-ImageNet datasets. Finally, few-shot learning is fine-tuned on HSI to extract discriminative spectral-spatial features and individual knowledge, which can improve the classification performance of the new classification task. Experiments conducted on two public HSI datasets demonstrate that the HFSL outperforms the existing few-shot learning methods and supervised learning methods for HSI classification with only a few labeled samples. Our source code is available at https://github.com/Li-ZK/HFSL. © 2004-2012 IEEE.","Few-shot learning; hyperspectral image (HSI); meta-learning; transfer learning","Classification (of information); Convolution; Data mining; Deep learning; Hyperspectral imaging; Image classification; Knowledge management; Sampling; Data set; Features extraction; Few-shot learning; Hyperspectral image classification; Kernel; Metalearning; Spatial informations; Task analysis; Training sample; Transfer learning; Spectroscopy; data set; experimental study; heterogeneity; image classification; machine learning; performance assessment; supervised learning",Article,Scopus,2-s2.0-85119624611
"Guan P., Lam E.Y.","57221619670;55694417300;","Multistage Dual-Attention Guided Fusion Network for Hyperspectral Pansharpening",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119618137&doi=10.1109%2fTGRS.2021.3114552&partnerID=40&md5=bf8c3661f9bdf6360a57fc40e6a00649","Deep learning, especially the convolutional neural network, has been widely applied to solve the hyperspectral pansharpening problem. However, most do not explore the intraimage characteristics and the interimage correlation concurrently due to the limited representation ability of the networks, which may lead to insufficient fusion of valuable information encoded in the high-resolution panchromatic images (HR-PANs) and low-resolution hyperspectral images (LR-HSIs). To cope with this problem, we develop a hyperspectral pansharpening method called multistage dual-attention guided fusion network (MDA-Net) to fully extract the important information and accurately fuse them. It employs a three-stream structure, which enables the network to incorporate the intrinsic characteristics of each input and correlation among them simultaneously. In order to combine as much information as possible, we merge the features extracted from three streams in multiple stages, where a dual-attention guided fusion block (DAFB) with spectral and spatial attention mechanisms is utilized to fuse the features efficiently. It identifies the useful components in both spatial and spectral domains, which are beneficial to improving the fusion accuracy. Moreover, we design a multiscale residual dense block (MRDB) to extract dense and hierarchical features, which improves the representation power of the network. Experiments are conducted on both real and simulated datasets. The evaluation results validate the superiority of the MDA-Net. 1558-0644 © 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.","Correlation; Data mining; Feature extraction; Fuses; Hyperspectral imaging; Pansharpening; Streaming media","Convolution; Deep learning; Feature extraction; Hyperspectral imaging; Media streaming; Neural networks; Spectroscopy; Convolutional neural network; Correlation; Dual-attention guided fusion; Features extraction; HyperSpectral; Hyperspectral pansharpening; Multi stage fusion; Multistage fusion.; Pan-sharpening; Streaming medium; Data mining; artificial neural network; correlation; image analysis; panchromatic image; spectral analysis",Article,Scopus,2-s2.0-85119618137
"Zhao X., Liu L., Wang Y., Zhang L., Plaza A.","57219329674;57202047220;55986579100;36064917000;7006613644;","DS4L: Deep Semisupervised Shared Subspace Learning for Hyperspectral Image Classification",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119592785&doi=10.1109%2fTGRS.2021.3115354&partnerID=40&md5=a4aca5ea6dcae87d63e14ca6dfd3abb7","Hyperspectral image (HSI) classification is essential in remote sensing image analysis. The classification methods based on deep learning have attracted more and more attention. However, classification accuracy is seriously affected by the quantity of labeled data and redundant information. Therefore, a deep semisupervised shared subspace learning (DS4L) model is developed to overcome these problems in this article. DS4L is composed of two parts. First, the basic feature extraction (BFE) network is constructed to preliminary extract high-dimensional-space features of multiscale data and fusion them to one shared subspace. Then, a deep shared subspace learning (DSSL) network is proposed to obtain a deeper and more representative low-dimensional subspace. Moreover, to obtain a more representative subspace and alleviate dependence on labeled samples, the regular, irregular constraint, and cross-entropy (CE) loss are integrated into the model. The regular constraint is adopted to reconstruct the multiscale patches to ensure the quality of the subspace in an unsupervised manner. The irregular constraint can well embed labeled and unlabeled samples into the procedure of subspace learning (SL). Then, the CE loss is used to extract more discriminative subspace using the limited labeled samples. Finally, we perform experiments on three widely used HSI datasets. Compared with the basic SL model, the DS4L’s classification accuracy on the popular Salinas, Indian Pines, and PaviaU datasets are increased by 4.68%, 4.25%, and 1.57%, respectively. 1558-0644 © 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.","Convolutional neural networks; Data mining; Feature extraction; Hyperspectral imaging; Image reconstruction; Kernel; Training","Classification (of information); Convolution; Data mining; Deep learning; Extraction; Feature extraction; Hyperspectral imaging; Image classification; Neural networks; Remote sensing; Space optics; Spectroscopy; Constraint; Convolutional neural network; Features extraction; Hyperspectral image classification; Images reconstruction; Kernel; Semi-supervised; Shared subspace learning .; Shared subspaces; Subspace learning; Image reconstruction; data set; detection method; genetic algorithm; image classification; remote sensing; satellite imagery; Salinas [California]",Article,Scopus,2-s2.0-85119592785
"Alfonsi A., Mandelli D., Parisi C., Rabiti C.","55479967700;16175402800;8638794300;23012996700;","Risk analysis virtual ENvironment for dynamic event tree-based analyses",2022,"Annals of Nuclear Energy","165",,"108754","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119509723&doi=10.1016%2fj.anucene.2021.108754&partnerID=40&md5=f198a07743eeddc67236ac4a499dc877","Conventional Event-Tree (ET) based methodologies are extensively used as tools to perform reliability and safety assessment of complex and critical engineering systems. One of the disadvantages of these methods is that timing/sequencing of events and system dynamics is not explicitly accounted for in the analysis. In order to overcome these limitations several techniques, also known as Dynamic Probabilistic Risk Assessment (DPRA), have been developed. Monte-Carlo (MC) and Dynamic Event Tree (DET) are two of the most widely used DPRA methodologies to perform safety assessment of Nuclear Power Plants (NPP). Since 2012, the Idaho National Laboratory (INL) is developing its own tool to perform Dynamic PRA: RAVEN (Risk Analysis and Virtual ENvironment). RAVEN has been designed in a high modular and pluggable way to enable easy integration of different programming languages (i.e., Python, C++) and coupling with other application including, among the others, several thermal–hydraulic and severe accident codes (e.g., RELAP5-3D, MELCOR, MAAP5, TRACE, etc.). RAVEN is aimed to provide a framework/container of capabilities for engineers and scientists to analyze the response of systems, physics and multi-physics, employing advanced numerical techniques and algorithms. Moreover, RAVEN models stochastic events, such as components failures, and performs uncertainty quantification (UQ). Such stochastic modeling is employed by using sampling strategies among which both MC and DET algorithms, which are going to be employed in this paper. In addition, RAVEN processes the large amount of data generated by sampling the physical models using data-mining based algorithms and risk assessment techniques. This paper provides an overview of the DET methodologies that have been deployed within the RAVEN framework, showing the potential of such techniques for the analysis of complex systems. A brief background of classical methodologies and their limitation is also reported and represent the motivation for the deployment of such dynamic technique. In addition, results from a pressurized water reactor loss of coolant accident scenario, using RELAP5-3D as physical model, are reported. © 2021","Dynamic Event Tree; Probabilistic risk assessment; RAVEN; RELAP5-3D; Uncertainty Quantification","C++ (programming language); Data mining; Dynamics; Monte Carlo methods; Nuclear fuels; Nuclear power plants; Pressurized water reactors; Probability distributions; Risk analysis; Risk assessment; Stochastic models; Stochastic systems; Virtual reality; Dynamic event tree; Dynamic events; Event-trees; Physical modelling; Probabilistic Risk Assessment; RELAP5-3d; Risk analyse and virtual environment; Safety assessments; Tree-based; Uncertainty quantifications; Uncertainty analysis",Article,Scopus,2-s2.0-85119509723
"Babour A., Khan J.I.","56426219500;57346225500;","Automatic Generation of a Metric Report: A Case Study of Scientometric Analytics",2022,"IEEE Access","10",,,"3923","3934",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119422891&doi=10.1109%2fACCESS.2021.3127207&partnerID=40&md5=7273d97232b0740d60a29ec2ef1a4c03","Automatic report generation is an emerging technology that mechanically generates documents in the form of a report consisting of text, tables, and figures about a specific topic. This paper proposes a model for automated generation of a scientometric research analysis report for any selected country. The scholarly database utilized in this study is Microsoft Academic Graph. Given the two-letter code of the selected country, starting study year, and ending study year data concepts to the employed database, the model extracts the datasets, including the scientific research information for the selected country in the specified period, and generates an in-depth analysis report about the country's research publications. The model consists of two main stages. The first stage extracts the datasets for the selected country from the utilized database using Azure Databricks and Azure Blob Storage services. The second stage utilizes a predefined scientometric research analysis report for generating a new report for the selected country. A case study on big data analysis for Saudi Arabian research publications was conducted. An evaluation was performed on the report within 10 evaluators to understand the practicability of the proposed model. They evaluated the report through four-point criteria on the user perceptions. The results indicated that the model was able to successfully create quite a pleasing scientific report in terms of factuality, coherency, sufficiency, and its ability to impart new knowledge for the readers. © 2013 IEEE.","Automated report generation; Big data; Coefficient collaboration; Collaborative research; Growth rate; scientometrics","Big data; Data mining; Digital storage; Growth rate; Automated report generation; Bibliometric; Case-studies; Coefficient collaboration; Collaboration; Collaborative research; Report generation; Research analysis; Scientometrics; Writing; Database systems",Article,Scopus,2-s2.0-85119422891
"Ling J., Wang Y., Ma L., Zheng Y., Tang H., Meng L., Zhang L.","55236952700;57346284300;57346284400;57346936200;57347090900;57346936300;56046918200;","KIF11, a plus end-directed kinesin, as a key gene in benzo(a)pyrene-induced non-small cell lung cancer",2022,"Environmental Toxicology and Pharmacology","89",,"103775","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119418486&doi=10.1016%2fj.etap.2021.103775&partnerID=40&md5=f396cc1de935ab4916902eb8c932c47a","Evidence indicates that Benzo(a)pyrenediol-epoxide (BPDE) can damage lung cells, resulting in carcinogenesis with complex mechanisms. We aimed to explore the genes and pathway variations in this process. First, the key gene was screened out and identified through data mining, and then, it was in turn validated by bioinformatics analysis and experimental methods. Consequently, 106 up-regulated and 260 down-regulated differentially expressed genes were yielded, which were enriched in various pathways, such as Cell cycle, and p53 signaling pathway. Then, KIF11 was identified as the key gene. Overexpression of KIF11 in lung cancer had a correlation with advanced pathological grade, advanced T stage, and presence of lymph node metastasis, which predicted poor prognosis. In summary, the present study revealed that KIF11 might be a key gene in the tumorigenesis of BPDE-related lung cancer, raising the possibility of KIF11 as a target for BPDE-induced lung cancer prevention and therapy. © 2021 Elsevier B.V.","Benzo[a]pyrene; Biomarkers; B[a]PDE; Immunohistochemistry assay; Lung cancer","benzo[a]pyrene; kinesin; protein p53; benzo(a,l)pyrene diol epoxide; benzopyran derivative; benzo[a]pyrene; epoxide; KIF11 protein, human; adult; Article; cancer staging; cell cycle; correlational study; data mining; down regulation; female; gene; gene expression; gene identification; genetic screening; genetic variation; human; immunoassay; immunohistochemistry; KIF11 gene; lymph node metastasis; major clinical study; male; middle aged; non small cell lung cancer; prediction; signal transduction; validation process; aged; carcinogenesis; gene expression regulation; genetics; lymph node metastasis; metabolism; non small cell lung cancer; pathology; tumor cell line; very elderly; Adult; Aged; Aged, 80 and over; Benzo(a)pyrene; Benzopyrans; Carcinogenesis; Carcinoma, Non-Small-Cell Lung; Cell Line, Tumor; Data Mining; Epoxy Compounds; Female; Gene Expression Regulation, Neoplastic; Humans; Immunohistochemistry; Kinesins; Lymphatic Metastasis; Male; Middle Aged",Article,Scopus,2-s2.0-85119418486
"Sun J., Zhao L., Liu Z., Li Q., Deng X., Wang Q., Jiang Y.","57194699710;57212621990;57189214821;57200602838;57345143700;56856235900;55733670600;","Practical differentially private online advertising",2022,"Computers and Security","112",,"102504","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119346726&doi=10.1016%2fj.cose.2021.102504&partnerID=40&md5=431b40401c20cba985df49a90102a3d6","Powered by machine learning technology, online advertising achieves accurate advertisement delivery to potential customers according to online user profiles. However, it raises serious privacy concerns since the learning process may reveal sensitive information in the profiles. It is highly desirable to provide high-quality advertisement recommendations while respecting an individual's privacy. To address the privacy issues, we propose a practical privacy-preserving system that predicts advertisement Click Through Rate (CTR) accurately without revealing any sensitive information of individuals. Our system combines both offline and online training to achieve the goal. In the offline training phase, we develop two differentially private algorithms built upon Gradient Boosting Decision Tree (GBDT) and Field-aware Factorization Machine (FFM) algorithms, respectively, to transform features and train a privacy-preserving offline model. In the online training phase, we propose a privacy-preserving online learning algorithm to compensate for the decline of offline performance when online feature changes occur. We perform extensive experiments to evaluate the performance of our system based on three real-world datasets. The results demonstrate that our system can protect user privacy without compromising the accuracy of CTR prediction, with reasonable computation overhead. © 2021","Data perturbation; Differential privacy; Learning and optimization; Online advertising; Recommendation system","Data mining; E-learning; Learning algorithms; Learning systems; Marketing; Online systems; Privacy-preserving techniques; User profile; Clickthrough rates (CTR); Data perturbation; Differential privacies; Learning and optimization; Off-line training; Online advertizing; Online training; Optimisations; Privacy preserving; Sensitive informations; Decision trees",Article,Scopus,2-s2.0-85119346726
"Kong Q., Xu W., Zhang D.","57020844800;55730741100;57198600396;","A comparative study of different granular structures induced from the information systems",2022,"Soft Computing","26","1",,"105","122",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119290925&doi=10.1007%2fs00500-021-06499-2&partnerID=40&md5=343db36d8edb89a0b43db7b16da74be8","Binary information table, multi-valued information table and set-valued information table are three kinds of information systems often encountered in information processing. For any information system, we can often induce different information granular structures, and then construct the corresponding rough set models. Generally speaking, for the same information system, three models of Pawlak rough set, covering rough set and multi-granulation rough set can be induced according to different rules. These three kinds of rough set models are effective tools for data mining and information processing. This paper studies the relationship among Pawlak rough set, covering rough set and multi-granularity rough set induced in binary information table, multi-valued information table and set-valued information table, and obtains many important conclusions. The research content of this paper effectively connects the theories, methods and applications of Pawlak rough set, covering rough set and multi-granularity rough set, which not only enriches the rough set theory, but also expands the application prospect of rough set. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Covering; Granular computing; Granular structure; Partition; Reduction; Rough sets","Computation theory; Data mining; Granular computing; Information systems; Information use; % reductions; Binary information; Covering; Covering rough sets; Granular structuress; Information table; Multi-valued; Partition; Pawlak rough set; Set coverings; Rough set theory",Article,Scopus,2-s2.0-85119290925
"Juan Y.-K., Lee P.-H.","12767232600;57344101600;","Applying data mining techniques to explore technology adoptions, grades and costs of green building projects",2022,"Journal of Building Engineering","45",,"103669","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119263241&doi=10.1016%2fj.jobe.2021.103669&partnerID=40&md5=5005b25424ba3271dafecfdbd4f5dc86","Many architects encounter problems during adoption of green building technologies and are unfamiliar with the benefits of green buildings. This study conducted two-stage data mining on 354 green building projects in Taiwan, in order to solve issues in the preliminary design phase of projects, such as technology adoptions, green building grades, and construction costs. This study adopted the association rules to explore the associations between different types and grades of green buildings and technology adoptions. Moreover, a prediction model based on the artificial neural network was constructed to predict the grades and costs of green building projects. The results indicate that different types and grades of green buildings are based on varying green building technologies. In particular, a green building with a high grade places more emphases on the green building technologies of air conditioning, CO2 reduction, and indoor environments. The green building technologies are affected by building types, regulations, costs, climate conditions, or geographic restrictions. This study also found that the accuracy of the artificial neural network in predicting green building grades and costs can reach above 80%. The systematic data mining method constructed herein can effectively assist architects and building owners to reduce preliminary design time and costs, and improve the success rates of green building projects. It is expected that the proposed approach can be adjusted in the future for other regions with different climates or their corresponding green building rating tools, so as to construct more suitable applications. © 2021 Elsevier Ltd","Artificial neural network; Association rules; Data mining; Green building technologies; Taiwan","Air conditioning; Architectural design; Data mining; Forecasting; Neural networks; Structural design; Building technologies; Construction costs; Data-mining techniques; Green building projects; Green building technology; Green buildings; Preliminary design phase; Taiwan; Technology adoption; Technology costs; Association rules",Article,Scopus,2-s2.0-85119263241
"Boudaghi M., Mahan F., Isazadeh A.","57343079200;23060709400;25122239900;","Using BTA Algorithm for finding Nash equilibrium problem aiming the extraction of rules in rule learning",2022,"Soft Computing","26","1",,"439","462",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119247464&doi=10.1007%2fs00500-021-06432-7&partnerID=40&md5=253ffd65892a486d790b7e17c4fa992a","It is crystal clear that discovering the rules for finding a specific pattern among given data for extraction of association rules in rule-based learning systems has been defined in previous researches. Making use of game theory for the processes contributing to discovery of rules can be seen in numerous researches. In recent years, modeling based on game theory in rule learning sphere has gained much more attention for computer scientists. When two or more players use different strategies independently, the strategy game modeling could be used. In this view, strategic play is a desirable model for situations with no permanent strategic relationship among interactions. In addition, Nash equilibrium is the most widely used solution concept in game theory. This concept is a state-of-the-art interpretation of a strategy game. Each player has an accurate prediction of other players’ behavior and acts according to such a rational prediction. In the present study, by extracting rules from frequent patterns we have presented a model that can extract learning rules by abstraction based on game theory, which can be used not only for association rules but also for rule-based learning systems. Also, the introduced method can be easily generalized to fuzzy data. To find Nash equilibrium (FNE) in the proposed method, we used meta-heuristic bus transportation algorithm. The results indicated that the method reduces computational complexity in the associate rule discovery process and rule learning, provided that FNE is solved. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Game theory; Meta-heuristic bus transportation algorithm; Nash equilibrium; Rule learning","Association rules; Buses; Data mining; Extraction; Game theory; Heuristic algorithms; Heuristic methods; Learning algorithms; Learning systems; Computer scientists; Game models; Meta-heuristic bus transportation algorithm; Metaheuristic; Model-based OPC; Nash equilibria; Nash equilibrium problems; Rule learning; Rule-based learning; Strategy games; Computation theory",Article,Scopus,2-s2.0-85119247464
"Ruan M., Liu J., Song P., Xu W.","56230184600;57221548750;57206871789;57205478905;","Meta-analysis of commercial Pt/C measurements for oxygen reduction reactions via data mining",2022,"Chinese Journal of Catalysis","43","1",,"116","121",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119205563&doi=10.1016%2fS1872-2067%2821%2963854-8&partnerID=40&md5=2ac879f33eadd107537caada68daae50","The rotating disk electrode technique is commonly used for screening and characterizing the performance of electrocatalysts for the oxygen reduction reaction (ORR). However, a reliable performance comparison of different electrocatalysts from different labs remains a challenge because of the inconsistency in the measurement of commercial Pt/C. Commercial Pt/C has been adopted extensively as a reference for evaluating the ORR performance of a new electrocatalyst. However, the reported ORR performances of commercial Pt/C from different labs could be significantly different because of multiple factors. Herein, we conducted a meta-analysis of the ORR performance of commercial Pt/C via data mining of the literature. This revealed the optimal testing conditions for the most repeatable ORR performance, with commercial Pt/C in both acid and alkaline electrolytes; the optimal Pt loading was 20 μg/cm2 on a 4 mm glassy carbon working electrode. The value of 0.84 ± 0.03 V was suggested as the “Golden reference” of the commercial Pt/C (with Pt 20 wt%) ORR half-wave potential for the performance evaluation of other ORR catalysts in both acid and alkaline electrolytes. The conclusion obtained through the meta-analysis was confirmed by experiments. This study provides general guidance for a reliable measurement of the ORR performance of commercial Pt/C as a reference. © 2022 Dalian Institute of Chemical Physics, the Chinese Academy of Sciences","Commercial Pt/C; Deep-analysis; Oxygen reduction reaction; Verification experiment","Data mining; Electrocatalysts; Electrolytes; Electrolytic reduction; Oxygen; Rotating disks; Acid electrolytes; Alkaline electrolytes; Commercial pt/C; Deep-analyse; Measurements of; Meta-analysis; Oxygen reduction reaction; Reaction performance; Rotating disk electrodes; Verification experiment; Electrodes",Article,Scopus,2-s2.0-85119205563
"Metzenmacher M., Hegedüs B., Forster J., Schramm A., Horn P.A., Klein C.A., Bielefeld N., Ploenes T., Aigner C., Theegarten D., Schildhaus H.-U., Siveke J.T., Schuler M., Lueong S.S.","55382271400;54983984500;57214896725;35622254200;7102146764;26642986500;54789437400;55316996200;6603706907;7006186806;6508083844;6603072868;7102690837;35976553200;","Combined multimodal ctDNA analysis and radiological imaging for tumor surveillance in Non-small cell lung cancer",2022,"Translational Oncology","15","1","101279","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119198585&doi=10.1016%2fj.tranon.2021.101279&partnerID=40&md5=be9f298234e5b0f6a3645f8aceecd24f","Background: Radiology is the current standard for monitoring treatment responses in lung cancer. Limited sensitivity, exposure to ionizing radiations and related sequelae constitute some of its major limitation. Non-invasive and highly sensitive methods for early detection of treatment failures and resistance-associated disease progression would have additional clinical utility. Methods: We analyzed serially collected plasma and paired tumor samples from lung cancer patients (61 with stage IV, 48 with stages I-III disease) and 61 healthy samples by means of next-generation sequencing, radiological imaging and droplet digital polymerase chain reaction (ddPCR) mutation and methylation assays. Results: A 62% variant concordance between tumor-reported and circulating-free DNA (cfDNA) sequencing was observed between baseline liquid and tissue biopsies in stage IV patients. Interestingly, ctDNA sequencing allowed for the identification of resistance-mediating p.T790M mutations in baseline plasma samples for which no such mutation was observed in the corresponding tissue. Serial circulating tumor DNA (ctDNA) mutation analysis by means of ddPCR revealed a general decrease in ctDNA loads between baseline and first reassessment. Additionally, serial ctDNA analyses only recapitulated computed tomography (CT) -monitored tumor dynamics of some, but not all lesions within the same patient. To complement ctDNA variant analysis we devised a ctDNA methylation assay (methcfDNA) based on methylation-sensitive restriction enzymes. cfDNA methylation showed and area under the curve (AUC) of &gt; 0.90 in early and late stage cases. A decrease in methcfDNA between baseline and first reassessment was reflected by a decrease in CT-derive tumor surface area, irrespective of tumor mutational status. Conclusion: Taken together, our data support the use of cfDNA sequencing for unbiased characterization of the molecular tumor architecture, highlights the impact of tumor architectural heterogeneity on ctDNA-based tumor surveillance and the added value of complementary approaches such as cfDNA methylation for early detection and monitoring © 2021","cfDNA methylation; ddPCR; Lung cancer; NGS; Surveillance","chloroplast DNA; circulating tumor DNA; formaldehyde; paraffin; Article; blood sampling; blood transfusion; cancer epidemiology; cancer staging; clinical outcome; cohort analysis; computer assisted tomography; controlled study; data mining; DNA extraction; DNA methylation; DNA sequencing; droplet digital polymerase chain reaction; fluorescence in situ hybridization; gene mutation; high throughput sequencing; human; human tissue; major clinical study; non small cell lung cancer; Sanger sequencing; treatment response; tumor volume",Article,Scopus,2-s2.0-85119198585
"Cruz-Martinez C., Reyes-Garcia C.A., Vanello N.","57193604091;55989773500;6505734003;","A novel event-related fMRI supervoxels-based representation and its application to schizophrenia diagnosis",2022,"Computer Methods and Programs in Biomedicine","213",,"106509","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119188963&doi=10.1016%2fj.cmpb.2021.106509&partnerID=40&md5=e4d56c08113ebab49e901faeb76604f8","Background and Objective: The schizophrenia diagnosis represents a difficult task because of the confusing descriptions of symptoms given by the patient, their similarity among several disorders, the lower familiarity with genetic predisposition, and the probably inadequate response to the treatment. Neuro-biological markers of schizophrenia, as a quantitative relationship between the psychiatrist's reports and the biology of the brain, could be used. Functional Magnetic Resonance Imaging (fMRI) obtain the subject's performance in cognitive tasks and may find significant differences between the patient's data and controls. The input data of classifiers may imply alterations in diagnosis; therefore, it is essential to ensure an adequate representation to describe the entire dataset classified. Methods: We propose a supervoxels-based representation calculated by two main steps: the short-range connectivity, supervoxels’ generation using a Fuzzy Iterative Clustering algorithm, and the long-range connectivity, employing Detrended Cross-Correlation Analysis among supervoxels. The unrelated supervoxels, through a statistical test based on critical points calculated empirically, are removed. The remainder supervoxels are the input for feature selectors to extract the discriminative supervoxels. We implement support vector machine classifiers using the correlation coefficient of the significant supervoxels. The dataset of 1.5 Tesla was downloaded from the SchizConnect site, where the fMRI data, during an auditory oddball task, was acquired. We calculate the performance of the classifiers using a leave-one-out cross-validation and compute the area under the Receiver Operating Characteristic curve and a permutation test to ensure no bias in the classifiers. Results: According to the permutation test, with p-values less than the significance level of 0.05, the classifiers extract discriminative class structure from data where no bias is shown. Our supervoxels-based representation gets the maximum values of sensitivity, specificity, and accuracy of 92.9%, 100%, and 96.4%, respectively. The discriminative brain regions, to discern among patients and controls, are extracted; these regions also are mentioned by the related works. Conclusions: The proposed representation, based on supervoxels, is a data-driven model that does not use predefined models of the signal nor pre-relocated brain regions of interest. The results are competitive against the related works, and the relevant supervoxels are related to the schizophrenia diagnosis. © 2021 Elsevier B.V.","Detrended cross-correlation analysis; fMRI; Fuzzy-SLIC; Schizophrenia; Supervoxel","Brain; Classification (of information); Clustering algorithms; Correlation methods; Data mining; Diagnosis; Diseases; Functional neuroimaging; Iterative methods; Patient treatment; Support vector machines; Brain regions; Detrended cross-correlations analysis; Event related functional magnetic resonance imaging; Functional magnetic resonance imaging; Fuzzy-SLIC; ITS applications; Performance; Permutation tests; Related works; Supervoxel; Magnetic resonance imaging; brain; brain mapping; diagnostic imaging; human; nuclear magnetic resonance imaging; schizophrenia; support vector machine; Brain; Brain Mapping; Humans; Magnetic Resonance Imaging; Schizophrenia; Support Vector Machine",Article,Scopus,2-s2.0-85119188963
"Santilli V.","57191539232;","Application of machine learning techniques to physical and rehabilitative medicine",2022,"Annali di Igiene Medicina Preventiva e di Comunita","34","1",,"79","83",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119059676&doi=10.7416%2fai.2021.2444&partnerID=40&md5=09487a1bb60009311ae1ea2c17230686","Nowadays, digital information has increased exponentially in every field to such an extent that it generates huge amounts of electronic data, namely Big Data. In the field of Artificial Intelligence, Machine Learning can be exploited in order to transform the large amount of information to improve decision-making. We retrospectively evaluated the data collected from 2016 to 2018, using the database of approximately 4000 rehabilitation hospital discharges (SDO) of the Latium Region (Italy). Three models of machine learning algorithms were considered: Support of vector machine; Neural networks; Random forests. Applying this model, the estimate of the average error is 9.077, and specifically, considering the distinction between orthopedic and neurological patients, the average error obtained is 7.65 for orthopedic and 10.73 for neurological patients. SDO information flow can be used to represent and quantify the potential inadequacy and inefficiency of rehabilitation hospitalizations, although there are limitations such as the absence of description of pre-pathological conditions, changes in health status from the beginning to the end of hospitalization, specific short- and long-term outcomes of rehabilitation, services provided during hospitalization, as well as psycho-social variables. Furthermore, information from wearable devices capable of providing clinical parameters and movement data could be integrated into the dataset. © Società Editrice Universo (SEU), Roma, Italy","big data; data mining; disability; Machine learning; rehabilitation","algorithm; artificial intelligence; factual database; human; machine learning; retrospective study; Algorithms; Artificial Intelligence; Databases, Factual; Humans; Machine Learning; Retrospective Studies",Article,Scopus,2-s2.0-85119059676
"Asiri Y.","57221523182;","Short text mining for classifying educational objectives and outcomes",2022,"Computer Systems Science and Engineering","41","1",,"35","50",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119018884&doi=10.32604%2fcsse.2022.020100&partnerID=40&md5=3ccf19163b7eeb477ec02a1a81f5472b","Most of the international accreditation bodies in engineering education (e.g., ABET) and outcome-based educational systems have based their assessments on learning outcomes and program educational objectives. However, mapping program educational objectives (PEOs) to student outcomes (SOs) is a challenging and time-consuming task, especially for a new program which is applying for ABET-EAC (American Board for Engineering and Technology the American Board for Engineering and Technology—Engineering Accreditation Commission) accreditation. In addition, ABET needs to automatically ensure that the mapping (classification) is reasonable and correct. The classification also plays a vital role in the assessment of students’ learning. Since the PEOs are expressed as short text, they do not contain enough semantic meaning and information, and consequently they suffer from high sparseness, multidimensionality and the curse of dimensionality. In this work, a novel associative short text classification technique is proposed to map PEOs to SOs. The datasets are extracted from 152 self-study reports (SSRs) that were produced in operational settings in an engineering program accredited by ABET-EAC. The datasets are processed and transformed into a representational form appropriate for association rule mining. The extracted rules are utilized as delegate classifiers to map PEOs to SOs. The proposed associative classification of the mapping of PEOs to SOs has shown promising results, which can simplify the classification of short text and avoid many problems caused by enriching short text based on external resources that are not related or relevant to the dataset. © 2022 CRL Publishing. All rights reserved.","ABET accreditation; Association rule mining; Associative classification; Educational data mining; Engineering education; Program educational objectives; Student outcomes","Accreditation; Association rules; Classification (of information); Engineering education; Mapping; Semantics; Students; Systems engineering; ABET accreditation; Associative classification; Educational data mining; Educational objectives; Engineering and technology; International accreditation; Program educational objectives; Short texts; Student outcomes; Text-mining; Data mining",Article,Scopus,2-s2.0-85119018884
"Deng Y., Mueller M., Rogers C., Olechowski A.","57337362900;57195966051;7402364162;55314243600;","The multi-user computer-aided design collaborative learning framework",2022,"Advanced Engineering Informatics","51",,"101446","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119004706&doi=10.1016%2fj.aei.2021.101446&partnerID=40&md5=32946d7147ed630d2baa9b4cec2232e9","New developments to computer-aided design (CAD) software transform a once solitary modelling task into a collaborative one. The emerging multi-user CAD (MUCAD) systems allow virtual, real-time collaboration, with the potential to expand the learning outcomes and teaching methods of CAD. This paper proposes a MUCAD collaborative learning framework (MUCAD-CLF) to interpret backend analytic data from commercially available MUCAD software. The framework builds on several existing metrics from the literature and introduces newly developed methods to classify CAD actions collected from users’ analytic data. The framework contains two different classification approaches of user actions, categorizing actions by action type (e.g., creating, revising, viewing) and by design space (e.g., constructive, organizing), for comparative analysis. Next, the analytical framework is applied via a collaborative design challenge, corresponding to over 20,000 actions collected from 31 participants. Illustrative analyses utilizing the MUCAD-CLF are presented to demonstrate the resulting insight. Differences in CAD behaviour, indicating differences in learning, are observed between teams made up entirely of novices, entirely of experienced users, or a mix. In pairs of experts and novices, we see both a perceived high-satisfaction apprenticeship experience for the novices and preliminary evidence of an increase in expert design behaviours for the novices. The proposed framework is critical for MUCAD systems to make the most of the educational possibility of combining technical skill-building with team collaboration. Preliminary evidence collected in a fully-virtual design learning activity, and analyzed using the proposed MUCAD-CLF, shows that novice students gain advanced CAD design knowledge when collaborating with experienced teammates. With the user data captured by modern MUCAD software and the MUCAD-CLF presented herein, instructors and researchers can more efficiently assess and visualize students’ performance over the design learning process. © 2021","CAD education; Collaborative learning; Computer-aided design; Data mining; Multi-user CAD; User analytics","Computer aided instruction; Computer software; Data mining; Learning systems; Real time systems; Students; CAD softwares; Collaborative learning; Computer-aided design; Computer-aided design education; Design Education; Learning frameworks; Multi-user computer-aided design; Multiusers; User analytic; Computer aided design",Article,Scopus,2-s2.0-85119004706
"Longa A., Cencetti G., Lepri B., Passerini A.","57221251335;56537036600;14008023300;7006983227;","An efficient procedure for mining egocentric temporal motifs",2022,"Data Mining and Knowledge Discovery","36","1",,"355","378",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118944370&doi=10.1007%2fs10618-021-00803-2&partnerID=40&md5=b8565819947bf7f501957d6ffbefc047","Temporal graphs are structures which model relational data between entities that change over time. Due to the complex structure of data, mining statistically significant temporal subgraphs, also known as temporal motifs, is a challenging task. In this work, we present an efficient technique for extracting temporal motifs in temporal networks. Our method is based on the novel notion of egocentric temporal neighborhoods, namely multi-layer structures centered on an ego node. Each temporal layer of the structure consists of the first-order neighborhood of the ego node, and corresponding nodes in sequential layers are connected by an edge. The strength of this approach lies in the possibility of encoding these structures into a unique bit vector, thus bypassing the problem of graph isomorphism in searching for temporal motifs. This allows our algorithm to mine substantially larger motifs with respect to alternative approaches. Furthermore, by bringing the focus on the temporal dynamics of the interactions of a specific node, our model allows to mine temporal motifs which are visibly interpretable. Experiments on a number of complex networks of social interactions confirm the advantage of the proposed approach over alternative non-egocentric solutions. The egocentric procedure is indeed more efficient in revealing similarities and discrepancies among different social environments, independently of the different technologies used to collect data, which instead affect standard non-egocentric measures. © 2021, The Author(s).","Graph mining; Network motifs; Social interaction networks; Sociopatterns; Temporal networks","Data mining; Change-over time; Complexes structure; Graph mining; Neighbourhood; Network motif; Relational data; Sociopattern; Subgraphs; Temporal graphs; Temporal networks; Complex networks",Article,Scopus,2-s2.0-85118944370
"Liu C.","57331226700;","Research on evaluation and promotion strategy of guilin tourism innovation ability based on big data analysis",2022,"Journal of Applied Science and Engineering (Taiwan)","25","2",,"321","327",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118939088&doi=10.6180%2fjase.202204_25%282%29.0008&partnerID=40&md5=fc31384a6ff0562d146806c5fe0bf832","In order to build an intelligent tourism innovation ability evaluation system, this paper uses data mining technology to mine tour-ism information, combines modern people's tourism needs to construct a regional innovation ability evaluation system, and detects the effect of data mining with the support of logistic regression analysis. Moreover, this paper uses the life cycle theory framework to analyze the life cycle stages of regional tourism destinations, analyzes the internal factors that form the characteristics and laws of specific stages, and takes timely measures to artificially control and adjust the problems that need attention. In addition, this paper constructs an evaluation system of tourism innovation capability with intelligent effects. On this basis, this paper combines experimental analysis to evaluate the system constructed in this paper, and takes Guilin regional tourism as an example to perform the analysis. The research results show that the system constructed in this paper has a certain effect, and the corresponding suggestions are given. © 2022 Tamkang University. All right reserved.","Ability evaluation; Big data analysis; Improvement strategy; Tourism innovation","3D modeling; Big data; Data mining; Life cycle; Regional planning; Regression analysis; Tourism; Ability evaluation; Big data analyse; Data mining technology; Evaluation strategies; Improvement strategies; Innovation abilities; Logistic regression analysis; Promotion strategies; Regional innovation; Tourism innovation; Data handling",Article,Scopus,2-s2.0-85118939088
"Fagerlind H., Harvey L., Humburg P., Davidsson J., Brown J.","36239685000;35242563900;57230583300;6701608768;57220078180;","Identifying individual-based injury patterns in multi-trauma road users by using an association rule mining method",2022,"Accident Analysis and Prevention","164",,"106479","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118896766&doi=10.1016%2fj.aap.2021.106479&partnerID=40&md5=83a875c6a25554992ba143e8d60e1dc4","In many road crashes the human body is exposed to high forces, commonly resulting in multiple injuries. This study of linked road crash data aimed to identify co-occurring injuries in multiple injured road users by using a novel application of a data mining technique commonly used in Market Basket Analysis. We expected that some injuries are statistically associated with each other and form Individual-Based Injury Patterns (IBIPs) and further that specific road users are associated with certain IBIPs. First, a new injury taxonomy was developed through a four-step process to allow the use of injury data recorded from either of the two major dictionaries used to document anatomical injury. Then data from the Swedish Traffic Accident Data Acquisition, which includes crash circumstances from the police and injury information from hospitals, was analysed for the years 2011 to 2017. The injury data was analysed using the Apriori algorithm to identify statistical association between injuries (IBIP). Each IBIP were then used as the outcome variable in logistic regression modelling to identify associations between specific road user types and IBIPs. A total of 48,544 individuals were included in the analysis of which 36,480 (75.1%) had a single injury category recorded and 12,064 (24.9%) were considered multiply injured. The data mining analysis identified 77 IBIPs in the multiply injured sample and 16 of these were associated with only one road user type. IBIPs and their relation to road user type are one step on the journey towards developing a tool to better understand and quantify injury severity and thereby improve the evidence-base supporting prioritisation of road safety countermeasures. © 2021 The Authors","Injury outcome; Injury pattern; Injury profiles; Multiple injuries; Road traffic crashes; Road trauma","data mining; human; injury; multiple trauma; police; statistical model; traffic accident; Accidents, Traffic; Data Mining; Humans; Logistic Models; Multiple Trauma; Police; Wounds and Injuries",Article,Scopus,2-s2.0-85118896766
"Park J.Y., Mistur E., Kim D., Mo Y., Hoefer R.","57200016003;57195278776;57329779300;56335840300;7003370691;","Toward human-centric urban infrastructure: Text mining for social media data to identify the public perception of COVID-19 policy in transportation hubs",2022,"Sustainable Cities and Society","76",,"103524","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118896710&doi=10.1016%2fj.scs.2021.103524&partnerID=40&md5=4bfce806b8785c61a43cbbea6163157e","The COVID-19 pandemic has made transportation hubs vulnerable to public health risks. In response, policies using nonpharmaceutical interventions have been implemented, changing the way individuals interact within these facilities. However, the impact of building design and operation on policy efficacy is not fully discovered, making it critical to investigate how these policies are perceived and complied in different building spaces. Therefore, we investigate the spatial drivers of user perceptions and policy compliance in airports. Using text mining, we analyze 103,428 Google Maps reviews of 64 major hub airports in the US to identify representative topics of passenger concerns in airports (i.e., Staff, Shop, Space, and Service). Our results show that passengers express having positive experiences with Staff and Shop, but neutral or negative experiences with Service and Space, which indicates how building design has impacted policy compliance and the vulnerability of health crises. Furthermore, we discuss the actual review comments with respect to 1) spatial design and planning, 2) gate assignment and operation, 3) airport service policy, and 4) building maintenance, which will construct the foundational knowledge to improve the resilience of transportation hubs to future health crises. © 2021 Elsevier Ltd","COVID-19 policy; Human-building interations; Human-centric urban infrastructures; Public perception; Social media data; Text mining","Airports; Architectural design; Data mining; Risk perception; Social networking (online); Urban transportation; Building design; COVID-19 policy; Human-building interation; Human-centric; Human-centric urban infrastructure; Policy compliance; Public perception; Social media datum; Transportation hubs; Urban infrastructure; Health risks",Article,Scopus,2-s2.0-85118896710
"Bahonar E., Chahardowli M., Ghalenoei Y., Simjoo M.","57328973200;56394450700;57329551600;14042635100;","New correlations to predict oil viscosity using data mining techniques",2022,"Journal of Petroleum Science and Engineering","208",,"109736","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118894686&doi=10.1016%2fj.petrol.2021.109736&partnerID=40&md5=58ab1de419fea7c22758969bb559e7ed","Oil viscosity is used in any fluid transport calculation in both subsurface and surface conditions. It is possible to determine oil viscosity from laboratory measurements or empirical correlations. However, laboratory measurements are not always available; and empirical correlations suffer from low accuracy. This work implements data mining algorithms to suggest a new correlation for oil viscosity calculation in a wide pressure range from subsurface to surface conditions. First, a scatter plot matrix is applied to analyze 1950 PVT experimental data points from Iranian oil reservoirs. Therefore, the most correlated parameters for predicting oil viscosity are determined. Next, 75% of data points are randomly selected to train the models. The remaining data, i.e., 25% of data points, are utilized to investigate the accuracy of the developed correlation. Then, a symbolic regression analysis is performed in all pressure ranges, i.e., dead oil viscosity, bubble point oil viscosity, below and above the bubble point pressure. Finally, a new oil viscosity correlation is proposed. The statistical and graphical evaluations reveal that the new correlation outperformed the previously proposed correlations by lowering average absolute errors. It can be concluded that the presented correlation improves the prediction accuracy in all pressure ranges. Consequently, it is inferred from the results that machine learning could provide a highly accurate prediction for oil viscosity in all pressure regions. Overall, the proposed correlation could be used to calculate oil viscosity in all pressure ranges with reasonable accuracy. © 2021 Elsevier B.V.","Correlation; Data mining; Machine learning; Oil viscosity; Scatter matrix plot; Symbolic regression","Data mining; Forecasting; Machine learning; Matrix algebra; Petroleum reservoir engineering; Regression analysis; Viscosity; Correlation; Datapoints; Matrix plots; New correlations; Oil viscosity; Pressure ranges; Scatter matrix; Scatter matrix plot; Surface conditions; Symbolic regression; Research laboratories",Article,Scopus,2-s2.0-85118894686
"Azimjonov J., Özmen A.","57219830959;35766528200;","Vision-based vehicle tracking on highway traffic using bounding-box features to extract statistical information",2022,"Computers and Electrical Engineering","97",,"107560","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118893992&doi=10.1016%2fj.compeleceng.2021.107560&partnerID=40&md5=4fac1a3c8d60e7b5186402c8fd93124a","In this study, a new bounding-box based vehicle tracking algorithm is presented to extract statistical information in the highway traffic. A novel shaking filter and a new voting approach are employed in the vehicle detection and tracking phases to reduce camera shaking effects that cause misdetection, misclassification, and mistracking. The algorithm uses image streams captured via ordinary cameras and successfully classifies and determines the time-dependent vehicle trajectory through successive frames. The novel tracking algorithm utilizes the Euclidean distance-based similarity measure to associate the detected vehicles in successive frames, or it predicts the next state of vehicles using the linear/polynomial prediction functions obtained from the trajectory vector when the observed vehicles are disappeared from the scene due to the occlusion or illusion problems. The comparative vehicle counting results show that the proposed algorithm performs approximately 7% better than the Kalman filter-based tracker. © 2021 Elsevier Ltd","Highway management; Vehicle counting; Vehicle detection; Vehicle tracking","Data mining; Kalman filters; Statistics; Tracking (position); Vehicles; Bounding-box; Detection and tracking; Highway management; Highway traffic; Statistical information; Tracking algorithm; Vehicle counting; Vehicles detection; Vision based; Voting approach; Cameras",Article,Scopus,2-s2.0-85118893992
"Tang T., Wu Y., Wu Y., Yu L., Li Y.","57194608001;57264291900;14828402200;36616058600;57198892029;","VideoModerator: A Risk-Aware Framework for Multimodal Video Moderation in E-Commerce",2022,"IEEE Transactions on Visualization and Computer Graphics","28","1",,"846","856",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118679051&doi=10.1109%2fTVCG.2021.3114781&partnerID=40&md5=7416a75bcc3c3d9d1bc14d5a18dfe46b","Video moderation, which refers to remove deviant or explicit content from e-commerce livestreams, has become prevalent owing to social and engaging features. However, this task is tedious and time consuming due to the difficulties associated with watching and reviewing multimodal video content, including video frames and audio clips. To ensure effective video moderation, we propose VideoModerator, a risk-Aware framework that seamlessly integrates human knowledge with machine insights. This framework incorporates a set of advanced machine learning models to extract the risk-Aware features from multimodal video content and discover potentially deviant videos. Moreover, this framework introduces an interactive visualization interface with three views, namely, a video view, a frame view, and an audio view. In the video view, we adopt a segmented timeline and highlight high-risk periods that may contain deviant information. In the frame view, we present a novel visual summarization method that combines risk-Aware features and video context to enable quick video navigation. In the audio view, we employ a storyline-based design to provide a multi-faceted overview which can be used to explore audio content. Furthermore, we report the usage of VideoModerator through a case scenario and conduct experiments and a controlled user study to validate its effectiveness. © 1995-2012 IEEE.","e-commerce livestreaming; video moderation; video visualization","Data visualization; Feature extraction; Flow visualization; Learning systems; Risk assessment; Video recording; E- commerces; E-commerce livestreaming; Features extraction; Multi-modal; Risk aware; Task analysis; Video contents; Video moderation; Video visualization; Visual analytics; Data mining; article; electronic commerce; human; human experiment; machine learning; videorecording",Article,Scopus,2-s2.0-85118679051
"Xie T., Ma Y., Kang J., Tong H., Maciejewski R.","57211999467;57094300000;57204946315;7201360533;23987550600;","FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models",2022,"IEEE Transactions on Visualization and Computer Graphics","28","1",,"368","377",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118674851&doi=10.1109%2fTVCG.2021.3114850&partnerID=40&md5=946daea2d517cb6024958e2506a8019f","Graph mining is an essential component of recommender systems and search engines. Outputs of graph mining models typically provide a ranked list sorted by each item's relevance or utility. However, recent research has identified issues of algorithmic bias in such models, and new graph mining algorithms have been proposed to correct for bias. As such, algorithm developers need tools that can help them uncover potential biases in their models while also exploring the impacts of correcting for biases when employing fairness-aware algorithms. In this paper, we present FairRankVis, a visual analytics framework designed to enable the exploration of multi-class bias in graph mining algorithms. We support both group and individual fairness levels of comparison. Our framework is designed to enable model developers to compare multi-class fairness between algorithms (for example, comparing PageRank with a debiased PageRank algorithm) to assess the impacts of algorithmic debiasing with respect to group and individual fairness. We demonstrate our framework through two usage scenarios inspecting algorithmic fairness. © 1995-2012 IEEE.","fairness; Graph ranking; visual analytics","Data structures; Job analysis; Learning algorithms; Machine learning; Search engines; Visualization; Algorithmics; Fairness; Graph mining; Graph ranking; Machine learning algorithms; Page ranks; Recent researches; Task analysis; Visual analytics; Data mining; algorithm; article; controlled study; mining",Article,Scopus,2-s2.0-85118674851
"Al Duhayyim M., Marzouk R., Al-Wesabi F.N., Alrajhi M., Hamza M.A., Zamani A.S.","57204360566;35749022100;57211901842;57212620403;57223407265;57295189700;","An improved evolutionary algorithm for data mining and knowledge discovery",2022,"Computers, Materials and Continua","71","1",,"1233","1247",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118656731&doi=10.32604%2fcmc.2022.021652&partnerID=40&md5=b6ea3b79065d1989eca4ea008ce7f27a","Recent advancements in computer technologies for data processing, collection, and storage have offered several chances to improve the abilities in production, services, communication, and researches.Datamining (DM) is an interdisciplinary field commonly used to extract useful patterns fromthe data. At the same time, educational data mining (EDM) is a kind of DM concept, which finds use in educational sector. Recently, artificial intelligence (AI) techniques can be used formining a large amount of data.At the same time, in DM, the feature selection process becomes necessary to generate subset of fea- tures and can be solved by the use of metaheuristic optimization algorithms. With this motivation, this paper presents an improved evolutionary algorithm based feature subsets election with neuro-fuzzy classification (IEAFSS-NFC) for data mining in the education sector. The presented IEAFSS-NFC model involves data pre-processing, feature selection, and classification. Besides, the Chaotic Whale Optimization Algorithm (CWOA) is used for the selection of the highly related feature subsets to accomplish improved classification results. Then, Neuro-Fuzzy Classification (NFC) technique is employed for the classification of education data. The IEAFSS-NFCmodel is tested against a benchmark Student Performance DataSet from the UCI repository. The simulation outcome has shown that the IEAFSS-NFC model is superior to other methods. © 2022 Tech Science Press. All rights reserved.","Classification; Educational data mining; Feature selection; Whale optimization","Benchmarking; Classification (of information); Data handling; Digital storage; Education computing; Feature extraction; Fuzzy inference; Fuzzy systems; Genetic algorithms; Computer technology; Data collection; Data mining and knowledge discovery; Educational data mining; Feature subset; Features selection; Neuro fuzzy classification; Optimisations; Optimization algorithms; Whale optimization; Data mining",Article,Scopus,2-s2.0-85118656731
"Tovanich N., Soulie N., Heulot N., Isenberg P.","57223325391;36941563400;56648058700;22957485100;","MiningVis: Visual Analytics of the Bitcoin Mining Economy",2022,"IEEE Transactions on Visualization and Computer Graphics","28","1",,"868","878",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118648346&doi=10.1109%2fTVCG.2021.3114821&partnerID=40&md5=933be1be42717dbf0bb1737e5620cfb0","We present a visual analytics tool, MiningVis, to explore the long-term historical evolution and dynamics of the Bitcoin mining ecosystem. Bitcoin is a cryptocurrency that attracts much attention but remains difficult to understand. Particularly important to the success, stability, and security of Bitcoin is a component of the system called mining. Miners are responsible for validating transactions and are incentivized to participate by the promise of a monetary reward. Mining pools have emerged as collectives of miners that ensure a more stable and predictable income. MiningVis aims to help analysts understand the evolution and dynamics of the Bitcoin mining ecosystem, including mining market statistics, multi-measure mining pool rankings, and pool hopping behavior. Each of these features can be compared to external data concerning pool characteristics and Bitcoin news. In order to assess the value of MiningVis, we conducted online interviews and insight-based user studies with Bitcoin miners. We describe research questions tackled and insights made by our participants and illustrate practical implications for visual analytics systems for Bitcoin mining. © 1995-2012 IEEE.","Bitcoin; Bitcoin mining; mining pools; pool hopping; Visual analytics","Bitcoin; Ecosystems; Lakes; Miners; Visualization; Analytic tools; Bitcoin mining; Hardware; Historical evolutions; Mining pool; Monetary rewards; Pool hopping; Security; Visual analytics; Data mining; adult; article; economic aspect; ecosystem; human; interview; miner",Article,Scopus,2-s2.0-85118648346
"Wang X., He J., Jin Z., Yang M., Wang Y., Qu H.","57211999463;57226405632;57209393570;57210798405;57193775768;7101947159;","M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis",2022,"IEEE Transactions on Visualization and Computer Graphics","28","1",,"802","812",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118648186&doi=10.1109%2fTVCG.2021.3114794&partnerID=40&md5=b2511882daa23799c337019d3d82c88f","Multimodal sentiment analysis aims to recognize people's attitudes from multiple communication channels such as verbal content (i.e., text), voice, and facial expressions. It has become a vibrant and important research topic in natural language processing. Much research focuses on modeling the complex intra- and inter-modal interactions between different communication channels. However, current multimodal models with strong performance are often deep-learning-based techniques and work like black boxes. It is not clear how models utilize multimodal information for sentiment predictions. Despite recent advances in techniques for enhancing the explainability of machine learning models, they often target unimodal scenarios (e.g., images, sentences), and little research has been done on explaining multimodal models. In this paper, we present an interactive visual analytics system, M2 Lens, to visualize and explain multimodal models for sentiment analysis. M2 Lens provides explanations on intra- and inter-modal interactions at the global, subset, and local levels. Specifically, it summarizes the influence of three typical interaction types (i.e., dominance, complement, and conflict) on the model predictions. Moreover, M2 Lens identifies frequent and influential multimodal features and supports the multi-faceted exploration of model behaviors from language, acoustic, and visual modalities. Through two case studies and expert interviews, we demonstrate our system can help users gain deep insights into the multimodal models for sentiment analysis. © 1995-2012 IEEE.","explainable machine learning; Multimodal models; sentiment analysis","Behavioral research; Character recognition; Communication channels (information theory); Data mining; Image enhancement; Learning algorithms; Modal analysis; Sentiment analysis; Visual languages; Communications channels; Computational modelling; Explainable machine learning; Facial Expressions; Modal interactions; Multi-modal; Multimodal models; Predictive models; Sentiment analysis; Deep learning; article; deep learning; human; human experiment; interview; language; prediction",Article,Scopus,2-s2.0-85118648186
"Zhao Z., Xu P., Scheidegger C., Ren L.","57202160802;36505244500;14018768500;57193058523;","Human-in-The-loop Extraction of Interpretable Concepts in Deep Learning Models",2022,"IEEE Transactions on Visualization and Computer Graphics","28","1",,"780","790",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118641506&doi=10.1109%2fTVCG.2021.3114837&partnerID=40&md5=c6d1c45d9c472196925f2462f4643908","The interpretation of deep neural networks (DNNs) has become a key topic as more and more people apply them to solve various problems and making critical decisions. Concept-based explanations have recently become a popular approach for post-hoc interpretation of DNNs. However, identifying human-understandable visual concepts that affect model decisions is a challenging task that is not easily addressed with automatic approaches. We present a novel human-in-The-Ioop approach to generate user-defined concepts for model interpretation and diagnostics. Central to our proposal is the use of active learning, where human knowledge and feedback are combined to train a concept extractor with very little human labeling effort. We integrate this process into an interactive system, ConceptExtract. Through two case studies, we show how our approach helps analyze model behavior and extract human-friendly concepts for different machine learning tasks and datasets and how to use these concepts to understand the predictions, compare model performance and make suggestions for model refinement. Quantitative experiments show that our active learning approach can accurately extract meaningful visual concepts. More importantly, by identifying visual concepts that negatively affect model performance, we develop the corresponding data augmentation strategy that consistently improves model performance. © 1995-2012 IEEE.","Deep Neural Network; Explainable AI; Model Interpretation; Visual Data Exploration","Data mining; Data visualization; Job analysis; Visualization; Computational modelling; Deep learning; Explainable AI; Human-in-the-loop; Model interpretations; Modeling performance; Predictive models; Task analysis; Visual concept; Visual data exploration; Deep neural networks; computer graphics; human; machine learning; Computer Graphics; Deep Learning; Humans; Machine Learning; Neural Networks, Computer",Article,Scopus,2-s2.0-85118641506
"Foo L.-K., Chua S.-L., Ibrahim N.","36088844300;57326771800;57326771900;","Attribute weighted naïve bayes classifier",2022,"Computers, Materials and Continua","71","1",,"1945","1957",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118635790&doi=10.32604%2fcmc.2022.022011&partnerID=40&md5=e7f9e3acb17e74b5f9221c67faf1f7a8","The naïve Bayes classifier is one of the commonly used data mining methods for classification. Despite its simplicity, naïve Bayes is effective and computationally efficient. Although the strong attribute independence assumption in the naïve Bayes classifier makes it a tractable method for learning, this assumption may not hold in real-world applications. Many enhancements to the basic algorithm have been proposed in order to alleviate the violation of attribute independence assumption. While these methods improve the classification performance, they do not necessarily retain the mathematical structure of the naïve Bayes model and some at the expense of computational time. One approach to reduce the naïveté of the classifier is to incorporate attribute weights in the conditional probability. In this paper, we proposed a method to incorporate attribute weights to naïve Bayes. To evaluate the performance of our method, we used the public benchmark datasets.We compared our method with the standard naïve Bayes and baseline attribute weighting methods. Experimental results show that our method to incorporate attribute weights improves the classification performance compared to both standard naïve Bayes and baseline attribute weighting methods in terms of classification accuracy and F1, especially when the independence assumption is strongly violated, which was validated using the Chi-square test of independence. © 2022 Tech Science Press. All rights reserved.","Attribute weighting; Classification; Information gain; Kullback-Leibler; Naïve Bayes","Bayesian networks; Benchmarking; Classifiers; Data mining; Statistical tests; Attribute independence assumption; Attribute weight; Attribute weighting; Classification performance; Information gain; Kullback-Leibler; Naive bayes; Naive Bayes classifiers; Weighted naive Bayes; Weighting methods; Classification (of information)",Article,Scopus,2-s2.0-85118635790
"Chen Z., Ye S., Chu X., Xia H., Zhang H., Qu H., Wu Y.","57189640527;57221854492;57221842843;57326788300;56979605300;7101947159;14828402200;","Augmenting Sports Videos with VisCommentator",2022,"IEEE Transactions on Visualization and Computer Graphics","28","1",,"824","834",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118620803&doi=10.1109%2fTVCG.2021.3114806&partnerID=40&md5=3a531712014d7b07ed42796ab2d72147","Visualizing data in sports videos is gaining traction in sports analytics, given its ability to communicate insights and explicate player strategies engagingly. However, augmenting sports videos with such data visualizations is challenging, especially for sports analysts, as it requires considerable expertise in video editing. To ease the creation process, we present a design space that characterizes augmented sports videos at an element-level (what the constituents are) and clip-level (how those constituents are organized). We do so by systematically reviewing 233 examples of augmented sports videos collected from TV channels, teams, and leagues. The design space guides selection of data insights and visualizations for various purposes. Informed by the design space and close collaboration with domain experts, we design VisCommentator, a fast prototyping tool, to eases the creation of augmented table tennis videos by leveraging machine learning-based data extractors and design space-based visualization recommendations. With VisCommentator, sports analysts can create an augmented video by selecting the data to visualize instead of manually drawing the graphical marks. Our system can be generalized to other racket sports (e.g., tennis, badminton) once the underlying datasets and models are available. A user study with seven domain experts shows high satisfaction with our system, confirms that the participants can reproduce augmented sports videos in a short period, and provides insightful implications into future improvements and opportunities. © 1995-2012 IEEE.","Augmented Sports Videos; Intelligent Design Tool; Sports visualization; Storytelling; Video-based Visualization","Data visualization; Learning systems; Sports; Visualization; Augmented sport video; Design spaces; Design tool; Domain experts; Intelligent design tool; Intelligent designs; Sport video; Sports visualizations; Storytelling; Video-based visualization; Data mining; adult; article; badminton; drawing; female; human; human experiment; machine learning; male; satisfaction; systematic review; tennis; videorecording",Article,Scopus,2-s2.0-85118620803
"Wang Q., Mazor T., Harbig T.A., Cerami E., Gehlenborg N.","57226562040;35082403300;57210260586;57205565604;57208464039;","ThreadStates: State-based Visual Analysis of Disease Progression",2022,"IEEE Transactions on Visualization and Computer Graphics","28","1",,"238","247",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118603278&doi=10.1109%2fTVCG.2021.3114840&partnerID=40&md5=bbae15a41f6bfd4a6f01cb45cb22aadc","A growing number of longitudinal cohort studies are generating data with extensive patient observations across multiple timepoints. Such data offers promising opportunities to better understand the progression of diseases. However, these observations are usually treated as general events in existing visual analysis tools. As a result, their capabilities in modeling disease progression are not fully utilized. To fill this gap, we designed and implemented ThreadStates, an interactive visual analytics tool for the exploration of longitudinal patient cohort data. The focus of ThreadStates is to identify the states of disease progression by learning from observation data in a human-in-The-loop manner. We propose a novel Glyph Matrix design and combine it with a scatter plot to enable seamless identification, observation, and refinement of states. The disease progression patterns are then revealed in terms of state transitions using Sankey-based visualizations. We employ sequence clustering techniques to find patient groups with distinctive progression patterns, and to reveal the association between disease progression and patient-level features. The design and development were driven by a requirement analysis and iteratively refined based on feedback from domain experts over the course of a 10-month design study. Case studies and expert interviews demonstrate that ThreadStates can successively summarize disease states, reveal disease progression, and compare patient groups. © 1995-2012 IEEE.","Disease Progression; Sequence Visualization; State Identification","Curricula; Data mining; Data visualization; Hidden Markov models; Iterative methods; Analysis tools; Cohort studies; Disease progression; Hidden-Markov models; Modelling disease; Sequence visualization; State based; State identification; Visual analysis; Visual analytics; Visualization; cluster analysis; computer graphics; disease exacerbation; human; longitudinal study; statistical analysis; Cluster Analysis; Computer Graphics; Data Interpretation, Statistical; Disease Progression; Humans; Longitudinal Studies",Article,Scopus,2-s2.0-85118603278
"Chen J., Guo X., Gan W., Chen C.-M., Ding W., Chen G.","55337226900;57222721844;57223999014;35072405000;57193448087;57217183830;","On-shelf utility mining from transaction database",2022,"Engineering Applications of Artificial Intelligence","107",,"104516","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118596969&doi=10.1016%2fj.engappai.2021.104516&partnerID=40&md5=02fcb368b6caade2fbf80734d69d3e38","As an important technique for dealing with transaction database in the field of data mining, utility-driven mining can be used to discover useful patterns (i.e., itemsets, sequences) which have a high utility. However, it has a bias towards the item/object combinations which have more exhibition period since they have more opportunity to generate a high utility. To address this, the on-shelf time period of items need to be considered, thus on-shelf utility mining (OSUM) can be applied in the application which is more closer to the actual situation. Currently several models have been proposed to deal with the OSUM problem, but they still suffer from the requirement that it needs to maintain a massive candidates in memory and to scan database many times. In this paper, we propose two effective one-phase algorithms named OSUMI (On-Shelf Utility Mining from transactIon database) and OSUMI+ (the improve version of OSUMI). Both OSUMI and OSUMI+ search all itemsets as a set-enumeration tree and discover the on-shelf itemsets with high utility in a more practical way. More precisely, in order to avoid the problems of high memory consumption, two algorithms apply some properties of the concept of on-shelf utility. Besides, two upper-bounds named subtree utility and local utility are applied to early filter out unpromising patterns and then prune the search space. Finally, an extensive experimental study on several real on-shelf datasets shows that our proposed algorithms can be significantly faster than the state-of-the-art algorithm. © 2021 Elsevier Ltd","High-utility itemset; On-shelf; Time period; Utility mining","Database systems; Filtration; High utility itemsets; Itemset; Memory consumption; Mining problems; On-shelf; Set enumeration tree; Time-periods; Transaction database; Useful patterns; Utility mining; Data mining",Article,Scopus,2-s2.0-85118596969
"Cui W., Wang J., Huang H., Wang Y., Lin C.-Y., Zhang H., Zhang D.","24587013600;57324647100;57202043133;57189645214;23009406500;54586310400;55717568500;","A Mixed-Initiative Approach to Reusing Infographic Charts",2022,"IEEE Transactions on Visualization and Computer Graphics","28","1",,"173","183",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118595457&doi=10.1109%2fTVCG.2021.3114856&partnerID=40&md5=c3bd7c0dfc304917bfb96ddb58903cf6","Infographic bar charts have been widely adopted for communicating numerical information because of their attractiveness and memorability. However, these infographics are often created manually with general tools, such as PowerPoint and Adobe Illustrator, and merely composed of primitive visual elements, such as text blocks and shapes. With the absence of chart models, updating or reusing these infographics requires tedious and error-prone manual edits. In this paper, we propose a mixed-initiative approach to mitigate this pain point. On one hand, machines are adopted to perform precise and trivial operations, such as mapping numerical values to shape attributes and aligning shapes. On the other hand, we rely on humans to perform subjective and creative tasks, such as changing embellishments or approving the edits made by machines. We encapsulate our technique in a PowerPoint add-in prototype and demonstrate the effectiveness by applying our technique on a diverse set of infographic bar chart examples. © 1995-2012 IEEE.","Automatic visualization; Graphic design; Infographics; Reusable templates","Data visualization; Semantics; Visualization; Automatic visualization; Bar chart; Graphic design; Image color analysis; Infographic; Mixed-initiative; Numerical information; Powerpoint; Reusable templates; Shape; Data mining",Article,Scopus,2-s2.0-85118595457
"Akram M., Ali G., Alcantud J.C.R.","57204684197;57197782381;57217136608;","Attributes reduction algorithms for m-polar fuzzy relation decision systems",2022,"International Journal of Approximate Reasoning","140",,,"232","254",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118576024&doi=10.1016%2fj.ijar.2021.10.005&partnerID=40&md5=a328b4dc3fec006b702a7947730de84b","Nowadays, attribute reduction has become a significant topic in relation decision systems. Their applications come from different domains of the computer sciences, including machine learning, data mining and pattern recognition, which often involve a large number of attributes in data. Several attribute reduction methods are presented in the literature in order to help solving decision-making problems efficiently. A common characterization for these approaches is still missing, that is, although attribute reduction methods of relation decision systems and fuzzy relation decision systems exist, a common generalization for them is still missing. This study presents a systematic discussion of attribute reduction based on m-polar fuzzy (mF, in short) relation systems and mF relation decision systems, which are respective extensions of fuzzy relation systems and fuzzy relation decision systems. This study provides mathematical results on the attribute reduction algorithms based upon mF relation systems and mF relation decision systems. Both are explained with numerical examples. The resulting algorithms permit to reinterpret the upshots of traditional reduction methods, providing them with larger generality and unification abilities. Afterwards, two real-life applications of the proposed attribute reduction approaches prove their validity and feasibility. Finally, the attribute reduction methods developed here are compared with some existing approaches to show their reliability. © 2021 The Author(s)","Attribute reduction; mF relation decision system; mF relation system; Redundant attributes","Decision making; Pattern recognition; Attribute reduction; Attribute reduction algorithm; Attribute reduction method; Decision systems; Decision-making problem; Different domains; Fuzzy relations; Mf relation decision system; Mf relation system; Still missing; Data mining",Article,Scopus,2-s2.0-85118576024
"Wang J., Cao J., Yuan S., Zhou X., Zhou P.","57221361691;55551562700;57217146611;57221187658;57095322700;","Spatiotemporal Synergistic Ensemble Deep Learning Method and Its Application to S-Wave Velocity Prediction",2022,"IEEE Geoscience and Remote Sensing Letters","19",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118563594&doi=10.1109%2fLGRS.2021.3123812&partnerID=40&md5=61ed82f6b6918ec5726af3fd994a1978","S-wave velocity (Vs) data are crucial in prestack seismic inversion, lithology interpretation, and fluid identification. However, most drilling wells currently lack Vs data. The use of traditional data-driven Vs prediction methods has limitations as these methods fail to adaptively extract effective features from logging signals and cannot fully account for the trends with depth. In this letter, we propose a novel spatiotemporal synergistic ensemble deep learning method, that is, the ensemble convolutional bidirectional memory network (ECBMN), to predict Vs. The ECBMN is built by integrating a convolutional neural network and a bidirectional long short-term memory network to leverage their complementary strengths and achieve a spatiotemporal synergistic learning. Using this method, Vs can be estimated from a series of input log data by accounting for the correlation of different log series and Vs, the variation trend, and context information with depth. The results of these applications to field data show that the proposed method can provide more reliable and accurate predictions compared with the existing methods, thereby demonstrating its potential for accurately estimating Vs from log data. © 2004-2012 IEEE.","Bidirectional long short-term memory (BLSTM); convolutional neural network (CNN); ensemble learning (EL); S-wave velocity prediction; spatiotemporal synergy","Acoustic wave velocity; Brain; Convolution; Estimation; Forecasting; Lithology; Long short-term memory; Remote sensing; Shear waves; Wave propagation; Bidirectional long short-term memory; Convolutional neural network; Ensemble learning; Features extraction; Geoscience and remote sensing; Geosciences; Predictive models; Remote-sensing; S-wave velocity predictions; Spatiotemporal phenomenon; Spatiotemporal synergy; Data mining; algorithm; artificial neural network; correlation; detection method; S-wave; velocity profile; wave velocity",Article,Scopus,2-s2.0-85118563594
"Wu J., Liu D., Guo Z., Xu Q., Wu Y.","57221513454;57188753761;57221694486;57221685918;14828402200;","TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports",2022,"IEEE Transactions on Visualization and Computer Graphics","28","1",,"835","845",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118562120&doi=10.1109%2fTVCG.2021.3114832&partnerID=40&md5=cefdd9cd5521169c29c22aaeec274eb4","Event sequence mining is often used to summarize patterns from hundreds of sequences but faces special challenges when handling racket sports data. In racket sports (e.g., tennis and badminton), a player hitting the ball is considered a multivariate event consisting of multiple attributes (e.g., hit technique and ball position). A rally (i.e., a series of consecutive hits beginning with one player serving the ball and ending with one player winning a point) thereby can be viewed as a multivariate event sequence. Mining frequent patterns and depicting how patterns change over time is instructive and meaningful to players who want to learn more short-Term competitive strategies (i.e., tactics) that encompass multiple hits. However, players in racket sports usually change their tactics rapidly according to the opponent's reaction, resulting in ever-changing tactic progression. In this work, we introduce a tailored visualization system built on a novel multivariate sequence pattern mining algorithm to facilitate explorative identification and analysis of various tactics and tactic progression. The algorithm can mine multiple non-overlapping multivariate patterns from hundreds of sequences effectively. Based on the mined results, we propose a glyph-based Sankey diagram to visualize the ever-changing tactic progression and support interactive data exploration. Through two case studies with four domain experts in tennis and badminton, we demonstrate that our system can effectively obtain insights about tactic progression in most racket sports. We further discuss the strengths and the limitations of our system based on domain experts' feedback. © 1995-2012 IEEE.","Multivariate Event Sequence; Progression Analysis; Sequential Pattern Mining; Sports Analytics","Data handling; Data mining; Data visualization; Job analysis; Sports; Domain experts; Event sequence; Multivariate event sequence; Progression analysis; Sequence mining; Sequential-pattern mining; Sport analytic; Sports data; Task analysis; Visual analytics; Visualization; algorithm; biomechanics; computer graphics; racquet sport; tennis; Algorithms; Biomechanical Phenomena; Computer Graphics; Racquet Sports; Tennis",Article,Scopus,2-s2.0-85118562120
"Mravik M., Vetriselvi T., Venkatachalam K., Sarac M., Bacanin N., Adamovic S.","57323881100;56338169000;57213001965;36462597700;37028223900;49561059300;","Diabetes prediction algorithm using recursive ridge regression l2",2022,"Computers, Materials and Continua","71","1",,"457","471",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118546994&doi=10.32604%2fcmc.2022.020687&partnerID=40&md5=077f425d9756d37d1493259e5d67f5c3","At present, the prevalence of diabetes is increasing because the human body cannot metabolize the glucose level. Accurate prediction of diabetes patients is an important research area.Many researchers have proposed techniques to predict this disease through data mining and machine learning methods. In prediction, feature selection is a key concept in preprocessing. Thus, the features that are relevant to the disease are used for prediction. This condition improves the prediction accuracy. Selecting the right features in the whole feature set is a complicated process, and many researchers are concentrating on it to produce a predictive model with high accuracy. In this work, a wrapper-based feature selection method called recursive feature elimination is combined with ridge regression (L2) to form a hybrid L2 regulated feature selection algorithm for overcoming the overfitting problem of data set. Overfitting is a major problem in feature selection, where the new data are unfit to the model because the training data are small. Ridge regression is mainly used to overcome the overfitting problem. The features are selected by using the proposed feature selection method, and random forest classifier is used to classify the data on the basis of the selected features. This work uses the Pima Indians Diabetes data set, and the evaluated results are compared with the existing algorithms to prove the accuracy of the proposed algorithm. The accuracy of the proposed algorithm in predicting diabetes is 100%, and its area under the curve is 97%. The proposed algorithm outperforms existing algorithms. © 2022 Tech Science Press. All rights reserved.","Feature selection; Machine learning; Random forest; Recursive feature elimination; Ridge regression","Classification (of information); Data mining; Decision trees; Forecasting; Machine learning; Random forests; Regression analysis; Accurate prediction; Feature selection methods; Features selection; Glucose level; Human bodies; Over fitting problem; Prediction algorithms; Random forests; Recursive feature elimination; Ridge regression; Feature extraction",Article,Scopus,2-s2.0-85118546994
"Wu Q., Miao S., Chai Z., Guo G.","35088967000;57324644900;17344881300;7402768270;","Fine-Grained Image Classification with Global Information and Adaptive Compensation Loss",2022,"IEEE Signal Processing Letters","29",,,"36","40",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118542180&doi=10.1109%2fLSP.2021.3123453&partnerID=40&md5=55c3de6f867053ecd02d4b74b673e398","Fine-grained image classification differs from traditional image classification in that the former needs to divide subclasses under a basic level of categories. Previous works always focus on how to locate discriminative parts of objects, but we find that the global and background information of objects neglected by them is also valuable in some situations. This letter proposes a method to combine the global information and discriminative parts information of objects to do classification, which includes three modules: (1) Activation map based crop-erase module localizes objects while avoiding localization bias due to excessive bias of the network to learn one discriminative part. (2) Part attention module helps learning discriminative part features of objects. (3) Two-level fusion module gives consideration to the global and local information of objects and some potentially effective background information. Meanwhile, we propose an adaptive compensation loss to distinguish easily confused categories. Experiments show that our method achieves state-of-the-art performance on three open benchmarks. © 1994-2012 IEEE.","Adaptive compensation loss; attention; feature fusion; fine-grained image classification; global information","Benchmarking; Classification (of information); Crops; Feature extraction; Image classification; Job analysis; Adaptive compensation; Adaptive compensation loss; Attention; Classification algorithm; Features extraction; Features fusions; Fine grained; Fine-grained image classification; Global informations; Images classification; Task analysis; Data mining",Article,Scopus,2-s2.0-85118542180
"Cai L., Wang H., Jiang F., Zhang Y., Peng Y.","57189595322;57210467973;57203845188;57226432168;35176363400;","A new clustering mining algorithm for multi-source imbalanced location data",2022,"Information Sciences","584",,,"50","64",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118511730&doi=10.1016%2fj.ins.2021.10.029&partnerID=40&md5=48293ae01250d7c0428d68c5c692e664","In the era of big data, clustering based on multi-source data fusion has become a hot topic in data mining field. Existing studies mainly focus on fusion models and algorithms of data sets in the same domain, but few studies consider imbalanced data sets from different domains. Furthermore, studies on imbalanced data sets mostly focus on classification and less on clustering problems. Therefore, we propose a novel clustering algorithm for mining fused location data. This algorithm can deal with imbalanced data sets with large density differences, find clusters generated by the minority class data, and reduce the time complexity of the clustering process. Since current evaluation indices are not suitable for evaluating clustering results of imbalanced data sets, we present a new comprehensive evaluation metric used in the clustering validity judgment. Urban hotspots mining is used as an example, and the effectiveness of the proposed method is validated using GPS trajectory data from the transport domain and check-in data from the social network. The experimental results demonstrate that the performance of the proposed algorithm outperforms that of the state-of-the-art clustering algorithms, and it can simultaneously discover urban hotspots formed by the majority and minority class data. © 2021 Elsevier Inc.","Adaptive grid partition; Clustering algorithm; Clustering quality; Data fusion; Imbalanced data set; Urban hotspots","Classification (of information); Cluster analysis; Data fusion; Data mining; Urban transportation; Adaptive grid partition; Adaptive grids; Clustering quality; Fusion model; Grid partition; Hot topics; Imbalanced dataset; Location data; Multi-Sources; Multisource data; Clustering algorithms",Article,Scopus,2-s2.0-85118511730
"Yin A.L., Guo W.L., Sholle E.T., Rajan M., Alshak M.N., Choi J.J., Goyal P., Jabri A., Li H.A., Pinheiro L.C., Wehmeyer G.T., Weiner M., Safford M.M., Campion T.R., Cole C.L., Weill Cornell COVID-19 Data Abstraction Consortium","57210809489;57323257100;57202978780;57322904600;57210211295;57199099880;57322904700;57217125373;57323257200;57322658100;57217125812;57322781100;57322540900;8432457900;22033428000;","Comparing automated vs. manual data collection for COVID-specific medications from electronic health records",2022,"International Journal of Medical Informatics","157",,"104622","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118504534&doi=10.1016%2fj.ijmedinf.2021.104622&partnerID=40&md5=2bd468d85ea1feea37d8338334f36991","Introduction: Data extraction from electronic health record (EHR) systems occurs through manual abstraction, automated extraction, or a combination of both. While each method has its strengths and weaknesses, both are necessary for retrospective observational research as well as sudden clinical events, like the COVID-19 pandemic. Assessing the strengths, weaknesses, and potentials of these methods is important to continue to understand optimal approaches to extracting clinical data. We set out to assess automated and manual techniques for collecting medication use data in patients with COVID-19 to inform future observational studies that extract data from the electronic health record (EHR). Materials and methods: For 4,123 COVID-positive patients hospitalized and/or seen in the emergency department at an academic medical center between 03/03/2020 and 05/15/2020, we compared medication use data of 25 medications or drug classes collected through manual abstraction and automated extraction from the EHR. Quantitatively, we assessed concordance using Cohen's kappa to measure interrater reliability, and qualitatively, we audited observed discrepancies to determine causes of inconsistencies. Results: For the 16 inpatient medications, 11 (69%) demonstrated moderate or better agreement; 7 of those demonstrated strong or almost perfect agreement. For 9 outpatient medications, 3 (33%) demonstrated moderate agreement, but none achieved strong or almost perfect agreement. We audited 12% of all discrepancies (716/5,790) and, in those audited, observed three principal categories of error: human error in manual abstraction (26%), errors in the extract-transform-load (ETL) or mapping of the automated extraction (41%), and abstraction-query mismatch (33%). Conclusion: Our findings suggest many inpatient medications can be collected reliably through automated extraction, especially when abstraction instructions are designed with data architecture in mind. We discuss quality issues, concerns, and improvements for institutions to consider when crafting an approach. During crises, institutions must decide how to allocate limited resources. We show that automated extraction of medications is feasible and make recommendations on how to improve future iterations. © 2021 Elsevier B.V.","Chart review; COVID-19; Data quality; Electronic health record; Research data repositories","Abstracting; Automation; Clinical research; Data mining; eHealth; Errors; Extraction; Automated extraction; Chart review; COVID-19; Data collection; Data extraction; Data quality; Data repositories; Observational research; Research data; Research data repository; Records management; angiotensin receptor antagonist; antibiotic agent; dipeptidyl carboxypeptidase inhibitor; diuretic agent; hydroxychloroquine; hydroxymethylglutaryl coenzyme A reductase inhibitor; hypertensive factor; immunoglobulin; inotropic agent; lopinavir plus ritonavir; nonsteroid antiinflammatory agent; oseltamivir; proteinase inhibitor; remdesivir; steroid; tocilizumab; adolescent; adult; aged; Article; child; comparative study; controlled study; coronavirus disease 2019; electronic health record; emergency ward; female; hospital patient; human; infant; information processing; interrater reliability; major clinical study; male; medical record review; medical research; observational study; outpatient; qualitative analysis; retrospective study; Severe acute respiratory syndrome coronavirus 2; total quality management",Article,Scopus,2-s2.0-85118504534
"Lyu Y., Wang Z., Ren Z., Ren P., Chen Z., Liu X., Li Y., Li H., Song H.","57323261100;57253673900;53985046100;56181249600;15749859500;57241489600;57205346557;36175959900;57224727334;","Improving legal judgment prediction through reinforced criminal element extraction",2022,"Information Processing and Management","59","1","102780","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118481517&doi=10.1016%2fj.ipm.2021.102780&partnerID=40&md5=75831b489e6cc026d81f9f52f0fd5d8b","Legal text mining is targeted at automatically analyzing the texts in the legal domain by employing various natural language processing techniques and has attracted enormous attention from the NLP community. As one of the most crucial tasks of legal text mining, Legal Judgment Prediction (LJP) aims to automatically predict judgment results (e.g., applicable law articles, charges, and terms of penalty) according to fact descriptions on law cases and becomes a promising application of artificial intelligence techniques. Unfortunately, ambiguous fact descriptions and law articles often appear due to a great number of shared words and legal concepts. Prior works are proposed to partially address these problems, focusing on introducing additional attributes to distinguish similar fact descriptions, or differentiating confusing law articles by grouping and distilling law articles. However, existing works still face two severe challenges: (1) indistinguishable fact descriptions with different criminals and targets and (2) misleading law articles with highly similar TF–IDF representations, both of which lead to serious misjudgments for the LJP task. In this paper, we present a novel reinforcement learning (RL) based framework, named Criminal Element Extraction Network (CEEN), to handle above challenges simultaneously. In CEEN, we propose four types of discriminative criminal elements, including the criminal, target, intentionality, and criminal behavior. To discriminate ambiguous fact descriptions, an reinforcement learning based extractor is designed to accurately locate elements for different cases. To enhance law article predictions, distinctive element representations are constructed for each type of criminal element. Finally, with the input of element representations, a multi-task predictor is adopted for the judgment predictions. Experimental results on real-world datasets show that extracting criminal elements is highly useful for predicting the judgment results. © 2021 Elsevier Ltd","Attention mechanism; Criminal element extraction; Legal judgment prediction; Legal text mining; Neural networks; Reinforcement learning","Crime; Data mining; Extraction; Forecasting; Learning algorithms; Natural language processing systems; Attention mechanisms; Criminal element extraction; Element extraction; Fact descriptions; Legal judgements; Legal judgment prediction; Legal text mining; Legal texts; Neural-networks; Text-mining; Reinforcement learning",Article,Scopus,2-s2.0-85118481517
"Matsuda K., Aoyagi S.","56067839400;7202461258;","Sparse autoencoder–based feature extraction from TOF–SIMS image data of human skin structures",2022,"Analytical and Bioanalytical Chemistry","414","2",,"1177","1186",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118429416&doi=10.1007%2fs00216-021-03744-3&partnerID=40&md5=6b6b8ffcfcbce89d6fee853cc9fd554f","Time-of-flight secondary ion mass spectrometry (TOF–SIMS) is a useful and versatile tool for surface analysis, enabling detailed compositional information to be obtained for the surfaces of diverse samples. Furthermore, in the case of two- or three-dimensional imaging, the measurement sensitivity in the higher molecular weight range can be improved by using a cluster ion source, thus further enriching the TOF–SIMS information. Therefore, appropriate analytical methods are required to interpret this TOF–SIMS data. This study explored the capabilities of a sparse autoencoder, a feature extraction method based on artificial neural networks, to process TOF–SIMS image data. The sparse autoencoder was applied to TOF–SIMS images of human skin keratinocytes to extract the distribution of endogenous intercellular lipids and externally penetrated drugs. The results were compared with those obtained using principal component analysis (PCA) and multivariate curve resolution (MCR), which are conventionally used for extracting features from TOF–SIMS data. This confirmed that the sparse autoencoder matches, and often betters, the feature extraction performance of conventional methods, while also offering greater flexibility. Graphical abstract: [Figure not available: see fulltext.]. © 2021, Springer-Verlag GmbH Germany, part of Springer Nature.","Artificial neural network; Autoencoder; Bioimaging; Feature extraction; TOF–SIMS","Data mining; Extraction; Feature extraction; Ion sources; Learning systems; Neural networks; Principal component analysis; Surface analysis; Auto encoders; Bio-imaging; Compositional information; Features extraction; Human skin; Image data; Mass spectrometry data; Skin structure; Time of flight secondary ion mass spectrometry; Versatile tools; Secondary ion mass spectrometry; diagnostic imaging; human; mass spectrometry; procedures; secondary ion mass spectrometry; skin; Humans; Mass Spectrometry; Skin; Spectrometry, Mass, Secondary Ion",Article,Scopus,2-s2.0-85118429416
"Pearce C., McLeod A., Supple J., Gardner K., Proposch A., Ferrigi J.","7101785869;8309403500;57320307500;37063277700;57215021081;57207250553;","Responding to COVID-19 with real-time general practice data in Australia",2022,"International Journal of Medical Informatics","157",,"104624","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118338040&doi=10.1016%2fj.ijmedinf.2021.104624&partnerID=40&md5=1ba6184808a8cfee9b3991458c67ec5f","Introduction: As SARS-CoV-2 spread around the world, Australia was no exception. Part of the Australian response was a robust primary care approach, involving changes to care models (including telehealth) and the widespread use of data to inform the changes. This paper outlines how a large primary care database responded to provide real-time data to inform policy and practice. Simply extracting the data is not sufficient. Understanding the data is. The POpulation Level Analysis and Reporting (POLAR) program is designed to use GP data for multiple objectives and is built on a pre-existing engagement framework established over a fifteen-year period. Initially developed to provide QA activities for general practices and population level data for General Practice support organisations, the POLAR platform has demonstrated the critical ability to design and deploy real-time data analytics solutions during the COVID-19 pandemic for a variety of stakeholders including state and federal government agencies. Methods: The system extracts and processes data from over 1,300 general practices daily. Data is de-identified at the point of collection and encrypted before transfer. Data cleaning for analysis uses a variety of techniques, including Natural Language Processing and coding of free text information. The curated dataset is then distilled into several analytic solutions designed to address specific areas of investigation of interest to various stakeholders. One such analytic solution was a model we created that used multiple data inputs to rank patient geographic areas by the likelihood of a COVID-19 outbreak. The model utilised pathology ordering, COVID-19 related diagnoses, indication of COVID-19 related concern (via progress notes) and also incorporated state based actual confirmed case figures. Results: Using the methods described, we were able to deliver real-time data feeds to practices, Primary Health Networks (PHN) and other agencies. In addition, we developed a COVID-19 geographic risk stratification based on local government areas (LGAs) to pro-actively inform the primary care response. Providing PHNs with a list of geographic priority hotspots allowed for better targeting and response of Personal Protective Equipment allocation and pop-up clinic placement. Conclusions: The program summarised here demonstrates the ability of a well-designed system underpinned by accurate and reliable data, to respond in real-time to a rapidly evolving public health emergency in a way which supports and enhances the health system response. © 2021","COVID-19; Data quality; Predictive tools; Primary care; SNOMED","Data Analytics; Decision trees; Diseases; Medical computing; Natural language processing systems; Population statistics; Protective clothing; SARS; Analytic solution; Australia; COVID-19; Data quality; Population levels; Predictive tools; Primary care; Real- time; Real-time data; SNOMED; Data mining; dipeptidyl carboxypeptidase inhibitor; hydroxychloroquine; ivermectin; Article; Australia; coronavirus disease 2019; data quality; general practice; government; human; natural language processing; pandemic; patient monitoring; primary medical care; risk assessment; Systematized Nomenclature of Medicine; time series analysis; Australia; epidemiology; Australia; COVID-19; General Practice; Humans; Pandemics; SARS-CoV-2",Article,Scopus,2-s2.0-85118338040
"Xing H., Qin H., Luo S., Dai P., Xu L., Cheng X.","25923226000;57320320300;56372997000;56790439400;35207681600;57320320400;","Spectrum sensing in cognitive radio: A deep learning based model",2022,"Transactions on Emerging Telecommunications Technologies","33","1","e4388","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118326709&doi=10.1002%2fett.4388&partnerID=40&md5=580f4a3d5ace21aca080dec25644cbcb","Spectrum sensing is an efficient technology for addressing the shortage of spectrum resources. Widely used methods usually employ model-based features as the test statistics, such as energies and eigenvalues, ignoring the temporal correlation aspect. Deep learning based methods have the potential to focus on various aspects, including temporal correlation. However, the existing ones are not good at capturing the temporal correlation features from spectrum data as traditional convolutional neural network (CNN) and long short-term memory network (LSTM) are used for feature extraction. Traditional CNNs were not designed to capture the global temporal correlations from time series data. Standard LSTM captures the temporal correlations based on the data collected from previous time slots only and cannot emphasize some important parts of a time series. This article proposes a data-driven deep learning based model to classify the received raw signals automatically, where the received signal data is considered time-series data. The proposed deep neural network (DNN) model is mainly featured with 1-dimensional convolutional neural network (1D CNN), bidirectional long short-term memory network (BiLSTM), and self-attention (SA). The 1D CNN and BiLSTM are responsible for extracting the local features and global correlations from the time series data, and BiLSTM could extract sufficient features in opposite directions. The SA layer enables the classifier network to emphasize those important features obtained by BiLSTM. The simulation results demonstrate that our model performs better than a number of existing DNN models in terms of the probabilities of missed detection and false alarm, especially when the signal to noise ratio is low. Moreover, the impacts of the modulation scheme and sample length on the detection performance are studied. © 2021 John Wiley & Sons, Ltd.",,"Brain; Cognitive radio; Convolution; Convolutional neural networks; Data mining; Deep neural networks; Eigenvalues and eigenfunctions; Signal to noise ratio; Statistical tests; Time series; Convolutional neural network; Efficient technology; Learning Based Models; Memory network; Model-based OPC; Neural network model; Spectra's; Spectrum sensing; Temporal correlations; Time-series data; Long short-term memory",Article,Scopus,2-s2.0-85118326709
"Chou T.-H., Chang S.-C.","8209454000;57221705999;","The RFM model analysis for VIP customer: A case study of golf clothing brand",2022,"International Journal of Knowledge Management","18","1",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118311121&doi=10.4018%2fIJKM.290025&partnerID=40&md5=85559282614f1dab9dc10bead7b81c12","Numerous firms accumulate large quantities of data or transactions after importing information systems and services, which leads to troubles with data procedure. Firms also have demands to find customers’ information from large datasets and to understand how to develop marketing strategies accurately to adjust their operational methods. Therefore, this study proposed customer ranking combined big data process based on the RFM model (recency, frequency, monetary) to develop a recommendation algorithm using an association rule, which finds greater recommendation to promote operational effects of firms. The authors adjust the weight of potential information to perform the customer ranking, which is conducted by using agglomerate hierarchical clustering. Finally, they present the recommendation by the association rule for each customer level. The datasets in this study use actual sales data; therefore, they are authentic and have been practically applied. The metrics of evaluation showed that the recommended system this study proposes is highly accurate. Copyright © 2022, IGI Global.","Association Rule; Data Mining; Recommended System; RFM Model","Association rules; Large dataset; Sales; Sports; Case-studies; Customer information; Large datasets; Marketing strategy; Modeling analyzes; Operational methods; Process-based; Recommendation algorithms; Recommended systems; RFM model; Data mining",Article,Scopus,2-s2.0-85118311121
"Markiewicz M., Koperwas J.","57211092909;23389701600;","Evaluation platform for ddm algorithms with the usage of nonuniform data distribution strategies",2022,"International Journal of Information Technologies and Systems Approach","15","1",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118311032&doi=10.4018%2fIJITSA.290000&partnerID=40&md5=6cc5bfcb1c5b0c1c709c1a63dedf3401","Huge amounts of data are collected in numerous independent data storage facilities around the world. However, how the data is distributed between physical locations remains unspecified. Downloading all of the data for the purpose of processing is undesirable and sometimes even impossible. Various methods have been proposed for performing data mining tasks, but the main problem is the lack of an objective strategy for comparing them. The authors present current research on a novel evaluation platform for distributed data mining (DDM) algorithms. The proposed platform opens up a new field to evaluate algorithms in terms of the quality of the results, transfer used, and speed, but also for the use of a non-uniform data distribution among independent nodes during algorithm evaluation. This work introduces a ‘data partitioning strategy’ term referring to a specific, not necessarily uniform data distribution. A brief evaluation for three clustering algorithms is also reported showing the usability and simplicity of identifying differences in processing with the use of the platform. Copyright © 2022, IGI Global.","Algorithm Evaluation; Benchmarking Platform; Classification; Clustering; Data Partitioning Strategies; Distributed Data Mining; Distributed Processing","Classification (of information); Data mining; Digital storage; Quality control; Algorithm evaluation; Benchmarking platforms; Clusterings; Data distribution; Data-partitioning strategy; Distributed data mining; Distributed processing; Distribution strategies; Evaluation platforms; Non uniform data; Clustering algorithms",Article,Scopus,2-s2.0-85118311032
"Dong J., Zhao W., Wang S.","57316935000;57316935100;57317660700;","Multiscale Context Aggregation Network for Building Change Detection Using High Resolution Remote Sensing Images",2022,"IEEE Geoscience and Remote Sensing Letters","19",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118255378&doi=10.1109%2fLGRS.2021.3121094&partnerID=40&md5=88f89b1e62b149973c72421d1c4cc3e4","The existing methods of building change detection (CD) using remote sensing (RS) images are still deficient in handling scale variation and class imbalance problems, indicating a decrease in the robustness of small-object detection and pseudo-change information. Thus, a novel building CD framework called the multiscale context aggregation network (MSCANet) is proposed. The high-resolution network is integrated into the feature extracting stage to maintain high-resolution representations throughout the whole process. Then, multiscale context information is aggregated using a scale-aware feature pyramid module (FPM). Recognition performance can be improved from discriminant feature representation learning by using a channel-spatial attention module. Furthermore, a class-balanced loss is proposed to reduce the impact of class imbalance in long-tail datasets. Experimental results from using the LEVIR-CD and SZTAKI AirChange benchmark datasets prove the superiority of the MSCANet over the other baseline methods, with improved maximum F1 scores of 5.28 and 8.47, respectively. © 2004-2012 IEEE.","Change detection (CD); class imbalance; deep convolutional network; optical remote sensing (RS) images","Data mining; Object detection; Remote sensing; Change detection; Class imbalance; Convolutional networks; Deep convolutional network; Features extraction; Optical remote sensing; Optical remote sensing image; Remote sensing images; Remote-sensing; Spatial resolution; Feature extraction; data set; image analysis; image processing; remote sensing",Article,Scopus,2-s2.0-85118255378
"Zhou X., Cheng H., Chen F.","57316358600;57315917100;55497713000;","A connection access mechanism of distributed network based on block chain",2022,"International Journal of Circuits, Systems and Signal Processing","16",,,"224","231",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118196101&partnerID=40&md5=ae92488dc9eda696e2693daf90fd6ce1","Cross-border payment optimization technology based on block chain has become a hot spot in the industry. The traditional method mainly includes the block feature detection method, the fuzzy access method, the adaptive scheduling method, which perform related feature extraction and quantitative regression analysis on the collected distributed network connection access data, and combine the fuzzy clustering method to optimize the data access design, and realize the group detection and identification of data in the block chain. However, the traditional method has a large computational overhead for distributed network connection access, and the packet detection capability is not good. This paper constructs a statistical sequence model of adaptive connection access data to extract the descriptive statistical features of the distributed network block chain adaptive connection access data similarity. The performance of the strategy retrieval efficiency in the experiment is tested based on the strategy management method. The experiment performs matching query tests on the test sets of different query sizes. The different parameters for error rate and search delay test are set to evaluate the impact of different parameters on retrieval performance. The calculation method of single delay is the total delay or the total number of matches. The optimization effect is mainly measured by the retrieval delay of the strategy in the strategy management contract; the smaller the delay, the higher the execution efficiency, and the better the retrieval optimization effect. © 2022, North Atlantic University Union NAUN. All rights reserved.","Big data; Block chain; Connection access; Distributed network; Internet finance","Data mining; Efficiency; Feature extraction; Fuzzy clustering; Regression analysis; Access mechanism; Block-chain; Connection access; Cross-border; Distributed networks; Internet finance; Network connection; Network-based; Optimization effects; Strategy management; Big data",Article,Scopus,2-s2.0-85118196101
"Roy K.K., Moon M.H.H., Rahman M.M., Ahmed C.F., Leung C.K.-S.","57223733857;57226308527;57199763102;24337639400;7402612526;","Mining weighted sequential patterns in incremental uncertain databases",2022,"Information Sciences","582",,,"865","896",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118191051&doi=10.1016%2fj.ins.2021.10.010&partnerID=40&md5=a9334815e0b938d945a358f38a378313","Due to the rapid development of science and technology, the importance of imprecise, noisy, and uncertain data is increasing at an exponential rate. Thus, mining patterns in uncertain databases have drawn the attention of researchers. Moreover, frequent sequences of items from these databases need to be discovered for meaningful knowledge with great impact. In many real cases, weights of items and patterns are introduced to find interesting sequences as a measure of importance. Hence, a constraint of weight needs to be handled while mining sequential patterns. Besides, due to the dynamic nature of databases, mining important information has become more challenging. Instead of mining patterns from scratch after each increment, incremental mining algorithms utilize previously mined information to update the result immediately. Several algorithms exist to mine frequent patterns and weighted sequences from incremental databases. However, these algorithms are confined to mine the precise ones. Therefore, in this work, we develop an algorithm to mine frequent sequences in an uncertain database in this work. Furthermore, we propose two new techniques for mining when the database is incremental. Extensive experiments have been conducted for performance evaluation. The analysis showed the efficiency of our proposed framework. © 2021 Elsevier Inc.","Data mining; Incremental database; Sequential pattern mining; Uncertain database; Weighted sequential patterns","Database systems; Development of science and technologies; Exponential rates; Frequent sequences; Imprecise data; Incremental database; Noisy data; Sequential-pattern mining; Uncertain database; Uncertain datas; Weighted sequential pattern; Data mining",Article,Scopus,2-s2.0-85118191051
"Ugwoke P.O., Bakpo F.S., Udanor C.N., Okoronkwo M.C.","56786561200;35794996800;57188661710;57211204570;","A framework for monitoring movements of pandemic disease patients based on GPS trajectory datasets",2022,"Wireless Networks","28","1",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118169345&doi=10.1007%2fs11276-021-02819-4&partnerID=40&md5=b803b696642419c820b424ddafa6083e","The rapid spread of contagious diseases poses a colossal threat to human existence. Presently, the emergence of coronavirus COVID-19 which has rightly been declared a global pandemic resulting in so many deaths, confusion as well as huge economic losses is a challenge. It has been suggested by the World Health Organization (WHO) in conjunction with different Government authorities of the world and non-governmental organizations, that efforts to curtail the COVID-19 pandemic should rely principally on measures such as social distancing, identification of infected persons, tracing of possible contacts as well as effective isolation of such person(s) for subsequent medical treatment. The aim of this study is to provide a framework for monitoring Movements of Pandemic Disease Patients and predicting their next geographical locations given the recent trend of infected COVID-19 patients absconding from isolation centres as evidenced in the Nigerian case. The methodology for this study, proposes a system architecture incorporating GPS (Global Positioning System) and Assisted-GPS technologies for monitoring the geographical movements of COVID-19 patients and recording of their movement Trajectory Datasets on the assumption that they are assigned with GPS-enabled devices such as smartphones. Accordingly, fifteen (15) participants (patients) were selected for this study based on the criteria of residency and business activity location. The ensuing participants movements generated 157, 218 Trajectory datasets during a period of 3 weeks. With this dataset, mining of the movement trace, Stay Points (hot spots), relationships, and the prediction of the next probable geographical location of a COVID-19 patient was realized by the application of Artificial Intelligence (AI) and Data Mining techniques such as supervised Machine Learning (ML) algorithms (i.e., Multiple Linear Regression (MLR), k-Nearest Neighbor (kNN), Decision Tree Regression (DTR), Random Forest Regression (RFR), Gradient Boosting Regression (GBR), and eXtreme Gradient Boosting regression(XGBR) as well as density-based clustering methods (i.e., DBSCAN) for the computation of Stay Points (hot spots) of COVID-19 patient. The result of this study showed clearly that it is possible to determine the Stay Points (hot spots) of a COVID-19 patient. In addition, this study demonstrated the possibility of predicting the next probable geographical location of a COVID-19 patient. Correspondingly, Six Machine Learning models (i.e., MLR, kNN, DTR, RFR, GBR, and XGBR) were compared for efficiency, in determining the next probable location of a COVID-19 patient. The result showed that the DTR model performed better compared to other models (i.e., MLR, kNN, RFR, GBR, XGBR) based on four evaluation matrices (i.e., ACCURACY, MAE, MSE, and R2) used. It is recommended that less developed Countries consider adopting this framework as a policy initiative for implementation at this burgeoning phase of COVID-19 infection and beyond. The same applies to the developed Countries. There is indication that GPS Trajectory dataset and Machine Learning algorithms as applied in this paper, appear to possess the potential of performing optimally in a real-life situation of monitoring a COVID-19 patient. This paper is unique given its ability to predict the next probable location of a COVID-19 patient. In the review of extant literature, prediction of the next probable location of a COVID-19 patient was not in evidence using the same Machine Learning algorithms. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Covid-19; Data mining; Machine learning; Movement; Pandemic; Trajectory dataset","Data mining; Decision trees; Diseases; Forecasting; Global positioning system; Linear regression; Location; Logistic regression; Losses; Nearest neighbor search; Supervised learning; Covid-19; Decision tree regression; Geographical locations; Gradient boosting; Hotspots; Machine learning algorithms; Movement; Multiple linear regressions; Pandemic; Trajectory dataset; Trajectories",Article,Scopus,2-s2.0-85118169345
"Wang W., Yang W., Shi W., Li C.-Q.","57195276286;57001294200;57205734071;57208269195;","Modeling of Corrosion Pit Growth in Buried Steel Pipes",2022,"Journal of Materials in Civil Engineering","34","1","04021386","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118140346&doi=10.1061%2f%28ASCE%29MT.1943-5533.0004023&partnerID=40&md5=985457e88714664f5e579e0414b6d2a9","A review of the published literature shows that although intensive research has been conducted on corrosion of steel in soils, accurate prediction of corrosion pit growth remains a serious challenge. This study aimed to develop a new model for predicting the maximum pit depth in buried steel pipes and verify it with data obtained from actual corrosion measurements in the field. A method was developed that integrates advanced data mining techniques, shape descriptive modeling, and evolutionary polynomial regression in deriving the underlying relationships between corrosion-influencing factors and model parameters. It was found that the area effect ψ is closely related to the ion content of the soil (Na+,K+, Mg2+, Cl-, and SO42-) and that the correlation between model parameter Ku and soil properties varies among soils with different aeration. It was also found that the developed predictive model exhibits superiority over existing models in quantifying multiple-phase corrosion growth. The proposed method can effectively correlate model parameters with the main contributing factors, which enables researchers and practitioners to accurately predict the maximum corrosion pit depth in buried steel pipes. © 2021 American Society of Civil Engineers.","Buried pipes; Field data; Maximum corrosion pit; Modeling; Shape prescriptive","Data mining; Forecasting; Kurchatovium; Pipeline corrosion; Pitting; Steel corrosion; Steel pipe; Underground corrosion; Buried pipes; Corrosion pits; Field data; Intensive research; Maximum corrosion pit; Modeling; Modeling of corrosions; Modeling parameters; Pit growth; Shape prescriptive; Soils; accuracy assessment; buried structure; corrosion; data mining; literature review; model test; pipeline; quantitative analysis; soil mechanics; soil property; soil-structure interaction; steel structure",Article,Scopus,2-s2.0-85118140346
"Stadlmann C., Zehetner A.","57222616321;57208480484;","Comparing AI-Based and Traditional Prospect Generating Methods",2022,"Journal of Promotion Management","28","2",,"160","174",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118119262&doi=10.1080%2f10496491.2021.1987973&partnerID=40&md5=27d4441984f91ef2c9e2f66bf4f7cd3e","This contribution deals with a comparison of one AI based data mining tool and two traditional approaches utilized to collect and interpret data for prospect generation. Traditional prospect generation methods, like manual web search or using purchased data from external providers may involve high costs and efforts and are subject to failures and waste through outdated and untargeted data. In contrast, AI based methods claim to provide better results at lower costs. Based on a real case, the authors compare effects of these three prospect generation methods. AI based data mining tools compensate for some weaknesses of other methods, especially because they do not need pre-defined selection criteria which might bias the results. In addition, they involve less effort from the researcher. However, the results in generating concrete prospects may be still weaker than with traditional methods if web crawling activities are influenced by underlying databases. For academic research in the field of prospect generation, this study provides a fact-based comparison of approaches. Implications for businesses include the advice to combine methods rather than to rely on a single approach. The time available for research and the complexity of the target market have an influence on the selection of the prospect generation approach. © 2021 The Author(s). Published with license by Taylor & Francis Group, LLC.","commercial data; data mining; method comparison; prospecting; systematic manual web research",,Article,Scopus,2-s2.0-85118119262
"Zhang W., Zhao L.","57212225953;57191225344;","The track, hotspot and frontier of international hyperspectral remote sensing research 2009–2019—— A bibliometric analysis based on SCI database",2022,"Measurement: Journal of the International Measurement Confederation","187",,"110229","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118098545&doi=10.1016%2fj.measurement.2021.110229&partnerID=40&md5=e829775fe19a0caa4636ffe127d7450f","Hyperspectral remote sensing is widely used in earth observation and environmental survey. However, the research on the development, applications and hot spots of hyperspectral remote sensing is very limited. Metadata taken from 4122 literature records is used to visualize the state of hyperspectral remote sensing research. The records were published between 2009 and 2019, and Citespace software is employed to visualize key data about the research contained on the SCI and SSCI platform. It has the following findings. In recent 10 years, hyperspectral remote sensing related research literature has shown a trend of rapid growth. There are more publications in China, the United States, Germany, and other countries, and the publishing institutions are mostly concentrated in the universities of various countries. Remote Sensing of Environment and IEEE journals are authoritative journals in this field. Anatoly Gitelson, Camps-valls, Asner GP and other authors have made important contributions to basic research. The highly cited documents of hyperspectral remote sensing are distributed in the research directions of spectral imaging technology, support vector machine, and spectral data classification, and 12 research clusters are formed, such as vegetation research, data feature extraction, SVM classification, and de-mixing algorithm. The research hotspots in this field are mainly in image classification, algorithm model, spectral resolution/reflectance and vegetation analysis. The research frontiers include spectral characteristics and reflectance, end element extraction, radar and data dimension reduction, UAV and so on. The development of hyperspectral remote sensing technology promotes the cross research in the fields of environment, ecology, chemistry, computer and so on. © 2021 Elsevier Ltd","Hot topics; Hyperspectral remote sensing; Image classification; Mapping knowledge; SVM; Unmixing algorithms","Classification (of information); Clustering algorithms; Data mining; Extraction; Information retrieval systems; Remote sensing; Spectroscopy; Support vector machines; Vegetation; Bibliometrics analysis; Earth observations; Environmental surveys; Hot topics; Hotspots; Hyperspectral remote sensing; Images classification; Observation surveys; SVM; Unmixing algorithms; Image classification",Article,Scopus,2-s2.0-85118098545
"Cordas C.M., Nguyen G.-S., Valério G.N., Jønsson M., Söllner K., Aune I.H., Wentzel A., Moura J.J.G.","15135081100;57221736854;57212444955;57312893500;57312676100;57312011400;6603572029;7102993187;","Discovery and characterization of a novel Dyp-type peroxidase from a marine actinobacterium isolated from Trondheim fjord, Norway",2022,"Journal of Inorganic Biochemistry","226",,"111651","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117939484&doi=10.1016%2fj.jinorgbio.2021.111651&partnerID=40&md5=9f21d2d1991279b127a19f11fe6e6518","A new dye-decolorizing peroxidase (DyP) was discovered through a data mining workflow based on HMMER software and profile Hidden Markov Model (HMM) using a dataset of 1200 genomes originated from a Actinobacteria strain collection isolated from Trondheim fjord. Instead of the conserved GXXDG motif known for Dyp-type peroxidases, the enzyme contains a new conserved motif EXXDG which has been not reported before. The enzyme can oxidize an anthraquinone dye Remazol Brilliant Blue R (Reactive Blue 19) and other phenolic compounds such as ferulic acid, sinapic acid, caffeic acid, 3-methylcatechol, dopamine hydrochloride, and tannic acid. The acidic pH optimum (3 to 4) and the low temperature optimum (25 °C) were confirmed using both biochemical and electrochemical assays. Kinetic and thermodynamic parameters associated with the catalytic redox center were attained by electrochemistry. © 2021 Elsevier Inc.","Actinobacteria; Catalysis; Data mining; Dye-decolorizing peroxidase; Electrochemistry; Profile hidden Markov model","amino acid; anthraquinone; brilliant blue; caffeic acid; chloride; ferulic acid; guaiacol; peroxidase; phenol derivative; sinapic acid; tannin; bacterial protein; peroxidase; Actinobacteria; Article; bacterium isolation; catalysis; controlled study; cyclic voltammetry; data mining; electrochemical analysis; electrochemistry; enzyme analysis; gene synthesis; hidden Markov model; low temperature; nonhuman; Norway; pH; physical chemistry; reduction (chemistry); sequence alignment; temperature dependence; workflow; aquatic species; chemistry; enzymology; estuary; genetics; isolation and purification; Actinobacteria; Aquatic Organisms; Bacterial Proteins; Estuaries; Norway; Peroxidase",Article,Scopus,2-s2.0-85117939484
"Fleming J.K., Katayev A., Moorer C.M., Ward-Jeffries D.A., Terrell C.L.","55419569300;35744718800;6505910704;57310372600;57310907800;","Development of nation-wide reference intervals using an indirect method and harmonized assays",2022,"Clinical Biochemistry","99",,,"20","59",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117854116&doi=10.1016%2fj.clinbiochem.2021.10.001&partnerID=40&md5=4eadceae03273a285062100c46fb70ef","Objectives: For many years, clinical laboratories have either verified or estimated reference intervals (RI) for laboratory tests. Those calculations have largely been performed by direct sampling analysis of ostensibly healthy individuals or by post-analysis biochemical screening. Recently however, indirect calculations have come to the forefront as an IFCC endorsed method by using normal and abnormal patient data. Design and methods: Using a large database of patient test results from Laboratory Corporation of America, age and gender based RIs, inclusive of neonatal, pediatric, and geriatric populations, were determined using a modified indirect method of Hoffmann, and represent a diverse population distributed across the United States from a nation-wide system of laboratories and is unbiased with respect to age, gender, race or geography. Results: The tabulation of RIs using big data by an indirect method represent 72 M patient test results. The table includes 266 individual analytes consisting of approximately 2,700 age categories, including tests across multiple medical disciplines. Conclusions: To our knowledge, this is the largest collection of RIs that were calculated by an indirect method representing clinical chemistry, endocrinology, coagulation, and hematology analytes that have been derived with very powerful “Ns” for each age bracket. This process provides more robust RIs and allows for the determination of pediatric and geriatric RIs that would otherwise be difficult to obtain using traditional direct RI determinations. © 2021 The Canadian Society of Clinical Chemists","Big Data; Computerization; Data Mining; Harmonization; Indirect Method; Reference Intervals","article; automation; big data; child; clinical chemistry; controlled study; data mining; endocrinology; female; gender; geography; hematology; human; human tissue; major clinical study; male; newborn; race; reference value; United States; adolescent; adult; aged; bioassay; factual database; infant; middle aged; preschool child; reference value; theoretical model; very elderly; Adolescent; Adult; Aged; Aged, 80 and over; Big Data; Biological Assay; Child; Child, Preschool; Databases, Factual; Female; Humans; Infant; Infant, Newborn; Laboratories, Clinical; Male; Middle Aged; Models, Theoretical; Reference Values",Article,Scopus,2-s2.0-85117854116
"Roy S.K., Kar P., Hong D., Wu X., Plaza A., Chanussot J.","56784776500;57224741790;56108179600;56108085300;7006613644;6602159365;","Revisiting Deep Hyperspectral Feature Extraction Networks via Gradient Centralized Convolution",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117769765&doi=10.1109%2fTGRS.2021.3120198&partnerID=40&md5=719737d331da310259f3209424864a19","The hyperspectral images are composed of a variety of textures across the different bands which increase the spectral similarity and make it difficult to predict the pixel-wise labels without inducing additional complexity at the feature level. To extract robust and discriminative features from the different regions of land cover, the hyperspectral research community is still seeking such type of convolutions which can efficiently deal with fine-grained texture information during the feature extraction phase, which often overlook this aspect by vanilla convolution. To overcome the above shortcoming, this article proposes a generalized gradient centralized 3D convolution (G2C-Conv3D) operation, which is a weighted combination between the vanilla and gradient centralized 3D convolutions (GC-Conv3D) to extract both the intensity-level semantic information and gradient-level information. This can be easily plugged into the existing HSI feature extraction networks to boost the performance of accurate prediction for land-cover types. To validate the feasibility of the proposed G2C-Conv3D, we have considered the existing CNN3D, MS3DNet, ContextNet, and SSRN feature extraction models and as well as CAE3D, VAE3D, and SAE3D autoencoder (AE) networks, respectively. All these networks are embedded with G2C-Conv3D convolution to implement both generalized gradient centralized feature extraction networks (G2C-FE) and generalized gradient centralized AE networks (G2C-AE) for fine-grained spectral-spatial feature learning. In addition, G2C-Conv2D is also considered with few networks. The extensive experiments are conducted on four most widely used hyperspectral datasets i.e., IP, KSC, UH, and UP, respectively, and compared with the nine methods. The results demonstrate that the proposed G2C-Conv3D can effectively enhance the feature learning ability of the existing networks and both the qualitative and quantitative results show the superiority and effectiveness of the proposed G2C-Conv3D. The source codes will be publicly available at https://github.com/danfenghong/G2C-Conv3D-HSI. © 1980-2012 IEEE.","Convolutional neural networks (CNNs); generalized gradient centralized 3D convolution (G2C-Conv3D); gradient centralized 3D convolution (GC-Conv3D); hyperspectral images (HSIs); ResNets","Data mining; Extraction; Feature extraction; Hyperspectral imaging; Job analysis; Learning systems; Neural networks; Semantics; Spectroscopy; Three dimensional displays; Centralised; Convolutional neural network; Features extraction; Generalized gradient centralized 3d convolution (G2C-conv3d); Generalized gradients; Gradient centralized 3d convolution (GC-conv3d); Hyperspectral image; Resnet; Task analysis; Three-dimensional display; Convolution; algorithm; detection method; network analysis; satellite imagery",Article,Scopus,2-s2.0-85117769765
"ElZahed M., Marzouk M.","57310114300;7005098184;","Smart archiving of energy and petroleum projects utilizing big data analytics",2022,"Automation in Construction","133",,"104005","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117756570&doi=10.1016%2fj.autcon.2021.104005&partnerID=40&md5=a2bf87466c74a2c400497770ba514c93","Complexity of the construction projects vary by the domain and type of the project. Due to the interaction between different disciplines and parties, Energy and Petroleum Projects (EPP) are considered among the most complex. This complexity produces a dense network of interrelated documents which are produced to cover the various aspects and details of the project. Analyzing this network which is composed of unstructured data is required to gain insights from old data, this task traditionally requires experience, knowledge, and awareness about the existence of the required data. Accordingly, a key asset of any company is the knowledge accumulated over the time from various projects. This research proposes a framework that enables salvaging traditional archives and include its data in searchable databases to increase the efficiency of archiving such accumulated data without affecting the normal workflow of companies, and hence decrease the manhours expenditure and reduce the time of archiving while not affecting the accuracy of the outcome. Due to the large diversity of the EPP projects, the research focuses on five main commodities which are selected based on the frequency of their existence in projects in addition to their monetary value as the main data to be stored which are Tanks, Air Coolers, Pumps, Generators and Distributed Control Systems (DCS). The key attributes of each commodity are identified based on technical questionnaires with technical specialists to act as the basis for building the proposed framework. The proposed framework integrates four modules to provide a complete solution to the problem, by implementing data mining techniques while harnessing the power of big data analytics tools to transform the existent unstructured data into structured data in the form of smart archives which can then be used to support ongoing business processes. © 2021 Elsevier B.V.","Archiving mechanisms; Big data; Data mining; Energy and petroleum projects; Optical character recognition","Advanced Analytics; Big data; Complex networks; Data Analytics; Distributed parameter control systems; Gasoline; Optical character recognition; Surveys; Archiving mechanism; Construction projects; Dense network; Energy; Energy and petroleum project; Gain insight; ITS data; Searchable database; Unstructured data; Work-flows; Data mining",Article,Scopus,2-s2.0-85117756570
"Ahlrichs J., Wenninger S., Wiethe C., Häckel B.","57219027467;57219027920;57218288747;36462447600;","Impact of socio-economic factors on local energetic retrofitting needs - A data analytics approach",2022,"Energy Policy","160",,"112646","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117725224&doi=10.1016%2fj.enpol.2021.112646&partnerID=40&md5=f45bd9edea33bcb204279664fbbc1bb8","Despite great efforts to increase energetic retrofitting rates in the residential building stock, greenhouse gas emissions are still too high to counteract climate change. One barrier is that policy measures are mostly national and do not address local differences. Even though there is plenty of research on instruments to overcome general barriers of energetic retrofitting, literature does not consider differences in local peculiarities. Thus, this paper aims to provide guidance for policy-makers by deriving evidence from over 19 million Energy Performance Certificates and socio-economic data from England, Scotland, and Wales. We find that building archetypes with their respective energetic retrofitting needs differ locally and that socio-economic factors show a strong correlation to the buildings’ energy efficiency, with the correlation varying depending on different degrees of this condition. For example, factors associated to employment mainly affect buildings with lower energy efficiency whereas the impact on more efficient buildings is limited. The findings of this paper allow for tailoring local policy instruments to fit the local peculiarities. We obtain a list of the most important socio-economic factors influencing the regional energy efficiency. Further, for two exemplary factors, we illustrate how local policy instruments should consider local retrofitting needs and socio-economic factors. © 2021 Elsevier Ltd","Data mining; Energy efficiency; Energy performance certificates; England; Environment; Local environmental policy; Residential building stock; Scotland; Socio-economic; Socio-economic effects; Wales","Climate change; Data Analytics; Data mining; Energy efficiency; Environmental protection; Gas emissions; Greenhouse gases; Housing; Retrofitting; Energy performance; Energy performance certificate; England; Environment; Environmental policy; Local environmental policy; Residential building stocks; Scotland; Socio-economic effects; Socio-economics; Wale; Economic and social effects; data mining; emission control; energy efficiency; energy resource; environmental economics; environmental policy; performance assessment; policy analysis; residential energy; resource allocation; socioeconomic conditions; England; Scotland; United Kingdom; Wales",Article,Scopus,2-s2.0-85117725224
"Jeon D., Ahn J.M., Kim J., Lee C.","57218263020;57189333120;57145217500;55700510500;","A doc2vec and local outlier factor approach to measuring the novelty of patents",2022,"Technological Forecasting and Social Change","174",,"121294","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117709676&doi=10.1016%2fj.techfore.2021.121294&partnerID=40&md5=1de57b4ef7b2e91c7af91a7ede442f5c","Patent analysis using text mining techniques is an effective way to identify novel technologies. However, the results of previous studies have been of limited use in practice because they require domain-specific knowledge and reflect the limited technological features of patents. As a remedy, this study proposes a machine learning approach to measuring the novelty of patents. At the heart of this approach are doc2vec to represent patents as vectors using textual information of patents and the local outlier factor to measure the novelty of patents on a numerical scale. A case study of 1,877 medical imaging technology patents confirms that our novelty scores are significantly correlated with the relevant patent indicators in the literature and that the novel patents identified have a higher technological impact on average. It is expected that the proposed approach could be useful as a complementary tool to support expert decision-making in identifying new technology opportunities, especially for small and medium-sized companies with limited technological knowledge and resources. © 2021","Doc2vec; Local outlier factor; Patent analysis; Patent novelty","Decision making; Medical imaging; Statistics; Case-studies; Doc2vec; Domain-specific knowledge; Local Outlier Factor; Machine learning approaches; Patent analysis; Patent novelty; Technological feature; Text mining techniques; Textual information; Patents and inventions; data mining; decision making; knowledge based system; numerical model; technological development",Article,Scopus,2-s2.0-85117709676
"Song B., Tang Y., Zhan T., Wu Z.","57221311679;57271114300;36669723100;57305375300;","BRCN-ERN: A Bidirectional Reconstruction Coding Network and Enhanced Residual Network for Hyperspectral Change Detection",2022,"IEEE Geoscience and Remote Sensing Letters","19",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117349369&doi=10.1109%2fLGRS.2021.3119859&partnerID=40&md5=247f830254ee963db64f9926dab6870f","Change detection (CD) is a hot issue in the field of remote sensing. Hyperspectral images (HSIs) contain rich spectral information and have gradually become an important data source in CD. Spectral-spatial combination is a commonly used strategy for suppressing the influence of noise on the spectrum. However, it is difficult to find a feature space that allows both spectral and spatial features to be optimally expressed. Therefore, this letter proposes a bidirectional reconstruction coding network and enhanced residual network for HSI CD (i.e., BRCN-ERN) based on the strategy of completely extracting spectral and spatial features separately and then fusing them together. In the spectral module, we use the spectrum of unchanged pixels at two time points to construct a bidirectional reconstruction network, and use the reconstruction error as a new source of spectral features. In the spatial module, we use advanced band selection algorithms to filter the bands with good spatial information and design an enhanced 2-D residual network to extract the spatial features of the change tensor. Finally, the obtained spectral and spatial feature vectors are fused and inputted into the fully connected classification network to obtain the final CD map. Real HSI experiments show that our proposed BRCN-ERN has a better CD effect and is more effective than most existing algorithms. © 2004-2012 IEEE.","Bidirectional reconstruction coding network (BRCN); change detection (CD); enhanced residual network (ERN); hyperspectral image (HSI); spectral-spatial combination","Codes (symbols); Data mining; Feature extraction; Filtration; Hyperspectral imaging; Image coding; Image enhancement; Image reconstruction; Network coding; Remote sensing; Bidirectional reconstruction coding network; Change detection; Encodings; Enhanced residual network; Features extraction; Hyperspectral image; Images reconstruction; Neural-networks; Residual neural network; Spatial combinations; Spectral-spatial combination; Spectroscopy; algorithm; artificial neural network; detection method; image analysis; image classification; reconstruction",Article,Scopus,2-s2.0-85117349369
"Zhang L., Leng X., Feng S., Ma X., Ji K., Kuang G., Liu L.","57203929982;56577492900;57224202465;57224201769;7005637029;57226626214;57054497500;","Domain Knowledge Powered Two-Stream Deep Network for Few-Shot SAR Vehicle Recognition",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117284311&doi=10.1109%2fTGRS.2021.3116349&partnerID=40&md5=baf428ead1395cc5917c4fbad4c3ad55","Synthetic aperture radar (SAR) target recognition faces the challenge that there are very little labeled data. Although few-shot learning methods are developed to extract more information from a small amount of labeled data to avoid overfitting problems, recent few-shot or limited-data SAR target recognition algorithms overlook the unique SAR imaging mechanism. Domain knowledge-powered two-stream deep network (DKTS-N) is proposed in this study, which incorporates SAR domain knowledge related to the azimuth angle, the amplitude, and the phase data of vehicles, making it a pioneering work in few-shot SAR vehicle recognition. The two-stream deep network, extracting the features of the entire image and image patches, is proposed for more effective use of the SAR domain knowledge. To measure the structural information distance between the global and local features of vehicles, the deep Earth mover’s distance is improved to cope with the features from a two-stream deep network. Considering the sensitivity of the azimuth angle in SAR vehicle recognition, the nearest neighbor classifier replaces the structured fully connected layer for <inline-formula> <tex-math notation=""LaTeX"">K </tex-math></inline-formula>-shot classification. All experiments are conducted under the configuration that the SARSIM and the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset work as a source and target task, respectively. Our proposed DKTS-N achieved 49.26% and 96.15% under ten-way one-shot and ten-way 25-shot, whose labeled samples are randomly selected from the training set. In standard operating condition (SOC) as well as three extended operating conditions (EOCs), DKTS-N demonstrated overwhelming advantages in accuracy and time consumption compared with other few-shot learning methods in <inline-formula> <tex-math notation=""LaTeX"">K </tex-math></inline-formula>-shot recognition tasks. 1558-0644 © 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.","Azimuth; Feature extraction; Image recognition; Radar polarimetry; Synthetic aperture radar; Target recognition; Task analysis","Automatic target recognition; Classification (of information); Data mining; Extraction; Feature extraction; Image recognition; Job analysis; Learning systems; Radar imaging; Radar target recognition; Synthetic aperture radar; Vehicles; Azimuth; Complex value information; Complex values; Features extraction; Few-shot learning; Images classification; Radar polarimetry; Synthetic aperture radar target recognition.; Target recognition; Task analysis; Image classification; artificial neural network; azimuth; data set; knowledge; synthetic aperture radar",Article,Scopus,2-s2.0-85117284311
"Ahmadi H., Argany M., Ghanbari A., Ahmadi M.","56377313500;36348139000;57298331000;57298531900;","Visualized spatiotemporal data mining in investigation of Urmia Lake drought effects on increasing of PM10 in Tabriz using Space-Time Cube (2004-2019)",2022,"Sustainable Cities and Society","76",,"103399","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117231534&doi=10.1016%2fj.scs.2021.103399&partnerID=40&md5=62d894496d4b56c8c7101fc5ea2a4d0b","The idea of a smart city seeks to improve the quality of life, which environmental sustainability is one of its main challenges, and low air quality is an important obstacle to achieving the sustainability. Accordingly, we investigated the relationships between Urmia Lake drying-up and PM10 concentration in Tabriz by 3D visualization of space-time cube in ArcGIS pro 2.7. So, Satellite images of the lake and PM10 data were collected. Next, a space-time cube was created for each season of the lake and PM10 during 2004 to 2019. Then Time series analysis, Emerging hot spot analysis and 3D visualization of these cubes were performed. The results showed that there is a clear relationship between drying-up of the lake and increasing of PM10 in Tabriz and we recognized that the drying-up has a significant impact on increasing of PM10 concentration in Tabriz. © 2021 Elsevier Ltd","3D visualization; Air pollution; Data mining; Salt storm; Spatiotemporal trend detection; Sustainability","Air quality; Data mining; Data visualization; Drying; Geometry; Sustainable development; Three dimensional computer graphics; Time series analysis; Visualization; 3D Visualization; Drying-up; PM10 concentration; Quality of life; Salt storm; Spacetime; Spatio-temporal data mining; Spatiotemporal trend detection; Spatiotemporal trends; Trend detection; Lakes",Article,Scopus,2-s2.0-85117231534
"Lafabregue B., Weber J., Gançarski P., Forestier G.","57202310663;57213136547;8557869900;55911405900;","End-to-end deep representation learning for time series clustering: a comparative study",2022,"Data Mining and Knowledge Discovery","36","1",,"29","81",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117218153&doi=10.1007%2fs10618-021-00796-y&partnerID=40&md5=19547b27fe2f2219eecf78221531ac26","Time series are ubiquitous in data mining applications. Similar to other types of data, annotations can be challenging to acquire, thus preventing from training time series classification models. In this context, clustering methods can be an appropriate alternative as they create homogeneous groups allowing a better analysis of the data structure. Time series clustering has been investigated for many years and multiple approaches have already been proposed. Following the advent of deep learning in computer vision, researchers recently started to study the use of deep clustering to cluster time series data. The existing approaches mostly rely on representation learning (imported from computer vision), which consists of learning a representation of the data and performing the clustering task using this new representation. The goal of this paper is to provide a careful study and an experimental comparison of the existing literature on time series representation learning for deep clustering. In this paper, we went beyond the sole comparison of existing approaches and proposed to decompose deep clustering methods into three main components: (1) network architecture, (2) pretext loss, and (3) clustering loss. We evaluated all combinations of these components (totaling 300 different models) with the objective to study their relative influence on the clustering performance. We also experimentally compared the most efficient combinations we identified with existing non-deep clustering methods. Experiments were performed using the largest repository of time series datasets (the UCR/UEA archive) composed of 128 univariate and 30 multivariate datasets. Finally, we proposed an extension of the class activation maps method to the unsupervised case which allows to identify patterns providing highlights on how the network clustered the time series. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.","Clustering; Deep learning; Time series","Cluster analysis; Clustering algorithms; Computer vision; Data mining; Deep learning; Network architecture; Clustering methods; Clusterings; Comparatives studies; Data annotation; Data mining applications; Deep learning; End to end; Time series clustering; Times series; Training time; Time series",Article,Scopus,2-s2.0-85117218153
"Matsui A., Moriwaki D.","57204815519;57212026898;","Online-to-offline advertisements as field experiments",2022,"Japanese Economic Review","73","1",,"211","242",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117196594&doi=10.1007%2fs42973-021-00101-y&partnerID=40&md5=113b6fc8ec70a6b722e2a7f5d7a21635","Online advertisements have become one of today’s most widely used tools for enhancing businesses partly because of their compatibility with A/B testing. A/B testing allows sellers to find effective advertisement strategie,s such as ad creatives or segmentations. Even though several studies propose a technique to maximize the effect of an advertisement, there is insufficient comprehension of the customers’ offline shopping behavior invited by the online advertisements. Herein, we study the difference in offline behavior between customers who received online advertisements and regular customers (i.e., the customers visits the target shop voluntary), and the duration of this difference. We analyze approximately three thousand users’ offline behavior with their 23.5 million location records through 31 A/B testings. We first demonstrate the externality that customers with advertisements traverse larger areas than those without advertisements, and this spatial difference lasts several days after their shopping day. We then find a long-run effect of this externality of advertising that a certain portion of the customers invited to the offline shops revisit these shops. Finally, based on this revisit effect findings, we utilize a causal machine learning model to propose a marketing strategy to maximize the revisit ratio. Our results suggest that advertisements draw customers who have different behavior traits from regular customers. This study demonstrates that a simple analysis may underrate the effects of advertisements on businesses, and an analysis considering externality can attract potentially valuable customers. © 2021, The Author(s).","Online advertisement; Online-to-offline advertisement; Trajectory mining","advertising; consumption behavior; data mining; experimental study; machine learning; marketing; spatial analysis; trajectory",Article,Scopus,2-s2.0-85117196594
"Kumar S., Sastry H.G., Marriboyina V., Alshazly H., Idris S.A., Verma M., Kaur M.","57301710800;57221199086;57209251058;56537040500;57226267577;57226164201;57209860887;","Semantic information extraction from multi-corpora using deep learning",2022,"Computers, Materials and Continua","70","3",,"5021","5038",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117026691&doi=10.32604%2fcmc.2022.021149&partnerID=40&md5=f626133c9ee7051018a0d5770853a96d","Information extraction plays a vital role in natural language processing, to extract named entities and events from unstructured data. Due to the exponential data growth in the agricultural sector, extracting significant information has become a challenging task. Though existing deep learning-based techniques have been applied in smart agriculture for crop cultivation, crop disease detection, weed removal, and yield production, still it is difficult to find the semantics between extracted information due to unswerving effects of weather, soil, pest, and fertilizer data. This paper consists of two parts. An initial phase, which proposes a data preprocessing technique for removal of ambiguity in input corpora, and the second phase proposes a novel deep learning-based long short-term memory with rectification in Adam optimizer and multilayer perceptron to find agricultural-based named entity recognition, events, and relations between them. The proposed algorithm has been trained and tested on four input corpora i.e., agriculture, weather, soil, and pest & fertilizers. The experimental results have been compared with existing techniques and it was observed that the proposed algorithm outperforms Weighted-SOM, LSTM+RAO, PLR-DBN, KNN, and Naïve Bayes on standard parameters like accuracy, sensitivity, and specificity. © 2022 Tech Science Press. All rights reserved.","Agriculture; Deep learning; Information extraction; Soil; Weather","Computational linguistics; Crops; Cultivation; Data mining; Information retrieval; Long short-term memory; Natural language processing systems; Semantics; Soils; Agricultural sector; Crop cultivation; Deep learning; Exponentials; Information extraction; Named entities; Semantic information extractions; Smart agricultures; Unstructured data; Weather; Fertilizers",Article,Scopus,2-s2.0-85117026691
"Chu J., Yu T., Huffman Hayes J., Han X., Zhao Y.","57295090000;36987016200;7403554664;57169256100;57295747600;","Effective fault localization and context-aware debugging for concurrent programs",2022,"Software Testing Verification and Reliability","32","1","e1797","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116984862&doi=10.1002%2fstvr.1797&partnerID=40&md5=cf58a5f33671031e3d6f21691b539ae5","Concurrent programs are difficult to debug because concurrency faults usually occur under specific inputs and thread interleavings. Fault localization techniques for sequential programs are often ineffective because the root causes of concurrency faults involve memory accesses across multiple threads rather than single statements. Previous research has proposed techniques to analyse passing and failing executions obtained from running a set of test cases for identifying faulty memory access patterns. However, stand-alone access patterns do not provide enough contextual information, such as the path leading to the failure, for developers to understand the bug. We present an approach, Coadec, to automatically generate interthread control flow paths that can link memory access patterns that occurred most frequently in the failing executions to better diagnose concurrency bugs. Coadec consists of two phases. In the first phase, we use feature selection techniques from machine learning to localize suspicious memory access patterns based on failing and passing executions. The patterns with maximum feature diversity information can point to the most suspicious pattern. We then apply a data mining technique and identify the memory access patterns that occurred most frequently in the failing executions. Finally, Coadec identifies faulty program paths by connecting both the frequent patterns and the suspicious pattern. We also evaluate the effectiveness of fault localization using test suites generated from different test adequacy criteria. We introduce and have evaluated Coadec on 10 real-world multithreaded Java applications. Results indicate that Coadec outperforms state-of-the-art approaches for localizing concurrency faults and that Coadec's context debugging can help developers understand concurrency fault by inspecting a small percentage of code. © 2021 John Wiley & Sons, Ltd","concurrent program; fault localization; software debugging","Concurrency control; Data mining; Program debugging; Concurrents programs; Context-Aware; Fault localization; Interleavings; Localization technique; Memory access; Memory access patterns; Root cause; Sequential programs; Software debugging; Memory architecture",Article,Scopus,2-s2.0-85116984862
"Bronda S., Ostrovsky M.G., Jain S., Malacarne A.","57294927900;57295581400;57214755072;57197821761;","The role of social media for patients with temporomandibular disorders: A content analysis of Reddit",2022,"Journal of Oral Rehabilitation","49","1",,"1","9",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116981361&doi=10.1111%2fjoor.13264&partnerID=40&md5=891308c969459c4c8ac69efb0da442c5","Background: Social media is frequently used to discuss health topics among users. Reddit is a popular social media platform particularly suit for discussion about chronic illness because of its anonymity that allow users to express uninhibited feelings. Temporomandibular disorders (TMD) represent a chronic painful disorder which has been rarely studied in terms of social media discussion. Objectives: By exploring how Reddit is used to discuss about TMD, we aim to raise awareness to clinicians involved in TMD management about the online discussion on this topic. Methods: A quantitative content analysis was performed on a pool of most relevant threads and comments about the topic “TMJ” on Reddit. Following a codebook, two independent coders assessed multiple clinically relevant variables. A third subject resolved eventual discrepancies. Results: Reddit is mostly used by subjects with TMD asking for advice to other users about symptoms and treatment modalities. The most discussed causes of TMD were bruxism and dental occlusion, and the most discussed treatments were oral appliance therapy and complementary and alternative treatments. The most favourable opinions about treatment modalities were for self-care and behavioural therapy while the least favourable opinions were for surgery and irreversible dental treatments. Conclusions: Reddit represents an excellent data-mining platform to retrieve valuable information about health-related discussion by the community. Our findings suggest an overall alignment of such discussion with evidence-based science about TMD; however, to further increase this trend, we encourage healthcare provider to take an active role in the digital spread of scientifically valid information. © 2021 John Wiley & Sons Ltd","chronic pain; data mining; online health; Reddit; social media; temporomandibular joint disorder","bruxism; emotion; human; social media; temporomandibular joint disorder; Bruxism; Emotions; Humans; Social Media; Temporomandibular Joint Disorders",Article,Scopus,2-s2.0-85116981361
"Eachempati P., Srivastava P.R., Kumar A., Muñoz de Prat J., Delen D.","57193355447;57225667071;57221973593;57205260155;55887961100;","Can customer sentiment impact firm value? An integrated text mining approach",2022,"Technological Forecasting and Social Change","174",,"121265","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116946304&doi=10.1016%2fj.techfore.2021.121265&partnerID=40&md5=6acd761d0ae51cf96d1f82d9ddb590ab","Developing measures to capture customer sentiment and securing a positive customer experience is a strategic necessity to improve firm profitability and shareholder value. The paper considers the relationship between customer satisfaction, earnings, and firm value as these drives change in stock prices, customer, and investor sentiment. The present study investigates the impact of customer sentiment polarity on stock prices based on Indian automobile sector databased such as the Indian Nifty Auto SNE (Maruti Suzuki, Tata Motors, and Eicher). A top-down approach is adopted to construct a financial proxy-based sentiment index completed with sentiment extracted from automobile news and customer reviews. The paper uses a text mining approach to holistically measure customer sentiment's impact on investor sentiment and stock prices. The study was initially performed at the overall individual stock from the Nifty Auto NSE but focused on the top three passenger vehicle manufacturing companies i.e., Maruti Suzuki, Tata Motors, and Eicher. It was found that the sentiment index was augmented with news and customer reviews allows predicting more accurately NIFTY AUTO stock price movements. This implies that customer sentiment is a major driver of investor sentiment which in turn impacts the stock market and the firm value. Thus, the present study is an integrated approach to holistically measure customer sentiment's impact on investor sentiment and stock prices. © 2021","Customer sentiment; Firm value; Research methods; Social media; Text mining","Automobiles; Automotive industry; Costs; Customer satisfaction; Data mining; Financial markets; Investments; Shareholders; Social networking (online); Customer experience; Customer review; Customer sentiment; Firm value; Investor's sentiments; Research method; Sentiment index; Social media; Stock price; Text-mining; Sales; firm size; innovation; integrated approach; profitability; social media; technological development; India; Lateolabrax japonicus",Article,Scopus,2-s2.0-85116946304
"Sameer S., Behadili S.F.","57291653200;57207243874;","Data mining techniques for Iraqi biochemical dataset analysis",2022,"Baghdad Science Journal","19","2",,"385","398",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116932117&doi=10.21123%2fBSJ.2022.19.2.0385&partnerID=40&md5=eeab5952009facf7309608b6dcb8ff37","This research aims to analyze and simulate biochemical real test data for uncovering the relationships among the tests, and how each of them impacts others. The data were acquired from Iraqi private biochemical laboratory. However, these data have many dimensions with a high rate of null values, and big patient numbers. Then, several experiments have been applied on these data beginning with unsupervised techniques such as hierarchical clustering, and k-means, but the results were not clear. Then the preprocessing step performed, to make the dataset analyzable by supervised techniques such as Linear Discriminant Analysis (LDA), Classification And Regression Tree (CART), Logistic Regression (LR), K-Nearest Neighbor (K-NN), Naïve Bays (NB), and Support Vector Machine (SVM) techniques. CART gives clear results with high accuracy between the six supervised algorithms. It is worth noting that the preprocessing steps take remarkable efforts to handle this type of data, since its pure data set has so many null values of a ratio 94.8%, then it becomes 0% after achieving the preprocessing steps. Then, in order to apply CART algorithm, several determined tests were assumed as classes. The decision to select the tests which had been assumed as classes were depending on their acquired accuracy. Consequently, enabling the physicians to trace and connect the tests result with each other, which extends its impact on patients’ health. © 2022 University of Baghdad. All rights reserved.","Biomedical; Classification and regression tree (CART); Data mining; Hierarchical clustering; K-means",,Article,Scopus,2-s2.0-85116932117
"Al-Behadili H.N.K.","57203142120;","Improved firefly algorithm with variable neighborhood search for data clustering",2022,"Baghdad Science Journal","19","2",,"409","421",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116916911&doi=10.21123%2fBSJ.2022.19.2.0409&partnerID=40&md5=97f73089306904fe66dbbaf58460ffb9","Among the metaheuristic algorithms, population-based algorithms are an explorative search algorithm superior to the local search algorithm in terms of exploring the search space to find globally optimal solutions. However, the primary downside of such algorithms is their low exploitative capability, which prevents the expansion of the search space neighborhood for more optimal solutions. The firefly algorithm (FA) is a population-based algorithm that has been widely used in clustering problems. However, FA is limited in terms of its premature convergence when no neighborhood search strategies are employed to improve the quality of clustering solutions in the neighborhood region and exploring the global regions in the search space. On these bases, this work aims to improve FA using variable neighborhood search (VNS) as a local search method, providing VNS the benefit of the trade-off between the exploration and exploitation abilities. The proposed FA-VNS allows fireflies to improve the clustering solutions with the ability to enhance the clustering solutions and maintain the diversity of the clustering solutions during the search process using the perturbation operators of VNS. To evaluate the performance of the algorithm, eight benchmark datasets are utilized with four well-known clustering algorithms. The comparison according to the internal and external evaluation metrics indicates that the proposed FA-VNS can produce more compact clustering solutions than the well-known clustering algorithms. © 2022 University of Baghdad. All rights reserved.","Data clustering; Data mining; Firefly algorithm; Machine learning; Variable neighborhood search",,Article,Scopus,2-s2.0-85116916911
"Li K., Zong J., Fei Y., Liang J., Hu G.","57205288794;57194041845;57291410900;57204721666;57209820480;","Simultaneous Seismic Deep Attribute Extraction and Attribute Fusion",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116903194&doi=10.1109%2fTGRS.2021.3113075&partnerID=40&md5=b5be8ac8109dc079802f25a2f4f12d70","Seismic attributes comprise an effective method for oil and gas reservoir characterization and prediction. Hundreds of seismic attributes have been introduced in the last 30 years. Among the seismic attributes targeting different reservoir features, the autoencoder (AE) receives a significant amount of attention, as it extracts deep attributes of seismic data, providing more details of seismic lateral features than other seismic waveform data and seismic attributes. However, data-driven deep attributes bring new challenges to interpretation as they lack the support of intrinsic physical mechanisms. Hence, a shared AE (S-AE) method is proposed in this article, which can extract seismic deep attributes and fuse traditional seismic attributes simultaneously. An S-AE is a revised version of an AE, which consists of an encoder and decoder. An S-AE takes the seismic waveform as the input of the encoder and obtains the deep attribute, and the decoder then transforms the deep attribute to reconstruct the seismic waveforms and attributes. In an S-AE, the network in front of the decoder is shared, while the networks after the decoder consist of independent layers. Such a network structure ensures the effect of reconstruction and associates seismic attributes with the extracted deep attribute, so as to achieve the purpose of attribute fusion and deep attribute extraction. The proposed S-AE method is compared with conventional seismic data fusion methods, such as RGB and principal component analysis, and the superiority of the S-AE is demonstrated in both synthetic and field applications. 1558-0644 © 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.","Data mining; Decoding; Feature extraction; Image color analysis; Neural networks; Principal component analysis; Reservoirs","Data fusion; Decoding; Extraction; Principal component analysis; Seismic response; Seismic waves; Signal encoding; Attributes extractions; Auto encoders; Deep learning; Features extraction; Image color analysis; Neural-networks; Principal-component analysis; Seismic attribute.; Seismic attributes; Seismic waveforms; Data mining; artificial neural network; data mining; principal component analysis; reconstruction; reservoir characterization; seismic data; waveform analysis",Article,Scopus,2-s2.0-85116903194
"Yosef A., Schneider M., Shnaider E.","57189641455;7404064576;6602828176;","Data Mining Method for Identifying Biased or Misleading Future Outlook",2022,"International Journal of Information Technology and Decision Making","21","1",,"109","141",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116897340&doi=10.1142%2fS0219622021500516&partnerID=40&md5=4f0c8e9e3e3d2e4caff20a6f003aed90","In this study, we introduce a data mining method to identify biased and/or misleading outlooks for future performance of various factors, such as income, corporate profits, production, countries' GDP, etc. The method consists of several components. One very important component involves building a general model, where the dependent variable is a factor suspected of projecting an over-optimistic impression in some records. Explanatory variables in the model are viewed as representing the potential for the satisfactory performance of the dependent variable. The second component involves evaluating the potential for the individual records of interest (specific countries, corporations, production facilities, etc.), and allows us to identify possible gaps between the upbeat/optimistic projections into the future (of the dependent variable) versus low and/or declining potential. In other words, low and/or declining potential basically tells us that the optimistic future performance of the dependent variable is unattainable, and could also represent misleading or deceitful information. The important novelty of this study is the capability to identify a highly exaggerated outlook of future performance, by utilizing a soft regression tool and the concept of ""performance potential"". The process is explained in detail, including the conditions for successful evaluations. Case studies to evaluate expected economic success are presented. © 2022 World Scientific Publishing Company.","cross-national model; Data mining; fuzzy logic; soft computing; soft regression","Fuzzy logic; Soft computing; Corporate profits; Cross-national; Cross-national model; Data mining methods; Dependent variables; Future performance; Fuzzy-Logic; Optimistics; Soft regression; Soft-Computing; Data mining",Article,Scopus,2-s2.0-85116897340
"Miao J., Yang T., Jin J.-W., Sun L., Niu L., Shi Y.","57190984415;9236907800;57195302188;57199010514;35729326200;57218707844;","Towards Compact Broad Learning System by Combined Sparse Regularization",2022,"International Journal of Information Technology and Decision Making","21","1",,"169","194",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116880809&doi=10.1142%2fS0219622021500553&partnerID=40&md5=dfc974b3b362edec9c9fc9e77bdbfc60","Broad Learning System (BLS) has been proven to be one of the most important techniques for classification and regression in machine learning and data mining. BLS directly collects all the features from feature and enhancement nodes as input of the output layer, which neglects vast amounts of redundant information. It usually leads to be inefficient and overfitting. To resolve this issue, we propose sparse regularization-based compact broad learning system (CBLS) framework, which can simultaneously remove redundant nodes and weights. To be more specific, we use group sparse regularization based on ℓ2,1 norm to promote the competition between different nodes and then remove redundant nodes, and a class of nonconvex sparsity regularization to promote the competition between different weights and then remove redundant weights. To optimize the resulting problem of the proposed CBLS, we exploit an efficient alternative optimization algorithm based on proximal gradient method together with computational complexity. Finally, extensive experiments on the classification task are conducted on public benchmark datasets to verify the effectiveness and superiority of the proposed CBLS. © 2022 World Scientific Publishing Company.","Broad learning system (BLS); group sparsity; nonconvex regularization; proximal gradient method","Classification (of information); Computational efficiency; Data mining; Learning systems; Broad learning system; Gradient's methods; Group sparsities; Nonconvex; Nonconvex regularization; Output layer; Proximal gradient method; Redundant nodes; Regularisation; Sparse regularizations; Gradient methods",Article,Scopus,2-s2.0-85116880809
"Richetti P.H.P., Jazbik L.S., Baião F.A., Campos M.L.M.","56342728100;57292704400;6603350945;7202803675;","Deviance mining with treatment learning and declare-based encoding of event logs",2022,"Expert Systems with Applications","187",,"115962","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116871597&doi=10.1016%2fj.eswa.2021.115962&partnerID=40&md5=a2ae3610fbbdaf3e032bc5b9fcb377b3","Business Process Deviance Mining is a research area that aims to characterize deviations of a business process from its expected outcomes. Techniques within this area discover which features of a set of process executions are associated to changes in process performance, providing insights on which process behavior leads to the best performance and also revealing behaviors that result in undesired process outcomes. In this sense, performance may refer to time, cost, resource dimensions or to any domain-specific performance indicator. Existing techniques for business process deviance mining are based on the extraction of patterns from event logs, using different pattern mining approaches. Up to date these extraction patterns have limited expressiveness, since they are not able to capture complex relationships that may be present in highly-flexible processes. In this work, we propose a new encoding technique for vector-based representation of process instances, and then apply Treatment Learning as a novel approach in the context of Deviance Mining to identify the characteristics of a process that mostly impact its performance. The proposed encoding technique is based on the fulfillment of Declare constraint templates, which makes it able to discover more expressive treatments. We compare our proposal with current process encoding techniques in a series of experiments with publicly available event logs from real-life processes. The results showed that treatment learning, together with our proposed Declare-based encoding, produced relevant and more expressive insights from the event logs, being a practical application for process analysis. © 2021 Elsevier Ltd","Declare; Deviance mining; Treatment learning","Data mining; Encoding (symbols); Extraction; Business Process; Declare; Deviance mining; Encoding techniques; Event logs; In-process; Performance; Process execution; Research areas; Treatment learning; Signal encoding",Article,Scopus,2-s2.0-85116871597
"Mahdavifar S., Deldar F., Mahdikhani H.","55537966600;36998303600;55538268300;","Personalized Privacy-Preserving Publication of Trajectory Data by Generalization and Distortion of Moving Points",2022,"Journal of Network and Systems Management","30","1","10","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116850420&doi=10.1007%2fs10922-021-09617-5&partnerID=40&md5=49ce6f1990fc04623e0b8ab25e66ac51","With the rising prevalence of location-aware devices such as mobile phones, Radio-Frequency Identification (RFID) tags, and Global Positioning Systems (GPSs), the amount of trajectory data is significantly increasing, resulting in various data mining applications. Improper publication of trajectory data may jeopardize the privacy of moving objects, so trajectories ought to be anonymized before making them accessible to the public. Many existing approaches for privacy-preserving publication of trajectory data provide only the same level of privacy protection for all moving objects, whereas different moving objects may require different amounts of privacy protection. In this paper, we address this issue by presenting WINR2D, a novel clustering-based approach for privacy-preserving publication of trajectory data. Being based on the concept of personalized privacy, the aim of WINR2D is to anonymize trajectories to some extent so that an adversary having some background knowledge cannot uniquely identify a specific trajectory, but with a maximum probability inversely proportional to the privacy protection requirement of the moving object that produced it. In doing so, we first assign a privacy level to each trajectory based on the privacy protection requirement of its moving object and then partition all the trajectories into a set of clusters based on a greedy strategy. Each cluster is created such that its size is proportional to the highest privacy level of trajectories within it. Eventually, we anonymize the trajectories of each cluster and generate a set of anonymized trajectories containing generalized and distorted moving points. Our experimental results show that WINR2D achieves a reasonable trade-off between the conflicting goals of data utility and data privacy according to the privacy protection requirements of moving objects. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Data privacy; Data utility; Moving object; Personalized anonymity; Trajectory anonymization; Trajectory clustering","Data mining; Data privacy; Economic and social effects; Global positioning system; Publishing; Radio frequency identification (RFID); Anonymization; Data utilities; Moving objects; Personalized anonymities; Privacy preserving; Privacy protection; Protection requirements; Trajectories datum; Trajectory anonymization; Trajectory clustering; Trajectories",Article,Scopus,2-s2.0-85116850420
"Chen Q., Pu N., Yin H., Zhang J., Zhao G., Lou W., Wu W.","57217850984;57195151272;57200607976;57217864458;56996489700;57217634494;55707447700;","A metabolism-relevant signature as a predictor for prognosis and therapeutic response in pancreatic cancer",2022,"Experimental Biology and Medicine","247","2",,"120","130",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116765737&doi=10.1177%2f15353702211049220&partnerID=40&md5=78a3a0a7bc185f8eabb21fa9feccd389","Although several altered metabolic genes have been identified to be involved in the tumorigenesis and advance of pancreatic cancer (PC), their prognostic values remained unclear. The purpose of this study was to explore new targets and establish a metabolic signature to predict prognosis and chemotherapy response for optimal individualized treatment. The expression data of PC patients from two independent cohorts and metabolism-related genes from KEGG were utilized and analyzed for the establishment of the signature via lasso regression. Then, the differentially expressed candidate genes were further confirmed via online data mining platform and qRT-PCR of clinical specimens. Then, the analyses of gene set enrichment, mutation, and chemotherapeutic response were performed via R package. As results showed, 109 differentially expressed metabolic genes were screened out in PC. Then a metabolism-related five-gene signature comprising B3GNT3, BCAT1, KYNU, LDHA, and TYMS was constructed and showed excellent ability for predicting survival. A novel nomogram coordinating the metabolic signature and other independent prognostic parameters was developed and showed better predictive power in predicting survival. In addition, this metabolic signature was significantly involved in the activation of multiple oncological pathways and regulation of the tumor immune microenvironment. The patients with high risk scores had higher tumor mutation burdens and were prone to be more sensitive to chemotherapy. In summary, our work identified a new metabolic signature and established a superior prognostic nomogram which may supply more indications to explore novel strategies for diagnosis and treatment. © 2021 by the Society for Experimental Biology and Medicine.","chemotherapeutic response; metabolic signature; Pancreatic cancer; prediction; prognosis","aminotransferase; branched chain amino acid transaminase 1; cisplatin; erlotinib; glycosyltransferase; kynureninase; lactate dehydrogenase A; paclitaxel; thymidylate synthase; udp GlcNAc:betagal beta 1,3 n acetylglucosaminyltransferase 3; unclassified drug; transcriptome; tumor protein; Article; bioinformatics; cancer chemotherapy; cancer prognosis; cancer staging; CD8+ T lymphocyte; cell infiltration; cohort analysis; controlled study; data mining; diagnostic test accuracy study; differential gene expression; gene expression; gene set enrichment analysis; human; human tissue; IC50; immunocompetent cell; immunohistochemistry; metabolism; mRNA expression level; overall survival; pancreas cancer; pancreatic ductal carcinoma; predictive value; protein expression; protein protein interaction; real time reverse transcription polymerase chain reaction; receiver operating characteristic; risk assessment; RNA extraction; sensitivity and specificity; somatic mutation; treatment response; tumor microenvironment; tumor volume; disease free survival; female; gene expression regulation; genetics; immunology; male; mortality; mutation; pancreas tumor; risk factor; survival rate; Disease-Free Survival; Female; Gene Expression Regulation, Neoplastic; Humans; Male; Mutation; Neoplasm Proteins; Pancreatic Neoplasms; Risk Factors; Survival Rate; Transcriptome; Tumor Microenvironment",Article,Scopus,2-s2.0-85116765737
"Vishnu P.R., Vinod P., Yerima S.Y.","57290131700;35114231400;22735916500;","A Deep Learning Approach for Classifying Vulnerability Descriptions Using Self Attention Based Neural Network",2022,"Journal of Network and Systems Management","30","1","9","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116743553&doi=10.1007%2fs10922-021-09624-6&partnerID=40&md5=c2a7b385eb602045755601cdccfd254e","Cyber threat intelligence (CTI) refers to essential knowledge used by organizations to prevent or mitigate against cyber attacks. Vulnerability databases such as CVE and NVD are crucial to cyber threat intelligence, but also provide information leveraged in hundreds of security products worldwide. However, previous studies have shown that these vulnerability databases sometimes contain errors and inconsistencies which have to be manually checked by security professionals. Such inconsistencies could threaten the integrity of security products and hamper attack mitigation efforts. Hence, to assist the security community with more accurate and time-saving validation of vulnerability data, we propose an automated vulnerability classification system based on deep learning. Our proposed system utilizes a self-attention deep neural network (SA-DNN) model and text mining approach to identify the vulnerability category from the description text contained within a report. The performance of the SA-DNN-based vulnerability classification system is evaluated using 134,091 vulnerability reports from the CVE details website.The experiments performed demonstrates the effectiveness of our approach, and shows that the SA-DNN model outperforms SVM and other deep learning methods i.e. CNN-LSTM and graph convolutional neural networks. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Common vulnerabilities and exposures; Cyber threat intelligence; Deep learning; Graph convolutional neural network; Self attention neural network; Text mining; Vulnerability classification","Classification (of information); Convolution; Convolutional neural networks; Data mining; Deep neural networks; Long short-term memory; Network security; Support vector machines; Text processing; Classification system; Common vulnerabilities and exposures; Cybe threat intelligence; Cyber threats; Deep learning; Neural-networks; Security products; Self attention neural network; Vulnerability classifications; Vulnerability database; Graph neural networks",Article,Scopus,2-s2.0-85116743553
"Shao C., Yang Y., Juneja S., GSeetharam T.","57288582200;57289088300;57210408722;57260822800;","IoT data visualization for business intelligence in corporate finance",2022,"Information Processing and Management","59","1","102736","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116678681&doi=10.1016%2fj.ipm.2021.102736&partnerID=40&md5=e5a7f7f9d7c1c7a5d12cfb730900852b","Business intelligence (BI) incorporates business research, data mining, data visualization, data tools,infrastructure, and best practices to help businesses make more data-driven choices.Business intelligence's challenging characteristics include data breaches, difficulty in analyzing different data sources, and poor data quality is consideredessential factors. In this paper, IoT-based Efficient Data Visualization Framework (IoT- EDVF) has been proposed to strengthen leaks' risk, analyze multiple data sources, and data quality management for business intelligence in corporate finance.Corporate analytics management is introduced to enhance the data analysis system's risk, and the complexity of different sources can allow accessing Business Intelligence. Financial risk analysis is implemented to improve data quality management initiative helps use main metrics of success, which are essential to the individual needs and objectives. The statistical outcomes of the simulation analysis show the increasedperformance with a lower delay response of 5ms and improved revenue analysis with the improvement of 29.42% over existing models proving the proposed framework's reliability. © 2021","Business intelligence; Corporate finance; Data visualization; IoT","Budget control; Data mining; Finance; Information management; Internet of things; Quality control; Reliability analysis; Risk analysis; Risk assessment; Visualization; Best practices; Business research; Business-intelligence; Corporate finance; Data driven; Data quality; Data quality management; Data tools; Data-source; Visualization framework; Data visualization",Article,Scopus,2-s2.0-85116678681
"Evans S.N., Dennis C.B.","57285487300;55836503700;","Interpersonal Associations and Completing a Substance Use Day Treatment Program",2022,"Journal of Evidence-Based Social Work (United States)","19","1",,"64","76",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116423406&doi=10.1080%2f26408066.2021.1982435&partnerID=40&md5=d3fdadfd069fb1668af4a92883037175","Purpose: Social influence can be important to people’s substance use recovery. It functions in professional substance use treatment programs and in the 12-step approach to recovery. Method: The study reported here employed clinical data mining methodology to study the role of interpersonal associations inside and outside of substance use treatment in predicting clients’ treatment success. Results: In this study, the interpersonal associations available in treatment predicted greater odds of completing a day treatment program. Two interpersonal associations outside of treatment had an effect on successful program completion, but only one was in the hypothesized direction. Discussion: Social learning theory and 12-Step philosophy offer a lens for viewing the role interpersonal associations might play in a client’s recovery process. © 2021 Taylor & Francis.","12-step philosophy; clinical data mining; Interpersonal; social learning; substance use treatment","article; data mining; human; philosophy; social learning; Social Learning Theory; substance use",Article,Scopus,2-s2.0-85116423406
"Makhalova T., Kuznetsov S.O., Napoli A.","56904199800;7202573378;7005873419;","Mint: MDL-based approach for Mining INTeresting Numerical Pattern Sets",2022,"Data Mining and Knowledge Discovery","36","1",,"108","145",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116331880&doi=10.1007%2fs10618-021-00799-9&partnerID=40&md5=02518ac6ec5ea4c83ec4fca2040c9a72","Pattern mining is well established in data mining research, especially for mining binary datasets. Surprisingly, there is much less work about numerical pattern mining and this research area remains under-explored. In this paper we propose Mint, an efficient MDL-based algorithm for mining numerical datasets. The MDL principle is a robust and reliable framework widely used in pattern mining, and as well in subgroup discovery. In Mint we reuse MDL for discovering useful patterns and returning a set of non-redundant overlapping patterns with well-defined boundaries and covering meaningful groups of objects. Mint is not alone in the category of numerical pattern miners based on MDL. In the experiments presented in the paper we show that Mint outperforms competitors among which IPD, RealKrimp, and Slim. © 2021, The Author(s).","Hyper-rectangles; Minimum Description Length principle; Numerical Data; Numerical Pattern Mining; Plug-in codes","Codes (symbols); Hyperrectangles; Minimum description length principle; Numerical data; Numerical datasets; Numerical pattern mining; Pattern mining; Pattern set; Plug-in code; Plug-ins; Research areas; Data mining",Article,Scopus,2-s2.0-85116331880
"Dubois P., Gomez T., Planckaert L., Perret L.","57207842326;36853859100;55754201700;7003760447;","Machine learning for fluid flow reconstruction from limited measurements",2022,"Journal of Computational Physics","448",,"110733","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116255113&doi=10.1016%2fj.jcp.2021.110733&partnerID=40&md5=b54251816efcedda0e3ab9d4bfc369b5","This paper investigates the use of data-driven methods for the reconstruction of unsteady fluid flow fields. The proposed framework is based on the combination of machine learning tools: dimensionality reduction to extract dominant spatial directions from data, reconstruction algorithm to recover encoded data by limited measurements and cross-validation for hyperparameter optimization. For the encoding part, linear and nonlinear extraction of patterns are considered: proper orthogonal decomposition (POD), linear autoencoder (LAE) and variational autoencoder (VAE). For the reconstruction part, regressive reconstruction (neural network, linear, support vector, gradient boosting) and library-based reconstruction are compared, each method being cross-validated to ensure good generalization on testing data. The position of sensors is optimized using an enhanced clustering algorithm. The robustness of regressive reconstructions to noise measurements is also addressed, showing the benefits of variational approaches in the reduction phase. The strategy is tested for three increasing complexity flows: 2D vortex shedding (Re=200), 2D spatial mixing layer and 3D vortex shedding (Re=20000). The results suggest that proper machine learning approaches to fluid flow data can lead to effective reconstruction models that can be used for the rapid estimation of complex flows. © 2021 Elsevier Inc.","Data-driven; Dimensionality reduction; Machine learning","Clustering algorithms; Complex networks; Data mining; Data reduction; Machine learning; Principal component analysis; Vortex flow; Auto encoders; Data driven; Data reconstruction; Data-driven methods; Dimensionality reduction; Flow reconstruction; Fluid flow field; Fluid-flow; Spatial direction; Vortex-shedding; Vortex shedding",Article,Scopus,2-s2.0-85116255113
"Pero-Gascon R., Hemeryck L.Y., Poma G., Falony G., Nawrot T.S., Raes J., Vanhaecke L., De Boevre M., Covaci A., De Saeger S.","56557127500;55339106000;49661979500;24385495600;6701621197;8602126700;23092256600;55200356800;57279625700;56785623200;","FLEXiGUT: Rationale for exposomics associations with chronic low-grade gut inflammation",2022,"Environment International","158",,"106906","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116046660&doi=10.1016%2fj.envint.2021.106906&partnerID=40&md5=c0ccfbdd3ee5e0217fd4146bc36ad605","FLEXiGUT is the first large-scale exposomics study focused on chronic low-grade inflammation. It aims to characterize human life course environmental exposure to assess and validate its impact on gut inflammation and related biological processes and diseases. The cumulative influences of environmental and food contaminants throughout the lifespan on certain biological responses related to chronic gut inflammation will be investigated in two Flemish prospective cohorts, namely the “ENVIRONAGE birth cohort”, which provides follow-up from gestation to early childhood, and the “Flemish Gut Flora Project longitudinal cohort”, a cohort of adults. The exposome will be characterised through biomonitoring of legacy and emerging contaminants, mycotoxins and markers of air pollution, by analysing the available metadata on nutrition, location and activity, and by applying state-of-the-art -omics techniques, including metagenomics, metabolomics and DNA adductomics, as well as the assessment of telomere length and measurement of inflammatory markers, to encompass both exposure and effect. Associations between exposures and health outcomes will be uncovered using an integrated -omics data analysis framework comprising data exploration, pre-processing, dimensionality reduction and data mining, combined with machine learning-based pathway analysis approaches. This is expected to lead to a more profound insight in mechanisms underlying disease progression (e.g. metabolic disorders, food allergies, gastrointestinal cancers) and/or accelerated biological ageing. © 2021 The Authors","Chronic exposure; Environmental and food contaminants; Exposome; Gut inflammation; Metabolomics; Microbiome","Data mining; 'omics'; Chronic exposure; Environmental contaminant; Exposome; Food contaminants; Gut inflammation; Large-scales; Low grade; Metabolomics; Microbiome; Pathology; mycotoxin; biomonitoring; cohort analysis; DNA; food consumption; metabolism; metabolite; microbial activity; adductomics; aging; air pollution; Article; biological monitoring; chronic inflammation; data integration; data processing; disease exacerbation; DNA adductomics; enteritis; environmental exposure; exposomics; food contamination; health; human; lifespan; metabolomics; metagenomics; multiomics; nonhuman; phenotype; telomere length; adult; adverse event; inflammation; preschool child; prospective study; Adult; Birth Cohort; Child, Preschool; Environmental Exposure; Exposome; Humans; Inflammation; Metagenomics; Prospective Studies",Article,Scopus,2-s2.0-85116046660
"Dusza H.M., Manz K.E., Pennell K.D., Kanda R., Legler J.","57203408310;57222718347;7004022008;7003741670;56425854300;","Identification of known and novel nonpolar endocrine disruptors in human amniotic fluid",2022,"Environment International","158",,"106904","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116028342&doi=10.1016%2fj.envint.2021.106904&partnerID=40&md5=6cee6457206ace3b09f650149be746fd","Background: Prenatal exposure to endocrine-disrupting compounds (EDCs) may contribute to endocrine-related diseases and disorders later in life. Nevertheless, data on in utero exposure to these compounds are still scarce. Objectives: We investigated a wide range of known and novel nonpolar EDCs in full-term human amniotic fluid (AF), a representative matrix of direct fetal exposure. Methods: Gas chromatography high-resolution mass spectrometry (GC-HRMS) was used for the targeted and non-targeted analysis of chemicals present in nonpolar AF fractions with dioxin-like, (anti-)androgenic, and (anti-)estrogenic activity. The contribution of detected EDCs to the observed activity was determined based on their relative potencies. The multitude of features detected by non-targeted analysis was tentatively identified through spectra matching and data filtering, and further investigated using curated and freely available sources to predict endocrine activity. Prioritized suspects were purchased and their presence in AF was chemically and biologically confirmed with GC-HRMS and bioassay analysis. Results: Targeted analysis revealed 42 known EDCs in AF including dioxins and furans, polybrominated diphenyl ethers, pesticides, polychlorinated biphenyls, and polycyclic aromatic hydrocarbons. Only 30% of dioxin activity and <1% estrogenic and (anti-)androgenic activity was explained by the detected compounds. Non-targeted analysis revealed 14,110 features of which 3,243 matched with library spectra. Our data filtering strategy tentatively identified 121 compounds. Further data mining and in silico predictions revealed in total 69 suspected EDCs. We selected 14 chemicals for confirmation, of which 12 were biologically active and 9 were chemically confirmed in AF, including the plasticizer diphenyl isophthalate and industrial chemical p,p'-ditolylamine. Conclusions: This study reveals the presence of a wide variety of nonpolar EDCs in direct fetal environment and for the first time identifies novel EDCs in human AF. Further assessment of the source and extent of human fetal exposure to these compounds is warranted. © 2021 The Authors","Effect-directed analysis; Endocrine disruptors; Human exposome; In utero exposure; Non-targeted analysis; Persistent organic pollutants","Data mining; Endocrine disrupters; Feature extraction; Gas chromatography; Industrial chemicals; Mass spectrometry; Organic chemicals; Polycyclic aromatic hydrocarbons; Amniotic fluid; Effect-directed analysis; Endocrine disrupting compound; Endocrine-disruptors; High resolution mass spectrometry; Human exposome; In utero exposure; Non-polar; Non-targeted; Non-targeted analyse; Organic pollutants; 4,4' ditolylamine; dioxin; endocrine disruptor; furan derivative; industrial chemical; pesticide; plasticizer; plasticizer diphenyl isophthalate; polybrominated diphenyl ether; polychlorinated biphenyl; polycyclic aromatic hydrocarbon; unclassified drug; diphenyl ether derivative; endocrine disruptor; polychlorinated biphenyl; polychlorinated dibenzodioxin; body fluid; dioxin; endocrine disruptor; PAH; PBDE; PCB; persistent organic pollutant; pesticide; pollution exposure; amnion fluid; Article; bioassay; chemical analysis; computer model; concentration (parameter); controlled study; female; gas chromatography; human; mass spectrometry; prediction; prenatal exposure; amnion fluid; chemistry; pregnancy; Amniotic Fluid; Endocrine Disruptors; Female; Halogenated Diphenyl Ethers; Humans; Polychlorinated Biphenyls; Polychlorinated Dibenzodioxins; Pregnancy",Article,Scopus,2-s2.0-85116028342
"Ebada A.I., Elhenawy I., Jeong C.-W., Nam Y., Elbakry H., Abdelrazek S.","57195680188;54915283100;57251541500;8971732400;6603728802;57216186536;","Applying apache spark on streaming big data for health status prediction",2022,"Computers, Materials and Continua","70","2",,"3511","3527",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115992206&doi=10.32604%2fcmc.2022.019458&partnerID=40&md5=be7e4b526110cb71296ff83b82c89ac6","Big data applications in healthcare have provided a variety of solutions to reduce costs, errors, and waste. This work aims to develop a real-time system based on big medical data processing in the cloud for the prediction of health issues. In the proposed scalable system, medical parameters are sent to Apache Spark to extract attributes from data and apply the proposed machine learning algorithm. In this way, healthcare risks can be predicted and sent as alerts and recommendations to users and healthcare providers. The proposed work also aims to provide an effective recommendation system by using streaming medical data, historical data on a user's profile, and a knowledge database to make the most appropriate real-time recommendations and alerts based on the sensor's measurements. This proposed scalable system works by tweeting the health status attributes of users. Their cloud profile receives the streaming healthcare data in real time by extracting the health attributes via a machine learning prediction algorithm to predict the users' health status. Subsequently, their status can be sent on demand to healthcare providers. Therefore, machine learning algorithms can be applied to stream health care data from wearables and provide users with insights into their health status. These algorithms can help healthcare providers and individuals focus on health risks and health status changes and consequently improve the quality of life. © 2022 Tech Science Press. All rights reserved.","Apache Spark; Big data; Healthcare data; IoT data processing; Machine learning; Streaming processing","Big data; Data handling; Data mining; Forecasting; Health risks; Interactive computer systems; Internet of things; Learning algorithms; Machine learning; Medical computing; Real time systems; Apache spark; Health care providers; Health status; Healthcare data; IoT data processing; Machine learning algorithms; Medical data; Real- time; Scalable systems; Streaming processing; Health care",Article,Scopus,2-s2.0-85115992206
"Mao Q., Wang W., You F., Zhao R., Li Z.","57277521200;57277189400;36640173500;57277351300;57015803000;","User behavior pattern mining and reuse across similar Android apps",2022,"Journal of Systems and Software","183",,"111085","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115971196&doi=10.1016%2fj.jss.2021.111085&partnerID=40&md5=f20a19d11eef9d90ad1485f95607691d","Nowadays, Android apps have penetrated all aspects of our lives. Despite their popularity, understanding their behaviors is still a challenging task. Considering that many Android apps are in the same category and share similar workflows, in this paper, we propose a user behavior pattern mining and reuse approach across similar Android apps, thereby reducing the cost of understanding new apps. Particularly, for a specific new app, to figure out its typical behaviors, the behavior patterns that refer to the frequently-occurring workflows can be obtained from another similar app and transferred to this app. Moreover, to reuse the behavior patterns on this app, a semantic-based event fuzzy matching strategy and continuous workflow generation strategy are raised to generate workflows for this app. To evaluate our approach's effectiveness and rationality, we conduct a series of experiments on 25 Android apps in five categories. Furthermore, the experimental results show that 88.3% of behavior patterns can be completely reused on similar apps, and the generated workflows cover 89.1% of the top 20% of important states. © 2021 Elsevier Inc.","Android apps; Behavior pattern reuse; GUI model; Semantic-based event fuzzy matching","Android (operating system); Behavioral research; Data mining; Android apps; Behavior pattern reuse; Behaviour patterns; Fuzzy matching; GUI model; Pattern mining; Reuse; Semantic-based event fuzzy matching; User behavior patterns; Work-flows; Semantics",Article,Scopus,2-s2.0-85115971196
"Ozkok F.O., Celik M.","57200860271;14024176500;","A hybrid CNN-LSTM model for high resolution melting curve classification",2022,"Biomedical Signal Processing and Control","71",,"103168","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115886818&doi=10.1016%2fj.bspc.2021.103168&partnerID=40&md5=820070a239eb8dd54ae8d9867e9b563f","High resolution melting (HRM) curve analysis is an efficient, correct, and rapid technique for analyzing real-time polymerase chain reaction (PCR) results. HRM curves are formed based on increasing temperature and decreasing amount of fluorescent dye in real-time PCR process. The shapes of them are unique for each species due to the sequence, length, and GC content of species' DNA. In the literature, the classification of HRM curves is usually conducted through visual inspection and a limited number of data mining methods have been used to classify these curves. However, it becomes challenging as the number of species and their samples and the number of closely related species increase. In this study, a hybrid classification model, which is based on convolutional neural network (CNN) and long short-term memory (LSTM) models, is proposed to classify HRM curves, efficiently. In the proposed CNN-LSTM model, CNN model was used for feature extraction, and LSTM model was used for classification. It takes both the HRM curves and derivative curves as inputs and gives the predicted species of HRM curves as outputs. The performance of the proposed CNN-LSTM model was compared with that of CNN and support vector machines (SVM) approaches. The results show that the proposed CNN-LSTM model outperforms other models. The accuracy, macro-average of F1, specificity, precision, and recall values of the proposed model were 0.96±0.02,0.95±0.02,1±0,0.96±0.02, and 0.96±0.02, respectively. © 2021 Elsevier Ltd","Classification; Convolutional neural network; Deep learning; High resolution melting curve; Long short-term memory; Real time PCR","Brain; Classification (of information); Convolution; Convolutional neural networks; Data mining; Deep neural networks; Melting; Polymerase chain reaction; Support vector machines; Convolutional neural network; Deep learning; Fluorescent dyes; High resolution; High resolution melting curve; Increasing temperatures; Melting curve analysis; Melting curves; Memory modeling; Real time polymerase chain reactions; Long short-term memory; article; convolutional neural network; data mining; deep learning; feature extraction; high resolution melting analysis; long short term memory network; real time polymerase chain reaction; recall; support vector machine",Article,Scopus,2-s2.0-85115886818
"Wan X., Wang R., Wang M., Deng J., Zhou Z., Yi X., Pan J., Du Y.","57218934453;57250921900;57274295000;57274596200;55587994600;57250454700;57275183100;57274745300;","Online Public Opinion Mining for Large Cross-Regional Projects: Case Study of the South-to-North Water Diversion Project in China",2022,"Journal of Management in Engineering","38","1","05021011","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115874985&doi=10.1061%2f%28ASCE%29ME.1943-5479.0000970&partnerID=40&md5=73656a84782598da72ddd95edc650d1e","Public perspective has long been ignored in ex-post evaluation of large cross-regional projects (LCPs), and the emergence of online public participation affords the opportunity to obtain and analyze public opinions from a wide range. Taking the South-to-North Water Diversion Project (SNWDP) as a typical LCP case, this study explored in depth the diverse and regionalized impacts of the project as reflected in online public opinion by developing an evaluation framework integrating post intensity, sentiment, and topic analyses. Leveraging lexicon-based sentiment analysis and latent Dirichlet allocation-based topic modeling technologies, patterns of public sentiments and topics were identified from spatial and temporal perspectives. The results showed that post intensity and public sentiment were significantly affected by project-related major events. A comparison of the water-donating and -receiving areas suggested that people in the water-receiving area had a more attentive and active attitude toward the project, and those in the water-donating area were predisposed to be more vigilant to negative events. Moreover, the variation of public sentiment across regions was found to largely depend on local social context. With regard to topic analysis, people in the water-donating and -receiving areas were more likely to focus on topics directly associated with their own interests, while the feasibility of the SNWDP garnered attention from both areas. In addition, project functions and significance as well as topical issues related to engineering corruption, water pollution, and security aroused widespread attention in all the areas under study. This study contributes to the literature on ex-post evaluation and public participation in the context of LCPs by incorporating online public opinion into the evaluation framework. The findings provide valuable insights for developing policies and strategies to improve the management of projects and to achieve an inclusive development process for projects of this type. © 2021 American Society of Civil Engineers.","Large cross-regional projects (LCP); Online public opinion mining; Sentiment analysis; South-to-North Water Diversion Project (SNWDP); Topic modeling","Data mining; Flood control; Social aspects; Statistics; Water pollution; Large cross-regional project; Online public opinion mining; Online public opinions; Project case; Public sentiments; Regional projects; Sentiment analysis; South-to-north water diversion project; Topic Modeling; Sentiment analysis",Article,Scopus,2-s2.0-85115874985
"Rong H., Teixeira A.P., Guedes Soares C.","57190866302;57272420000;56978160800;","Maritime traffic probabilistic prediction based on ship motion pattern extraction",2022,"Reliability Engineering and System Safety","217",,"108061","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115767098&doi=10.1016%2fj.ress.2021.108061&partnerID=40&md5=df485903e8fb655298f3a52cffe517d2","This paper proposes a novel maritime traffic prediction method based on ship motion pattern extraction, considering ship destination prediction and ship trajectory prediction within a specific route. To extract ship motion patterns from historical Automatic Identification System data, traffic departure-arrival areas are determined based on the Order Points to Identify the Clustering Structure algorithm and ship trajectories following the same itinerary are clustered. A maritime traffic network abstraction consisting of nodes that represent waypoint areas and navigational legs is constructed to represent the maritime traffic at a larger scale. Multinomial Logistic Regression and Gaussian Process regression models are developed and applied for predicting probabilistically the ships’ destinations and their trajectories along the ship route, respectively. Based on these models, the uncertainty on the ship's future position can be estimated given its current state. The proposed method is capable of long-term ship position prediction and provides information on the maritime traffic 10, 30 and 60 min ahead when the method is applied to all ships navigating in a study area. The presented method may assist maritime authorities to improve the efficiency of maritime traffic surveillance and to develop strategies to improve navigation safety. © 2021","AIS data; Gaussian Process regression models; Maritime traffic; Maritime traffic prediction; Multinomial Logistic Regression models; Ship motion pattern","Automation; Clustering algorithms; Data mining; Errors; Extraction; Forecasting; Gaussian distribution; Gaussian noise (electronic); Motion estimation; Regression analysis; Ships; Time and motion study; Trajectories; AIS data; Gaussian process regression model; Maritime traffic; Maritime traffic prediction; Motion pattern; Multinomial logistic regression models; Ship motion; Ship motion pattern; Traffic prediction; Uncertainty analysis",Article,Scopus,2-s2.0-85115767098
"Bai Y., Zhao M., Li R., Xin P.","57219601777;57226644701;57307509700;57273252800;","A new data mining method for time series in visual analysis of regional economy",2022,"Information Processing and Management","59","1","102741","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115739894&doi=10.1016%2fj.ipm.2021.102741&partnerID=40&md5=29751b909ff10a7484e053454e8330d1","In order to improve the effect of visual analysis of regional economy, this paper uses machine learning algorithms to analyze time series data, uses various models and methods of intelligent data analysis to mine data laws from huge data, statistical data reports, and find problems in economic development. Moreover, this paper combines the time series algorithm to design and plan the functional structure of the system, and design a separate module structure from the actual situation of regional economic analysis, and build a model system from the overall structure. After constructing the system, this paper tests the system. From the results of the experimental research, we can see that the regional economic visualization system based on time series constructed in this paper has perfect system functions and can meet the needs of regional economic analysis. © 2021","Regional economy; Time series; Visualization","Data mining; Data visualization; Economic analysis; Learning algorithms; Machine learning; Regional planning; Time series analysis; Visualization; Data mining methods; Economics analysis; Intelligent data analysis; Machine learning algorithms; Regional economic; Regional economy; Statistical datas; Time-series data; Times series; Visual analysis; Time series",Article,Scopus,2-s2.0-85115739894
"Saad O.M., Oboue Y.A.S.I., Bai M., Samy L., Yang L., Chen Y.","57201504788;57223158334;55695781700;57201336852;57215054335;56311736500;","Self-Attention Deep Image Prior Network for Unsupervised 3-D Seismic Data Enhancement",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115711402&doi=10.1109%2fTGRS.2021.3108515&partnerID=40&md5=55d5cbcab27482f7a23a98b2e6af1061","We develop a deep learning framework based on deep image prior (DIP) and attention networks for 3-D seismic data enhancement. First, the 3-D noisy data are divided into several overlapped patches. Second, the DIP network has a U-NET architecture, where the input patches are encoded to extract the significant latent features, while the decoder tries to reconstruct the input patches using these extracted features. Besides, the attention network is used to scale the extracted features from the encoder and the decoder. Third, the attention network output of the encoder is concatenated with that of the decoder to obtain high-order features and guide the network to extract the most significant information related to the seismic signals and discard the others. Finally, the 3-D seismic data are reconstructed using the output patches obtained by the DIP network. The proposed algorithm is an iterative and unsupervised approach, which does not require labeled data. We evaluate the proposed algorithm using several synthetic and field data examples. As a result, the proposed algorithm shows the ability to enhance the 3-D seismic data by attenuating the random noise and preserving the 3-D seismic signal with minimal signal leakage. Moreover, the proposed algorithm shows good denoising performance when tested using various types of events, e.g., linear, hyperbolic, low and high dominant frequencies, and weak amplitude. In addition, the proposed method outperforms the predictive filtering (PF) and damped rank-reduction (DRR) methods. To further understand the principle of the proposed method inside the DIP network, we analyze the weighting matrices in the encoder and decoder parts in detail. We attribute the denoising ability of the DIP network to the improvement of the extracted basis features from the encoder to the decoder layers through a deep network. © 2021 IEEE.","Convolutional neural networks; Data mining; Decoding; Electronics packaging; Feature extraction; Noise reduction; Signal to noise ratio","Data mining; Decoding; Deep learning; Image enhancement; Iterative methods; Seismic response; Seismic waves; Signal to noise ratio; 3-D seismic denoising; 3-D seismics; Convolutional neural network; De-noising; Deep leaning.; Electronic Packaging; Features extraction; Image priors; Seismic signals; Squeeze-and-excitation network; Neural networks; algorithm; image analysis; information; seismic data",Article,Scopus,2-s2.0-85115711402
"Ramírez-Noriega A., Martínez-Ramírez Y., Jiménez S., Soto-Vega J., Figueroa-Pérez J.F.","57188557062;57189523444;55933652400;57272111500;57201498659;","inDev: A software to generate an MVC architecture based on the ER model",2022,"Computer Applications in Engineering Education","30","1",,"259","274",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115678370&doi=10.1002%2fcae.22455&partnerID=40&md5=c2152420b560f236faa7e44f418cba70","Model-view-controller (MVC) design pattern is employed as software architecture. This pattern has the objective of separating the code into three elements, maintaining layers with defined functions. MVC pattern is used to structure and organize code in software development; therefore, it is an important topic in the teaching of software engineering. However, understanding and implementing this design pattern is not easy for students. Therefore, this investigation proposes a Computer Aided Software Engineering tool called inDev, which is capable of generating an application based on an Entity Relationship (ER) diagram, generating the model, the controller, and the view. The student can interact with the system by visualizing the changes produced by the inputs in the ER diagram in the output as the MVC architecture. To test the scope of the project as a teaching strategy, an experiment was designed with a control group and an experimental group. The experimental group that used the application, inDev, showed better results in learning than the control group, which did not use it. The inDev tool proved to be a useful educational tool for dealing with a topic like the MVC design pattern. © 2021 Wiley Periodicals LLC",,"Application programs; Codes (symbols); Computer aided instruction; Computer aided software engineering; Controllers; Data mining; Professional aspects; Angular; Case tool; Codegeneration; Entity relationship diagrams; Entity/Relationship model; Model view controller design patterns; Model-view controller; Model-view-controller architectures; Model-view-controller design patterns; Model-view-controller pattern; Software design",Article,Scopus,2-s2.0-85115678370
"Bhatti U.A., Yu Z., Chanussot J., Zeeshan Z., Yuan L., Luo W., Nawaz S.A., Bhatti M.A., Ain Q.U., Mehmood A.","57194505499;23491178000;6602159365;56520165100;57271206800;57271615400;57207104583;57217055773;57220388160;57200864193;","Local Similarity-Based Spatial–Spectral Fusion Hyperspectral Image Classification With Deep CNN and Gabor Filtering",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115666688&doi=10.1109%2fTGRS.2021.3090410&partnerID=40&md5=00168acee83344e254b81b39685ca474","Currently, the different deep neural network (DNN) learning approaches have done much for the classification of hyperspectral images (HSIs), especially most of them use the convolutional neural network (CNN). HSI data have the characteristics of multidimensionality, correlation, nonlinearity, and a large amount of data. Therefore, it is particularly important to extract deeper features in HSIs by reducing dimensionalities which help improve the classification in both spectral and spatial domains. In this article, we present a spatial–spectral HSI classification algorithm, local similarity projection Gabor filtering (LSPGF), which uses local similarity projection (LSP)-based reduced dimensional CNN with a 2-D Gabor filtering algorithm. First, use the local similarity analysis to reduce the dimensionality of the hyperspectral data, and then we use the 2-D Gabor filter to filter the reduced hyperspectral data to generate spatial tunnel information. Second, use the CNN to extract features from the original hyperspectral data to generate spectral tunnel information. Third, the spatial tunnel information and the spectral tunnel information are fused to form the spatial–spectral feature information, which is input into the deep CNN to extract more effective features; and finally, a dual optimization classifier is used to classify the final extracted features. This article compares the performance of the proposed method with other algorithms in three public HSI databases and shows that the overall accuracy of the classification of LSPGF outperforms all datasets. © 2021 IEEE.","Classification algorithms; Convolutional neural networks; Data mining; Feature extraction; Hyperspectral imaging; Principal component analysis; Training","Classification (of information); Convolution; Data mining; Data reduction; Deep neural networks; Hyperspectral imaging; Image classification; Principal component analysis; Spectroscopy; Classification algorithm; Convolutional neural network; Deep convolutional neural network; Features extraction; Gabor feature; Hyperspectral image classification.; Hyperspectral image classification; Principal-component analysis; Gabor filters; image classification; learning; optimization; spatial analysis",Article,Scopus,2-s2.0-85115666688
"Sun Y., Fu Z., Sun C., Hu Y., Zhang S.","57410211400;57221034264;57271940000;57271108600;57272077500;","Deep Multimodal Fusion Network for Semantic Segmentation Using Remote Sensing Image and LiDAR Data",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115665120&doi=10.1109%2fTGRS.2021.3108352&partnerID=40&md5=cddcfa175e264f57071cff0c52604fb2","Extracting semantic information from very-high-resolution (VHR) aerial images is a prominent topic in the Earth observation research. An increasing number of different sensor platforms are appearing in remote sensing, each of which can provide corresponding multimodal supplemental or enhanced information, such as optical images, light detection and ranging (LiDAR) point clouds, infrared images, or inertial measurement unit (IMU) data. However, these current deep networks for LiDAR and VHR images have not fully utilized the complete potential of multimodal data. The stacked multimodal fusion network (MFNet) ignores the structural differences between the modalities and the manual statistical characteristics within the modalities. For multimodal remote sensing data and its corresponding carefully designed handcrafted features, we designed a novel deep MFNet that can use multimodal VHR aerial images and LiDAR data and the corresponding intramodal features, such as LiDAR-derived features [slope and normalized digital surface model (NDSM)] and imagery-derived features [infrared–red–green (IRRG), normalized difference vegetation index (NDVI), and difference of Gaussian (DoG)]. Technically, we introduce the attention mechanism and multimodal learning to adaptively fuse intermodal and intramodal features. Specifically, we designed a multimodal fusion mechanism, pyramid dilation blocks, and a multilevel feature fusion module. Through these modules, our network realized the adaptive fusion of multimodal features, improved the receptive field, and enhanced the global-to-local contextual fusion effect. Moreover, we used a multiscale supervision training scheme to optimize the network. Extensive experimental results and ablation studies on the ISPRS semantic dataset and IEEE GRSS DFC Zeebrugge dataset show the effectiveness of our proposed MFNet. © 2021 IEEE.","Feature extraction; Image segmentation; Laser radar; Semantics; Sensors; Sun; Task analysis","Antennas; Data mining; Geometrical optics; Image enhancement; Image fusion; Image segmentation; Infrared imaging; Neural networks; Optical radar; Radar imaging; Remote sensing; Aerial images; Attention mechanisms; Convolutional neural network; Features extraction; Images segmentations; Multi-modal fusion; Semantic labeling; Semantic labeling.; Task analysis; Semantics; image analysis; laser; lidar; remote sensing; semantic standardization; sensor; Belgium; West Flanders; Zeebrugge",Article,Scopus,2-s2.0-85115665120
"Wei X., Qian Y., Sun C., Sun J., Liu Y.","57270153900;57205745973;57189869017;55634319700;16200075200;","A survey of location-based social networks: problems, methods, and future research directions",2022,"GeoInformatica","26","1",,"159","199",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115649400&doi=10.1007%2fs10707-021-00450-1&partnerID=40&md5=8595dad5f7aa287536f8468ebbadf338","The development of mobile devices and positioning technology has facilitated the rapid growth of location-based social networks (LBSNs). Users in these networks can share geo-related information in real-time, including locations, trajectories, geo-tagged pictures, and tweets. LBSNs record massive amounts of spatiotemporal data and offer a great opportunity to analyze human and location-specific spatiotemporal characteristics. It plays an important role in various applications, such as marketing, recommendations, and urban planning. In this study, we collect relevant literature about LBSNs research in the past 10 years and use a topic model, latent Dirichlet allocation (LDA), to uncover the highly heterogeneous area of research related to LBSNs. Then, we conduct a systematic review of those works. In doing so, we organize identified literature into eight fine-grained directions. For each direction, we sum up the major research focus and contributions. We also systematize future research into four main themes concerning data simulation and fusion, privacy-aware methods, new applications and services, and technological innovations. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Data mining; LDA; Mobility; Privacy; Recommendation; Spatio-temporal","Data mining; Data privacy; Social networking (online); Social sciences computing; Statistics; Device technologies; Future research directions; Latent Dirichlet allocation; Location-based social networks; Mobility; Network problems; Positioning technologies; Privacy; Recommendation; Spatio-temporal; Location; data mining; innovation; mobile phone; social network; spatial data; spatiotemporal analysis",Article,Scopus,2-s2.0-85115649400
"Fan T., Wang H.","57270870800;57211369297;","Research of Chinese intangible cultural heritage knowledge graph construction and attribute value extraction with graph attention network",2022,"Information Processing and Management","59","1","102753","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115614038&doi=10.1016%2fj.ipm.2021.102753&partnerID=40&md5=8eeb7aecca1fc3156bc7f378dc4de88f","The development of digital technology promotes the construction of the Intangible cultural heritage (ICH) database but the data is still unorganized and not linked well, which makes the public hard to master the overall knowledge of the ICH. An ICH knowledge graph (KG) can help the public to understand the ICH and facilitate the protection of the ICH. However, a general framework of ICH KG construction is lacking now. In this study, we take the Chinese ICH (nation-level) as an example and propose a framework to build a Chinese ICH KG combining multiple data sources from Baike and the official website, which can extend the scale of the KG. Besides, the data of ICH grows daily, requiring us to design an efficient model to extract the knowledge from the data to update the KG in time. The built KG is based on the triple 〈entity, attribute, attribute value〉 and we introduce the attribute value extraction (AVE) task. However, the public Chinese ICH annotated AVE corpus is lacking. To solve that, we construct a Chinese ICH AVE corpus based on the Distant Supervision (DS) automatically rather than employing traditional manual annotation. Currently, AVE is usually seen as the sequence tagging task. In this paper, we take the ICH AVE as a node classification task and propose an AVE model BGC, combining the BiLSTM and graph attention network, which can fuse and utilize the word-level and character-level information by means of the ICH lexicon generated from the KG. We conduct extensive experiments and compare the proposed model with other state-of-the-art models. Experimental results show that the proposed model is of superiority. © 2021","Attribute value extraction; Digital humanity; Intangible cultural heritage; Knowledge graph","Classification (of information); Data mining; Extraction; Attribute value extraction; Attribute values; Corpus-based; Digital humanities; Digital technologies; Graph construction; Intangible cultural heritages; Knowledge graphs; Manual annotation; Multiple data sources; Knowledge graph",Article,Scopus,2-s2.0-85115614038
"Zhang Z., Chen C., Chang Y., Hu W., Zheng Z., Zhou Y., Sun L.","57270796700;56442806000;57211988140;57208648487;25224189400;7405367554;57270796800;","HIN2Grid: A disentangled CNN-based framework for heterogeneous network learning",2022,"Expert Systems with Applications","187",,"115823","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115611419&doi=10.1016%2fj.eswa.2021.115823&partnerID=40&md5=365b5213dfa108965aee8bc4cb9a8cab","Recently, graph convolutional networks (GCNs) have been applied to heterogeneous information network (HIN) learning and have shown promising performance. However, the performance of GCNs degrades attributed to the recursive propagation, which leads to an indistinguishable embedding for the distinctly heterogeneous node. Besides, the inherently coupled paradigm of GCNs limits their applications on large-scale graphs. In this paper, we tackle these problems by proposing a disentangled framework named Heterogeneous Information Network to Grid (HIN2Grid) for heterogeneous network learning. We innovatively design an effective and efficient strategy to transform the graph data into semantic-specific grid-like data, which can be effectively processed by convolutional neural networks (CNNs), thus explicitly overcoming the drawbacks of the inherent paradigm of GCNs. Such a CNN-based learning scheme also contributes to extracting more expressive features and consume less time and memory. We further propose dual attention mechanisms to capture the importance of various grid-like data and heterogeneous semantics, thus providing interpretability and robustness for HIN2Grid. We conduct experiments on four datasets and the results show that HIN2Grid significantly outperforms the state-of-the-art methods, gaining a improvement on node classification of about 2% to 10% and a 2 to 5 times promotion on running speed. © 2021","Data mining; Graph convolutional network; Heterogeneous graph learning; Network embedding","Backpropagation; Classification (of information); Convolution; Convolutional neural networks; Data mining; Embeddings; Heterogeneous networks; Information services; Semantics; Convolutional networks; Convolutional neural network; Graph convolutional network; Heterogeneous graph; Heterogeneous graph learning; Heterogeneous information; Information networks; Network embedding; Networks learning; Performance; Network embeddings",Article,Scopus,2-s2.0-85115611419
"Zhang S., Xu H., Zhu G., Chen X., Li K.C.","35303797500;57217829437;35757177600;57217825865;57224659283;","A data processing method based on sequence labeling and syntactic analysis for extracting new sentiment words from product reviews",2022,"Soft Computing","26","2",,"853","866",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115602715&doi=10.1007%2fs00500-021-06228-9&partnerID=40&md5=8a40264ddd0fbeb76558286313ef17d7","New sentiment words in product reviews are valuable resources that are directly close to users. The data processing of new sentiment word extraction can provide information service better for users and provide theoretical support for the related research of edge computing. Traditional methods for extracting new sentiment words generally ignored the context and syntactic information, which leads to the low accuracy and recall rate in the process of extracting new sentiment words. To tackle the mentioned issue, we proposed a data processing method based on sequence labeling and syntactic analysis for extracting new sentiment words from product reviews. Firstly, the probability that the new word is a sentiment word is calculated through the location rules derived from the sequence labeling result, and the candidate set of new sentiment words is obtained according to the probability. Then, the candidate set of new sentiment words is supplemented with the method of matching appositive words based on edit distance. Finally, the final set of new sentiment words is collected through fine-grained filtering, including the calculation of point mutual information and difference coefficient of positive and negative corpus (DC-PNC). The experimental results illustrate the effectiveness of new sentiment words extracted by the proposed method which can obviously improve the accuracy and recall rate of sentiment analysis. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","New sentiment words; Product reviews; Sequence labeling; Syntactic analysis","Data handling; Data mining; Information filtering; Information services; Sentiment analysis; Accuracy rate; Candidate sets; Context information; Data processing methods; Edge computing; New sentiment word; Product reviews; Recall rate; Sequence Labeling; Syntactic analysis; Syntactics",Article,Scopus,2-s2.0-85115602715
"Huynh H.M., Nguyen L.T.T., Vo B., Oplatková Z.K., Fournier-Viger P., Yun U.","56039325700;55195057700;35147075900;15043128400;14048484800;8958234600;","An efficient parallel algorithm for mining weighted clickstream patterns",2022,"Information Sciences","582",,,"349","368",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115427566&doi=10.1016%2fj.ins.2021.08.070&partnerID=40&md5=16836766adef7636478564b4e297f286","In the Internet age, analyzing the behavior of online users can help webstore owners understand customers’ interests. Insights from such analysis can be used to improve both user experience and website design. A prominent task for online behavior analysis is clickstream mining, which consists of identifying customer browsing patterns that reveal how users interact with websites. Recently, this task was extended to consider weights to find more impactful patterns. However, most algorithms for mining weighted clickstream patterns are serial algorithms, which are sequentially executed from the start to the end on one running thread. In real life, data is often very large, and serial algorithms can have long runtimes as they do not fully take advantage of the parallelism capabilities of modern multi-core CPUs. To address this limitation, this paper presents two parallel algorithms named DPCompact-SPADE (Depth load balancing Parallel Compact-SPADE) and APCompact-SPADE (Adaptive Parallel Compact-SPADE) for weighted clickstream pattern mining. Experiments on various datasets show that the proposed parallel algorithm is efficient, and outperforms state-of-the-art serial algorithms in terms of runtime, memory consumption, and scalability. © 2021 Elsevier Inc.","Frequent pattern mining; Parallelism; Weighted clickstream patterns","Data mining; Program processors; Websites; Behavior analysis; Browsing patterns; Clickstreams; Frequent patterns minings; Online behaviours; Online users; Parallelism; Runtimes; Serial algorithms; Weighted clickstream pattern; Parallel algorithms",Article,Scopus,2-s2.0-85115427566
"Yu Y., Huang H.","57044986300;57211013474;","Eliciting symptom-diagnosis knowledge from online medical Q&A",2022,"Expert Systems","39","1","e12821","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115423166&doi=10.1111%2fexsy.12821&partnerID=40&md5=6ecd8b4cf94a9d61127f6951c97d00bd","With the objective to automatically detect diseases from symptoms in free-text data, a methodology to extract symptom-diagnosis knowledge from online medical textual data in Q&A domain is proposed in this paper: (1) a term frequency-inverse document frequency and PRECISION method is adopted to retrieve symptom words from unstructured text; (2) a variable precision rough set based genetic algorithm is applied to reduce redundant symptom words, and a rough set based rule is utilized for adding discriminative symptom words assisting to discriminate diseases sharing similar symptoms; (3) by employing fuzzy linguistic variables to express the risk level of disease or severity level of symptoms, a knowledge base with fuzzy belief structure is generated. Using data extracted from a Chinese medical Q&A forum for training and testing, some classical gastrointestinal diseases serve as a case study to evaluate the efficiency of the proposed methodology. Subsequently performance comparisons are made between the proposed methodology and some other classifiers, such as the decision tree algorithms including ID3 and J45, and the Bayesian network classifier. The comparative results demonstrate that the proposed methodology outperforms the decision tree algorithms and the Bayesian network classifier. © 2021 John Wiley & Sons Ltd.",,"Bayesian networks; Data mining; Decision trees; Genetic algorithms; Inverse problems; Knowledge based systems; Rough set theory; Text processing; Bayesian network classifiers; Decision-tree algorithm; Dempster's rule of combination; Free texts; Rough-set based; Term frequencyinverse document frequency (TF-IDF); Text data; Textual data; TF-IDF; Variable precision rough sets; Diagnosis",Article,Scopus,2-s2.0-85115423166
"Kim H., Arisato Y., Inoue J.","57210863804;57267771300;7201840505;","Unsupervised segmentation of microstructural images of steel using data mining methods",2022,"Computational Materials Science","201",,"110855","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115392539&doi=10.1016%2fj.commatsci.2021.110855&partnerID=40&md5=a02ea260d25ea0b46f03aa20f10260cf","A novel and efficient data mining method for the segmentation of microstructural images of low-carbon steel is presented. Microstructural characterization has been the focus of many works in the field of materials science because microstructure is the fundamental element in understanding the link between process and property. Recently, deep-learning-based methods have been actively employed for microstructural classification since it has shown outstanding performance for solving image classification problems. However, previous applications of deep learning models to microstructural classification revealed limitations in that not only do they require the time-consuming labeling process but it is also still difficult to obtain a satisfactory result, especially for steel microstructures containing substances developed by displacive transformation. In this study, we propose a rule-based segmentation method that not only works without labeled images but also requires no prior knowledge of the number of microstructural constituents in each image. This unsupervised inference algorithm captures the morphological features of each microstructure and automatically finds the optimal number of microstructures having similar characteristics using a Bayesian Gaussian mixture model. The viability of our method is demonstrated by qualitative and quantitative evaluations with optical microscopy images of steel composed of different microstructures taken under different imaging conditions. © 2021 Elsevier B.V.","And unsupervised learning; Machine learning; Microstructure segmentation; Steel microstructure","Data mining; Deep learning; Gaussian distribution; Image segmentation; Inference engines; Low carbon steel; Unsupervised learning; And unsupervised learning; Data mining methods; Low-carbon steels; Material science; Micro-structural; Microstructural characterizations; Microstructural images; Microstructure segmentation; Steel microstructure; Unsupervised segmentation; Microstructure",Article,Scopus,2-s2.0-85115392539
"Mahdi M.A., Hosny K.M., Elhenawy I.","57189577760;57205214086;54915283100;","FR-Tree: A novel rare association rule for big data problem",2022,"Expert Systems with Applications","187",,"115898","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115364647&doi=10.1016%2fj.eswa.2021.115898&partnerID=40&md5=11ba9c26bb9d87c30a3ab3200771fb1e","In some situations, finding the rare association rule is of higher importance than the frequent itemset. Unique rules represent rare cases, activities, or events in real-world applications. It is essential to extract exceptional critical activity from vast routine data. This paper proposes a new algorithm called FR-Tree to mine the association rules and produce essential rules. This work aims to demonstrate that this algorithm is suitable for extracting rare association rules with high confidence. The proposed algorithm generates, filters, and classifies the all-important rules, either frequent or rare. The rare rules were produced without needing to set an additional threshold. Therefore, the proposed algorithm has an advantage incomparable with the other rare association rule techniques. The generated rules were tested using well-known datasets, and the performance was compared with the other rare association rule techniques. The results proved that our method outperformed the existing rare association rule techniques. © 2021 Elsevier Ltd","Association rules; Categorical data; Clustering; Data mining; Rare association rules","Association rules; Big data; Clustering algorithms; Filtration; Trees (mathematics); Categorical data; Clusterings; Critical activities; Data problems; Frequent itemset; High confidence; Performance; Rare association rules; Real-world; Data mining",Article,Scopus,2-s2.0-85115364647
"Krier J., Singh R.R., Kondić T., Lai A., Diderich P., Zhang J., Thiessen P.A., Bolton E.E., Schymanski E.L.","57225119199;57217371625;57222357908;57202036830;57225119495;55992279000;7004023983;16686522000;24068144000;","Discovering pesticides and their TPs in Luxembourg waters using open cheminformatics approaches",2022,"Environment International","158",,"106885","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115305430&doi=10.1016%2fj.envint.2021.106885&partnerID=40&md5=031875ed168407c6f8a8cd02ad07bf0d","The diversity of hundreds of thousands of potential organic pollutants and the lack of (publicly available) information about many of them is a huge challenge for environmental sciences, engineering, and regulation. Suspect screening based on high-resolution liquid chromatography-mass spectrometry (LC-HRMS) has enormous potential to help characterize the presence of these chemicals in our environment, enabling the detection of known and newly emerging pollutants, as well as their potential transformation products (TPs). Here, suspect list creation (focusing on pesticides relevant for Luxembourg, incorporating data sources in 4 languages) was coupled to an automated retrieval of related TPs from PubChem based on high confidence suspect hits, to screen for pesticides and their TPs in Luxembourgish river samples. A computational workflow was established to combine LC-HRMS analysis and pre-screening of the suspects (including automated quality control steps), with spectral annotation to determine which pesticides and, in a second step, their related TPs may be present in the samples. The data analysis with Shinyscreen (https://gitlab.lcsb.uni.lu/eci/shinyscreen/), an open source software developed in house, coupled with custom-made scripts, revealed the presence of 162 potential pesticide masses and 96 potential TP masses in the samples. Further identification of these mass matches was performed using the open source approach MetFrag (https://msbi.ipb-halle.de/MetFrag/). Eventual target analysis of 36 suspects resulted in 31 pesticides and TPs confirmed at Level-1 (highest confidence), and five pesticides and TPs not confirmed due to different retention times. Spatio-temporal analysis of the results showed that TPs and pesticides followed similar trends, with a maximum number of potential detections in July. The highest detections were in the rivers Alzette and Mess and the lowest in the Sûre and Eisch. This study (a) added pesticides, classification information and related TPs into the open domain, (b) developed automated open source retrieval methods - both enhancing FAIRness (Findability, Accessibility, Interoperability and Reusability) of the data and methods; and (c) will directly support “L'Administration de la Gestion de l'Eau” on further monitoring steps in Luxembourg. © 2021","Data mining; High resolution tandem mass spectrometry; Non-target screening; Pesticides; Suspect screening; Transformation products","Automation; C (programming language); Chemical detection; Classification (of information); Data mining; Liquid chromatography; Mass spectrometry; Metadata; Open source software; Open systems; Organic pollutants; Polychlorinated biphenyls; Quality control; Reusability; Screening; Water pollution; Cheminformatics; High confidence; High-resolution tandem mass spectrometries; Liquid chromatography - mass spectrometries; Luxembourg; Non-target screenings; Open-source; Potential transformations; Suspect screening; Transformation products; Pesticides; pesticide; pesticide residue; river water; transformation product; unclassified drug; pesticide; data mining; detection method; liquid chromatography; organic pollutant; pesticide; pollution monitoring; river water; spatiotemporal analysis; water quality; Article; automation; cheminformatics; data analysis; data mining; liquid chromatography-mass spectrometry; Luxembourg; mathematical analysis; quality control; river; spatiotemporal analysis; water monitoring; water pollution; water pollutant; Alzette River; Luxembourg [Benelux]; Luxembourg [Luxembourg (NTN)]; Mess River; Cheminformatics; Luxembourg; Pesticides; Rivers; Water Pollutants, Chemical",Article,Scopus,2-s2.0-85115305430
"Xu Z., Dang Y., Wang Q.","57191840489;8223823300;57262777500;","Potential buyer identification and purchase likelihood quantification by mining user-generated content on social media",2022,"Expert Systems with Applications","187",,"115899","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115185042&doi=10.1016%2fj.eswa.2021.115899&partnerID=40&md5=db3c5ea23fa05ffb58efa0f7b5e29900","Understanding the purchase likelihood of potential buyers is an important prerequisite for marketers to carry out targeted marketing. Massive authentic and personalized user-generated content (UGC) generated on social media, reflecting the content creator's purchase intent, provides a new possibility for decision-makers to accomplish this task yet remain mostly untapped by many firms. As such, the current research develops a two-stage approach where potential buyers are first identified based on the premise of classifying user's posts into before buying and after buying, and their purchase likelihood is quantified by a novel Weighted Recency, Focus, and Sentiment (WRFS) model. Data from the Honda Civic community in the Bitauto automotive forum are employed to verify the proposed method. 2492 from 10,229 users in the Honda Civic community were identified as potential buyers, and their purchase likelihood is obtained by the WRFS model. The actual purchases of these potential buyers are then observed and verified. The results highlight that the higher the purchase likelihood, the higher the proportion of users who purchase, which illustrates the accuracy of the proposed method. © 2021 Elsevier Ltd","Automotive forum; Data mining; Potential buyer identification; Purchase likelihood quantification; Social media; User-generated content","Commerce; Decision making; Sales; Social networking (online); Automotive forum; Automotives; Honda civic; Potential buyer identification; Potential buyers; Purchase likelihood quantification; Social media; Targeted marketing; User-generated; User-generated content; Data mining",Article,Scopus,2-s2.0-85115185042
"Zhou W., Jin J., Lei J., Hwang J.-N.","24781509400;57263469400;7201403736;7403896543;","CEGFNet: Common Extraction and Gate Fusion Network for Scene Parsing of Remote Sensing Images",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115121581&doi=10.1109%2fTGRS.2021.3109626&partnerID=40&md5=864154177c55098f11f0e6ee95083513","Scene parsing of high spatial resolution (HSR) remote sensing images has achieved notable progress in recent years by the adoption of convolutional neural networks. However, for scene parsing of multimodal remote sensing images, effectively integrating complementary information remains challenging. For instance, the decrease in feature map resolution through a neural network causes loss of spatial information, likely leading to blurred object boundaries and misclassification of small objects. In addition, object scales on a remote sensing image vary substantially, undermining the parsing performance. To solve these problems, we propose an end-to-end common extraction and gate fusion network (CEGFNet) to capture both high-level semantic features and low-level spatial details for scene parsing of remote sensing images. Specifically, we introduce a gate fusion module to extract complementary features from spectral data and digital surface model data. A gate mechanism removes redundant features in the data stream and extracts complementary features that improve multimodal feature fusion. In addition, a global context module and a multilayer aggregation decoder handle scale variations between objects and the loss of spatial details due to downsampling, respectively. The proposed CEGFNet was quantitatively evaluated on benchmark scene parsing datasets containing HSR remote sensing images, and it achieved state-of-the-art performance. © 2021 IEEE.","Data mining; Decoding; Feature extraction; Logic gates; Remote sensing; Semantics; Spatial resolution","Benchmarking; Convolutional neural networks; Data mining; Data streams; Extraction; Image fusion; Image processing; Semantics; Complementary features; Digital surface models; High spatial resolution; High-level semantic features; Multimodal feature fusions; Remote sensing images; Spatial informations; State-of-the-art performance; Remote sensing; aggregation; data; image analysis; remote sensing; sequential extraction; spatial resolution",Article,Scopus,2-s2.0-85115121581
"Hu Y.-H.","57223101412;","Effects of the COVID-19 pandemic on the online learning behaviors of university students in Taiwan",2022,"Education and Information Technologies","27","1",,"469","491",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115103453&doi=10.1007%2fs10639-021-10677-y&partnerID=40&md5=20e79f89844a5c73d0a51d5b5a30139c","Prior to the coronavirus disease 2019 (COVID-19) pandemic, due to the rarity of pandemics in recent centuries, suitable conditions did not exist in educational institutions for the implementation of asynchronous distance teaching. No empirical studies have been conducted on whether the considerable environmental changes caused by COVID-19 have affected students’ online learning behaviors. Therefore, this study collected information on students’ online learning behaviors during the COVID-19 pandemic and other periods to examine whether pandemic-caused environmental changes affected students’ online learning behaviors. This study focuses on the 60-day transmission after the beginning of the second semester of the 2019 academic year. The data source was from a comparative assessment between the pandemic group (331 students) and the control group (101 students). The Spearman Rank Correlation Test and the Wilcoxon signed-rank test were used as our statistical methods. This paper presents preliminary results on how COVID-19 has affected students’ online learning behaviors and proposes asynchronous online learning as a method for maintaining university students’ learning during the COVID-19 pandemic. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Asynchronous online learning; COVID-19 pandemic; Data mining; e-Learning; Learning analytics",,Article,Scopus,2-s2.0-85115103453
"Torres S., Durán I., Marulanda A., Pavas A., Quirós-Tortós J.","56937972300;55923460900;27467497500;25632912500;55210868700;","Electric vehicles and power quality in low voltage networks: Real data analysis and modeling",2022,"Applied Energy","305",,"117718","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115035325&doi=10.1016%2fj.apenergy.2021.117718&partnerID=40&md5=8d5f329819f284c85522bb030f921498","Electric vehicles (EVs) will help to decarbonize energy systems. However, their connection to on-board level 2 chargers (7.2 kW) at household facilities brings challenges to Distribution Network Operators (DNOs) as they can affect the power quality of low voltage (LV) networks. In order to truly assess these effects, the electrical behavior of the on-board charger in terms of its non-linear content, power demand, and charge rate must be understood first. Nonetheless, most modeling methodologies with this aim result in circuital approaches, and thus, in heavy computational burdens, or assume simplified representations that do not correspond to the reality of the charge. To overcome this, we present a new methodology to model the power quality characteristics of EVs based on measured data from the harmonic spectra of the charger. The model provides a precise and efficient electrical characterization, where probabilistic models of the harmonic spectra are used to compute the power demand during every stage of the charge. Due to its probabilistic nature, these harmonic spectra are represented using Gaussian Mixture Models. We validate the model contrasting simulated data versus real measured one. Then, we illustrate a case study of the model in a LV network power quality assessment with different EV penetration levels, considering time-series harmonic power flows with 10-min resolution under a Monte Carlo approach. Obtained results revealed an increase in the network chargeability and voltage unbalance, along with an increased content of the third harmonic, which appears to be the most intense. © 2021 Elsevier Ltd","Electric vehicle; EV electric model; EV harmonic spectrum; Power quality","Electric power utilization; Electric vehicles; Harmonic analysis; Analysis and models; Board-level; Electric models; Electric vehicle electric model; Electric vehicle harmonic spectrum; Energy systems; Harmonic spectrum; Low-voltage networks; Power demands; Real data analysis; Power quality; data mining; electric vehicle; electricity supply; energy storage; energy use",Article,Scopus,2-s2.0-85115035325
"Qian W., Xiong Y., Yang J., Shu W.","57206747603;57260354900;54792477500;7103148783;","Feature selection for label distribution learning via feature similarity and label correlation",2022,"Information Sciences","582",,,"38","59",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114997380&doi=10.1016%2fj.ins.2021.08.076&partnerID=40&md5=a9d4aebae80160ffc73c12365c4d9520","Feature selection plays a crucial role in machine learning and data mining, and improves the performance of learning models by selecting a distinguishing feature subset and eliminating irrelevant features. Existing feature selection methods are mainly used for single-label learning and multi-label learning; however, there are only a few feature selection methods for label distribution learning. Label distribution learning has the “curse of dimensionality” problem, similar to that in multi-label learning. In label distribution learning, the related labels of each sample have different levels of importance. Therefore, multi-label feature selection algorithms can not be directly applied to label distribution data, and discretizing the label distribution data into multi-label data would result in the loss of some important supervised information. To solve this problem, a novel feature selection algorithm for label distribution learning is proposed in this paper. The proposed method utilizes neighborhood granularity to explore feature similarity, and it uses a correlation coefficient to generate the label correlations. In addition, sparse learning is used to improve the robustness and control complexity. Experimental results indicate that our proposed method is more effective than five state-of-art feature selection algorithms on twelve datasets, with respect to six representative evaluation measures. © 2021 Elsevier Inc.","Feature selection; Feature similarity; Label correlation; Label distribution learning","Data mining; Learning algorithms; Learning systems; Feature selection algorithm; Feature selection methods; Feature similarity; Features selection; Label correlations; Label distribution; Label distribution learning; Multi-label learning; Multi-labels; Performance; Feature extraction",Article,Scopus,2-s2.0-85114997380
"Chen B., Huang Z., Liu C., Wu Z.","56948852200;57194379930;27170953000;8261698100;","Spatio-temporal data mining method for joint cracks in concrete dam based on association rules",2022,"Structural Control and Health Monitoring","29","1","e2848","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114926019&doi=10.1002%2fstc.2848&partnerID=40&md5=8fd00fec8b483515e60b23f87c7a2aee","Structural health monitoring (SHM) has been widely employed to reveal the hidden safety information and to diagnose the safety status in dam engineering fields. As one of the most important parameters of SHM, crack opening displacement (COD) is often used to evaluate the cracks or joints of concrete dams. In this paper, a new dam health analytic perspective is introduced by integrating the data mining method into SHM field, focusing on revealing the association rules in COD monitoring data. The association rules are investigated systematically, considering the cause–effect relations between external loads and structural response, the temporal characteristics of time series for a single sensor, the spatial characteristics of monitoring data for multisensors, and the abnormal characteristics for different items of structural responses. The association relation is quantified by proposing the quantitative indexes, including support degree, confidence degree, and promotion degree. The methods are used in the COD monitoring data of the Baishan concrete gravity-arch dam, which is located in a severely cold area in northeastern China. Results show that 4 out of 24 cause–effect association rules are extracted by calculating the association degree of monitored COD values, and 21 out of 24 crack sensors present a temporal association relationship, among which the confidence degree of two sensors reaches 100%. The variation trend of COD values is relevant with the locations of the crack sensors. These results are consistent with the dam safety monitoring theories and models, which would be very useful for extracting the SHM information between different sensors, predicting the trend of COD value and repairing the monitoring data series of COD sensors, or even for discovering an abnormal signal for the operation safety of dams. © 2021 John Wiley & Sons, Ltd.","association rules; concrete gravity-arch dam; crack opening displacement; pattern recognition; spatiotemporal data mining; Structural health monitoring","Arch dams; Association rules; Concrete dams; Concretes; Gravity dams; Monitoring; Safety engineering; Structural health monitoring; Abnormal characteristics; Crack opening displacements; Spatial characteristics; Spatio-temporal data mining; Structural health monitoring (SHM); Structural response; Temporal association; Temporal characteristics; Data mining",Article,Scopus,2-s2.0-85114926019
"Wang L., Teng C., Jiang K., Jiang C., Zhu S.","57196340847;57258661300;57216692191;57204547826;57258207500;","D-InSAR Monitoring Method of Mining Subsidence Based on Boltzmann and Its Application in Building Mining Damage Assessment",2022,"KSCE Journal of Civil Engineering","26","1",,"353","370",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114876971&doi=10.1007%2fs12205-021-1042-5&partnerID=40&md5=1d61f02a7830eb48bb337216a6f67622","Monitoring and predicting mining area subsidence caused by coal mining help effectively control geological disasters. Information regarding small surface deformations can be obtained using a differential interferometric synthetic aperture radar (D-InSAR), which exhibits high monitoring accuracy and can cover large areas; thus, D-InSAR are being applied for mining area settlement monitoring. However, mining areas are prone to large gradient deformations in short durations; obtaining information regarding such deformations is outside the scope of D-InSAR monitoring. To adapt to the characteristics of D-InSAR monitoring, this study selected a Boltzmann function model with a slow boundary convergence rate to address the problem of probabilistic integration methods exhibiting a high boundary convergence rate. The three-dimensional surface deformation of the mining area can be accurately obtained. According to the projection relationship between D-InSAR line of sight (LOS) directional deformation, subsidence, and horizontal movement, a D-InSAR monitoring equation for mining subsidence assisted by Boltzmann is derived. This equation is combined with the shuffled frog leaping algorithm (SFLA) to obtain the parameters to be estimated from the equation. The D-InSAR LOS deformation information of the 13121 working face in the Huainan mining area was obtained from July 14 to October 30, 2019. The proposed method was then used to obtain the predicted parameters of the working face under insufficient mining. The predicted parameters of mining subsidence were calculated to obtain the predicted parameters when it was fully mined. Then, the revised parameters were used to predict the subsidence and horizontal movement of the mining area and compared with the actual leveling observations. The results show that the mean square error of predicted subsidence is 97.1 mm, which is about 3.09% of the maximum subsidence value; the mean square error of predicted horizontal movement is 46.1 mm, which is about 4.1% of the maximum horizontal movement value. The predicted results of the aforementioned method were used to analyze the damage to the buildings above the working face and determine the damage level of the buildings to provide a reference for the demolition and maintenance of the Zhaimiao village. © 2021, Korean Society of Civil Engineers.","Boltzmann function; D-InSAR; Mining damage assessment; Prediction of mining subsidence; Shuffled frog leaping algorithm","Boltzmann equation; Coal industry; Cutting machines (mining); Damage detection; Deformation; Mean square error; Monitoring; Motion compensation; Subsidence; Synthetic aperture radar; Differential interferometric synthetic aperture radars; Geological disaster; Horizontal movements; Probabilistic integration; Settlement monitoring; Shuffled frog leaping algorithm (SFLA); Surface deformation; Three-dimensional surface; Data mining",Article,Scopus,2-s2.0-85114876971
"Yao J., Lu B., Zhang J.","57226890104;57226881368;57226894759;","Tool remaining useful life prediction using deep transfer reinforcement learning based on long short-term memory networks",2022,"International Journal of Advanced Manufacturing Technology","118","3-4",,"1077","1086",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114781232&doi=10.1007%2fs00170-021-07950-2&partnerID=40&md5=5fcd64268c3c5ddf4b47e115a131fe86","Tool wear and faults will affect the quality of machined workpiece and damage the continuity of manufacturing. The accurate prediction of remaining useful life (RUL) is significant to guarantee the processing quality and improve the productivity of automatic system. At present, the most commonly used methods for tool RUL prediction are trained by history fault data. However, when researching on new types of tools or processing high value parts, fault datasets are difficult to acquire, which leads to RUL prediction a challenge under limited fault data. To overcome the shortcomings of above prediction methods, a deep transfer reinforcement learning (DTRL) network based on long short-term memory (LSTM) network is presented in this paper. Local features are extracted from consecutive sensor data to track the tool states, and the trained network size can be dynamically adjusted by controlling time sequence length. Then in DTRL network, LSTM network is employed to construct the value function approximation for smoothly processing temporal information and mining long-term dependencies. On this basis, a novel strategy of Q-function update and transfer is presented to transfer the deep reinforcement learning (DRL) network trained by historical fault data to a new tool for RUL prediction. Finally, tool wear experiments are performed to validate effectiveness of the DTRL model. The prediction results demonstrate that the proposed method has high accuracy and generalization for similar tools and cutting conditions. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Deep reinforcement learning; Remaining useful life (RUL) prediction; Tool wear monitoring; Transfer learning","Brain; Cutting tools; Data mining; Deep learning; Forecasting; Reinforcement learning; Transfer learning; Wear of materials; Accurate prediction; Long-term dependencies; Prediction methods; Processing quality; Remaining useful life predictions; Remaining useful lives; Temporal information; Value function approximation; Long short-term memory",Article,Scopus,2-s2.0-85114781232
"Ai J., Mao Y., Luo Q., Jia L., Xing M.","35319586300;57221601419;55774570900;55541053400;7005922869;","SAR Target Classification Using the Multikernel-Size Feature Fusion-Based Convolutional Neural Network",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114745555&doi=10.1109%2fTGRS.2021.3106915&partnerID=40&md5=259a4231cd407bd020449e8777a88dcd","It is well-known that the convolutional neural network (CNN) is an effective method for synthetic aperture radar (SAR) target classification. In the convolutional layer of CNN, convolutional kernels of different sizes can extract different feature information of the target. The small-size kernel can extract the local texture feature information, and the large-size kernel can extract the global contour feature information. Traditional CNN methods usually use fixed-size kernels for convolution, and they generally lose part of the target's feature information, resulting in the inaccurate classification of the SAR targets. This article proposes a novel CNN model based on multikernel-size feature fusion (MKSFF-CNN) for SAR target classification. MKSFF-CNN designs a convolutional methodology with a multichannel parallel topology, it uses convolutional kernels of different sizes to extract the multikernel-size deep features of the SAR target, and then, these features are fused in an optimal way to acquire the lowest loss. Moreover, MKSFF-CNN concatenates the fused features extracted by the convolutional layers of different dimensions to achieve the finest classification. MKSFF-CNN greatly elevates the feature representation completeness of the SAR targets so that more useful feature information can be exploited for SAR target classification. Undoubtedly, MKSFF-CNN can achieve a better classification performance compared with traditional CNN models with a fixed kernel size. The superiority of MKSFF-CNN is validated on the moving and stationary target acquisition and recognition (MSTAR) dataset with the detailed objective and subjective evaluation. © 2021 IEEE.","Convolution; Convolutional neural networks; Data mining; Feature extraction; Kernel; Synthetic aperture radar; Training","Convolution; Convolutional neural networks; Radar target recognition; Synthetic aperture radar; Textures; Classification performance; Convolutional kernel; Feature information; Feature representation; Local texture feature; Objective and subjective evaluations; Stationary targets; Target Classification; Classification (of information); artificial neural network; data mining; data set; design; synthetic aperture radar",Article,Scopus,2-s2.0-85114745555
"Qian Y., Li C.-X., Zou X.-G., Feng X.-B., Xiao M.-H., Ding Y.-Q.","57253227700;57252618200;55551790900;54787483500;18234453600;57253349200;","Research on predicting learning achievement in a flipped classroom based on MOOCs by big data analysis",2022,"Computer Applications in Engineering Education","30","1",,"222","234",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114615518&doi=10.1002%2fcae.22452&partnerID=40&md5=cdb722fa2a7a788a1d145aeda7d481a1","In the era of big data mining, educational data mining has become a principal research focus, with online education mining, such as massive open online courses' (MOOC) data analysis, representing an important source of it. Recent studies have found that learners have low passing rates on MOOCs. A number of studies have proposed prediction models for the dropout rate of learners on MOOCs. The improvement of MOOCs and the promotion of personalized education are the key points of online education. However, the selection and intervention of students with a tendency to drop out slows down the efficiency of teaching and increases the burden on teachers. This study's aim is to utilize back propagation neural networks and radar graphs in a flipped classroom based on MOOCs to predict students' future grades and to analyze the influence of teaching from various perspectives to support the promotion and reform of teaching and curriculum. Compared with the previous year, after the forecast and the adjustment, this year's student scores increase significantly. © 2021 Wiley Periodicals LLC",,"Backpropagation; Big data; Curricula; Data mining; E-learning; Education computing; Forecasting; Predictive analytics; Back propagation neural networks; Educational data mining; Learning achievement; Massive open online course; On-line education; Prediction model; Previous year; Research focus; Students",Article,Scopus,2-s2.0-85114615518
"Tan Q., Yang P., Wen G.","8423288700;56066634400;15043398600;","Deep non-negative tensor factorization with multi-way EMG data",2022,"Neural Computing and Applications","34","2",,"1307","1317",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114286245&doi=10.1007%2fs00521-021-06474-w&partnerID=40&md5=7bb61abfc36ceafb6f2a3679ec8e9031","Tensor decomposition is widely used in a variety of applications such as data mining, biomedical informatics, neuroscience, and signal processing. In this paper, we propose a deep non-negative tensor factorization (DNTF) model to learn intrinsic and hierarchical structures from multi-way data. The DNTF model takes the Tucker tensor decomposition as a building block to stack up a multi-layer structure. In such a way, we can gradually learn the more abstract structures in a higher layer. The benefit is that it helps to mine intrinsic correlations and hierarchical structures from multi-way data. The non-negative constraints allow for clustering interpretation of the extracted data-dependent components. The objective of DNTF is to minimize the total reconstruction loss resulting from using the core tensor in the highest layer and the mode matrices in each layer to reconstruct the data tensor. Then, a deep decomposition algorithm based on multiplicative update rules is proposed to solve the optimization problem. It first conducts layer-wise tensor factorization and then fine-tunes the weights of all layers to reduce the total reconstruction loss. The experimental results on biosignal sensor data demonstrate the effectiveness and robustness of the proposed approach. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Deep tensor factorization; EMG data; Multi-way data analysis; Non-negative","Factorization; Signal processing; Tensors; Biomedical informatics; Decomposition algorithm; Hierarchical structures; Multilayer structures; Multiplicative updates; Optimization problems; Tensor decomposition; Tensor factorization; Data mining",Article,Scopus,2-s2.0-85114286245
"Gao L., Zhao Z., Li C., Zhao J., Zeng Q.","57213829519;56171564000;55696014000;7410313439;7401806588;","Deep cognitive diagnosis model for predicting students’ performance",2022,"Future Generation Computer Systems","126",,,"252","262",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114246185&doi=10.1016%2fj.future.2021.08.019&partnerID=40&md5=055e04ad425d79e7bf4064352ade3034","Cognitive model is playing very important role in predicting students’ performance and recommending learning resources. Thus, it has received a great deal of attention from researchers. However, most of the existing work design models from the aspect of students, ignoring the internal relation between problems and skills. To address this problem, we propose a deep cognitive diagnosis framework to obtain students’ mastery of skills and problems by enhancing traditional cognitive diagnosis methods with deep learning. First, we model the skill proficiency of students according to their responses to objective and subjective problems. Second, students’ mastery on problems is modeled based on attention mechanism and neural network, considering both the importance and the interactions of skills. Finally, considering the facts that students may carelessly select or simply guess the answer, we predict students’ performance via the proposed model. Extensive experiments are carried out on two real-world data sets, and the results have proved the effectiveness and interpretability of this work. © 2021 Elsevier B.V.","Cognitive diagnosis; Deep learning; Educational data mining; Learning analysis; Student modeling","Deep learning; Forecasting; Students; Cognitive diagnosis; Cognitive modeling; Deep learning; Diagnosis model; Educational data mining; Learning analyse; Learning resource; Student Models; Student performance; Work design; Data mining",Article,Scopus,2-s2.0-85114246185
"Sasank V.V.S., Venkateswarlu S.","57215600669;56973963400;","An automatic tumour growth prediction based segmentation using full resolution convolutional network for brain tumour",2022,"Biomedical Signal Processing and Control","71",,"103090","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114127663&doi=10.1016%2fj.bspc.2021.103090&partnerID=40&md5=9a6a2604eeca57836101feebf5d70c74","Segmenting the brain tumour from MRI using a tumour growth model is a developing research field because the intensity feature obtained from first scan point using Lattice Boltzmann Method (LBM) largely improves tumour segmentation result. However, the random selection of LBM parameters reduces the effectiveness of tumour growth model. To overcome that, the Modified Sunflower Optimization (MSFO) algorithm is hybrid along with LBM which optimally selects the parameters that maximize the performance of tumour growth model. Along with these intensity features, the texture features (fractal and multi-fractal Brownian motion (mBm)) are extracted. Before going for feature extraction, the data needs to be pre-processed. Therefore, a scalable range based adaptive bilateral filter (SCRAB) is used at the pre-processing step which removes the noise from the data and improves the edges. Finally, the extracted features are combined and provided as an input to fully resolution convolutional network (FrCN) for segmentation. The performance of the proposed approach is analysed using different metrics viz. accuracy, precision, recall, sensitivity, specificity, and F1-score. Further, the error attained by proposed method is also evaluated using mean absolute percentage error (MAPE), and Root mean square error (RMSE). Three benchmark BRATS dataset such as BRATS 2020, BRATS 2019, and BRATS 2018 are used in this work for performance analysis. The resultant values are compared with the performance of existing methods. The overall accuracy attained by proposed approach for three different datasets are 97% (2020), 95.56% (2019) and 95.23% (2018) respectively. © 2021 Elsevier Ltd","Brain tumour; Cell density pattern; Convolution network; Segmentation; Tumour growth model","Benchmarking; Brain; Brownian movement; Data mining; Errors; Fractals; Image resolution; Image segmentation; Mean square error; Textures; Tumors; Brain tumors; Cell density pattern; Convolution network; Convolutional networks; Intensity features; Lattice Boltzmann method; Performance; Segmentation; Tumor growth; Tumor growth models; Convolution; Article; benchmarking; brain tumor; comparative study; controlled study; convolutional neural network; feature extraction; human; mathematical computing; nonhuman; nuclear magnetic resonance imaging; predictive value; segmentation algorithm; sensitivity and specificity; tumor growth",Article,Scopus,2-s2.0-85114127663
"Dubey R., Kumar M., Upadhyay A., Pachori R.B.","57205667272;57214630296;56609850100;14632337000;","Automated diagnosis of muscle diseases from EMG signals using empirical mode decomposition based method",2022,"Biomedical Signal Processing and Control","71",,"103098","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114123472&doi=10.1016%2fj.bspc.2021.103098&partnerID=40&md5=bcedfed6556628e84457fb3def512f44","Muscle activity decreases due to various conditions like age factors and muscle diseases namely, amyotrophic lateral sclerosis (ALS) and myopathy. Electromyogram (EMG) signals are regularly explored by specialists to analyze the irregularity of muscles. Manual investigation of EMG signals is a tedious task for medical practitioners. Therefore, this work proposes a new method for classifying the ALS, myopathy, and normal EMG signals. In the proposed method, the empirical mode decomposition (EMD) method is applied to decompose the EMG signals into intrinsic mode functions (IMFs). The suitable IMFs for feature selection are selected using the t-test based approach and used to compute the foot distances denoted as fp1 and fp2 by constructing the complex plane plot. The quadrilateral is drawn over a complex plot by considering fp1 and fp2 as a diagonal of it, followed by calculating the area (A) and circumference (CF) of the quadrilateral. These measures are utilized for separating the three classes of myopathy, ALS, and normal EMG signals. The proposed algorithm has been trained and validated using a feed forward neural network (FFNN), support vector machine (SVM), and decision tree. The algorithm, when tested with a FFNN, achieved the maximum classification accuracy, sensitivity, and specificity of 99.53%, 99.25% and 99.60%, respectively. © 2021","Amyotrophic lateral sclerosis; Electromyogram; Empirical mode decomposition; Intrinsic mode functions; Myopathy","Biomedical signal processing; Complex networks; Data mining; Decision trees; Diagnosis; Muscle; Neurodegenerative diseases; Support vector machines; Amyotrophic lateral sclerosis; Automated diagnosis; Electromyo grams; Emg signals (Electromyogram); Empirical Mode Decomposition; Feed-forward neural network; Intrinsic Mode functions; Muscle activities; Muscle disease; Myopathy; Functions; adult; aged; amyotrophic lateral sclerosis; Article; automation; biceps brachii muscle; classification algorithm; clinical article; controlled study; decision tree; electromyography; empirical mode decomposition; feature extraction; feature selection; feed forward neural network; female; Hilbert transform; human; intrinsic mode function; male; middle aged; motor unit potential; muscle contraction; myopathy; neuromuscular disease; number of crossings; polymyositis; radiculopathy; sensitivity and specificity; support vector machine; surface electromyography; vastus muscle",Article,Scopus,2-s2.0-85114123472
"Dehghani M., Shooshtarian M.R., Moosavi P., Zare F., Derakhshan Z., Ferrante M., Conti G.O., Jafari S.","55339569500;57195057580;57242500400;57242677500;55770305700;57143880700;24332549000;57243021200;","A process mining approach in big data analysis and modeling decision making risks for measuring environmental health in institutions",2022,"Environmental Research","203",,"111804","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114110504&doi=10.1016%2fj.envres.2021.111804&partnerID=40&md5=3cb284539a28e71b6b952a23c3a33c5f","This paper aimed to introduce a process-mining framework for measuring the status of environmental health in institutions. The methodology developed a new software-based index namely Institutional Environmental Health Index (IEHI) that was integrated from ontology-based Multi-Criteria Group Decision-Making models based on the principles of fuzzy modeling and consensus evaluation. Fuzzy Ordered Weighting Average (OWA) with the capability of modeling the uncertainties and decision-making risks along with Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) were employed as the computation engine. The performance of the extended index was examined through an applied example on 20 mosques as public institutions. IEHI could analyze big data collected by environmental health investigators and convert them to a single and interpretable number. The index detected the mosques with very unsuitable health conditions that should be in priority of sanitation and suitable ones as well. Due to the capability of defining the type and numbers of criteria and benefitting from specific and user-friendly software namely Group Fuzzy Decision-Making, this index is highly flexible and practical. The methodology could be used for numerating the environmental health conditions in any intended institution or occupation. The proposed index would provide e-health assessment by more efficient analysis of big data and risks that make more realistic decisions in environmental health system. © 2021 Elsevier Inc.","Data mining; Decision support models; Environmental health; Fuzzy logic; Risk","data mining; decision making; environmental effect; fuzzy mathematics; methodology; numerical model; software; article; big data; consensus; data mining; decision support system; environmental health; fuzzy logic; human; occupation; ontology; risk assessment; sanitation; sensitivity analysis; software; technique for order preference by similarity to ideal solution; telehealth; uncertainty; data analysis; decision making; environmental health; Big Data; Data Analysis; Decision Making; Environmental Health; Fuzzy Logic",Article,Scopus,2-s2.0-85114110504
"Abhaya A., Patra B.K.","57238282600;57238282700;","RDPOD: an unsupervised approach for outlier detection",2022,"Neural Computing and Applications","34","2",,"1065","1077",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113899851&doi=10.1007%2fs00521-021-06432-6&partnerID=40&md5=7c368445516081b9a475569e908883de","Outliers are the data points which deviate significantly from the majority of the data points. Finding outliers is an important task in various applications, especially in data mining. The unsupervised technique is very popular to mine outliers in a dataset over supervised techniques. Various unsupervised approaches have been proposed over the last decades. Clustering-based, distance-based, and density-based outlier approaches are found to be successful for detecting outlier points. However, the main focus of clustering-based method is to identifying clustering structure. Many distance-based and density-based techniques are not suitable for varying density datasets, and they are also very sensitive with their parameter (number of nearest-neighbor (k)). In this paper, we propose a hybrid approach named RDPOD, which utilizes distance-based and density-based clustering approaches efficiently for identifying the density of each point correctly. We obtain local density and relative distance of each data instance. From this density and distance information, we identify outlier points. Experimental results with real-world datasets show that our proposed approach outperforms the popular techniques LOF, LDOF, symmetric neighborhood, and recently introduced approaches NOF and RDOS. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Density peaks clustering; Local distance-based outlier factor (LDOF); Local outlier factor (LOF); Natural outlier factor (NOF); Outlier detection; Relative density-based factor (RDOS); Symmetric neighborhood (INFLO)","Anomaly detection; Statistics; Density-based Clustering; Distance information; Hybrid approach; Nearest neighbors; Real-world datasets; Relative distances; Unsupervised approaches; Unsupervised techniques; Data mining",Article,Scopus,2-s2.0-85113899851
"Pereg D., Cohen I., Vassiliou A.A.","57193488863;7401818333;6603779635;","Convolutional Sparse Coding Fast Approximation With Application to Seismic Reflectivity Estimation",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113826405&doi=10.1109%2fTGRS.2021.3105300&partnerID=40&md5=a55a3634c9c038be51326576768be599","In sparse coding, we attempt to extract features of input vectors, assuming that the data is inherently structured as a sparse superposition of basic building blocks. Similarly, neural networks perform a given task by learning features of the training dataset. Recently, both data- and model-driven feature extracting methods have become extremely popular and have achieved remarkable results. Nevertheless, practical implementations are often too slow to be employed in real-life scenarios, especially for real-time applications. We propose a speed-up upgraded version of the classic iterative thresholding algorithm (ITA), which produces a good approximation of the convolutional sparse code (CSC) within 2–5 iterations. The speed advantage is gained mostly from the observation that most solvers are slowed down by inefficient global thresholding. The main idea is to normalize each data point by the local receptive field energy, before applying a threshold. This way, the natural inclination toward strong feature expressions is suppressed, so that one can rely on a global threshold that can be easily approximated, or learned during training. The proposed algorithm can be employed with a known predetermined dictionary, or with a trained dictionary. The trained version is implemented as a neural net designed as the unfolding of the proposed solver. The performance of the proposed solution is demonstrated via the seismic inversion problem in both synthetic and real data scenarios. We also provide theoretical guarantees for a stable support recovery, namely we prove that under certain conditions, the true support is perfectly recovered within the first iteration. © 2021 IEEE.","Approximation algorithms; Coherence; Convolution; Dictionaries; Encoding; Feature extraction; Image coding","Approximation algorithms; Convolution; Iterative methods; Neural networks; Seismology; Basic building block; Feature extracting method; Global thresholding; Iterative thresholding; Real-time application; Seismic reflectivity; Synthetic and real data; Theoretical guarantees; Data mining; algorithm; deconvolution; estimation method; machine learning; reflectivity; threshold",Article,Scopus,2-s2.0-85113826405
"Zolfaghari Z., Aslani A., Moshari A., Malekli M.","57193839059;55093935000;57234344500;57234097000;","Direct air capture from demonstration to commercialization stage: A bibliometric analysis",2022,"International Journal of Energy Research","46","1",,"383","396",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113683065&doi=10.1002%2fer.7203&partnerID=40&md5=b005895201c109600752c47a9bde4430","Concerns related to increasing CO2 emission and its effects on global warming and climate change have been increased with increasing the global consumption of fossil fuels. One solution to respond to this challenge is the development and utilization of carbon capturing and storage technologies. Among different carbon capturing technologies, direct air capture (DAC) reduces CO2 emissions from air. While the technology readiness level (TRL) of DAC is in the demonstration stage, identifying the commercialization research gaps and possible opportunities can help with diffusion and adoption of the technology. This research uses a knowledge discovery in research databases, based on bibliometric analysis and data mining, to understand DAC research and development's current status and future. Then, we identify the critical areas of the research gap for commercialization. The bibliometric analysis results show that DAC has not yet reached its maturity level compared with other carbon capture technologies (CCTs). However, there are different opportunities for the development of this technology. The results indicate that (a) new systematic designs, improvement in nano-catalysts, increase in the capturing capacity, (b) economic and investment improvements in combination with the environmental assessment of the optimized DAC technology, (c) assessment of future prospects, (d) integration with alternative energy supply sources especially renewable energy to respond to the required energy and process integration with current carbon emitted processes, (e) technology demonstration and readiness assessment, and (f) policy and uncertainty analysis of the market are the key areas that should be investigated for the success of this technology in the competitive market. © 2021 John Wiley & Sons Ltd.","bibliometric analysis; carbon capture; direct air capture; patent analysis","Carbon capture; Carbon dioxide; Commerce; Data mining; Demonstrations; Digital storage; Energy policy; Environmental technology; Fossil fuels; Global warming; Investments; Nanocatalysts; Alternative energy supplies; Bibliometric analysis; Development and utilizations; Environmental assessment; Global warming and climate changes; Readiness assessment; Research and development; Technology readiness levels; Uncertainty analysis",Article,Scopus,2-s2.0-85113683065
"Wang J., Hu X., Shi T., He L., Hu W., Wu G.","57191893435;57224724221;55349704100;57192420381;56422852600;57230124900;","Assessing toxic metal chromium in the soil in coal mining areas via proximal sensing: Prerequisites for land rehabilitation and sustainable development",2022,"Geoderma","405",,"115399","","",,6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113410712&doi=10.1016%2fj.geoderma.2021.115399&partnerID=40&md5=f02feeaa521a4b9f152567eeaf123086","The rapid and accurate determination of soil chromium (Cr) is crucial for preventing toxic element pollution in soils and ensuring ecological security. Proximal sensing technology uses visible and near-infrared (Vis-NIR) diffuse reflectance spectroscopy, which has been demonstrated to be a viable approach for monitoring soil Cr concentrations. However, at trace levels, soil Cr is not especially spectrally active, thus limiting the practical application of using corresponding spectral data for quantifying soil Cr concentrations. In this study, we hypothesized that fused proximal sensing and soil auxiliary attributes (including organic matter (OM) and pH) could improve estimation of Cr concentrations in the soil. Additionally, the introduction of best-fit variogram models was theoretically possible to improve spatial visualization. To address these hypotheses, we collected 168 soil samples from the open coal mine area in the Eastern Junggar Basin, China. Fractional-order derivative (FOD) pretreatment and optimal band combination methods were implemented for spectral data mining and the derivation of spectral parameters, respectively. Soil Cr estimation models were calibrated with a partial least squares (PLS) approach through four designed strategies with different predictors: (I) full Vis-NIR variables, (II) effective three-band spectral indices (TBIs), (III) the effective TBIs and OM, and (IV) the effective TBIs, OM, and the pH. The results suggest that FOD could identify abundant spectral variability. Compared with full Vis-NIR variables, the effective TBIs can effectively magnify the subtle spectral signals concerning soil Cr. The optimal estimation model was determined as Strategy IV, indicating that the introduction of soil auxiliary attributes (pH and OM) can improve the estimation performance of the model; notably, the coefficient of determination (R2) and ratio of performance to interquartile distance (RPIQ) were 0.87 and 2.68, respectively. Based on the optimal semivariance model, we used kriging interpolation to map regional soil Cr. In the study area, the soil Cr distribution features strong spatial dependence and strong associations. Our study might inspire further research on soil contamination mapping based on proximal Vis-NIR sensors. © 2021 Elsevier B.V.","Auxiliary soil attributes; Proximal soil sensing; Soil chromium (Cr); Soil pollution mapping; Three-band spectral indices (TBI); Variogram model","Coal mines; Data mining; Infrared devices; Interpolation; Least squares approximations; Photomapping; Soil pollution; Soil surveys; Auxiliary soil attribute; Chromium concentration; Organics; Proximal sensing; Proximal soil sensing; Soil chromium; Soil pollution mapping; Three-band spectral index; Variogram models; Visible and near infrared; Soils; chromium; coal mining; infrared spectroscopy; soil; sustainable development; toxic substance; China; Croatia; Junggar Basin; Vis; Xinjiang Uygur",Article,Scopus,2-s2.0-85113410712
"Troisi O., Fenza G., Grimaldi M., Loia F.","55555898300;15065479100;57195626795;56888726900;","Covid-19 sentiments in smart cities: The role of technology anxiety before and during the pandemic",2022,"Computers in Human Behavior","126",,"106986","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113356265&doi=10.1016%2fj.chb.2021.106986&partnerID=40&md5=d31a32510f198dee459b674b9c142a6c","The spread of Covid-19 profoundly changed citizens' daily lives due to the introduction of new modes of work and access to services based on smart technologies. Although the relevance of new technologies as strategic levers for crisis resolution has been widely debated before the pandemic, especially in the smart cities' context, how individuals have agreed to include the technological changes dictated by the pandemic in their daily interactions remains an open question. This paper aims at detecting citizens' sentiment toward technology before and after the emergence of the Covid-19 pandemic using Fuzzy Formal Concept Analysis (FFCA) to analyze a large corpus of tweets. Specifically, citizens' attitudes in five cities (Berlin, Dublin, London, Milan, and Madrid) were explored to extract and classify the key topics related to the degree of confidence, familiarity and approval of new technologies. The results shed light on the complex technology acceptance process and help managers identify the potential negative effects of smart technologies. In this way, the study enhances scholars' and practitioners' understanding of the strategies for enabling the use of technology within smart cities to manage the transformations introduced by the health emergency and guide citizens’ behaviour. © 2021 Elsevier Ltd","Citizen behaviour; Covid-19; Fuzzy formal concept analysis; Pandemic; Smart cities; Technology anxiety","Data mining; Formal concept analysis; Citizen behavior; Covid-19; Daily lives; Fuzzy formal concept analyse; Pandemic; Role of technologies; Service-based; Smart technology; Technological change; Technology anxiety; Smart city",Article,Scopus,2-s2.0-85113356265
"Rocha D., Aloise D., Aloise D.J., Contardo C.","57230334700;12759644500;6504045903;26032523300;","Visual attractiveness in vehicle routing via bi-objective optimization",2022,"Computers and Operations Research","137",,"105507","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113354651&doi=10.1016%2fj.cor.2021.105507&partnerID=40&md5=34b18b9e58a0b0c01f509d4b7516393a","We consider the problem of designing vehicle routes in a distribution system that are at the same time cost-effective and visually attractive. In this paper we argue that clustering, a popular data mining task, provides a good proxy for visual attractiveness. Our claim is supported by the proposal of a bi-objective capacitated vehicle routing problem in which, in addition to seek for traveling cost minimization, optimizes clustering criteria defined over the customers partitioned in the different routes. The model is solved by a multi-objective evolutionary algorithm to approximate its Pareto frontier. We show, by means of computational experiments, that our model is able to characterize vehicle routing solutions with low routing costs which are, at the same time, attractive according to the visual metrics proposed in the literature. © 2021 Elsevier Ltd","Clustering; Vehicle routing problem; Visual attractiveness","Cost effectiveness; Evolutionary algorithms; Vehicle routing; Vehicles; Bi-objective optimization; Capacitated vehicle routing problem; Clustering criteria; Computational experiment; Cost minimization; Data mining tasks; Distribution systems; Multi objective evolutionary algorithms; Data mining",Article,Scopus,2-s2.0-85113354651
"Zhang P., Wang H., Nie Z.","55547103187;57226805690;57214364372;","Agile innovation process model based on computer-aided patent knowledge mining and functional analogy",2022,"Computer-Aided Design and Applications","19","2",,"346","374",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112728576&doi=10.14733%2fCADAPS.2022.346-374&partnerID=40&md5=868179dd1be6ded2677c1099115c104b","As the variability and timeliness of customer demands pose new challenges to traditional design methods, it is urgent for a new innovative design method to meet the rapid and agile requirements of new product. Therefore, this paper proposed an agile innovation process model based on computer-aided patent knowledge mining and function analogy by combining computer-aided patent knowledge mining with product function analogy. First, the text information in the patent is excavated through computer assistance and stored in the form of functional structure and functional model. Second, the customer demand is analyzed and the functional decomposition is carried out. Then the functional structure of the new product is established in the form of a functional base. Third, similar functional structures and functional models are retrieved among the functional structures and functional models abstracted in the patent. Fourth, the functional design method of functional similarity matrix and computer-aided data processing is used in this paper. Finally, the final agile innovative design plan is obtained by solving the problems existing in the new product function model through analyzing the available resource analysis and material-field model analysis of TRIZ. In order to prove the scientificity and effectiveness of the proposed method, it is verified by the solar panel natural wind dust removal system in this paper. © 2022 CAD Solutions, LLC.","Computer-aided innovation design; Functional analogy; Functional model; Functional structure; Patent knowledge mining","Data handling; Data mining; Patents and inventions; Product design; Agile requirements; Dust removal systems; Functional decomposition; Functional similarity; Functional structure; Innovation process; Innovative design methods; Product functions; Structural design",Article,Scopus,2-s2.0-85112728576
"Xiao J., Li J., Yuan Q., Zhang L.","57224185327;57214207213;36635300800;8359720900;","A Dual-UNet With Multistage Details Injection for Hyperspectral Image Fusion",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112563929&doi=10.1109%2fTGRS.2021.3101848&partnerID=40&md5=de7c8e9ea3896c7e0a104833f4225e9f","Enhancement of hyperspectral image (HSI) resolution is significant for better application in practice. In this article, a dual U-Net (D-UNet) is proposed to improve the spatial resolution of HSI. The whole network contains two parts. One is the detail extraction network, whose network architecture is encoder–decoder and mainly extracts various spatial features from multispectral images (MSIs). Another is the spatio-spectral fusion network (SSFN), which aims at injecting the features from the detail extraction network into HSI for better reconstruction. Furthermore, in the primary stage of the whole network, a novel multiscale spatio-spectral attention module (MSSAM) is utilized to pay more attention to important features at different scales. Considering the complex ground scenes, the features of different scale and depth are continually extracted and fused in the whole network. The experimental results show that the proposed method is more effective compared with the state-of-the-art methods. © 2021 IEEE.","Convolution; Data mining; Feature extraction; Image reconstruction; Sparse matrices; Spatial resolution; Tensors","Extraction; Image enhancement; Image fusion; Spectroscopy; Encoder-decoder; Important features; Multispectral images; Spatial features; Spatial resolution; State-of-the-art methods; Network architecture; architecture; data mining; extraction method; remote sensing; spatial resolution; spectral analysis",Article,Scopus,2-s2.0-85112563929
"Sellami A., Tabbone S.","56581605600;6602855250;","Deep neural networks-based relevant latent representation learning for hyperspectral image classification",2022,"Pattern Recognition","121",,"108224","","",,6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112515783&doi=10.1016%2fj.patcog.2021.108224&partnerID=40&md5=a8968dcd2c98605579675ffb2e019431","The classification of hyperspectral image is a challenging task due to the high dimensional space, with large number of spectral bands, and low number of labeled training samples. To overcome these challenges, we propose a novel methodology for hyperspectral image classification based on multi-view deep neural networks which fuses both spectral and spatial features by using only a small number of labeled samples. Firstly, we process the initial hyperspectral image in order to extract a set of spectral and spatial features. Each spectral vector is the spectral signature of each pixel of the image. The spatial features are extracted using a simple deep autoencoder, which seeks to reduce the high dimensionality of data taking into account the neighborhood region for each pixel. Secondly, we propose a multi-view deep autoencoder model which allows fusing the spectral and spatial features extracted from the hyperspectral image into a joint latent representation space. Finally, a semi-supervised graph convolutional network is trained based on thee fused latent representation space to perform the hyperspectral image classification. The main advantage of the proposed approach is to allow the automatic extraction of relevant information while preserving the spatial and spectral features of data, and improve the classification of hyperspectral images even when the number of labeled samples is low. Experiments are conducted on three real hyperspectral images respectively Indian Pines, Salinas, and Pavia University datasets. Results show that the proposed approach is competitive in classification performances compared to state-of-the-art. © 2021","Deep learning; Feature extraction; Hyperspectral image classification; Representation learning","Classification (of information); Data mining; Deep neural networks; Extraction; Feature extraction; Image enhancement; Pixels; Spectroscopy; Supervised learning; Autoencoders; Deep learning; Features extraction; HyperSpectral; Hyperspectral image classification; Multi-views; Neural-networks; Representation learning; Spatial features; Spectral feature; Image classification",Article,Scopus,2-s2.0-85112515783
"Ma X., Tan S., Xie X., Zhong X., Deng J.","34969286900;57209244526;57226697906;57221849226;55922714500;","Joint multi-label learning and feature extraction for temporal link prediction",2022,"Pattern Recognition","121",,"108216","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112380369&doi=10.1016%2fj.patcog.2021.108216&partnerID=40&md5=90b2a3dc611659c91b75dd5a2b35bf47","Networks derived from various disciplinary of sociality and nature are dynamic and incomplete, and temporal link prediction has wide applications in recommendation system and data mining system, etc. The current algorithms first obtain features by exploiting the topological or latent structure of networks, and then predict temporal links based on the obtained features. These algorithms are criticized by the separation of feature extraction and link prediction, which fails to fully characterize the dynamics of networks, resulting in undesirable performance. To overcome this problem, we propose a novel algorithm by joint multi-label learning and feature extraction (called MLjFE), where temporal link prediction and feature extraction are integrated into an overall objective function. The main advantage of MLjFE is that the features and parameter matrix for temporal link prediction are simultaneously learned during optimization procedure, which is more precise to capture dynamics of networks, improving the performance of algorithms. The experimental results on a number of artificial and real-world temporal networks demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods, showing joint learning with feature extraction and temporal link prediction is promising. © 2021 Elsevier Ltd","Dynamic networks; Multi-label learning; Non-negative matrix factorization; Temporal link prediction","Data mining; Extraction; Factorization; Feature extraction; Learning algorithms; Learning systems; Matrix algebra; 'current; Data mining system; Dynamic network; Dynamics of networks; Features extraction; Link prediction; Multi-label learning; Nonnegative matrix factorization; Temporal link prediction; Topological structure; Forecasting",Article,Scopus,2-s2.0-85112380369
"Zhu Y., Chen L., Gao Y., Jensen C.S.","57205161882;55355122300;14038661300;35453638700;","Pivot selection algorithms in metric spaces: a survey and experimental study",2022,"VLDB Journal","31","1",,"23","47",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112324607&doi=10.1007%2fs00778-021-00691-4&partnerID=40&md5=6c1d3cae15ac6e57e8b20f23197ff14d","Similarity search in metric spaces is used widely in areas such as multimedia retrieval, data mining, data integration, to name but a few. To accelerate metric similarity search, pivot-based indexing is often employed. Pivot-based indexing first computes the distances between data objects and pivots and then exploits filtering techniques that use the triangle inequality on pre-computed distances to prune search space during search. The performance of pivot-based indexing depends on the quality of the pivots used, and many algorithms have been proposed for selecting high-quality pivots. We present a comprehensive empirical study of pivot selection algorithms. Specifically, we classify all existing algorithms into three categories according to the types of distances they use for selecting pivots. We also propose a new pivot selection algorithm that exploits the power law probabilistic distribution. Next, we report on a comprehensive empirical study of the search performance enabled by different pivot selection approaches, using different datasets and indexes, thus contributing new insight into the strengths and weaknesses of existing selection techniques. Finally, we offer advice on how to select appropriate pivot selection algorithms for different settings. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Metric index; Metric space; Pivot; Similarity search","Data integration; Indexing (of information); Probability distributions; Set theory; Topology; Filtering technique; Multimedia Retrieval; Pivot based indexing; Probabilistic distribution; Search performance; Selection techniques; Similarity search; Triangle inequality; Data mining",Article,Scopus,2-s2.0-85112324607
"Wang J., Lu W., Li Y.","57261138900;7404214627;55876100200;","A Multitask Learning-Based Dynamic Wavelet Amplitude Spectra Extraction Method and Its Application in Q Estimation",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112181401&doi=10.1109%2fTGRS.2021.3086266&partnerID=40&md5=a1492164dd81bc3137815a947087ccb1","Dynamic wavelet amplitude spectra extraction (DWASE), which is an ill-posed problem, is of great importance for nonstationary seismic data processing. The most difficult challenge is how to decouple the dynamic wavelets and reflection coefficients. The traditional DWASE methods solve the ill-posed problem depending on some prior information, such as the piecewise stationary hypothesis or estimation of the attenuation factor $Q$. In this article, we propose a multitask learning-based DWASE method and apply the method for $Q$ estimation. Our proposed method can reduce the multiplicity of the ill-posed problem by estimating the logarithmic time-frequency amplitude spectrum (logarithmic TFAS) of both reflection coefficients and dynamic seismic wavelets, simultaneously. In our method, a parameter-sharing U-net is used to extract the logarithmic TFAS of the reflection coefficients and dynamic wavelets from the logarithmic TFAS of the nonstationary seismic data. To verify the accuracy of the DWASE results of our method, we make a quantitative analysis of the synthetic seismic data, which are obtained by our method and some traditional methods. We also apply the DWASE results of our method for $Q$ estimation and attenuation compensation in both synthetic and field seismic data, to prove the effectiveness of the method. Also, comparisons with some traditional methods are given. © 1980-2012 IEEE.","Attenuation compensation; dynamic wavelet amplitude spectra extraction (DWASE); multitask learning (MTL); Q estimation","Data handling; Data mining; Extraction; Geophysical prospecting; Multi-task learning; Reflection; Seismic response; Seismic waves; Wavelet analysis; Attenuation compensation; Attenuation factors; Extraction method; Ill posed problem; Parameter sharing; Piecewise stationaries; Prior information; Seismic data processing; Learning systems; abundance estimation; algorithm; amplitude; detection method; estimation method; quantitative analysis; seismic data; wavelet; wavelet analysis",Article,Scopus,2-s2.0-85112181401
"Yang L., Wang S., Chen X., Saad O.M., Chen W., Oboue Y.A.S.I., Chen Y.","57215054335;56040342200;57001991300;57201504788;57192091350;57223158334;57258344600;","Unsupervised 3-D Random Noise Attenuation Using Deep Skip Autoencoder",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112164561&doi=10.1109%2fTGRS.2021.3100455&partnerID=40&md5=cb03a37c7c205afff97ff0dbac949c38","Effective random noise attenuation is critical for subsequent processing of seismic data, such as velocity analysis, migration, and inversion. Thus, the removal of seismic random noise with an uncertainty level is meaningful. Attenuating 3-D random noise in a supervised way based on deep learning (DL) is challenging because clean labels are difficult to obtain. Therefore, it is necessary to develop an adaptive unsupervised-based method for random noise attenuation. In this article, we propose a deep-denoising unsupervised learning (DDUL) network to attenuate random noise in 2-D/3-D seismic data. A patching technique is used to split 2-D/3-D seismic data into several patches to be fed into the network, which helps to expand the number of samples for training. We use the fully symmetrical structure of the autoencoder to construct the network. In each corresponding encoder and decoder layer, skip connections are added to enhance the learning of seismic data features. We construct three blocks to extract waveform features in seismic data, i.e., encoder, decoder, and skip blocks. Among them, the skip is connected between the encoder and decoder blocks of each hidden layer. The use of multiple blocks not only improves the network's ability to extract seismic data features but also solves the problem of excessive training parameters caused by hidden layer stacking. Five 2-D/3-D synthetic and field seismic datasets are used to test the denoising performance of our proposed method. The denoising results demonstrate that our proposed method has good signal-preserving and noise attenuation capabilities in real-world applications. © 1980-2012 IEEE.","3-D random noise attenuation; autoencoder (AE); deep learning (DL); U-Net","Data handling; Decoding; Deep learning; Geophysical prospecting; Learning systems; Seismic response; Seismic waves; Signal encoding; Auto encoders; Fully symmetrical structure; Hidden layers; Noise attenuation; Number of samples; Training parameters; Velocity analysis; Waveform features; Data mining; algorithm; data set; detection method; instrumentation; parameter estimation; seismic data; three-dimensional modeling; unsupervised classification",Article,Scopus,2-s2.0-85112164561
"Caballero D., Rodríguez P.G., Caro A., Ávila M.D.M., Torres J.P., Antequera T., Perez-Palacios T.","56041924300;16639726700;22333786700;57226296532;57212172979;7004624442;15049710600;","An experimental protocol to determine quality parameters of dry-cured loins using low-field Magnetic Resonance Imaging",2022,"Journal of Food Engineering","313",,"110750","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112128684&doi=10.1016%2fj.jfoodeng.2021.110750&partnerID=40&md5=1ef70c01908846c249d6acc6b9869388","The objective of this study was to achieve an experimental protocol (EP) to determine quality characteristics of dry-cured loins non-destructively by using low-field (LF) Magnetic Resonance Imaging (MRI). The MRI procedure is composed of three main stages: MRI acquisition, MRI analysis (computer vision techniques) and data analysis (data mining methods). Two procedures have been implemented within a EP and validated with real samples from the meat industry (dry-cured loins, n = 100) by means of different quality measures. The validation results may indicate the use of both implemented procedures and the development of an EP to determine quality characteristics of loins by LF MRI-computer vision-data mining in a non-destructive way, with high accuracy and reducing the dispersion of the values. This brings the possibility of implementing this methodology in meat processing plants. © 2021 Elsevier Ltd","Dry-cured loins; Experimental protocol; Magnetic resonance imaging; Optimum procedures; Quality parameters","Computer vision; Data mining; Drying; Magnetic resonance imaging; Magnetism; Resonance; 'Dry' [; Computer vision techniques; Dry-cured loin; Experimental protocols; Imaging analysis; Low field magnetic resonance; Optimum procedure; Quality characteristic; Quality parameters; Vision data; Curing",Article,Scopus,2-s2.0-85112128684
"Desbordes J.K., Zhang K., Xue X., Ma X., Luo Q., Huang Z., Hai S., Jun Y.","57206770530;55769748056;57211946084;57206612475;36606630600;35792109900;57195471338;35090604500;","Dynamic production optimization based on transfer learning algorithms",2022,"Journal of Petroleum Science and Engineering","208",,"109278","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112019162&doi=10.1016%2fj.petrol.2021.109278&partnerID=40&md5=911988a2be5abefcff29796369fc8fc5","Dynamic production optimization involves continuous cycle of model-predictive control initiated at specified times to maximize production net present value (NPV) throughout the expected life of the reservoir. Re-evaluating predictive models using traditional methods is computationally expensive. The existing methods for updating production controls, do not take the inter-cycle correlation into account. But such information is valuable for boosting the problem-solving efficiency on the current cycle by utilizing the experience or knowledge extracted from the previous developed cycles. Originally, transfer learning algorithms are difficult to be implemented into the oil and gas industry, due to its high computational cost and random or unavailable learning samples. Therefore, we propose a new transfer learning based optimization framework for dynamic production optimization problems. First, domain adaptation learning (DAL) is used to represent data between two inter-cycles, to decrease the dissimilarity between them. Second, extended boundary constraints (EBC) is a technique used to embed the optimization problem into the learning samples during DAL stage. EBC reduces the burden on computational facilities and makes the algorithm suitable for production optimization. Third, a transfer component analysis (TCA) method is used to simplify the data representation and also extract the data correlation. Then, the extracted correlation is used to produce an effective population for MOEAs. The developed framework is incorporated into three well-known evolutionary algorithms, nondominated sorting genetic algorithm II (NSGAII), multiobjective particle swarm optimization (MOPSO), and multiobjective evolutionary algorithm based on decomposition (MOEA/D) and one single objective optimizer, particle swarm optimization (PSO) for NPV maximization and robust optimization respectively. The proposed method is tested on a series of dynamic benchmark problems dynamic and a practical case based on a three channel reservoir model. Results showed that the proposed method reduces the number of simulation calls needed to reach optimum control options when using population-based evolutionary algorithms. Also, using the proposed technique, a higher NPV and better convergence speed in comparison to their original evolutionary algorithms is achieved. © 2021","Dynamic optimization; Multiobjective optimization; Production optimization; Transfer learning; Waterflooding","Data mining; Genetic algorithms; Learning algorithms; Particle swarm optimization (PSO); Population statistics; Production control; Screening; Boundary constraints; Domain adaptation learning; Dynamic optimization; Dynamic production; Learning samples; Multi-objectives optimization; Optimization problems; Production optimization; The net present value (NPV); Transfer learning; Multiobjective optimization; algorithm; correlation; dynamic analysis; flooding; hydrocarbon reservoir; machine learning; oil production; optimization",Article,Scopus,2-s2.0-85112019162
"Nenova Z., Shang J.","57206181429;57226542538;","Chronic Disease Progression Prediction: Leveraging Case-Based Reasoning and Big Data Analytics",2022,"Production and Operations Management","31","1",,"259","280",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111892702&doi=10.1111%2fpoms.13532&partnerID=40&md5=2ceefd1edd55830c6d42c4332b2bcba5","Physicians caring for chronically ill individuals need to predict patients' disease progression, as accurate disease projections can facilitate better treatment decisions. The power of prediction is prevention, as it is easier to prevent than to reverse. In this research, we propose a data-driven model for accurate and fast disease trajectory prediction, using electronic health records (EHRs) from Veterans Affairs Hospitals. EHRs contain tremendous amount of frequently updated, highly dimensional and not equally spaced data in diverse formats (e.g., numeric, textual, images, and videos). We propose an intelligent case-based reasoning (iCBR) approach to better predict kidney disease progression, which can help prevent patients' health deterioration and prolong lives. Our iCBR contributes to the literature by enhancing the automation and personalization capabilities of the conventional case-based reasoning (CBR). Through the iCBR, we advance the utilization of patient's laboratory data, vital sign, clinic visit, and comorbidity information. We examine (1) if the number of cases chosen for predicting the new patient's disease progression should be tailored, and (2) what the best number of prediction cases should be if customization is warranted. We link the number of cases selected for disease prediction with patient's disease characteristics. By comparing the results of the iCBR and popular machine learning and statistics models adapted to our problem, we find that the iCBR outperforms other methods. While the proposed model is applied to patients with chronic kidney disease, it can be readily applied to other chronic diseases such as diabetes, due to its similar data structure. © 2021 Production and Operations Management Society","case-based reasoning; data mining; disease progression; forecast; healthcare analytics",,Article,Scopus,2-s2.0-85111892702
"Bykov K., Patorno E., D’Andrea E., He M., Lee H., Graff J.S., Franklin J.M.","55535907200;35793933000;39560945300;57198802034;55810034100;54891855800;55602764900;","Prevalence of Avoidable and Bias-Inflicting Methodological Pitfalls in Real-World Studies of Medication Safety and Effectiveness",2022,"Clinical Pharmacology and Therapeutics","111","1",,"209","217",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111751421&doi=10.1002%2fcpt.2364&partnerID=40&md5=9fc4d2d38112e6d88d86bdbd38ac9ff0","Many real-word evidence (RWE) studies that utilize existing healthcare data to evaluate treatment effects incur substantial but avoidable bias from methodologically flawed study design; however, the extent of preventable methodological pitfalls in current RWE is unknown. To characterize the prevalence of avoidable methodological pitfalls with potential for bias in published claims-based studies of medication safety or effectiveness, we conducted an English-language search of PubMed for articles published from January 1, 2010 to May 20, 2019 and randomly selected 75 studies (10 case-control and 65 cohort studies) that evaluated safety or effectiveness of cardiovascular, diabetes, or osteoporosis medications using US health insurance claims. General and methodological study characteristics were extracted independently by two reviewers, and potential for bias was assessed across nine bias domains. Nearly all studies (95%) had at least one avoidable methodological issue known to incur bias, and 81% had potentially at least one of the four issues considered major due to their potential to undermine study validity: time-related bias (57%), potential for depletion of outcome-susceptible individuals (44%), inappropriate adjustment for postbaseline variables (41%), or potential for reverse causation (39%). The median number of major issues per study was 2 (interquartile range (IQR), 1–3) and was lower in cohort studies with a new-user, active-comparator design (median 1, IQR 0–1) than in cohort studies of prevalent users with a nonuser comparator (median 3, IQR 3–4). Recognizing and avoiding known methodological study design pitfalls could substantially improve the utility of RWE and confidence in its validity. © 2021 The Authors. Clinical Pharmacology & Therapeutics © 2021 American Society for Clinical Pharmacology and Therapeutics",,"adverse drug reaction; case control study; cohort analysis; data analysis; data mining; factual database; human; insurance; methodology; prevalence; procedures; statistical bias; Bias; Case-Control Studies; Cohort Studies; Data Analysis; Data Mining; Databases, Factual; Drug-Related Side Effects and Adverse Reactions; Humans; Insurance Claim Review; Methods; Prevalence; Research Design",Article,Scopus,2-s2.0-85111751421
"Xue Y.-J., Cao J.-X., Wang X.-J., Du H.-K.","36976507000;55551562700;57192631911;56320845900;","Reservoir permeability estimation from seismic amplitudes using variational mode decomposition",2022,"Journal of Petroleum Science and Engineering","208",,"109293","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111694340&doi=10.1016%2fj.petrol.2021.109293&partnerID=40&md5=1fe0fa2a194a7c22d99623e1e7c517fd","A new qualitative Reservoir Permeability estimation approach using variational mode decomposition named RPE-VMD is proposed which directly use seismic amplitude and frequency attributes. The method reveals amplitude anomalies that may reflect details about the heterogenous distribution of reservoir permeability. The main procedure is carried out in the time-frequency domain which generates by using a generalized S transform. Reflection coefficient information is used as seismic amplitude attribute and extracted by using variational mode decomposition combined with mutual information theory from seismic data using the logarithmic spectrum. The dominant frequency extracted in the time-frequency domain is used as seismic frequency attribute for estimating reservoir thickness information. The RPE-VMD approach uses dominant reflection coefficient information from the seismic data combined with reservoir thickness information for permeability estimation. Application of this approach to field data from a sandstone reservoir in China illustrates that it can provide permeability estimation with a high resolution and accuracy. The proposed method directly uses seismic data to measure the reservoir permeability avoiding the need of seismic velocity in the conventional estimation methods. © 2021 Elsevier B.V.","Permeability estimation; Reflection coefficient; Reservoir thickness; Seismic amplitude; Variational mode decomposition","Data mining; Frequency domain analysis; Geophysical prospecting; Information theory; Petroleum reservoir engineering; Reservoirs (water); Seismic response; Seismic waves; Estimation approaches; Mode decomposition; Permeability estimation; Reservoir permeability; Reservoir thickness; Seismic amplitudes; Seismic datas; Seismic frequencies; Time frequency domain; Variational mode decomposition; Reflection; amplitude; estimation method; heterogeneity; permeability; reservoir; seismic data; seismic reflection; seismic velocity; China",Article,Scopus,2-s2.0-85111694340
"Nikabadi S., Zabihi H., Shahcheraghi A.","57226459073;55326794900;55210005000;","Evaluating the Effective Factors of Hospital Rooms on Patients’ Recovery Using the Data Mining Method",2022,"Health Environments Research and Design Journal","15","1",,"97","114",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111679604&doi=10.1177%2f19375867211031305&partnerID=40&md5=888c69212ff390482ca4cf99962985fd","Objectives: This study aims to investigate the effective environmental factors of hospital rooms in patients’ recovery through data mining techniques. Background: Previous studies have shown the positive effect of the interior environment of the hospitals on patients’ recovery. The methods of these studies were mainly based on the evidence and patients’ perception while hospital environments are associated with a large amount of data that make them an appropriate case for data mining studies. But data mining studies in hospitals mainly focused on medical and management purposes rather than evaluating the interior environment condition. Methods: We analyzed the hospital information system data of a hospital using Python programming language and some of its libraries. Preprocessing and eliminating the outliers, labeling and clustering of diseases, data visualization and analysis, final evaluation, and concluding were done using the knowledge discovery in databases process. Results: Pearson coefficient value for rooms’ area was.5 and, respectively, for the distance from the ward entrance and nursing station were.75 and.70. The χ2 values for the variables of room types, location, and occupation were 24.62, 18.98, and 21.53, respectively, and for the beds’ location was 0.12. Conclusions: The results confirmed the correlation of the length of stay with the room types, location, and occupation, distance from the nursing station and ward entrance and also showed a moderate correlation with the rooms’ area. However, no evidence was found about the relationship between the beds’ location in rooms and patients’ length of hospital stay. © The Author(s) 2021.","data mining; hospital information system; hospital room features; length of stay; Python programming language","data mining; health care facility; hospital; human; methodology; nursing station; Data Mining; Hospitals; Humans; Nursing Stations; Patients' Rooms; Research Design",Article,Scopus,2-s2.0-85111679604
"Jiang B., Sun P., Luo B.","56890202300;57219693427;57203411639;","GLMNet: Graph learning-matching convolutional networks for feature matching",2022,"Pattern Recognition","121",,"108167","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111519060&doi=10.1016%2fj.patcog.2021.108167&partnerID=40&md5=a57c968ce36fc030cb3a53b8f7997787","Recently, graph convolutional networks (GCNs) have been employed for graph matching problem. It can integrate graph node feature embedding, node-wise affinity learning and matching optimization together in a unified end-to-end model. However, first, the matching graphs feeding to existing graph matching networks are generally fixed and independent of graph matching task, which thus are not guaranteed to be optimal for the graph matching task. Second, existing methods generally employ smoothing-based graph convolution to generate graph node embeddings, in which extensive smoothing convolution operation may dilute the desired discriminatory information of graph nodes. To overcome these issues, we propose a novel Graph Learning-Matching Network (GLMNet) for graph matching problem. GLMNet has three main aspects. (1) It integrates graph learning into graph matching which thus adaptively learns a pair of optimal graphs for graph matching task. (2) It further employs a Laplacian sharpening graph convolution to generate more discriminative node embeddings for graph matching. (3) A new constraint regularized loss is designed for GLMNet training which can encode the desired one-to-one matching constraints in matching optimization. Experiments demonstrate the effectiveness of GLMNet. © 2021 Elsevier Ltd","Graph convolutional network; Graph learning; Graph matching; Laplacian sharpening","Convolution; Data mining; Embeddings; Graph theory; Pattern matching; Convolutional networks; Embeddings; Graph convolutional network; Graph learning; Graph matching problems; Graph matchings; Laplacian sharpening; Matching networks; Matching optimization; Networks/graphs; Laplace transforms",Article,Scopus,2-s2.0-85111519060
"Theis J., Galanter W.L., Boyd A.D., Darabi H.","57220411457;6603094520;10038853600;7003569437;","Improving the In-Hospital Mortality Prediction of Diabetes ICU Patients Using a Process Mining/Deep Learning Architecture",2022,"IEEE Journal of Biomedical and Health Informatics","26","1",,"388","399",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111203645&doi=10.1109%2fJBHI.2021.3092969&partnerID=40&md5=5f3f594bbb6707336b64de48be1fdeb6","Diabetes intensive care unit (ICU) patients are at increased risk of complications leading to in-hospital mortality. Assessing the likelihood of death is a challenging and time-consuming task due to a large number of influencing factors. Healthcare providers are interested in the detection of ICU patients at higher risk, such that risk factors can possibly be mitigated. While such severity scoring methods exist, they are commonly based on a snapshot of the health conditions of a patient during the ICU stay and do not specifically consider a patient's prior medical history. In this paper, a process mining/deep learning architecture is proposed to improve established severity scoring methods by incorporating the medical history of diabetes patients. First, health records of past hospital encounters are converted to event logs suitable for process mining. The event logs are then used to discover a process model that describes the past hospital encounters of patients. An adaptation of Decay Replay Mining is proposed to combine medical and demographic information with established severity scores to predict the in-hospital mortality of diabetes ICU patients. Significant performance improvements are demonstrated compared to established risk severity scoring methods and machine learning approaches using the Medical Information Mart for Intensive Care III dataset. © 2013 IEEE.","deep learning; diabetes; in-hospital mortality; intensive care; Process mining; risk assessment","Indium compounds; Intensive care units; Learning systems; Demographic information; Diabetes patients; Health care providers; Hospital mortality; Learning architectures; Machine learning approaches; Medical information; Time-consuming tasks; Data mining; adult; article; controlled study; data mining; deep learning; demography; diabetic patient; diagnostic imaging; human; in-hospital mortality; intensive care unit; medical history; medical information; Petri net; prediction; process model; risk assessment; intensive care unit; mining",Article,Scopus,2-s2.0-85111203645
"Xiong Y., Peng W., Chen Q., Huang Z., Tang B.","57208557645;57220993199;34869206800;8593282000;35115621400;","A Unified Machine Reading Comprehension Framework for Cohort Selection",2022,"IEEE Journal of Biomedical and Health Informatics","26","1",,"379","387",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111038447&doi=10.1109%2fJBHI.2021.3095478&partnerID=40&md5=9a195587ac62308412bb86c80741e8c7","Cohort selection is an essential prerequisite for clinical research, determining whether an individual satisfies given selection criteria. Previous works for cohort selection usually treated each selection criterion independently and ignored not only the meaning of each selection criterion but the relations among cohort selection criteria. To solve the problems above, we propose a novel unified machine reading comprehension (MRC) framework. In this MRC framework, we design simple rules to generate questions for each criterion from cohort selection guidelines and treat clues extracted by trigger words from patients' medical records as passages. A series of state-of-the-art MRC models based on BiDAF, BIMPM, BERT, BioBERT, NCBI-BERT, and RoBERTa are deployed to determine which question and passage pairs match. We also introduce a cross-criterion attention mechanism on representations of question and passage pairs to model relations among cohort selection criteria. Results on two datasets, that is, the dataset of the 2018 National NLP Clinical Challenge (N2C2) for cohort selection and a dataset from the MIMIC-III dataset, show that our NCBI-BERT MRC model with cross-criterion attention mechanism achieves the highest micro-averaged F1-score of 0.9070 on the N2C2 dataset and 0.8353 on the MIMIC-III dataset. It is competitive to the best system that relies on a large number of rules defined by medical experts on the N2C2 dataset. Comparing these two models, we find that the NCBI-BERT MRC model mainly performs worse on mathematical logic criteria. When using rules instead of the NCBI-BERT MRC model on some criteria regarding mathematical logic on the N2C2 dataset, we obtain a new benchmark with an F1-score of 0.9163, indicating that it is easy to integrate rules into MRC models for improvement. © 2013 IEEE.","Cohort selection; machine reading comprehension; phenotyping, self-attention","Clinical research; Computer circuits; Attention mechanisms; F1 scores; Medical experts; Medical record; Modeling relations; Reading comprehension; Selection criteria; State of the art; Large dataset; adult; article; attention; cohort analysis; data mining; female; human; logic; male; medical expert; medical record; phenotype; practice guideline; reading; attention; logic; practice guideline",Article,Scopus,2-s2.0-85111038447
"Brasil Y.L., Cruz-Tirado J.P., Barbin D.F.","57214181552;57194466483;54913625700;","Fast online estimation of quail eggs freshness using portable NIR spectrometer and machine learning",2022,"Food Control","131",,"108418","","",,10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109929234&doi=10.1016%2fj.foodcont.2021.108418&partnerID=40&md5=c05cb16460f80ae34eef5a19223faa61","Quail eggs are one of the main natural sources of essential nutrients, presenting high amounts of protein, antioxidants, calcium, iron and phosphorus. However, its quality assessment demands laborious methods and chemicals, and there is currently no standard method do quantify its freshness. This work aimed to investigate the performance of a portable NIR spectrometer, in combination with machine learning, to estimate the freshness of quail eggs. Since there is no standard index to classify quail eggs, we compared Haugh Unit (HU), Yolk Index (YI) and the Egg Quality Index (EQI) as reference methods. Partial Least Squares Regression (PLSR) and Support Vector Machine Regression (SVMR) were used to build prediction models, and Partial Least Squares-Discriminant Analysis (PLSDA) and Support Vector Machine Classification (SVMC) for the development of classification models. For the first time, we demonstrated that EQI, which is a parameter that measures egg freshness according to the quality of the yolk and the albumen, is the best way to express the freshness of quail eggs. The best prediction models were obtained for YI and EQI, using SVMR, with RPD = 2.0–2.5 and RER >10, indicating good predictive capacity. PLSDA and SVMC models showed similar performance, correctly classifying more than 80% of the samples. The results obtained demonstrate the potential of portable NIR spectrometer for monitoring quail eggs freshness during storage. © 2021 Elsevier Ltd","Chemometrics; Data mining; Near infrared spectroscopy; Shelf life",,Article,Scopus,2-s2.0-85109929234
"Ocaña M., Chapela-Campa D., Álvarez P., Hernández N., Mucientes M., Fabra J., Llamazares Á., Lama M., Revenga P.A., Bugarín A., García-Garrido M.A., Alonso J.M.","56410080500;57203988184;35271271700;35760976400;6506530208;15019133700;36632755700;12239276700;12790320200;55910844200;13005796000;55618144400;","Automatic linguistic reporting of customer activity patterns in open malls",2022,"Multimedia Tools and Applications","81","3",,"3369","3395",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109612951&doi=10.1007%2fs11042-021-11186-3&partnerID=40&md5=6b57179b8c8370c47b0ffffab7f95cfa","In this work, we present a complete system to produce an automatic linguistic reporting about the customer activity patterns inside open malls, a mixed distribution of classical malls joined with the shops on the street. These reports can assist to design marketing campaigns by means of identifying the best places to catch the attention of customers. Activity patterns are estimated with process mining techniques and the key information of localization. Localization is obtained with a parallelized solution based on WiFi fingerprint system to speed up the solution. In agreement with the best practices for human evaluation of natural language generation systems, the linguistic quality of the generated report was evaluated by 41 experts who filled in an online questionnaire. Results are encouraging, since the average global score of the linguistic quality dimension is 6.17 (0.76 of standard deviation) in a 7-point Likert scale. This expresses a high degree of satisfaction of the generated reports and validates the adequacy of automatic natural language textual reports as a complementary tool to process model visualization. © 2021, The Author(s).","Automatic linguistic reporting; Data mining techniques; Localization; Parallelization strategies; Social workflows","Data mining; Electronic assessment; Linguistics; Natural language processing systems; Online systems; Sales; Complementary tools; Degree of satisfaction; Fingerprint systems; Marketing campaign; Mixed distribution; Natural language generation systems; Online questionnaire; Standard deviation; Quality control",Article,Scopus,2-s2.0-85109612951
"Homocianu D., Dospinescu O., Sireteanu N.-A.","37023064100;57193915121;56067480700;","Exploring the Influences of Job Satisfaction for Europeans Aged 50 + from Ex-communist vs. Non-communist Countries",2022,"Social Indicators Research","159","1",,"235","279",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109377390&doi=10.1007%2fs11205-021-02754-z&partnerID=40&md5=7f069f0d1d01f00ae31d9f3c43852025","The paper deals with the analysis of the influences of job satisfaction among Europeans aged 50 + (SHARE-ERIC’s data set-Wave7) filtered on main residences and education before 1989. Besides confirming the leading role of the workplace atmosphere and own efforts recognition (dual-core), it further validates the assumption that education and residence in former communist countries count when analyzing job satisfaction and brings two particular types of models. We used many methods based on data mining and variable selection, ordinal and binary logistic and probit regressions, cross-validations via LASSO and mixed-effects modeling with random effects on countries, average marginal effects, and logistic-based prediction nomograms. We discovered seven common influences that count the most when analyzing job satisfaction in these circumstances. It is about the dual-core above and the ones corresponding to older respondents, the better-educated ones (ISCED2011), those with computer skills, the ones endowed with thoroughness, and the ones having higher values of the CASP index of life quality. Depending on each of the two specific models, we discovered peculiarities related to the role of some economic (GDP and SMC to GDP) and institutional (WGI) indicators. For the ex-communist models, we found significant negative influences for both categories while, for non-communist ones, only the second category matters and has a positive role. © 2021, The Author(s), under exclusive licence to Springer Nature B.V.","Data mining and LASSO; Ex-communist vs. non-communist countries; Job satisfaction; Logistic and probit models; Mixed and marginal effects; Prediction capability","data mining; Gross Domestic Product; modeling; prediction; workplace; Europe",Article,Scopus,2-s2.0-85109377390
"Youssef N., Abdulkader H., Abdelwahab A.","57219292883;56572688700;24721237300;","Enhanced parallel mining algorithm for frequent sequential rules",2022,"Ain Shams Engineering Journal","13","1","101505","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108547933&doi=10.1016%2fj.asej.2021.05.019&partnerID=40&md5=77d45e2ebae484553e348ee18611bb2d","Sequential rule mining is an important data mining technique that discovers relationships between occurrences of sequential patterns. The main challenge is to avoid time-consuming, especially in large search spaces. This can be achieved through developing an efficient sequential rule mining algorithm without redundancy in a long sequence dataset. In this paper, an algorithm named PNRD-CloGen is proposed. It can be used to mine sequential rules from both frequent closed dynamic bit vectors with sequential generator patterns at the same time. It helps in speedily eliminating uninteresting candidates and compact the representations. Additionally, we apply a parallel approach utilizing multi-core architecture. An experimental evaluation was performed using five real sequence datasets: BMSWebView1, Sign, FIFA, Korsarak, and MSNBC. The proposed algorithm has been compared with two non-redundant sequential rule algorithms called: TRuleGrowth, and NRD-DBV algorithm. Experimental results show the time saving, especially for large sequence datasets and low minimum support threshold. © 2021 THE AUTHORS","Closed sequential patterns; Dynamic bit vector; Multi-core processors; Non-redundant rule; Sequential generator patterns","Computer architecture; Large dataset; Experimental evaluation; Large sequences; Long sequences; Minimum support thresholds; Multicore architectures; Parallel mining algorithms; Sequential patterns; Sequential rule; Data mining",Article,Scopus,2-s2.0-85108547933
"Bao Y., Wang B., Guo P., Wang J.","57224546145;57210246809;57224533340;55929423300;","Chemical process fault diagnosis based on a combined deep learning method",2022,"Canadian Journal of Chemical Engineering","100","1",,"54","66",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107843786&doi=10.1002%2fcjce.24153&partnerID=40&md5=a0e78af81055928d8c43c21bd5956d16","The study on fault detection and diagnosis (FDD) of chemical processes has always been the top priority of the chemical process safety. In this paper, a fault diagnosis method combining the deep convolutional with the recurrent neural network (DCRNN) is proposed. In this method, the data from chemical processes are input to the deep convolutional neural network (DCNN) to extract features in spatial domains, and then, the features are fused into the bidirectional recurrent neural network (BRNN). Due to the powerful capabilities of DCNN to extract features in spatial domains and the sensitivity to time series of RNN, the combined method can adaptively learn the dynamic information of the raw data in both spatial and temporal domains and has unique advantages in multivariate chemical processes. The application of the DCRNN model in the Tennessee Eastman (TE) process demonstrates the high accuracy of this proposal in identifying abnormal conditions for the chemical process, compared with the traditional fault identification algorithms of deep learning. © 2021 Canadian Society for Chemical Engineering.","DCNN; deep learning; fault diagnosis; RNN; Tennessee Eastman","Chemical detection; Convolution; Convolutional neural networks; Data mining; Deep neural networks; Failure analysis; Fault detection; Learning systems; Abnormal conditions; Bidirectional recurrent neural networks; Chemical process safety; Dynamic information; Fault detection and diagnosis; Fault diagnosis method; Fault identifications; Tennessee Eastman; Recurrent neural networks",Article,Scopus,2-s2.0-85107843786
"Chu W., Ho C.-S., Liao P.-H.","35336091100;57224511886;24825115100;","Comparison of different predicting models to assist the diagnosis of spinal lesions",2022,"Informatics for Health and Social Care","47","1",,"92","102",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107759540&doi=10.1080%2f17538157.2021.1939355&partnerID=40&md5=6a2f84ce508545755c7c5597f60af008","In neurosurgical or orthopedic clinics, the differential diagnosis of lower back pain is often time-consuming and costly. This is especially true when there are several candidate diagnoses with similar symptoms that might confuse clinic physicians. Therefore, methods for the efficient differential diagnosis can help physicians to implement the most appropriate treatment and achieve the goal of pain reduction for their patients. In this study, we applied data-mining techniques from artificial intelligence technologies, in order to implement a computer-aided auxiliary differential diagnosis for a herniated intervertebral disc, spondylolithesis, and spinal stenosis. We collected questionnaires from 361 patients and analyzed the resulting data by using a linear discriminant analysis, clustering, and artificial neural network techniques to construct a related classification model and to compare the accuracy and implementation efficiency of the different methods. Our results indicate that a linear discriminant analysis has obvious advantages for classification and diagnosis, in terms of accuracy. We concluded that the judgment results from artificial intelligence can be used as a reference for medical personnel in their clinical diagnoses. Our method is expected to facilitate the early detection of symptoms and early treatment, so as to reduce the social resource costs and the huge burden of medical expenses, and to increase the quality of medical care. © 2021 The Author(s). Published with license by Taylor & Francis Group, LLC.","auxiliary diagnosis; Data mining; spinal disease","artificial intelligence; human; questionnaire; Artificial Intelligence; Humans; Neural Networks, Computer; Surveys and Questionnaires",Article,Scopus,2-s2.0-85107759540
"Bugatti M., Colosimo B.M.","57203618949;6507082990;","Towards real-time in-situ monitoring of hot-spot defects in L-PBF: a new classification-based method for fast video-imaging data analysis",2022,"Journal of Intelligent Manufacturing","33","1",,"293","309",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107745486&doi=10.1007%2fs10845-021-01787-y&partnerID=40&md5=9afa3b8d6ce59d976c6c2c6c7262bc75","The increasing interest towards additive manufacturing (AM) is pushing the industry to provide new solutions to improve process stability. Monitoring is a key tool for this purpose but the typical AM fast process dynamics and the high data flow required to accurately describe the process are pushing the limits of standard statistical process monitoring (SPM) techniques. The adoption of novel smart data extraction and analysis methods are fundamental to monitor the process with the required accuracy while keeping the computational effort to a reasonable level for real-time application. In this work, a new framework for the detection of defects in metal additive manufacturing processes via in-situ high-speed cameras is presented: a new data extraction method is developed to efficiently extract only the relevant information from the regions of interest identified in the high-speed imaging data stream and to reduce the dimensionality of the anomaly detection task performed by three competitor machine learning classification methods. The defect detection performance and computational speed of this approach is carefully evaluated through computer simulations and experimental studies, and directly compared with the performance and computational speed of other existing methods applied on the same reference dataset. The results show that the proposed method is capable of quickly detecting the occurrence of defects while keeping the high computational speed that would be required to implement this new process monitoring approach for real-time defect detection. © 2021, The Author(s).","Image-based process monitoring; In-situ defect detection; Laser Powder Bed Fusion (L-PBF); Machine learning; Neural network","3D printers; Additives; Anomaly detection; Classification (of information); Data streams; Defects; Extraction; High speed cameras; Process monitoring; Speed; Statistical process control; Classification based methods; Computational effort; Detection of defects; Machine learning classification; Real time in situ monitoring; Real-time application; Regions of interest; Statistical process monitoring; Data mining",Article,Scopus,2-s2.0-85107745486
"Xiaolei W.","55344239000;","Similar Simulation Test of Overlying Rock Failure and Crack Evolution in Fully Mechanized Caving Face with Compound Roof",2022,"Geotechnical and Geological Engineering","40","1",,"73","82",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107659253&doi=10.1007%2fs10706-021-01892-y&partnerID=40&md5=bf354cf4d65186f1830e7d30d59f1015","In the process of coal mining, the characteristics of overburden failure and the evolution of cracks were of great significance to the prevention and control of mine water and gas by using similar simulation test to study the overlying rock failure and crack evolution law of Yuwu Coal Industry's compound roof and large cutting height fully mechanized caving face. The results show that: the height of the caving zone is 32.6 m and the height of the fracture zone is 64.6 m; The influence of coal seam mining process on the supporting pressure of the roof overlying rock, the near seam was larger, the far seam was smaller; The inclination angle of the overburden mining fissures was medium angle, the width was mainly medium width, and the number of fissures gradually decreases as they move away from the coal seam; During the mining process, the overlying strata of the coal seams at a short distance are the gathering areas of mining cracks, and the density curve of the overlying cracks presents a ""wave"" shape. © 2021, The Author(s), under exclusive licence to Springer Nature Switzerland AG.","Crack evolution; Fully mechanized caving mining; Mining engineering; Overburden failure; Similar simulation","Coal; Coal deposits; Cracks; Data mining; Failure (mechanical); Groundwater; Mine roof control; Roofs; Evolution of cracks; Fully-mechanized caving faces; Inclination angles; Large cutting height; Overburden failures; Overlying strata; Prevention and controls; Simulation tests; Coal industry; caving; coal mining; coal seam; computer simulation; cracking (fracture); engineering geology; failure analysis; overburden",Article,Scopus,2-s2.0-85107659253
"Zhang C., Zhao Y., Zhou Y., Zhang X., Li T.","57206684840;55443372800;57224363242;7410281588;57196459776;","A real-time abnormal operation pattern detection method for building energy systems based on association rule bases",2022,"Building Simulation","15","1",,"69","81",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107450446&doi=10.1007%2fs12273-021-0791-x&partnerID=40&md5=cd41faf32cd7bf3ea6ce94caa273d95a","Expert systems are effective for anomaly detection in building energy systems. However, it is usually inefficient to establish comprehensive rule bases manually for complex building energy systems. Association rule mining is available to accelerate the establishment of the rule bases due to its powerful capability of discovering rules from numerous data. This paper proposes a real-time abnormal operation pattern detection method towards building energy systems. It can benefit from both expert systems and association rule mining. Association rules are utilized to establish association rule bases of abnormal and normal operation patterns. The established rule bases are then utilized to develop an expert system for real-time detection of abnormal operation patterns. The proposed method is applied to an actual chiller plant for evaluating its performance. Results show that 15 types of known abnormal operation patterns and 11 types of unknown abnormal operation patterns are detected successfully by the proposed method. © 2021, Tsinghua University Press and Springer-Verlag GmbH Germany, part of Springer Nature.","anomaly detection; association rule mining; building energy conservation; building energy systems; expert systems","Anomaly detection; Association rules; Data mining; Expert systems; Pattern recognition; Abnormal operation; Building energy systems; Chiller plants; Complex buildings; In-buildings; Normal operations; Real-time detection; Rule basis; Real time systems",Article,Scopus,2-s2.0-85107450446
"Wang X., Yang F., Zhang H., Su D., Wang Z., Xu F.","57208133495;23010905700;57194513662;57193845177;57224205749;57212413221;","Registration of Airborne LiDAR Bathymetry and Multibeam Echo Sounder Point Clouds",2022,"IEEE Geoscience and Remote Sensing Letters","19",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107234775&doi=10.1109%2fLGRS.2021.3076462&partnerID=40&md5=af25e6fca6f8998eabe036609f8799e4","Airborne light detection and ranging (LiDAR) bathymetry (ALB) and multibeam echo sounder (MBES) are both active remote sensing technologies that are complementary in terms of survey scope. The registration of ALB and MBES data can provide complete overwater and underwater geoinformation on a measurement target. However, in the overlapping area of the ALB and MBES data, there are different point densities and few identifiable structure features. Although the existing multiplatform registration strategies can provide good results for overwater datasets, they are difficult to adapt for the registration of ALB and MBES data. Therefore, to address these problems, a new registration method for ALB and MBES datasets is proposed in this letter. First, a triangulated irregular network (TIN) is constructed with control points extracted from the MBES data. Then, the features of the TIN facets are extracted to identify the data gaps. Finally, the transformation parameters are iteratively calculated by minimizing the distances between the ALB points and MBES TIN facets. Five samples with different characteristics captured around Yuanzhi Island in the South China Sea are selected to evaluate the performance of the proposed method. The mean root mean square error (RMSE) of the five samples is approximately 0.2 m. The results indicate that the proposed method performs well for the registration of ALB and MBES datasets, with advantages in accuracy and robustness. © 2004-2012 IEEE.","Airborne laser scanning (ALS); data gaps; multibeam echo sounder (MBES); registration","Acoustic devices; Bathymetry; Data mining; Iterative methods; Mean square error; Remote sensing; Light detection and ranging bathymetries; Multi-beam echo sounders; Multibeam echo sounder; Registration methods; Remote sensing technology; Root mean square errors; Transformation parameters; Triangulated irregular networks; Optical radar; airborne sensing; bathymetry; data set; echo sounder; lidar; Pacific Ocean; South China Sea",Article,Scopus,2-s2.0-85107234775
"Wang L., Gui L., Zhu H.","57188740375;57223909626;57204294170;","Incremental fuzzy temporal association rule mining using fuzzy grid table",2022,"Applied Intelligence","52","2",,"1389","1405",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106418946&doi=10.1007%2fs10489-021-02407-1&partnerID=40&md5=71aa7e8232cdcd4b2621bced329c8370","Traditional temporal association rules mining algorithms cannot dynamically update the temporal association rules within the valid time interval with increasing data. In this paper, a new algorithm called incremental fuzzy temporal association rule mining using fuzzy grid table (IFTARMFGT) is proposed by combining the advantages of boolean matrix with incremental mining. First, multivariate time series data are transformed into discrete fuzzy values that contain the time intervals and fuzzy membership. Second, in order to improve the mining efficiency, the concept of boolean matrices was introduced into the fuzzy membership to generate a fuzzy grid table to mine the frequent itemsets. Finally, in view of the Fast UPdate (FUP) algorithm, fuzzy temporal association rules are incrementally mined and updated without repeatedly scanning the original database by considering the lifespan of each item and inheriting the information from previous mining results. The experiments show that our algorithm provides better efficiency and interpretability in mining temporal association rules than other algorithms. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Fuzzy grid table; Fuzzy temporal association rule; Incremental mining; Item lifespan","Association rules; Efficiency; Matrix algebra; Boolean matrices; Boolean Matrix; Fuzzy membership; Incremental mining; Interpretability; Multivariate time series; Temporal association rule; Temporal association rule minings; Data mining",Article,Scopus,2-s2.0-85106418946
"Du P., Wang J., Yang W., Niu T.","57190880827;56380147600;57191974753;57193523842;","A novel hybrid fine particulate matter (PM2.5) forecasting and its further application system: Case studies in China",2022,"Journal of Forecasting","41","1",,"64","85",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106220632&doi=10.1002%2ffor.2785&partnerID=40&md5=b18f2a649720f2a4c29af692190f526d","Air pollution has received more attention from many countries and scientists due to its high threat to human health. However, air pollution prediction remains a challenging task because of its nonstationarity, randomness, and nonlinearity. In this research, a novel hybrid system is successfully developed for PM2.5 concentration prediction and its application in health effects and economic loss assessment. First, an efficient data mining method is adopted to capture and extract the primary characteristic of PM2.5 dataset and alleviate the noises' adverse effects. Second, Harris hawks optimization algorithm is introduced to tune the extreme learning machine model with high prediction accuracy, then the optimized extreme learning machine can be established to obtain the forecasting values of PM2.5 series. Next, PM2.5-related health effects and economic costs was estimated based on the predicted PM2.5 values, the related health effects, and environmental value assessment methods. Several experiments are designed using three daily PM2.5 datasets from Beijing, Tianjin, and Shijiazhuang. Lastly, the corresponding experimental results showed that this proposed system can not only provide early warning information for environmental management, assist in the formulation of effective measures to reduce air pollutant emissions, and prevent health problems but also help for further research and application in different fields, such as health issues due to PM2.5 pollutant. © 2021 John Wiley & Sons, Ltd.","extreme learning machine; Harris hawks optimization algorithm; health effects and economic loss assessment; PM2.5 prediction and application system","Air pollution; Economic and social effects; Environmental management; Forecasting; Health; Health risks; Hybrid systems; Information management; Knowledge acquisition; Losses; Machine learning; Particles (particulate matter); Air pollutant emission; Air pollution predictions; Application systems; Environmental values; Extreme learning machine; Fine particulate matter (PM2.5); Optimization algorithms; Research and application; Data mining",Article,Scopus,2-s2.0-85106220632
"Lei D., Huang Y., Zhang L., Li W.","36627356400;57223427483;57200044819;36067507500;","Multibranch Feature Extraction and Feature Multiplexing Network for Pansharpening",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105893594&doi=10.1109%2fTGRS.2021.3074624&partnerID=40&md5=e877af33bf004d228d0ea26fa6b21ae5","With the continuous development of deep neural networks in the visual field, their application to panchromatic sharpening has received increasing attention from researchers; however, the existing panchromatic sharpening methods generally lack the ability to combine knowledge of the panchromatic sharpening field for feature extraction with neural networks, which have certain limitations in feature extraction and the discovery of new features. This article proposes a simple, modular, multibranched feature extraction and reuses network architecture designed not only to support feature reuse but to learn well-expressed new features for use in panchromatic sharpening approaches. In addition, we fused field knowledge of panchromatic sharpening to extract spatial structure information of panchromatic maps through gradient calculators and design structural and spectral compensation to fully extract and preserve the spatial structural and spectral information of images. We conducted experiments on the QuickBird and WorldView-3 satellite data sets, and the experimental results reveal that our proposed method has advantages over the best methods currently available, achieving excellent results not only on objective evaluation metrics, such as full-reference and no-reference metrics, but also on subjective visual evaluation. © 1980-2012 IEEE.","Deep neural networks; feature extraction; feature multiplexing; multibranch architecture","Data mining; Deep neural networks; Extraction; Feature extraction; Network architecture; Continuous development; Multiplexing networks; No-reference metrics; Objective evaluation; Satellite data sets; Spatial structure information; Spectral compensation; Spectral information; Neural networks; remote sensing",Article,Scopus,2-s2.0-85105893594
"El-Bably M.K., Fleifel K.K., Embaby O.A.","35242140000;57194075910;56233580900;","Topological approaches to rough approximations based on closure operators",2022,"Granular Computing","7","1",,"","",,7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105862719&doi=10.1007%2fs41066-020-00247-x&partnerID=40&md5=247de9558df104b0f63d5128b8f81ae6","The main goal of this paper is to integrate the relationships among rough set theory and topology. We introduce different closure operators by using binary relations. Using these operators, we construct generalized approximation operators in the theory of rough sets. In addition, new methods for generating different topologies from any binary relation (without using base or subbase) are provided. The main properties of suggested structures are investigated. Comparisons among the suggested operations and the previous works are constructed. The suggested methods depend, basically, on the “j-neighborhood space” that given by (Abd El-Monsef et al., Int J Granul Comput Rough Sets Intell Syst 3:292–305, 2014). These methods generate new granulation for rough sets. Finally, a practical example is introduced as a simple application for the suggested structures. We think that at the level of theoretical foundations and real-life implementations, the proposed approaches place emphasis on ties between rough sets, granular computing, and information discovery and data mining. © 2021, Springer Nature Switzerland AG.","Closure operators; Neighborhood spaces; Topology and Rough Sets","Approximation algorithms; Computation theory; Data mining; Topology; Approximation operators; Binary relation; Closure operators; Neighborhood space; Neighbourhood; Property; Rough approximations; Simple++; Topological approach; Topology and rough set; Rough set theory",Article,Scopus,2-s2.0-85105862719
"Li W., Luo Z., Xiao Z., Chen Y., Wang C., Li J.","57220088316;57200602927;56579557400;56233739200;36990982800;57235557700;","A GCN-Based Method for Extracting Power Lines and Pylons from Airborne LiDAR Data",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105846559&doi=10.1109%2fTGRS.2021.3076107&partnerID=40&md5=f4bb1b545e470e01d5d547ea78f124ff","Extracting the power lines and pylons automatically and accurately from airborne LiDAR data is a critical step in inspecting the routine power line, especially in the remote mountainous areas. However, challenges arise in using existing methods to extract the targets from large scenarios of remote mountainous areas since the terrain is undulating, and the features are difficult to distinguish. In this article, to overcome these challenges, we propose a graph convolutional network (GCN)-based method to extract power lines and pylons from Airborne LiDAR point clouds. First, data augmentation and near-ground filtering methods are developed to overcome the problems of insufficient and imbalanced samples in the LiDAR data. Then, a GCN-based framework is proposed to extract the power lines and pylons, which consist of two main modules, i.e., the neighborhood dimension information (NDI) module and the neighborhood geometry information aggregation (NGIA) module. These two modules are designed to strengthen the model's ability to portray local geometric details. Besides, an attention fusion module is investigated to further improve the NDI and NGIA features. Finally, a line structure constraint algorithm is proposed to identify individual power lines, where the power corridor is reconstructed using a polynomial-based algorithm. Numerical experiments are conducted based on two different power line scenarios acquired in mountainous areas. The results demonstrate the superior performances of the proposed method over several existing algorithms, where the F_{1} score and quality of the power line are 99.3% and 98.6%, and the results of the pylon are 96% and 92.4%, respectively. The identification rate of power line identification is above 98%. © 1980-2012 IEEE.","Airborne LiDAR; graph convolutional network (GCN); neighborhood information; power line extraction; pylon extraction","Convolutional neural networks; Nondestructive examination; Optical radar; Airborne lidar data; Convolutional networks; Data augmentation; Geometric details; Geometry information; Ground filtering; Identification rates; Numerical experiments; Data mining; accuracy assessment; airborne sensing; extraction method; lidar; power line; remote sensing",Article,Scopus,2-s2.0-85105846559
"Oulhiq R., Benjelloun K., Kali Y., Saad M.","57188710582;6603543630;57188881386;7202075533;","A data mining based approach for process identification using historical data",2022,"International Journal of Modelling and Simulation","42","2",,"335","349",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105215118&doi=10.1080%2f02286203.2021.1905375&partnerID=40&md5=bb1482e7d57c1c34d30861f79eea72e1","In this paper, a data mining based methodology for process identification from historical data was proposed. Thereon, it considers the phases of process understanding, data collection, data preparation, data modeling, and model evaluation. As some parts of historical data are irrelevant, a data selection step, based on the Gaussian Mixture Model (GMM) clustering algorithm, was considered. Additionally, the methodology includes a data informativity step to study the richness of data. In this regard, the condition number (CN) and the extended CN for ridge regression (RR CN) were used. To evaluate the approach, 2 years of industrial thickener historical data were used. Thereafter, data were prepared and an ARX (Auto-Regressive with eXogenous inputs) model structure was adopted to identify the model. To estimate input delays, Granger causality was used. As for fit criteria, least square regression was tested and compared to ridge regression as a less sensitive method to multicollinearity. The results were then evaluated based on the 20-step ahead prediction and compared to existing methods. In this context, the proposed approach gave the best results with an R 2 of 98.11% and 62.70% for 1 and 20-step ahead predictions, respectively. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","data mining; gaussian mixture model; historical data; industrial thickener; methodology; Process identification; ridge regression","Clustering algorithms; Data acquisition; Gaussian distribution; Least squares approximations; Number theory; Regression analysis; Condition numbers; Gaussian Mixture Model; Granger Causality; Industrial thickeners; Least square regression; Multicollinearity; Process identification; Process understanding; Data mining",Article,Scopus,2-s2.0-85105215118
"Jiang J., Ren H., Zhang M.","57217988653;23101165700;57230229600;","A Convolutional Autoencoder Method for Simultaneous Seismic Data Reconstruction and Denoising",2022,"IEEE Geoscience and Remote Sensing Letters","19",,,"","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105059437&doi=10.1109%2fLGRS.2021.3073560&partnerID=40&md5=fd24b5d707947f637fdf8399ebfe1e7b","Petroleum geophysical exploration is based on seismic data and has been widely affected by deep learning technology in recent years. As a consequence of the high efficiency and nonlinear fitting ability of deep learning models, we propose an improved convolutional autoencoder (CAE) method to achieve simultaneous reconstruction and denoising of seismic data. The architecture of the improved CAE is based on group convolution and inception structures, which have powerful feature extraction capabilities for seismic data. The CAE method regards the reconstruction and denoising of seismic data as a feature extraction process of the target seismic signals; this enables the method to simultaneously reconstruct the seismic signal accurately and suppress the random noise mixed in the seismic data. During the training of the CAE, the mean absolute error (MAE) loss function and Adam optimization algorithm were used. Because this is a data-driven method and does not require the threshold to be input manually, it can process a large amount of seismic data quickly and intelligently. Synthetic and field data examples demonstrate the effectiveness of the CAE method. © 2004-2012 IEEE.","Convolutional autoencoder (CAE); deep learning; denoising; seismic data reconstruction","Convolution; Data mining; Deep learning; Extraction; Feature extraction; Geophysical prospecting; Petroleum prospecting; Seismic response; Seismic waves; Data-driven methods; Extraction capability; Geophysical exploration; Learning technology; Mean absolute error; Optimization algorithms; Seismic data reconstruction; Simultaneous reconstruction; Learning systems; algorithm; architecture; extraction method; fossil fuel; optimization; reconstruction; seismic data; threshold",Article,Scopus,2-s2.0-85105059437
"Islam M.A., Rafi M.R., Azad A.-A., Ovi J.A.","57224751549;57223124298;57223125605;57209212090;","Weighted frequent sequential pattern mining",2022,"Applied Intelligence","52","1",,"254","281",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105004042&doi=10.1007%2fs10489-021-02290-w&partnerID=40&md5=a60a0098727074b16370b218fa71881e","Trillions of bytes of data are generated every day in different forms, and extracting useful information from that massive amount of data is the study of data mining. Sequential pattern mining is a major branch of data mining that deals with mining frequent sequential patterns from sequence databases. Due to items having different importance in real-life scenarios, they cannot be treated uniformly. With today’s datasets, the use of weights in sequential pattern mining is much more feasible. In most cases, as in real-life datasets, pushing weights will give a better understanding of the dataset, as it will also measure the importance of an item inside a pattern rather than treating all the items equally. Many techniques have been introduced to mine weighted sequential patterns, but typically these algorithms generate a massive number of candidate patterns and take a long time to execute. This work aims to introduce a new pruning technique and a complete framework that takes much less time and generates a small number of candidate sequences without compromising with completeness. Performance evaluation on real-life datasets shows that our proposed approach can mine weighted patterns substantially faster than other existing approaches. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Data mining; Pattern mining; Sequence mining; Weighted sequence","Artificial intelligence; Candidate patterns; Candidate sequences; Frequent sequential patterns; Pruning techniques; Real life datasets; Sequence database; Sequential-pattern mining; Weighted sequential pattern; Data mining",Article,Scopus,2-s2.0-85105004042
"Wang Y., Guo W.G., Yue X.","57221508697;57217496903;57195393572;","Tensor decomposition to compress convolutional layers in deep learning",2022,"IISE Transactions","54","5",,"481","495",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104534497&doi=10.1080%2f24725854.2021.1894514&partnerID=40&md5=7d834cff125f4fc569fdd2489f305631","Feature extraction for tensor data serves as an important step in many tasks such as anomaly detection, process monitoring, image classification, and quality control. Although many methods have been proposed for tensor feature extraction, there are still two challenges that need to be addressed: (i) how to reduce the computation cost for high dimensional and large volume tensor data; (ii) how to interpret the output features and evaluate their significance. The most recent methods in deep learning, such as Convolutional Neural Network, have shown outstanding performance in analyzing tensor data, but their wide adoption is still hindered by model complexity and lack of interpretability. To fill this research gap, we propose to use CP-decomposition to approximately compress the convolutional layer (CPAC-Conv layer) in deep learning. The contributions of our work include three aspects: (i) we adapt CP-decomposition to compress convolutional kernels and derive the expressions of forward and backward propagations for our proposed CPAC-Conv layer; (ii) compared with the original convolutional layer, the proposed CPAC-Conv layer can reduce the number of parameters without decaying prediction performance. It can combine with other layers to build novel Deep Neural Networks; (iii) the value of decomposed kernels indicates the significance of the corresponding feature map, which provides us with insights to guide feature selection. © Copyright © 2021 “IISE”.","convolutional neural network; CP-decomposition; feature selection; model compression; tensor","Anomaly detection; Convolution; Convolutional neural networks; Data mining; Deep neural networks; Extraction; Feature extraction; Multilayer neural networks; Process monitoring; Quality control; Tensors; Computation costs; Convolutional kernel; Forward-and-backward; High-dimensional; Interpretability; Model complexity; Prediction performance; Tensor decomposition; Deep learning",Article,Scopus,2-s2.0-85104534497
"Hu Z.-Z., Leng S., Lin J.-R., Li S.-W., Xiao Y.-Q.","36147587900;57216968702;56703744700;55523768500;57209294238;","Knowledge Extraction and Discovery Based on BIM: A Critical Review and Future Directions",2022,"Archives of Computational Methods in Engineering","29","1",,"335","356",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104156752&doi=10.1007%2fs11831-021-09576-9&partnerID=40&md5=681ddfc3acfd78eda40b8098faaf3a3d","In the past, knowledge in the fields of Architecture, Engineering and Construction (AEC) industries mainly come from experiences and are documented in hard copies or specific electronic databases. In order to make use of this knowledge, a lot of studies have focused on retrieving and storing this knowledge in a systematic and accessible way. The Building Information Modeling (BIM) technology proves to be a valuable media in extracting data because it provides physical and functional digital models for all the facilities within the life-cycle of the project. Therefore, the combination of the knowledge science with BIM shows great potential in constructing the knowledge map in the field of the AEC industry. Based on literature reviews, this article summarizes the latest achievements in the fields of knowledge science and BIM, in the aspects of (1) knowledge description, (2) knowledge discovery, (3) knowledge storage and management, (4) knowledge inference and (5) knowledge application, to show the state-of-arts and suggests the future directions in the application of knowledge science and BIM technology in the fields of AEC industries. The review indicates that BIM is capable of providing information for knowledge extraction and discovery, by adopting semantic network, knowledge graph and some other related methods. It also illustrates that the knowledge is helpful in the design, construction, operation and maintenance periods of the AEC industry, but now it is only at the beginning stage. © 2021, The Author(s).",,"Architectural design; Digital storage; Electronics industry; Extraction; Knowledge management; Knowledge representation; Life cycle; Semantics; Architecture , engineering and construction industries; Building Information Model - BIM; Electronic database; Knowledge application; Knowledge extraction; Knowledge science; Literature reviews; Operation and maintenance; Data mining",Article,Scopus,2-s2.0-85104156752
"Zhou B., Hu W., Chen T.","57207878997;56687975700;55557599700;","Pattern extraction from industrial alarm flood sequences by a modified clofast algorithm",2022,"IEEE Transactions on Industrial Informatics","18","1",,"288","296",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103900246&doi=10.1109%2fTII.2021.3071361&partnerID=40&md5=b8fe4be60d637395299d12b4d29ea138","Alarm systems are critical for process safety and efficiency of complex industrial facilities. However, the presence of alarm floods severely compromises the performance of alarm systems. To cope with alarm floods, data mining has been applied to discover interesting patterns from historical alarm data, and such patterns can be used for alarm suppression, root cause analysis, and decision supports. However, most existing methods ignored the timestamps in pattern extraction or obtained complete patterns with significant redundancy. In this article, a new method is proposed to extract alarm flood patterns using a modified CloFAST algorithm. The contributions are twofold: first, a closed alarm sequence mining approach is proposed based on the CloFAST algorithm with improvements to incorporate timestamps and tolerate alarm order switchings; second, a pattern distillation strategy is designed to merge similar alarm sequences and export compact alarm sequential patterns. The proposed method is capable of avoiding influences of order ambiguities and also minimizing the redundancy of extracted patterns. The effectiveness of the proposed method is demonstrated by an industrial case study involving alarm data from a large-scale industrial facility. © 2005-2012 IEEE.","Alarm flood; Alarm system; Clofast algorithm; Pattern mining; Process monitoring; Process safety","Accident prevention; Data mining; Decision support systems; Distillation; Extraction; Floods; Redundancy; Decision supports; Industrial case study; Industrial facilities; Pattern extraction; Process safety; Root cause analysis; Sequence mining; Sequential patterns; Alarm systems",Article,Scopus,2-s2.0-85103900246
"Rasti B., Koirala B., Scheunders P., Ghamisi P.","36069933000;57208229179;7003845862;53663404300;","UnDIP: Hyperspectral Unmixing Using Deep Image Prior",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103771765&doi=10.1109%2fTGRS.2021.3067802&partnerID=40&md5=665b301067f61857efed3db25d1dbcd1","In this article, we introduce a deep learning-based technique for the linear hyperspectral unmixing problem. The proposed method contains two main steps. First, the endmembers are extracted using a geometric endmember extraction method, i.e., a simplex volume maximization in the subspace of the data set. Then, the abundances are estimated using a deep image prior. The main motivation of this work is to boost the abundance estimation and make the unmixing problem robust to noise. The proposed deep image prior uses a convolutional neural network to estimate the fractional abundances, relying on the extracted endmembers and the observed hyperspectral data set. The proposed method is evaluated on simulated and three real remote sensing data for a range of SNR values (i.e., from 20 to 50 dB). The results show considerable improvements compared to state-of-the-art methods. The proposed method was implemented in Python (3.8) using PyTorch as the platform for the deep network and is available online: https://github.com/BehnoodRasti/UnDIP. © 1980-2012 IEEE.","Convolutional neural network; Deep learning; Deep prior; Endmember extraction; Hyperspectral image; Unmixing","Convolutional neural networks; Data mining; Remote sensing; Signal to noise ratio; Abundance estimation; Endmember extraction; Hyperspectral Data; Hyperspectral unmixing; Image priors; Remote sensing data; Simplex volume maximizations; State-of-the-art methods; Deep learning; abundance estimation; artificial neural network; data set; extraction method; imagery; noise; remote sensing",Article,Scopus,2-s2.0-85103771765
"Chittor Sundaram R., Naghizade E., Borovica-Gajic R., Tomko M.","57216083057;56275377300;55258719200;23398794500;","Can you fixme? An intrinsic classification of contributor-identified spatial data issues using topic models",2022,"International Journal of Geographical Information Science","36","1",,"1","30",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103569038&doi=10.1080%2f13658816.2021.1893323&partnerID=40&md5=8e446c95b38adca865648751b887c4ad","Assessing OpenStreetMap (OSM) data quality against authoritative data sources may not always be viable. This is primarily because of the multi-dimensional nature and heterogeneity of the maps, yet the activity is pivotal for targeted data cleansing and quality enhancement undertakings in these data sets. A salient facet of OSM, allowing contributors to flag potential problems encountered during the mapping process, is the FIXME tag. In this article, we examine and discuss OSM data quality through the vast expanse of issues (knowledge) documented via FIXME. We present a classification and analysis of these quality issues, exposed as topic models and grounded in the ISO-19157 standard, across USA and Australia. Regional distributions of these topics are further qualitatively analyzed to ascertain the variation of key issues in OSM. We also present a comparison of the intrinsic issue classification against those identified in an issue corpus of an authoritative map data source. Due to the considerable heterogeneity in user mapping and reporting, OSM issue detection and classification remains problematic. This research presents a flexible and intrinsic data-mining approach, linking established ISO data quality standards to OSM issue categorization. Our work, thus informs the development of automated error correction methods for VGI datasets. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","FIXME;fixme; L-LDA; latent labeled dirichlet allocation; LDA; OpenStreetMap; spatial data quality; text mining; Topic modelling; VGI","data mining; data quality; error correction; GIS; mapping; modeling; spatial data; Australia; United States",Article,Scopus,2-s2.0-85103569038
"Randall N., Šabanović S., Milojević S., Gupta A.","57196248253;14036199300;16029209200;57222603516;","Top of the Class: Mining Product Characteristics Associated with Crowdfunding Success and Failure of Home Robots",2022,"International Journal of Social Robotics","14","1",,"149","163",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103369372&doi=10.1007%2fs12369-021-00776-8&partnerID=40&md5=0b382cf324e1cc278153c1c446a8253e","Recent high-profile failures of domestic robotic products suggest more research is needed on factors that impact consumers’ willingness to purchase robots, and the success rates of the consumer robotics industry compared with other innovative technologies. Using data from two crowdfunding sites (Kickstarter and Indiegogo), we summarize the applications, forms, prices, contexts of use, target populations, and sociality of potential consumer and home robots. We then use statistical analysis, predictive modeling, and word co-occurrence to determine which characteristics are associated with increased product support by early market consumers, finding that health and fitness, security and monitoring, and general education applications, cartoon-like and animal-like robot forms, and single user group robots have significantly more backers. We also find that social robots have a mean of 1.2–3.2 times as many backers as non-social robots and that every twofold increase in price results in a 20% decrease in financial supporters. Product reviews from these sites are additionally used to identify product features consumers found important. Finally, analyses of the failure rates of social and home robots find that these products are not failing more frequently than other innovative products overall. This research is among the first to study factors influencing consumers’ purchasing behavior of home robots, and to use data mining methods to gain insights into home and consumer robot design. © 2021, The Author(s), under exclusive licence to Springer Nature B.V.","Consumer behavior; Consumer robotics; Data-driven design; Design strategy; Domestic robots; Home robots; Intelligent systems product design; Market-driven product design; Social robots; Strategic design","Agricultural robots; Consumer behavior; Crowdsourcing; Data mining; Failure (mechanical); Failure analysis; Industrial robots; Machine design; Population statistics; Predictive analytics; Robotics; Social robots; Data mining methods; Innovative product; Innovative technology; Predictive modeling; Product characteristics; Purchasing behaviors; Security and monitoring; Word co-occurrence; Educational robots",Article,Scopus,2-s2.0-85103369372
"Wang J., Wei M., Xing X.","55742680800;57221540918;57215845292;","Static gain estimation for nonlinear dynamic systems from steady-state values hidden in historical data",2022,"ISA Transactions","120",,,"78","88",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102835351&doi=10.1016%2fj.isatra.2021.03.007&partnerID=40&md5=1c6c7552ad09794ae333b639fcaeb93b","Static gains are often required for control, diagnosis and optimization of nonlinear dynamic systems. This paper proposes a new approach to estimate static gains for nonlinear dynamic systems from steady-state values hidden in historical data. First, steady-state values of system inputs and outputs are extracted by automatically finding data segments in steady-state conditions. Second, static gains of nonlinear dynamic systems in different operating conditions are estimated via linear regression from these steady-state values. The proposed approach has two practical features: (i) estimated static gains can be verified in a convincing way, because the validness of extracted steady-state values is confirmed by visualizing data segments in steady-state conditions, and the accuracy of estimated static gains is verified by comparing the extracted steady-state values and their estimates; (ii) the proposed approach is simple to understand and implement in practice, since it only involves a linear equation between steady-state values and static gains, as well as a basic technique of linear regression. Numerical simulation and industrial application demonstrate the effectiveness of the proposed approach. © 2021 ISA","Linear regression; Nonlinear dynamic systems; Piece-wise linear representation; Static gains; Steady state values; System identification","Data mining; Nonlinear dynamical systems; Data segment; Different operating conditions; Gain estimation; Historical data; New approaches; Steady-state condition; Steady-state values; Equations of state; article; computer simulation; linear regression analysis; steady state",Article,Scopus,2-s2.0-85102835351
"Yu W., Zhang Y., Chen Z., Ai T.","56443033900;57214255854;26322275400;55645790600;","Sparse reconstruction with spatial structures to automatically determine neighbors",2022,"International Journal of Geographical Information Science","36","2",,"338","359",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101641104&doi=10.1080%2f13658816.2021.1885675&partnerID=40&md5=571a23568bde29521b94a8afd6436ea3","Previous research has tended to use a global threshold of proximity to determine neighbors, neglecting spatial heterogeneity. Flexible thresholds implemented by adaptive search radii methods account for either the spatial structures or the non-spatial similarities of objects, but few consider both. By combining the spatial and non-spatial information of objects, we propose a novel approach that can automatically determine the neighbors that are strongly related to the object of interest. We introduce the sparse reconstruction technique from the signal processing domain, which aims to remove trivial relationships in a dataset. We extend the sparse reconstruction model by assuring three principles in spatial data, including retention of the correlation of data in the non-spatial attribute domain, preservation of local dependencies in the spatial domain, and removal of trivial relationships. Extensive experiments, based on road network missing value imputation and building clustering, show that our approach can make better use of both spatial and non-spatial information than a simple addition of them. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","sparse coding; spatial data mining; Spatial database; spatial inference","data mining; database; GIS; reconstruction; signal processing; spatial data",Article,Scopus,2-s2.0-85101641104
"Dallagassa M.R., dos Santos Garcia C., Scalabrin E.E., Ioshii S.O., Carvalho D.R.","57208618288;57222004338;6506948841;6601997686;55887138200;","Opportunities and challenges for applying process mining in healthcare: a systematic mapping study",2022,"Journal of Ambient Intelligence and Humanized Computing","13","1",,"165","182",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100927134&doi=10.1007%2fs12652-021-02894-7&partnerID=40&md5=ba0c9854ec522858690f47cd8db6255d","Process mining applies robust methodologies using data mining and machine learning for pattern recognition, using models that represent the process flow identified by the sequence of events, their timing, and the assessment of resources used. To evaluate the use of process mining in health care, with emphasis on the identification of characteristics, health care studies were selected based on a systematic review of the literature, well-defined eligibility criteria, and guided research questions. Such questions address the strategy and algorithm adopted, the location used, and the main contributions for the identified application. A total of 270 articles were selected. Among the identified applications, the discovery of process models was the most frequent, followed by resource analysis and evaluation. The most adopted algorithms were identified, the Fuzzy Miner and Heuristic Miner. One may highlight, among the main contributions, the analysis and discovery of process models for the evaluation of patient care and the evaluation of process conformity, focused on medical protocols and clinical guidelines. This review highlighted the significant use of process models discovery in their evaluation, thus supporting the proposal of changing the health care model so that it favors resources evaluation and care quality. There is also an important challenge regarding the use of such technique; on the one hand, concerning data integration and a more automatic recognition of standards and, on the other hand, concerning the application of standards focused on needs for compliance evaluation between discovered models, medical protocols and clinical guidelines. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.","Artificial intelligence; Health care; Knowledge discovery; Process mining","Data integration; Data mining; Health care; Heuristic algorithms; Miners; Pattern recognition; Regulatory compliance; Application of standards; Automatic recognition; Clinical guideline; Compliance evaluations; Eligibility criterion; Resources evaluation; Sequence of events; Systematic mapping studies; Quality control",Article,Scopus,2-s2.0-85100927134
"Zhu L., Fan H., Luo Y., Xu M., Yang Y.","57149022100;57200617264;55766236300;55703591000;56159216600;","Temporal Cross-Layer Correlation Mining for Action Recognition",2022,"IEEE Transactions on Multimedia","24",,,"668","676",,4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100847683&doi=10.1109%2fTMM.2021.3057503&partnerID=40&md5=d5c60cab640575375678153edcde96e9","Neighboring frames are more correlated compared to frames from further temporal distances. In this paper, we aim to explore the temporal correlations among neighboring frames and exploit cross-layer multi-scale features for action recognition. First, we present a Temporal Cross-Layer Correlation (TCLC) framework for temporal correlation learning. The unified framework uncovers both local and global structures from video data, enabling a better exploration of temporal context and assisting cross-layer spatio-temporal feature learning. Second, we propose a novel cross-layer attention and a center-guided attention mechanism to integrate features with contextual knowledge from multiple scales. Our method is a two-stage process for effective cross-layer feature learning. The first stage incorporates the cross-layer attention module to decide the importance weight of the convolutional layers. The second stage leverages the center-guided attention mechanism to aggregate local features from each layer for the generation of a final video representation. We leverage global centers to extract shared semantic knowledge among videos. We evaluate TCLC on three action recognition datasets, i.e., UCF-101, HMDB-51 and Kinetics. Our experimental results demonstrate the superiority of our proposed temporal correlation mining method. © 1999-2012 IEEE.","action recognition; Deep learning; frame correlation mining; video classification; video feature learning","Aggregates; Knowledge management; Mining; Semantics; Action recognition; Attention mechanisms; Contextual knowledge; Discriminative ability; Frame correlations; Multi-scale features; Temporal correlations; Video representations; Data mining",Article,Scopus,2-s2.0-85100847683
"Kshirsagar D., Kumar S.","46161349100;57218539729;","A feature reduction based reflected and exploited DDoS attacks detection system",2022,"Journal of Ambient Intelligence and Humanized Computing","13","1",,"393","405",,5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099925202&doi=10.1007%2fs12652-021-02907-5&partnerID=40&md5=be37ebacf3db75fb0021a11673f7c5fc","The hacker attempts distributed denial of service (DDoS) attacks towards network resources to disturb or deny services. The hacker degrades the quality of service to legitimate users by performing reflection and exploitation based DDoS attacks with a trusted third party server that hides information of the attacker. It is, therefore, necessary to propose an intelligent intrusion detection system to detect reflection and exploitation based DDoS attacks efficiently and effectively. The present study proposes a feature reduction method by the combination of information gain (IG) and correlation (CR) feature selection techniques. This study presents a DDoS attack detection framework to detect reflection and exploitation based DDoS attacks in an efficient manner. The framework is tested on the latest DDoS evaluation (CICDDoS2019) dataset with J48 classifier. The feature reduction method obtains minimum and maximum reduction by 56 and 82.92% respectively, of the original features. The experimentation results show that the proposed framework outperforms using a reduced features subset. The validation of the proposed framework on knowledge discovery and data mining (KDD Cup 1999) dataset provides improvement in performance for binary and multi-level classification using feature reduction by 60.97% of the original features. The proposed feature reduction method is also compared to the relevant existing feature selection methods used for intrusion detection on CICDoS 2019 and KDD Cup 1999 datasets. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.","Correlation; Distributed denial of service (DDoS); Feature reduction; Information gain; Intrusion detection","Classification (of information); Data mining; Data reduction; Feature extraction; Intrusion detection; Network security; Personal computing; Quality of service; Distributed denial of service attack; Feature reduction; Feature selection methods; Intelligent Intrusion detection systems; Knowledge discovery and data minings; Multi-level classifications; Selection techniques; Trusted third parties; Denial-of-service attack",Article,Scopus,2-s2.0-85099925202
"Nan Y., Feng Z., Li B., Liu E.","57211097158;57211095982;57221601375;7202240258;","Multiscale Fusion Signal Extraction for Spaceborne Photon-Counting Laser Altimeter in Complex and Low Signal-to-Noise Ratio Scenarios",2022,"IEEE Geoscience and Remote Sensing Letters","19",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099583113&doi=10.1109%2fLGRS.2020.3016995&partnerID=40&md5=421d30000c7735f166171b97878efac7","Extracting signal photons from noisy raw data is one of the critical processes for the new generation of spaceborne photon-counting laser altimeter. Affected by vast noise photon-counting events, the extraction of weak signal events still faces challenges in complex scenarios with low signal-to-noise ratio (SNR). Aiming to improve the extraction ability of signal photon events in these scenarios, a multiscale fusion signal extraction method was proposed, characterized by combining global spatial correlation constraint with optimized local spatial correlation constraint. The local constraint is implemented based on a density-based spatial clustering of applications with noise (DBSCAN) clustering method with adaptive parameter estimation, which is used to extract possible signal photons. A subsequent global constraint based on the spatial correlation of the terrain profiles is designed to remove the pseudo-signal photons clustered in the local constraints' step. The global constraint is implemented based on a cost function, which is used to quantify different candidate paths. Our method was verified based on the actual Ice, Cloud, and land Elevation satellite-(ICESat2) data containing vegetation, mountains, and residential areas. The experimental results show that compared with the ICESat-2 extraction method, our method can significantly improve the precision and recall rate of signal photon events from the low SNR photon-counting data. © 2004-2012 IEEE.","and land Elevation satellite-2 (ICESat-2); Cloud; Ice; laser altimeter; low signal-to-noise ratio (SNR); multiscale fusion; photon-counting","Aneroid altimeters; Constrained optimization; Cost functions; Data mining; Extraction; Meteorological instruments; Photons; Radio altimeters; Signal processing; Adaptive parameter estimation; Clustering methods; Density-based spatial clustering of applications with noise; Global constraints; Land elevation satellites; Low signal-to-noise ratio; Precision and recall; Spatial correlations; Signal to noise ratio; algorithm; altimeter; complexity; laser method; signal-to-noise ratio",Article,Scopus,2-s2.0-85099583113
"Zhang H., Zhou X., Tang G., Zhang X., Qin J., Xiong L.","57192483889;56896348800;57196403563;55715721800;57194214800;57203879234;","Detecting Colocation Flow Patterns in the Geographical Interaction Data",2022,"Geographical Analysis","54","1",,"84","103",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099059898&doi=10.1111%2fgean.12274&partnerID=40&md5=29639f99dae940b0396287b1bf563fe5","The detection of colocation pattern is an important and widely used method to analyze the spatial associations of geographical objects and events. Existing studies primarily focus on discovering colocation patterns and association rules based on point data. A broad range of flow data types, such as population flow, logistics, and information flow, have emerged in recent years. However, colocation patterns and association rules based on flow data are difficult to detect because of their complex structure. This work proposes a colocation pattern detection and spatial association rule discovery approach that treats origin-destination (OD) flow as Boolean spatial features, while considering the spatial proximity of the origins and destinations of OD flows and its direction similarity. The effectiveness of this approach is verified by an artificial data set. Finally, this work analyzes the data of tourists who are traveling from different countries or regions to diverse cities in China. It also proves the application value of the proposed approach, which has general applicability to the mining of colocation patterns and association rules from any type of OD flow data. © 2021 The Ohio State University",,"data mining; data set; detection method; spatiotemporal analysis; tourist behavior; tourist destination; travel behavior; China",Article,Scopus,2-s2.0-85099059898
"Aliev R.A., Pedrycz W., Guirimov B.G., Huseynov O.H.","7102561275;56854903200;17134685500;35781871200;","Acquisition of Z-Number-Valued Clusters by Using a New Compound Function",2022,"IEEE Transactions on Fuzzy Systems","30","1",,"279","286",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098779746&doi=10.1109%2fTFUZZ.2020.3037969&partnerID=40&md5=ea815474352c3242842745e84563e92b","A large number of clustering methods exist including deterministic, probabilistic, and fuzzy clustering. All these methods are devoted to handling different types of uncertainty. No studies have been encountered on clustering taking into account a confluence of probabilistic and fuzzy information. In the existing studies, the reliability of extracted knowledge is one of the important issues to be investigated. The concept of Z-number arises as a formal construct that expresses reliability of information under bimodal distribution. In this article, we propose an approach to construction of Z-number-valued clusters of a dataset for evaluation of reliability of extracted data-driven knowledge. Real-world applications are given that confirm the usefulness of the proposed method. © 1993-2012 IEEE.","Clustering; data mining; reliability; Z-number","Reliability; Bimodal distribution; Clustering methods; Data driven; Data set; Fuzzy information; Real-world; Reliability of information; Data mining",Article,Scopus,2-s2.0-85098779746
"Tan A., Shi S., Wu W.-Z., Li J., Pedrycz W.","56402500200;57210435186;34868730000;56222923500;56854903200;","Granularity and Entropy of Intuitionistic Fuzzy Information and Their Applications",2022,"IEEE Transactions on Cybernetics","52","1",,"192","204",,11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097888793&doi=10.1109%2fTCYB.2020.2973379&partnerID=40&md5=c12ae9f245579ff587721163bf774370","A granular structure of intuitionistic fuzzy (IF) information presents simultaneously the similarity and diversity of samples. However, this structural representation has rarely displayed its technical capability in data mining and information processing due to the lack of suitable constructive methods and semantic interpretation for IF information with regard to real data. To pursue better performance of the IF-based technique in real-world data mining, in this article, we examine information granularity, information entropy of IF granular structures, and their applications to data reduction of IF information systems. First, several types of partial-order relations at different hierarchical levels are defined to reveal the granularity of IF granular structures. Second, the granularity invariance between different IF granular structures is characterized by using relational mappings. Third, Shannon's entropies are generalized to IF entropies and their relationships with the partial-order relations are addressed. Based on the theoretical analysis above, the significance of intuitionistic attributes using the information measures is then introduced and the information-preserving algorithm for data reduction of IF information systems is constructed. Finally, by inducing substantial IF relations from public datasets that take both the similarity/diversity between the samples from the same/different classes into account, a collection of numerical experiments is conducted to confirm the performance of the proposed technique. © 2013 IEEE.","Attribute reduction; granular structure; granularity; information entropy; intuitionistic fuzzy (IF) relation; uncertainty measure","Data mining; Data reduction; Information systems; Information use; Semantics; Uncertainty analysis; Attribute reduction; Fuzzy information systems; Granular structuress; Granularity; Information entropy; Intuitionistic fuzzy; Intuitionistic fuzzy informations; Intuitionistic fuzzy relations; Performance; Uncertainty measures; Fuzzy sets; algorithm; data mining; entropy; fuzzy logic; Algorithms; Data Mining; Entropy; Fuzzy Logic",Article,Scopus,2-s2.0-85097888793
"Koh J.X., Liew T.M.","57219939413;56478539800;","How loneliness is talked about in social media during COVID-19 pandemic: Text mining of 4,492 Twitter feeds",2022,"Journal of Psychiatric Research","145",,,"317","324",,13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096156826&doi=10.1016%2fj.jpsychires.2020.11.015&partnerID=40&md5=c68068a338bd06551b0777ec5c47866e","Background: Loneliness is a public health problem that is expected to rise during the COVID-19 pandemic, given the widespread policy of quarantine. The literature is unclear whether loneliness during COVID-19 is similar to those of non-pandemic seasons. This study examined the expression of loneliness on Twitter during COVID-19 pandemic, and identified key areas of loneliness across diverse communities. Methods: Twitter was searched for feeds that were:(1) in English; (2) posted from May 1, 2020 to July 1, 2020; (3) posted by individual users (not organisations); and (4) contained the words ‘loneliness’ and ‘COVID-19’. A machine-learning approach (Topic Modeling) identified key topics from the Twitter feeds; Hierarchical Modeling identified overarching themes. Variations in the prevalence of the themes were examined over time and across the number of followers of the Twitter users. Results: 4492 Twitter feeds were included and classified into 3 themes: (1) Community impact of loneliness during COVID-19; (2) Social distancing during COVID-19 and its effects on loneliness; and (3) Mental health effects of loneliness during COVID-19. The 3 themes demonstrated temporal variations. Particularly in Europe, Theme 1 showed a drastic reduction over time, with a corresponding rise in Theme 3. The themes also varied across number of followers. Highly influential users were more likely to talk about Theme 3 and less about Theme 2. Conclusions: The findings reflect close-to-real-time public sentiments on loneliness during the COVID-19 pandemic and demonstrated the potential usefulness of social media to keep tabs on evolving mental health issues. It also provides inspiration for potential interventions to address novel problems–such as loneliness–during COVID-19 pandemic. © 2020","COVID-19; Loneliness; Mental health; Natural Language Processing; Social media; Topic modeling; Twitter","Africa; anxiety disorder; Article; Asia; Australia; controlled study; coping behavior; coronavirus disease 2019; data mining; depression; Europe; geographic distribution; human; loneliness; machine learning; major clinical study; mental health; natural language processing; North America; pandemic; prevalence; social distancing; social interaction; social isolation; social media; South America; thematic analysis; data mining; loneliness; pandemic; COVID-19; Data Mining; Humans; Loneliness; Pandemics; SARS-CoV-2; Social Media",Article,Scopus,2-s2.0-85096156826
"Miao P., Yokota H., Zhang Y.","57194454104;57210456549;57212883210;","Extracting procedures of key data from a structural maintenance database",2022,"Structure and Infrastructure Engineering","18","2",,"219","229",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094807673&doi=10.1080%2f15732479.2020.1838561&partnerID=40&md5=7aa6d67e6adb2017d07f477677b08c95","Maintenance continues for structure’s life cycle, which usually costs a lot. Inspection and/or monitoring are widely implemented to investigate the conditions of the structures. Existing databases are sometimes referred to for understanding the performance of structure with inspection/monitoring data. Since structural performance is related to various irregularly time-shifted factors, it is complicated to analyze a database efficiently. To improve the situation, the key data selection (KDS) method was proposed to extract key data from a database in this paper. The KDS method is first introduced in the context of conventional maintenance procedure. Afterwards, the detailed principle, implementation procedure, and possible applications of the KDS method are explained. Verification is performed with the data from the natural vibration of a bridge girder produced by vehicles (normal conditions) and from the lateral displacement of a bridge girder during a typhoon (accidental conditions). The results showed that the method can lessen the amount of data without changing their tendency and keep the error within an acceptable range. As a result, the KDS method is simple, reliable, and capable of reducing data under normal and accidental conditions. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","data extraction; Database; inspection; key data selection method; maintenance; monitoring; structures","Accidents; Database systems; Highway bridges; Life cycle; Maintenance; Plate girder bridges; Vibrations (mechanical); Bridge girder; Data Selection; Lateral displacements; Maintenance procedures; Natural vibration; Normal condition; Structural maintenance; Structural performance; Data mining",Article,Scopus,2-s2.0-85094807673
"Yang L., Fan W., Bouguila N.","57218353175;46460901900;6603545988;","Clustering Analysis via Deep Generative Models with Mixture Models",2022,"IEEE Transactions on Neural Networks and Learning Systems","33","1",,"340","350",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092909172&doi=10.1109%2fTNNLS.2020.3027761&partnerID=40&md5=5cda5d192ead256a20a8179d329ae537","Clustering is a fundamental problem that frequently arises in many fields, such as pattern recognition, data mining, and machine learning. Although various clustering algorithms have been developed in the past, traditional clustering algorithms with shallow structures cannot excavate the interdependence of complex data features in latent space. Recently, deep generative models, such as autoencoder (AE), variational AE (VAE), and generative adversarial network (GAN), have achieved remarkable success in many unsupervised applications thanks to their capabilities for learning promising latent representations from original data. In this work, first we propose a novel clustering approach based on both Wasserstein GAN with gradient penalty (WGAN-GP) and VAE with a Gaussian mixture prior. By combining the WGAN-GP with VAE, the generator of WGAN-GP is formulated by drawing samples from the probabilistic decoder of VAE. Moreover, to provide more robust clustering and generation performance when outliers are encountered in data, a variant of the proposed deep generative model is developed based on a Student's-T mixture prior. The effectiveness of our deep generative models is validated though experiments on both clustering analysis and samples generation. Through the comparison with other state-of-Art clustering approaches based on deep generative models, the proposed approach can provide more stable training of the model, improve the accuracy of clustering, and generate realistic samples. © 2012 IEEE.","Clustering; generative adversarial network (GAN); mixture models; student's-T mixture model; variational autoencoder (AE); variational inference; Wasserstein GAN","Data mining; Learning systems; Pattern recognition; Adversarial networks; Clustering analysis; Clustering approach; Gaussian mixtures; Novel clustering; Robust clustering; Shallow structure; Traditional clustering; Clustering algorithms; article; autoencoder; controlled study; drawing; human; human experiment; punishment",Article,Scopus,2-s2.0-85092909172
"Maroli N., Bhasuran B., Natarajan J., Kolandaivel P.","57195554584;57191285268;55372581000;7004716894;","The potential role of procyanidin as a therapeutic agent against SARS-CoV-2: a text mining, molecular docking and molecular dynamics simulation approach",2022,"Journal of Biomolecular Structure and Dynamics","40","3",,"1230","1245",,12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091287548&doi=10.1080%2f07391102.2020.1823887&partnerID=40&md5=5a1dd6832cdc784586dd97e843ac044b","A novel coronavirus (SARS-CoV-2) has caused a major outbreak in human all over the world. There are several proteins interplay during the entry and replication of this virus in human. Here, we have used text mining and named entity recognition method to identify co-occurrence of the important COVID 19 genes/proteins in the interaction network based on the frequency of the interaction. Network analysis revealed a set of genes/proteins, highly dense genes/protein clusters and sub-networks of Angiotensin-converting enzyme 2 (ACE2), Helicase, spike (S) protein (trimeric), membrane (M) protein, envelop (E) protein, and the nucleocapsid (N) protein. The isolated proteins are screened against procyanidin-a flavonoid from plants using molecular docking. Further, molecular dynamics simulation of critical proteins such as ACE2, Mpro and spike proteins are performed to elucidate the inhibition mechanism. The strong network of hydrogen bonds and hydrophobic interactions along with van der Waals interactions inhibit receptors, which are essential to the entry and replication of the SARS-CoV-2. The binding energy which largely arises from van der Waals interactions is calculated (ACE2=-50.21 ± 6.3, Mpro=-89.50 ± 6.32 and spike=-23.06 ± 4.39) through molecular mechanics Poisson-Boltzmann surface area also confirm the affinity of procyanidin towards the critical receptors. Communicated by Ramaswamy H. Sarma. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","covid; molecular docking; molecular dynamics simulation; procyanidin; Text mining","angiotensin converting enzyme 2; coronavirus envelope protein; coronavirus nucleocapsid protein; coronavirus spike glycoprotein; helicase; membrane protein; procyanidin; coronavirus spike glycoprotein; proanthocyanidin; protein binding; Article; coronavirus disease 2019; drug protein binding; drug structure; enzyme inhibition; gene cluster; gene interaction; human; hydrogen bond; hydrophobicity; molecular docking; molecular dynamics; network analysis; protein interaction; protein isolation; Severe acute respiratory syndrome coronavirus 2; surface area; virus entry; virus replication; data mining; metabolism; COVID-19; Data Mining; Humans; Molecular Docking Simulation; Molecular Dynamics Simulation; Proanthocyanidins; Protein Binding; SARS-CoV-2; Spike Glycoprotein, Coronavirus",Article,Scopus,2-s2.0-85091287548
"Zhang C., Tian G., Fathollahi-Fard A.M., Wang W., Wu P., Li Z.","57208554904;36619290600;57196414092;57207967312;55816858600;55588010000;","Interval-Valued Intuitionistic Uncertain Linguistic Cloud Petri Net and Its Application to Risk Assessment for Subway Fire Accident",2022,"IEEE Transactions on Automation Science and Engineering","19","1",,"163","177",,17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090440863&doi=10.1109%2fTASE.2020.3014907&partnerID=40&md5=49179dd6328e30516cde359aaf61ddab","This article proposes a risk assessment method based on interval intuitionistic integrated cloud Petri net (IIICPN). The cloud model is widely used in data mining and knowledge discovery, especially in risk assessment problems with linguistic variables. However, the cloud models proposed in the literature do not express interval-valued intuitionistic linguistic satisfactorily, and the reasoning methods based on the cloud models cannot perform risk assessment well. The work in this article includes the definition of IIIC and IIICPN, the method of converting the interval-valued intuitionistic uncertain linguistic numbers into IIIC, and the reasoning method of IIICPN. As proofs, a subway fire accident model is adopted to confirm the feasibility of the proposed method, and comparison experiments between the IIICPN with general fuzzy Petri net and the trapezium cloud model are conducted to verify the superiority of the proposed model. Note to Practitioners-This work deals with the subway fire risk assessment problem. It proposes a cloud model based on interval-valued intuitionistic uncertain linguistic and builds a cloud-based Petri net model. The methods of fire risk assessment use the existing fault trees or aggregation operators to combine all the factors into consideration, but they do not take the interaction of factors. The goal of this work is to assess the risk of subway fire accident of subway, using fuzzy linguistic decision variables. The simulation results indicate that the proposed method is highly effective. The obtained results can help assessors better determine which factors may cause the disaster. © 2004-2012 IEEE.","Cloud model; Fuzzy Petri net (FPN); Interval-valued intuitionistic number; Risk Assessment; Uncertain linguistic","Accidents; Cloud computing; Data mining; Fuzzy neural networks; Linguistics; Petri nets; Railroads; Data mining and knowledge discovery; Fuzzy Petri nets; ITS applications; Linguistic variable; Reasoning methods; Risk assessment methods; Trapezium cloud; Uncertain linguistic; Risk assessment",Article,Scopus,2-s2.0-85090440863
"Bhaktikul K., Sharif M.","15622832400;35874506200;","Evaluating changes in flood regime in Canadian watersheds using peaks over threshold approach",2022,"ISH Journal of Hydraulic Engineering","28","S1",,"433","438",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084806061&doi=10.1080%2f09715010.2020.1764873&partnerID=40&md5=e09807d5822ca6beb83db8dceb8fc533","Flood behaviour is likely to alter as a result of the impacts of climate change. This paper investigates the non-stationarity in the flood regime of several Canadian rivers through an analysis of peaks over threshold (POT) data. Identification of thresholds and ensuring the independence of POT events are the two major challenges in the implementation of a POT approach. In the present research, a semi-automatic approach based on the POT package in R–an open-source software environment for statistical computing has been used. A total of 127 hydrometric reference streamflow gauging stations that are reasonably free from human intervention have been considered in the present analysis. This ensured the reliability of the trends identified in the flow and timing measures considered herein. The POT data have been extracted from daily streamflow data for each hydrometric station. Four flow and timing measures, such as (1) duration of POT events, (2) volume of POT events, (3) annual sum of durations in POT events, and (4) annual sum of volumes of POT events, were extracted using the daily flow data. The trends in the flow and timing measures were investigated using the Mann–Kendall nonparametric test, and the significance of trends was evaluated using the bootstrap resampling approach. The results of the analysis clearly indicated a predominance of decreasing trends at a greater number of stations in all the four measures of flow and timings considered. It can be concluded that the present analysis indicates a remarkable change in the flood regime of the Canadian watersheds considered herein. © 2020 Indian Society for Hydraulics.","Canada; change; Climate; peaks; peaks over threshold; threshold","Climate change; Floods; Open source software; Open systems; Stream flow; Watersheds; Bootstrap resampling; Gauging stations; Human intervention; Hydrometric stations; Non-parametric test; Non-stationarities; Peaks over threshold; Statistical computing; Data mining",Article,Scopus,2-s2.0-85084806061
"Zou W., Sun Y., Zhou Y., Lu Q., Nie Y., Sun T., Peng L.","57211032781;57205544752;55556256200;57215366021;57215353765;56605729800;56156413200;","Limited Sensing and Deep Data Mining: A New Exploration of Developing City-Wide Parking Guidance Systems",2022,"IEEE Intelligent Transportation Systems Magazine","14","1",,"198","215",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083010911&doi=10.1109%2fMITS.2020.2970185&partnerID=40&md5=a226607f895d26a85078f6f0a1beb071","With the increasing automobiles in China, the parking difficulty has gradually spread over the city. Developing city-wide parking guidance systems (CPGS) has become urgent to many local governments of China nowadays. However, there haven't been any effective systems available yet, because the economic investments on sensors and the time costs of negotiation with proprietors of parking lots are unaffordable from city-wide perspective. The lack of parking data is the most critical problem in development of CPGS. In this paper, we propose a new lightweight technical solution to implement CPGS by strengthening data mining and utilization under the current conditions. We start with mining the public information of parking lots, to quantify the service capability and pick out some of the most important as the samples. Next, we build a RGAN to generate data for those parking lots without data, patching them based on the samples with the similar surroundings. The technology of data generation will help us rebuild the city-wide parking data at low costs, both time and money. Finally, we design a model of recommendation based on the real-time driving data to implement intelligent parking guidance, featuring active recommendation and automatic adjustment with vehicle moving. The solution makes the best use of the existing data while reducing the reliance on sensors significantly. And we develop an experimental system in Shenzhen, achieving the rather satisfying guidance effect in practice. Hence, it's a novel and effective exploration of CPGS, breaking through the dilemma of heavy dependence on sensors. © 2009-2012 IEEE.",,"Remote control; 'current; Condition; Critical problems; Developing cities; Effective systems; Local government; Parking guidance systems; Parking lots; Technical solutions; Time cost; Data mining",Article,Scopus,2-s2.0-85083010911
"Sarıyer G., Ataman M.G.","57189867008;57192943136;","The likelihood of requiring a diagnostic test: Classifying emergency department patients with logistic regression",2022,"Health Information Management Journal","51","1",,"13","22",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082932687&doi=10.1177%2f1833358320908975&partnerID=40&md5=71493f02ae36a28f2c3422e3c05ba751","Background: Emergency departments (EDs) play an important role in health systems since they are the front line for patients with emergency medical conditions who frequently require diagnostic tests and timely treatment. Objective: To improve decision-making and accelerate processes in EDs, this study proposes predictive models for classifying patients according to whether or not they are likely to require a diagnostic test based on referral diagnosis, age, gender, triage category and type of arrival. Method: Retrospective data were categorised into four output patient groups: not requiring any diagnostic test (group A); requiring a radiology test (group B); requiring a laboratory test (group C); requiring both tests (group D). Multivariable logistic regression models were used, with the outcome classifications represented as a series of binary variables: test (1) or no test (0); in the case of group A, no test (1) or test (0). Results: For all models, age, triage category, type of arrival and referral diagnosis were significant predictors whereas gender was not. The main referral diagnosis with high model coefficients varied by designed output groups (groups A, B, C and D). The overall accuracies of the logistic regression models for groups A, B, C and D were, respectively, 74.11%, 73.07%, 82.47% and 85.79%. Specificity metrics were higher than the sensitivities for groups B, C and D, meaning that these models were better able to predict negative outcomes. Implications: These results provide guidance for ED triage staff, researchers and practitioners in making rapid decisions regarding patients’ diagnostic test requirements based on specified variables in the predictive models. This is critical in ED operations planning as it potentially decreases waiting times, while increasing patient satisfaction and operational performance. © The Author(s) 2022.","algorithms; classification techniques; data analysis; data mining; diagnostic test; electronic medical records; emergency department; health information management; logistic regression; referral diagnosis","adult; algorithm; article; case report; clinical article; data analysis; data mining; decision making; electronic medical record; emergency health service; emergency ward; female; gender; human; laboratory test; male; medical information system; outcome assessment; patient referral; patient satisfaction; radiology; retrospective study; sensitivity and specificity; emergency health service; hospital emergency service; statistical model; Emergency Service, Hospital; Humans; Logistic Models; Retrospective Studies; Triage",Article,Scopus,2-s2.0-85082932687
"Su Y., Han L., Wang H., Wang J.","57212318159;57204731358;57214057136;57212313261;","The workshop scheduling problems based on data mining and particle swarm optimisation algorithm in machine learning areas",2022,"Enterprise Information Systems","16","2",,"363","378",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076432021&doi=10.1080%2f17517575.2019.1700551&partnerID=40&md5=8b202f9c05d7710a0e37862a9e77abe6","The optimisation process and results are classified and stored to guide the future workshop scheduling and improve the retrieval efficiency. The results show that the random inertia weight strategy is added to the standard particle swarm optimisation (PSO) algorithm. The idea of crossover and mutation in genetic algorithm (GA) is introduced to increase the diversity of population and prevent it from falling into local optimal solution. Finally, the global optimal solution can be searched by using the strong ability of genetic algorithm to jump out of local optimal to ensure that population evolution is stagnated. © 2019 Informa UK Limited, trading as Taylor & Francis Group.","genetic algorithm; Job shop scheduling; machine learning; particle swarm optimisation","Data mining; Genetic algorithms; Job shop scheduling; Learning algorithms; Learning systems; Machine learning; Optimal systems; Scheduling; Swarm intelligence; Crossover and mutation; Diversity of populations; Global optimal solutions; Local optimal solution; Particle swarm optimisation; Particle swarm optimisation algorithms; Population evolution; Retrieval efficiency; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-85076432021
"Farag A., Abdelkader H., Salem R.","57211231968;55000601900;57023267400;","Parallel graph-based anomaly detection technique for sequential data",2022,"Journal of King Saud University - Computer and Information Sciences","34","1",,"1446","1454",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073067995&doi=10.1016%2fj.jksuci.2019.09.009&partnerID=40&md5=1ab36a3b9a6dd00f9e3d0fc73107f6dc","In data mining, outlier detection is applied in different domains. It has very large applications such as energy consumption analysis, forecasting hurricanes in meteorological data, fraud and intrusion detection, event detection and system monitoring in sensor networks, etc. Most of existing outlier detection techniques depend on the properties of a particular type of data and can not deal with a large volume of data well, which mean that there is a necessity for improved methodologies and techniques to be applied to a large amount of data with different types in other application areas. In this paper, a parallel outlier detection technique is developed to detect the outliers in the sequential data. Although there are many types of outliers, this paper concentrates on the contextual anomalies. The proposed technique uses a graph approach to detect the outliers. It is very flexible, fast, and no labeled data is needed comparing to many previous approaches. The experimental results show the detected contextual outliers in the sequential data, as well as the efficient scaling up to handle the massive data by increasing the number of processors. The results prove that the parallelism of the proposed technique is very valuable. © 2019 The Authors","Data mining; Outlier detection; Parallel graph-based algorithm; Region outliers",,Article,Scopus,2-s2.0-85073067995
