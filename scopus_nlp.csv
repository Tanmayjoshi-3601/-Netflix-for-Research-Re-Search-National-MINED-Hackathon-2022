Authors,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,Link,Abstract,Author Keywords,Index Keywords,Document Type,Source,EID
"Jin Y., Yang Y.","57223046312;56151669300;","ProtPlat: an efficient pre-training platform for protein classification based on FastText",2022,"BMC Bioinformatics","23","1","66","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124578474&doi=10.1186%2fs12859-022-04604-2&partnerID=40&md5=aefca152c2b6d54c5092f8b447606491","Background: For the past decades, benefitting from the rapid growth of protein sequence data in public databases, a lot of machine learning methods have been developed to predict physicochemical properties or functions of proteins using amino acid sequence features. However, the prediction performance often suffers from the lack of labeled data. In recent years, pre-training methods have been widely studied to address the small-sample issue in computer vision and natural language processing fields, while specific pre-training techniques for protein sequences are few. Results: In this paper, we propose a pre-training platform for representing protein sequences, called ProtPlat, which uses the Pfam database to train a three-layer neural network, and then uses specific training data from downstream tasks to fine-tune the model. ProtPlat can learn good representations for amino acids, and at the same time achieve efficient classification. We conduct experiments on three protein classification tasks, including the identification of type III secreted effectors, the prediction of subcellular localization, and the recognition of signal peptides. The experimental results show that the pre-training can enhance model performance effectively and ProtPlat is competitive to the state-of-the-art predictors, especially for small datasets. We implement the ProtPlat platform as a web service (https://compbio.sjtu.edu.cn/protplat) that is accessible to the public. Conclusions: To enhance the feature representation of protein amino acid sequences and improve the performance of sequence-based classification tasks, we develop ProtPlat, a general platform for the pre-training of protein sequences, which is featured by a large-scale supervised training based on Pfam database and an efficient learning model, FastText. The experimental results of three downstream classification tasks demonstrate the efficacy of ProtPlat. © 2022, The Author(s).","Pre-training; Protein sequence classification; ProtPlat; Web server","Amino acids; Classification (of information); Database systems; Forecasting; HTTP; Learning algorithms; Learning systems; Natural language processing systems; Network layers; Physicochemical properties; Web services; Amino acid sequence; Classification tasks; Down-stream; Pre-training; Protein Classification; Protein sequence classification; Protein sequences; Protplat; Training platform; Web servers; Proteins; protein; amino acid sequence; machine learning; natural language processing; Amino Acid Sequence; Machine Learning; Natural Language Processing; Neural Networks, Computer; Proteins",Article,Scopus,2-s2.0-85124578474
"Pressat-Laffouilhère T., Balayé P., Dahamna B., Lelong R., Billey K., Darmoni S.J., Grosjean J.","57206664920;57209643532;55931827000;56724139200;57217245257;55404507100;45161346600;","Evaluation of Doc’EDS: a French semantic search tool to query health documents from a clinical data warehouse",2022,"BMC Medical Informatics and Decision Making","22","1","34","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124323777&doi=10.1186%2fs12911-022-01762-4&partnerID=40&md5=f061018ceaf941da201dd19fb28ce285","Background: Unstructured data from electronic health records represent a wealth of information. Doc’EDS is a pre-screening tool based on textual and semantic analysis. The Doc’EDS system provides a graphic user interface to search documents in French. The aim of this study was to present the Doc’EDS tool and to provide a formal evaluation of its semantic features. Methods: Doc’EDS is a search tool built on top of the clinical data warehouse developed at Rouen University Hospital. This tool is a multilevel search engine combining structured and unstructured data. It also provides basic analytical features and semantic utilities. A formal evaluation was conducted to measure the impact of Natural Language Processing algorithms. Results: Approximately 18.1 million narrative documents are stored in Doc’EDS. The formal evaluation was conducted in 5000 clinical concepts that were manually collected. The F-measures of negative concepts and hypothetical concepts were respectively 0.89 and 0.57. Conclusion: In this formal evaluation, we have shown that Doc’EDS is able to deal with language subtleties to enhance an advanced full text search in French health documents. The Doc’EDS tool is currently used on a daily basis to help researchers to identify patient cohorts thanks to unstructured data. © 2022, The Author(s).","Clinical data warehouse; Cohort identification; Electronic health record; Information retrieval; Semantics","adult; algorithm; article; cohort analysis; data warehouse; electronic health record; female; human; human experiment; information retrieval; male; narrative; natural language processing; search engine; semantics; university hospital",Article,Scopus,2-s2.0-85124323777
"You Y., Jiang J., Jiang Z., Yang P., Liu B., Feng H., Wang X., Li N.","57439994500;57221178709;57202645862;57219603466;8444144500;57439651200;9239023700;56424359800;","TIM: threat context-enhanced TTP intelligence mining on unstructured threat data",2022,"Cybersecurity","5","1","3","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124044077&doi=10.1186%2fs42400-021-00106-5&partnerID=40&md5=fb087c5640cac29d9282ea3d09553ef2","TTPs (Tactics, Techniques, and Procedures), which represent an attacker’s goals and methods, are the long period and essential feature of the attacker. Defenders can use TTP intelligence to perform the penetration test and compensate for defense deficiency. However, most TTP intelligence is described in unstructured threat data, such as APT analysis reports. Manually converting natural language TTPs descriptions to standard TTP names, such as ATT&CK TTP names and IDs, is time-consuming and requires deep expertise. In this paper, we define the TTP classification task as a sentence classification task. We annotate a new sentence-level TTP dataset with 6 categories and 6061 TTP descriptions from 10761 security analysis reports. We construct a threat context-enhanced TTP intelligence mining (TIM) framework to mine TTP intelligence from unstructured threat data. The TIM framework uses TCENet (Threat Context Enhanced Network) to find and classify TTP descriptions, which we define as three continuous sentences, from textual data. Meanwhile, we use the element features of TTP in the descriptions to enhance the TTPs classification accuracy of TCENet. The evaluation result shows that the average classification accuracy of our proposed method on the 6 TTP categories reaches 0.941. The evaluation results also show that adding TTP element features can improve our classification accuracy compared to using only text features. TCENet also achieved the best results compared to the previous document-level TTP classification works and other popular text classification methods, even in the case of few-shot training samples. Finally, the TIM framework organizes TTP descriptions and TTP elements into STIX 2.1 format as final TTP intelligence for sharing the long-period and essential attack behavior characteristics of attackers. In addition, we transform TTP intelligence into sigma detection rules for attack behavior detection. Such TTP intelligence and rules can help defenders deploy long-term effective threat detection and perform more realistic attack simulations to strengthen defense. © 2022, The Author(s).","Advanced persistent threat (APT); Natural language processing (NLP); Threat intelligence; TTPs","Classification (of information); Information retrieval systems; Network security; Text processing; Advanced persistent threat; Attack behavior; Classification accuracy; Classification tasks; Essential features; Evaluation results; Natural language processing; Penetration test; Tactic, technique, and procedure; Threat intelligence; Natural language processing systems",Article,Scopus,2-s2.0-85124044077
"Pan X., Huang P., Li S., Cui L.","56155626800;57439435000;57439953400;57199474030;","MCRWR: a new method to measure the similarity of documents based on semantic network",2022,"BMC Bioinformatics","23","1","56","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124017916&doi=10.1186%2fs12859-022-04578-1&partnerID=40&md5=003bffcd53ee5f1966efb851ae8d667d","Background: Besides Boolean retrieval with medical subject headings (MeSH), PubMed provides users with an alternative way called “Related Articles” to access and collect relevant documents based on semantic similarity. To explore the functionality more efficiently and more accurately, we proposed an improved algorithm by measuring the semantic similarity of PubMed citations based on the MeSH-concept network model. Results: Three article similarity networks are obtained using MeSH-concept random walk with restart (MCRWR), MeSH random walk with restart (MRWR) and PubMed related article (PMRA) respectively. The area under receiver operating characteristic (ROC) curve of MCRWR, MRWR and PMRA is 0.93, 0.90, and 0.67 respectively. Precisions of MCRWR and MRWR under various similarity thresholds are higher than that of PMRA. Mean value of P5 of MCRWR is 0.742, which is much higher than those of MRWR (0.692) and PMRA (0.223). In the article semantic similarity network of “Genes & Function of organ & Disease” based on MCRWR algorithm, four topics are identified according to golden standards. Conclusion: MeSH-concept random walk with restart algorithm has better performance in constructing article semantic similarity network, which can reveal the implicitly semantic association between documents. The efficiency and accuracy of retrieving semantic-related documents have been improved a lot. © 2022, The Author(s).","Medical subject headings; Network analysis; Random walk with restart algorithm; Semantic similarity network","Mesh generation; Random processes; Semantic Web; Document-based; Improved * algorithm; Medical subject headings; Random walk with restart; Random walk with restart algorithm; Relevant documents; Semantic similarity; Semantic similarity network; Semantics networks; Similarity network; Semantics; algorithm; Medical Subject Headings; Medline; semantic web; semantics; Algorithms; Medical Subject Headings; PubMed; Semantic Web; Semantics",Article,Scopus,2-s2.0-85124017916
"Garland J., Ghazi-Zahedi K., Young J.-G., Hébert-Dufresne L., Galesic M.","56581387600;56896147400;55796127900;36865979800;35606385600;","Impact and dynamics of hate and counter speech online",2022,"EPJ Data Science","11","1","3","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123624342&doi=10.1140%2fepjds%2fs13688-021-00314-6&partnerID=40&md5=923b70abc1720b642dd2c620d23aef7f","Citizen-generated counter speech is a promising way to fight hate speech and promote peaceful, non-polarized discourse. However, there is a lack of large-scale longitudinal studies of its effectiveness for reducing hate speech. To this end, we perform an exploratory analysis of the effectiveness of counter speech using several different macro- and micro-level measures to analyze 180,000 political conversations that took place on German Twitter over four years. We report on the dynamic interactions of hate and counter speech over time and provide insights into whether, as in ‘classic’ bullying situations, organized efforts are more effective than independent individuals in steering online discourse. Taken together, our results build a multifaceted picture of the dynamics of hate and counter speech online. While we make no causal claims due to the complexity of discourse dynamics, our findings suggest that organized hate speech is associated with changes in public discourse and that counter speech—especially when organized—may help curb hateful rhetoric in online discourse. © 2022, The Author(s).","Counter speech; Hate speech; Natural language processing; Time-series analysis; Twitter","Dynamics; Natural language processing systems; Speech; Time series analysis; Counter speech; Dynamic interaction; Exploratory analysis; Hate speech; Large-scales; Longitudinal study; Micro level; Social networking (online)",Article,Scopus,2-s2.0-85123624342
"Wang W., Jiang X., Tian S., Liu P., Dang D., Su Y., Lookman T., Xie J.","57419420500;57189508121;57420467600;57216910377;57420467700;13408937200;7003587333;7402994953;","Automated pipeline for superalloy data by text mining",2022,"npj Computational Materials","8","1","9","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123212834&doi=10.1038%2fs41524-021-00687-2&partnerID=40&md5=dded4be4ba764e6011738c84e5daab7f","Data provides a foundation for machine learning, which has accelerated data-driven materials design. The scientific literature contains a large amount of high-quality, reliable data, and automatically extracting data from the literature continues to be a challenge. We propose a natural language processing pipeline to capture both chemical composition and property data that allows analysis and prediction of superalloys. Within 3 h, 2531 records with both composition and property are extracted from 14,425 articles, covering γ′ solvus temperature, density, solidus, and liquidus temperatures. A data-driven model for γ′ solvus temperature is built to predict unexplored Co-based superalloys with high γ′ solvus temperatures within a relative error of 0.81%. We test the predictions via synthesis and characterization of three alloys. A web-based toolkit as an online open-source platform is provided and expected to serve as the basis for a general method to search for targeted materials using data extracted from the literature. © 2022, The Author(s).",,"Chemical analysis; Cobalt alloys; Data mining; Learning algorithms; Natural language processing systems; Pipelines; Superalloys; Chemical composition data; Data driven; High quality; Large amounts; Materials design; Property; Property data; Scientific literature; Text-mining; γ'solvus temperature; Forecasting",Article,Scopus,2-s2.0-85123212834
"Sharma A., Jain P., Mahgoub A., Zhou Z., Mahadik K., Chaterji S.","57344403300;57223339591;56878717700;57385176900;54583962500;18133715900;","Lerna: transformer architectures for configuring error correction tools for short- and long-read genome sequencing",2022,"BMC Bioinformatics","23","1","25","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122518709&doi=10.1186%2fs12859-021-04547-0&partnerID=40&md5=55efd92e4c4e8b4dc231396ce8ce1189","Background: Sequencing technologies are prone to errors, making error correction (EC) necessary for downstream applications. EC tools need to be manually configured for optimal performance. We find that the optimal parameters (e.g., k-mer size) are both tool- and dataset-dependent. Moreover, evaluating the performance (i.e., Alignment-rate or Gain) of a given tool usually relies on a reference genome, but quality reference genomes are not always available. We introduce Lerna for the automated configuration of k-mer-based EC tools. Lerna first creates a language model (LM) of the uncorrected genomic reads, and then, based on this LM, calculates a metric called the perplexity metric to evaluate the corrected reads for different parameter choices. Next, it finds the one that produces the highest alignment rate without using a reference genome. The fundamental intuition of our approach is that the perplexity metric is inversely correlated with the quality of the assembly after error correction. Therefore, Lerna leverages the perplexity metric for automated tuning of k-mer sizes without needing a reference genome. Results: First, we show that the best k-mer value can vary for different datasets, even for the same EC tool. This motivates our design that automates k-mer size selection without using a reference genome. Second, we show the gains of our LM using its component attention-based transformers. We show the model’s estimation of the perplexity metric before and after error correction. The lower the perplexity after correction, the better the k-mer size. We also show that the alignment rate and assembly quality computed for the corrected reads are strongly negatively correlated with the perplexity, enabling the automated selection of k-mer values for better error correction, and hence, improved assembly quality. We validate our approach on both short and long reads. Additionally, we show that our attention-based models have significant runtime improvement for the entire pipeline—18× faster than previous works, due to parallelizing the attention mechanism and the use of JIT compilation for GPU inferencing. Conclusion: Lerna improves de novo genome assembly by optimizing EC tools. Our code is made available in a public repository at: https://github.com/icanforce/lerna-genomics. © 2021, The Author(s).","Automated configuration tuning; Error correction; Nanopore reads; Natural language processing (NLP); PacBio reads; Parameter search space; Perplexity metric; Transformer networks","Automation; Bioinformatics; Ion beams; Nanopores; Natural language processing systems; Automated configuration; Automated configuration tuning; Errors correction; Language model; Nanopore read; Natural language processing; Pacbio read; Parameter search space; Perplexity metric; Transformer network; Error correction; algorithm; DNA sequence; genomics; high throughput sequencing; nucleotide sequence; software; Algorithms; Base Sequence; Genomics; High-Throughput Nucleotide Sequencing; Sequence Analysis, DNA; Software",Article,Scopus,2-s2.0-85122518709
"Lastra-Díaz J.J., Lara-Clares A., Garcia-Serrano A.","56904281200;57198489345;55930769700;","HESML: a real-time semantic measures library for the biomedical domain with a reproducible survey",2022,"BMC Bioinformatics","23","1","23","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122499743&doi=10.1186%2fs12859-021-04539-0&partnerID=40&md5=b37d4bcb5a825efceed7000b3b721944","Background: Ontology-based semantic similarity measures based on SNOMED-CT, MeSH, and Gene Ontology are being extensively used in many applications in biomedical text mining and genomics respectively, which has encouraged the development of semantic measures libraries based on the aforementioned ontologies. However, current state-of-the-art semantic measures libraries have some performance and scalability drawbacks derived from their ontology representations based on relational databases, or naive in-memory graph representations. Likewise, a recent reproducible survey on word similarity shows that one hybrid IC-based measure which integrates a shortest-path computation sets the state of the art in the family of ontology-based semantic measures. However, the lack of an efficient shortest-path algorithm for their real-time computation prevents both their practical use in any application and the use of any other path-based semantic similarity measure. Results: To bridge the two aforementioned gaps, this work introduces for the first time an updated version of the HESML Java software library especially designed for the biomedical domain, which implements the most efficient and scalable ontology representation reported in the literature, together with a new method for the approximation of the Dijkstra’s algorithm for taxonomies, called Ancestors-based Shortest-Path Length (AncSPL), which allows the real-time computation of any path-based semantic similarity measure. Conclusions: We introduce a set of reproducible benchmarks showing that HESML outperforms by several orders of magnitude the current state-of-the-art libraries in the three aforementioned biomedical ontologies, as well as the real-time performance and approximation quality of the new AncSPL shortest-path algorithm. Likewise, we show that AncSPL linearly scales regarding the dimension of the common ancestor subgraph regardless of the ontology size. Path-based measures based on the new AncSPL algorithm are up to six orders of magnitude faster than their exact implementation in large ontologies like SNOMED-CT and GO. Finally, we provide a detailed reproducibility protocol and dataset as supplementary material to allow the exact replication of all our experiments and results. © 2021, The Author(s).","Gene ontology; HESML; Information content models; MeSH; Ontology-based semantic similarity measures; Semantic measures library; SNOMED-CT; WordNet","Approximation algorithms; Benchmarking; Bioinformatics; Computational efficiency; Computer software; Gene Ontology; Genes; Graph theory; Mesh generation; Semantics; Surveys; Content modeling; Gene ontology; HESML; Information content model; Information contents; MeSH; Ontology-based; Ontology-based semantic similarity measure; Semantic measure library; Semantic measures; Semantic similarity measures; SNOMED-CT; Wordnet; Libraries; article; Dijkstra's algorithm; gene ontology; library; medical ontology; reproducibility; software; Systematized Nomenclature of Medicine; taxonomy; biological ontology; Medical Subject Headings; semantics; Systematized Nomenclature of Medicine; Biological Ontologies; Medical Subject Headings; Reproducibility of Results; Semantics; Systematized Nomenclature of Medicine",Article,Scopus,2-s2.0-85122499743
"Elangovan A., Li Y., Pires D.E.V., Davis M.J., Verspoor K.","56950365000;57192526255;26028060700;35419216300;12772581800;","Large-scale protein-protein post-translational modification extraction with distant supervision and confidence calibrated BioBERT",2022,"BMC Bioinformatics","23","1","4","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122283263&doi=10.1186%2fs12859-021-04504-x&partnerID=40&md5=c6110b010fc432b081ea0b8f506e5557","Motivation: Protein-protein interactions (PPIs) are critical to normal cellular function and are related to many disease pathways. A range of protein functions are mediated and regulated by protein interactions through post-translational modifications (PTM). However, only 4% of PPIs are annotated with PTMs in biological knowledge databases such as IntAct, mainly performed through manual curation, which is neither time- nor cost-effective. Here we aim to facilitate annotation by extracting PPIs along with their pairwise PTM from the literature by using distantly supervised training data using deep learning to aid human curation. Method: We use the IntAct PPI database to create a distant supervised dataset annotated with interacting protein pairs, their corresponding PTM type, and associated abstracts from the PubMed database. We train an ensemble of BioBERT models—dubbed PPI-BioBERT-x10—to improve confidence calibration. We extend the use of ensemble average confidence approach with confidence variation to counteract the effects of class imbalance to extract high confidence predictions. Results and conclusion: The PPI-BioBERT-x10 model evaluated on the test set resulted in a modest F1-micro 41.3 (P =5 8.1, R = 32.1). However, by combining high confidence and low variation to identify high quality predictions, tuning the predictions for precision, we retained 19% of the test predictions with 100% precision. We evaluated PPI-BioBERT-x10 on 18 million PubMed abstracts and extracted 1.6 million (546507 unique PTM-PPI triplets) PTM-PPI predictions, and filter ≈ 5700 (4584 unique) high confidence predictions. Of the 5700, human evaluation on a small randomly sampled subset shows that the precision drops to 33.7% despite confidence calibration and highlights the challenges of generalisability beyond the test set even with confidence calibration. We circumvent the problem by only including predictions associated with multiple papers, improving the precision to 58.8%. In this work, we highlight the benefits and challenges of deep learning-based text mining in practice, and the need for increased emphasis on confidence calibration to facilitate human curation efforts. © 2021, The Author(s).","BioBERT; Deep learning; Distant supervision; Natural language processing; Post-translational modifications; Protein-protein interaction","Abstracting; Cost effectiveness; Deep learning; Filtration; Forecasting; Natural language processing systems; Proteins; BioBERT; Confidence predictions; Deep learning; Distant supervision; High confidence; Human curation; Large scale proteins; Post-translational modifications; Protein-protein interactions; Test sets; Calibration; protein; data mining; human; Medline; protein processing; Data Mining; Humans; Protein Processing, Post-Translational; Proteins; PubMed",Article,Scopus,2-s2.0-85122283263
"Ayadi M.G., Bouslimi R., Akaichi J.","57189028782;57120092800;55961061400;","Medical social networks content mining for a semantic annotation",2022,"Social Network Analysis and Mining","12","1","17","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120952140&doi=10.1007%2fs13278-021-00848-7&partnerID=40&md5=92700922ca51c0cd93754793f62cdf8b","The interactions between subscribers of the health-related social networking (HSNs) platforms rise the production and sharing of a huge amount of multimedia content, daily, by permitting them to upload their medical images. These images become the centre of communication in various multilingual expressions immediately describing observations, comments and health checkups. As a part of this exchange, it is clear that these spaces are a valuable source of subscribers-generated information. Besides, it is still an open question to enable subscribers to investigate relevant information, due to the diversity of the available content. So, it is vital to engage new mechanisms in order to pull out information and acquaintance from this content. For this purpose, we have implemented a content analysis model of health-related information to get an overview of the medical content available. We present a semantic terms-based approach to pull out pertinent terms and concepts from the text material. As a result, notable extracted terms and keywords will be applied, subsequently, to present to annotate medical images, to direct users to an appropriate seeking task, through the SN site. So, the analysis method concentrates on algorithms based on statistical methods and external multilingual semantic resources to cover and treat this situation. It is essential also to deal with such ambiguities causing the efficacy decreasing of the search function. Our study is validated by a set of experiments and compared with some existing models. Experimental results have ensured that the presented model has better findings, in terms of performance and satisfaction. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature.","Health information; Medical social network; MeSh thesaurus; Multilingual mixed approach; Semantic indexation","Medical imaging; MESH networking; Semantic Web; Semantics; Social networking (online); Content mining; Health informations; Medical social network; Mesh thesaurus; Mixed approach; Multilingual mixed approach; Pull-out; Semantic annotations; Semantic indexation; Social-networking; Mesh generation",Article,Scopus,2-s2.0-85120952140
"Prasad R.S., Chakkaravarthy M.","35234918100;57446568500;","State of the Art in Authorship Attribution with Impact Analysis of Stylometric Features on Style Breach Prediction",2022,"Journal of Cases on Information Technology","24","4",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124343862&doi=10.4018%2fJCIT.296716&partnerID=40&md5=596adc6cd1b4536c6f8f3054894f5850","The most influential research was studied that spans the domains from authorship attribution and stylometry. The reference material contributes robust classifiers with reasonable array of feature extraction techniques, such as Dirichlet-multinomial change point regression to extract the progress of inscription elegance with time, comprising plodding variations in stylishness as the author ages and unexpected vicissitudes. This paper presents quantifiable evaluation of the research in terms of year-wise research output, diversity of applications, nature of collaboration, characteristics of highly productive techniques, and the benchmark of performance criteria by eminent high impact researchers. The outcomes of this study can by deployed for dialectology analysis and corpus linguistics, stylistics, natural language processing, classification, literary and historical analysis, and forensic analysis. © 2022 IGI Global. All rights reserved.","Authorship; Linguistic feature; Stylometric","Benchmarking; Classification (of information); Extraction; Linguistics; Authorship; Authorship attribution; Feature extraction techniques; Impact analysis; Linguistic features; Reference material; State of the art; Stylometric features; Stylometrics; Stylometry; Natural language processing systems",Article,Scopus,2-s2.0-85124343862
"Tamine L., Goeuriot L.","55921878300;36145674900;","Semantic Information Retrieval on Medical Texts: Research Challenges, Survey, and Open Issues",2022,"ACM Computing Surveys","54","7","146","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115444382&doi=10.1145%2f3462476&partnerID=40&md5=e150fc5f524c641a223a0f1d6a907ede","The explosive growth and widespread accessibility of medical information on the Internet have led to a surge of research activity in a wide range of scientific communities including health informatics and information retrieval (IR). One of the common concerns of this research, across these disciplines, is how to design either clinical decision support systems or medical search engines capable of providing adequate support for both novices (e.g., patients and their next-of-kin) and experts (e.g., physicians, clinicians) tackling complex tasks (e.g., search for diagnosis, search for a treatment). However, despite the significant multi-disciplinary research advances, current medical search systems exhibit low levels of performance. This survey provides an overview of the state of the art in the disciplines of IR and health informatics, and bridging these disciplines shows how semantic search techniques can facilitate medical IR. First,we will give a broad picture of semantic search and medical IR and then highlight the major scientific challenges. Second, focusing on the semantic gap challenge, we will discuss representative state-of-the-art work related to feature-based as well as semantic-based representation and matching models that support medical search systems. In addition to seminal works, we will present recent works that rely on research advancements in deep learning. Third, we make a thorough cross-model analysis and provide some findings and lessons learned. Finally, we discuss some open issues and possible promising directions for future research trends. © 2021 ACM.","evaluation; Information retrieval; knowledge resources; medical texts; relevance","Clinical research; Decision support systems; Deep learning; Diagnosis; Medical informatics; Patient treatment; Safety devices; Search engines; Semantic Web; Semantics; Surveys; Evaluation; Health informatics; Knowledge resource; Medical information; Medical text; Relevance; Search system; Semantic information retrieval; Semantic search; State of the art; Information retrieval",Article,Scopus,2-s2.0-85115444382
"Gumiel Y.B., Oliveira L.E.S.E., Claveau V., Grabar N., Paraiso E.C., Moro C., Carvalho D.R.","57016767100;57022994800;23088341000;6602326045;56028874700;36717883800;55887138200;","Temporal Relation Extraction in Clinical Texts",2022,"ACM Computing Surveys","54","7","144","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115442922&doi=10.1145%2f3462475&partnerID=40&md5=3ae85faa661ae0581054282264e1be62","Unstructured data in electronic health records, represented by clinical texts, are a vast source of healthcare information because they describe a patient's journey, including clinical findings, procedures, and information about the continuity of care. The publication of several studies on temporal relation extraction from clinical texts during the last decade and the realization of multiple shared tasks highlight the importance of this research theme. Therefore, we propose a review of temporal relation extraction in clinical texts. We analyzed 105 articles and verified that relations between events and document creation time, a coarse temporality type, were addressed with traditional machine learning-based models with few recent initiatives to push the state-of-the-art with deep learning-based models. For temporal relations between entities (event and temporal expressions) in the document, factors such as dataset imbalance because of candidate pair generation and task complexity directly affect the system's performance. The state-of-the-art resides on attention-based models, with contextualized word representations being fine-tuned for temporal relation extraction. However, further experiments and advances in the research topic are required until real-time clinical domain applications are released. Furthermore, most of the publications mainly reside on the same dataset, hindering the need for new annotation projects that provide datasets for different medical specialties, clinical text types, and even languages. © 2021 ACM.","clinical data; natural language processing; Temporal relation extraction","Clinical research; Data handling; Data mining; Deep learning; Extraction; Learning algorithms; Clinical data; Continuity of cares; Learning Based Models; Relation extraction; State of the art; Task complexity; Temporal expressions; Temporal relation; Temporal relation extraction; Unstructured data; Natural language processing systems",Article,Scopus,2-s2.0-85115442922
"Klingvall E., Heinat F.","55115825100;43461526500;","The effects of quantifier size on the construction of discourse models",2022,"Journal of Neurolinguistics","63",,"101066","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125144932&doi=10.1016%2fj.jneuroling.2022.101066&partnerID=40&md5=fe0f5cdd65ab5192c71816c14ba37009","Sentences with quantified expressions involve mental representations of sets of individuals for which some property holds (the reference set), as well as of sets for which the property does not hold (the complement set). Both sets can receive discourse focus with negative quantifiers, while the reference set is strongly preferred with positive quantifiers, complement set focus however being possible if contextually motivated. In an offline semantic plausibility study and two online EEG studies, we investigated whether the complement set is an available discourse entity inherently for positive quantifiers, as it is for negative quantifiers. The results show that while the default focus patterns induced by positive and negative quantifiers are robust, both complement and reference set are represented as discourse entities and this is to our knowledge the first study to show that even positive quantifiers make both reference and complement set mentally represented during discourse processing without contextual influence. We also discuss the impact the results from the two ERP studies have on the functional interpretation of two well known ERP effects: the N400 and the P600. © 2022 The Author(s)","Acceptability study; ERP; P600; Semantics; Sentence processing; Swedish","article; electroencephalogram; semantics",Article,Scopus,2-s2.0-85125144932
"Gao J., Liu X., Chen Y., Xiong F.","55503957000;57385521000;43260919500;57211756597;","MHGCN: Multiview Highway Graph Convolutional Network for Cross-Lingual Entity Alignment",2022,"Tsinghua Science and Technology","27","4",,"719","728",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121748036&doi=10.26599%2fTST.2021.9010056&partnerID=40&md5=25d7e22c58d8d8984d39da2fddd484e7","Knowledge graphs (KGs) provide a wealth of prior knowledge for the research on social networks. Cross-lingual entity alignment aims at integrating complementary KGs from different languages and thus benefits various knowledge-driven social network studies. Recent entity alignment methods often take an embedding-based approach to model the entity and relation embedding of KGs. However, these studies mostly focus on the information of the entity itself and its structural features but ignore the influence of multiple types of data in KGs. In this paper, we propose a new embedding-based framework named multiview highway graph convolutional network (MHGCN), which considers the entity alignment from the views of entity semantic, relation semantic, and entity attribute. To learn the structural features of an entity, the MHGCN employs a highway graph convolutional network (GCN) for entity embedding in each view. In addition, the MHGCN weights and fuses the multiple views according to the importance of the embedding from each view to obtain a better entity embedding. The alignment entities are identified based on the similarity of entity embeddings. The experimental results show that the MHGCN consistently outperforms the state-of-the-art alignment methods. The research also will benefit knowledge fusion through cross-lingual KG entity alignment. © 1996-2012 Tsinghua University Press.","Entity alignment; Graph convolutional network; Knowledge graph","Convolution; Embeddings; Knowledge graph; Semantics; Alignment methods; Convolutional networks; Cross-lingual; Embeddings; Entity alignment; Graph convolutional network; Knowledge graphs; Multi-views; Prior-knowledge; Structural feature; Alignment",Article,Scopus,2-s2.0-85121748036
"Jiang S., Fu S., Lin N., Fu Y.","57385976000;57200513560;57207732160;57220176939;","Pretrained Models and Evaluation Data for the Khmer Language",2022,"Tsinghua Science and Technology","27","4",,"709","718",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121666084&doi=10.26599%2fTST.2021.9010060&partnerID=40&md5=3ac342332088109ccce435453e8ee036","Trained on a large corpus, pretrained models (PTMs) can capture different levels of concepts in context and hence generate universal language representations, which greatly benefit downstream natural language processing (NLP) tasks. In recent years, PTMs have been widely used in most NLP applications, especially for high-resource languages, such as English and Chinese. However, scarce resources have discouraged the progress of PTMs for low-resource languages. Transformer-based PTMs for the Khmer language are presented in this work for the first time. We evaluate our models on two downstream tasks: Part-of-speech tagging and news categorization. The dataset for the latter task is self-constructed. Experiments demonstrate the effectiveness of the Khmer models. In addition, we find that the current Khmer word segmentation technology does not aid performance improvement. We aim to release our models and datasets to the community in hopes of facilitating the future development of Khmer NLP applications. © 1996-2012 Tsinghua University Press.","Khmer language; News categorization; Part-of-speech (POS) tagging; Pretrained models; Word segmentation","Computational linguistics; Down-stream; In contexts; Khmer language; Large corpora; Natural language processing applications; News categorization; Part of speech tagging; Parts-of-speech tagging; Pretrained model; Word segmentation; Natural language processing systems",Article,Scopus,2-s2.0-85121666084
"Yalcin K., Cicekli I., Ercan G.","57463706000;6603079400;18834717000;","An external plagiarism detection system based on part-of-speech (POS) tag n-grams and word embedding",2022,"Expert Systems with Applications","197",,"116677","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125128428&doi=10.1016%2fj.eswa.2022.116677&partnerID=40&md5=51e7f1b62ccbc7ab877089bc42f0d4e4","The aim of this paper is to present an automatic plagiarism detection system to identify plagiarized passages of documents. Our plagiarism detection system uses both syntactic and semantic similarities to identify plagiarized passages. Our proposed method is a novel contribution because of its usage of part-of-speech tag n-grams (POSNG) which are able to show syntactic similarities between source and suspicious sentences. Each source document is indexed according to part-of-speech (POS) tag n-grams by a search engine in order to access rapidly to sentences that are possible plagiarism candidates. Even though our plagiarism detection system obtains very good results just using POS tag n-grams, its performance is further improved with the usage of semantic similarities. The semantic relatedness between words is measured with the word embedding technique called Word2Vec and the longest common subsequence approach is used to measure the semantic similarity between source and suspicious sentences. There are several types of plagiarism such as verbatim, paraphrasing, source-code, and cross-lingual. The high obfuscation paraphrasing is a type of plagiarism and its detection is one of the most difficult plagiarism detection tasks. Our proposed method, which is based on POS tag n-grams, improves the detection performance of the high obfuscation paraphrasing type and is the main contribution of this paper. For this study, we use the large dataset called PAN-PC-11 which is created for the evaluation of automatic plagiarism detection algorithms. Our experiments are conducted with the four types of paraphrasing in PAN-PC-11 which are none, low, high and simulated obfuscation paraphrasing types. We defined various threshold and parameter settings in order to assess the diversity of our results. We compared the performance of our method with the plagiarism detectors in the 3rd International Competition on Plagiarism Detection (PAN11). According to the experimental results, the proposed method achieved the best performance in terms of plagdet measure in the types of high and low obfuscation paraphrasing and produced competitive results in the other paraphrasing types. © 2022 Elsevier Ltd","N-grams; part-of-speech (POS) tagging; Plagiarism detection; Semantic similarity; Word embedding","Competition; Computational linguistics; Intellectual property; Large dataset; Search engines; Semantics; Syntactics; Detection system; Embeddings; N-grams; Part of speech tagging; Part-of-speech tagger; Part-of-speech tags; Parts-of-speech tagging; Plagiarism detection; Semantic similarity; Word embedding; Embeddings",Article,Scopus,2-s2.0-85125128428
"Sharma R., Morwal S., Agarwal B.","57210948935;57192427663;55634231900;","Named entity recognition using neural language model and CRF for Hindi language",2022,"Computer Speech and Language","74",,"101356","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123394781&doi=10.1016%2fj.csl.2022.101356&partnerID=40&md5=f6c4dbacb838c2fdd9970ae077034d94","Named Entity Recognition (NER) plays an important role in various Natural Language Processing (NLP) applications to extract the key information from a huge amount of unstructured text data. NER is a task of identifying and classifying the named entities into predefined categories for a given text. Recently, language models are highly appreciable in several NLP tasks as these state-of-the-art models result better even in resource scarcity. In this paper, we perform NER task on the Hindi language by incorporating the recently released multilingual language model MuRIL which stands for Multilingual Representation for Indian Languages. MuRIL is specially trained for 16 Indian languages. We develop a Hindi NER system using MuRIL with a conditional random field (CRF) layer and fine-tune the model on the ICON 2013 Hindi NER dataset. Further, in the proposed approach, we compute the addition of the last 4 layers representations of the MuRIL model instead of just using the last layer's representation and fine-tune the whole model. Several variants of this model are presented by applying different computations on token representations provided by different layers of 12-layered MuRIL architecture. The proposed model achieves state-of-the-art results as 87.89% precision, 83.74% recall and 85.77% F1-score and outperforms all other existing Hindi NER systems developed on the ICON 2013 dataset. Additionally, we develop a similar Hindi NER system by replacing the MuRIL language model with another state-of-the-art language model, called multilingual Bidirectional Encoder Representations from Transformers (mBERT) to analyze the efficiency of both language models over the Hindi NER task. © 2022 Elsevier Ltd","Language models; Multilingual BERT; MuRIL; Neural network; Sequence labeling; Transfer-learning","Character recognition; Computational linguistics; Learning systems; Natural language processing systems; Indian languages; Language model; Multilingual BERT; MuRIL; Named entity recognition; Neural-networks; Recognition systems; Sequence Labeling; State of the art; Transfer learning; Random processes",Article,Scopus,2-s2.0-85123394781
"AlMousa M., Benlamri R., Khoury R.","57203297062;57203235506;16021635100;","A novel word sense disambiguation approach using WordNet knowledge graph",2022,"Computer Speech and Language","74",,"101337","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122705398&doi=10.1016%2fj.csl.2021.101337&partnerID=40&md5=ac2d90607397c902c5eaef4ae5251b0c","Various applications in computational linguistics and artificial intelligence rely on high-performing word sense disambiguation techniques to solve challenging tasks such as information retrieval, machine translation, question answering, and document clustering. While text comprehension is intuitive for humans, machines face tremendous challenges in processing and interpreting a human's natural language. This paper presents a novel knowledge-based word sense disambiguation algorithm, namely Sequential Contextual Similarity Matrix Multiplication (SCSMM). The SCSMM algorithm combines semantic similarity, heuristic knowledge, and document context to respectively exploit the merits of local sense-based context between consecutive terms, human knowledge about terms, and a document's main topic in disambiguating terms. Unlike other algorithms, the SCSMM algorithm guarantees the capture of the maximum sentence context while maintaining the terms’ order within the sentence. The proposed algorithm outperformed all other algorithms when disambiguating nouns on the combined gold standard datasets, while demonstrating comparable results to current state-of-the-art word sense disambiguation systems when dealing with each dataset separately. Furthermore, the paper discusses the impact of granularity level, ambiguity rate, sentence size, and part of speech distribution on the performance of the proposed algorithm. © 2022 Elsevier Ltd","Knowledge graph; Knowledge-based; Semantic word sense disambiguation; WordNet","Knowledge graph; Natural language processing systems; Ontology; Knowledge based; Knowledge graphs; Machine translations; Matrix multiplication algorithm; Question Answering; Semantic word sense disambiguation; Similarity matrix; Word Sense Disambiguation; Word sense disambiguation techniques; Wordnet; Semantics",Article,Scopus,2-s2.0-85122705398
"Li D., Yan L., Yang J., Ma Z.","54403297800;55227476500;57456937700;13205045700;","Dependency syntax guided BERT-BiLSTM-GAM-CRF for Chinese NER",2022,"Expert Systems with Applications","196",,"116682","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124795925&doi=10.1016%2fj.eswa.2022.116682&partnerID=40&md5=6dc57ef2dffe01b351bcadebd8c4160f","Traditional named entity recognition (NER) methods derived from deep learning (DL) often ignore the long-distance syntactic dependence relationships between words. In such methods, the representation of Chinese word vectors is too simple to solve the problem of polysemy. To address these shortcomings, this paper proposes a syntactic dependency guided BERT-BiLSTM-GAM-CRF model for Chinese NER. First, the self-attention method guided by the dependency syntactic parsing tree is embedded in the transformer computing framework of the BERT model. This can not only obtain the deep two-way linguistic representation of a word according to the context information of the word, but it can also better express the long-distance syntactic dependency relationships between words; Second, the trained word vector sequence is input into the BiLSTM layer embedded in the global attention mechanism (GAM), and then the most important whole situation semantic information in the sentence is obtained. Finally, the CRF is employed to learn the dependence relationships between adjacent labels to obtain the best sentence level label sequence. A large number of experiments on the CLUENER-2020 corpus, MSRA corpus, Weibo corpus and OntoNotes4 corpus prove that the constructed model has good results for the Chinese NER task, and the F1 values are 81.08%, 94.97%, 63.60% and 75.64% respectively. © 2022 Elsevier Ltd","Attention mechanism; BiLSTM; Deep learning; Dependency syntactic parsing; Name entity recognition","Deep learning; Natural language processing systems; Semantics; Attention mechanisms; BiLSTM; Chinese named entity recognition; Deep learning; Dependence relationships; Dependency syntactic parsing; Name entity recognition; Syntactic dependencies; Syntactic parsing; Word vectors; Syntactics",Article,Scopus,2-s2.0-85124795925
"Feldman J., Choi L.-S.","7402824370;57463758200;","Meaning and reference from a probabilistic point of view",2022,"Cognition","223",,"105058","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125129911&doi=10.1016%2fj.cognition.2022.105058&partnerID=40&md5=348d4f1e1bd01e24c9ea7697ea5fa4b5","The rise of Bayesian models of cognition requires that traditional questions in epistemology and metaphysics, such as how models relate to reality and how one observer's models relate to another's, be reframed in probabilistic terms. In this paper we take up these questions beginning from a subjective (Bayesian) conception of probability, in which distinct observers hold potentially different probabilistic models of the world, with no one observer necessarily possessing the “true” one. The key question is what terms in a probabilistic theory mean—that is, what they refer to and what their truth conditions are. We address this question with tools from information theory. We introduce the translation uncertainty, a generalization of the Kullback–Leibler divergence that expresses the discrepancy between two observers’ probabilistic models of a common environment. We derive a number of basic information-theoretic relationships among observers, showing for example that the probability that two Bayesian observers will classify the world similarly (called the concordance) depends on the translation uncertainty between their respective models of the world. Our framework suggests a pathway to a semantics for a “probabilistic language of thought.” © 2022 Elsevier B.V.","Bayesian cognitive science; Information theory; Kullback-Leibler divergence; Meaning; Reference","article; conception; human; human experiment; information science; language; probability; psychology; semantics; theoretical study; uncertainty",Article,Scopus,2-s2.0-85125129911
"Wahid J.A., Shi L., Gao Y., Yang B., Wei L., Tao Y., Hussain S., Ayoub M., Yagoub I.","57212572513;57193762604;57194080412;57200115803;47562085200;23092066600;57222866929;57449989100;57204149069;","Topic2Labels: A framework to annotate and classify the social media data through LDA topics and deep learning models for crisis response",2022,"Expert Systems with Applications","195",,"116562","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124568929&doi=10.1016%2fj.eswa.2022.116562&partnerID=40&md5=3f1a584950c2cca76bd6d21b2c0ca206","The abundant use of social media impacts every aspect of life, including crisis management. Disaster management needs real-time data to be used in machine learning and deep learning models to aid their decision making. Mostly the data that is newly generated from social media is unstructured and unlabeled. Current text classification models based on supervised deep learning models heavily rely on human-labeled data that very small size and imbalanced in the context of disasters, ultimately affecting the generalization of models. In this study, we propose Topic2labels (T2L) framework which provides an automated way of labeling the data through LDA (latent dirichlet allocation) topic modeling approach and utilize Bert (the bidirectional encoder representation from transformer) embeddings for construction of feature vector to be employed to classify the data contextually. Our framework consists of three layers. In the first layer, we adopt LDA to generate the topics from the data, and develop a new algorithm to rank the topics, and map the highest ranked dominant topic into label to annotate the data. In the second layer, we transform the labeled text into feature representation through Bert embeddings and in the third layer we leveraged deep learning models as classifiers to classify the textual data into multiple categories. Experimental results on crisis-related datasets show that our framework performs better in terms of classification performance and yields improvement as compared to other baseline approaches. © 2022 Elsevier Ltd","Annotation; Classification; Crisis response; Natural language processing; Neural network; Social media; Topic modeling; Transformer","Classification (of information); Decision making; Deep learning; Disaster prevention; Disasters; Embeddings; Modeling languages; Natural language processing systems; Neural networks; Statistics; Text processing; Annotation; Crisis response; Embeddings; Latent Dirichlet allocation; Learning models; Neural-networks; Social media; Social media datum; Topic Modeling; Transformer; Social networking (online)",Article,Scopus,2-s2.0-85124568929
"Yang S., Tang Y.","57211496553;57446154700;","News topic detection based on capsule semantic graph",2022,"Big Data Mining and Analytics","5","2",,"98","109",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124321674&doi=10.26599%2fBDMA.2021.9020023&partnerID=40&md5=e554f3b38f48ffb526fff6bf0d8332d2","Most news topic detection methods use word-based methods, which easily ignore the relationship among words and have semantic sparsity, resulting in low topic detection accuracy. In addition, the current mainstream probability methods and graph analysis methods for topic detection have high time complexity. For these reasons, we present a news topic detection model on the basis of capsule semantic graph (CSG). The keywords that appear in each text at the same time are modeled as a keyword graph, which is divided into multiple subgraphs through community detection. Each subgraph contains a group of closely related keywords. The graph is used as the vertex of CSG. The semantic relationship among the vertices is obtained by calculating the similarity of the average word vector of each vertex. At the same time, the news text is clustered using the incremental clustering method, where each text uses CSG; that is, the similarity among texts is calculated by the graph kernel. The relationship between vertices and edges is also considered when calculating the similarity. Experimental results on three standard datasets show that CSG can obtain higher precision, recall, and F1 values than several latest methods. Experimental results on large-scale news datasets reveal that the time complexity of CSG is lower than that of probabilistic methods and other graph analysis methods. © 2018 Tsinghua University Press.","capsule semantic graph; graph kernel; news topic detection","Large dataset; Semantics; Capsule semantic graph; Detection accuracy; Detection methods; Graph analysis method; Graph kernels; News topic detections; Semantic graphs; Subgraphs; Time complexity; Topic detection; Graphic methods",Article,Scopus,2-s2.0-85124321674
"Zhou T., Law K.M.Y.","57262608000;36803886300;","Semantic Relatedness Enhanced Graph Network for aspect category sentiment analysis",2022,"Expert Systems with Applications","195",,"116560","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124241998&doi=10.1016%2fj.eswa.2022.116560&partnerID=40&md5=ef9a1fee078993bc30b5f2a37de198b9","As a variant problem of aspect-based sentiment analysis (ABSA), aspect category sentiment analysis (ACSA) aims to identify the aspect categories discussed in sentences and predict their sentiment polarities. However, most aspect-based sentiment analysis (ABSA) research focuses on predicting the sentiment polarities of given aspect categories or aspect terms explicitly discussed in sentences. In contrast, aspect categories are often discussed implicitly. Additionally, most of the research does not consider the relations between contextual words and aspect categories. This paper proposes a novel Semantic Relatedness-enhanced Graph Network (SRGN) model which integrates the semantic relatedness information through an Edge-gated Graph Convolutional Network (EGCN). We introduce an ontology-based approach and a distributional approach to calculate the semantic relatedness values between contextual words and aspect categories. EGCN with the capability to aggregate multi-channel edge features, is then applied to model the semantic relatedness values in a graphical structure. We also employ an aspect–context attention module to generate aspect-specific representations. The proposed SRGN is evaluated on five datasets constructed based on SemEval 2015, SemEval 2016 and MAMC-ACSA datasets. Experimental results indicate that our proposed model outperforms the baseline models in both accuracy and F1 score. © 2022 Elsevier Ltd","Aspect category sentiment analysis; Aspect–context attention; Edge-Gated Graph Convolutional Network; Semantic relatedness","Convolution; Convolutional neural networks; Ontology; Semantic Web; Semantics; Aspect category sentiment analyse; Aspect–context attention; Contextual words; Convolutional networks; Edge-gated graph convolutional network; Graph networks; Network models; Research focus; Semantic relatedness; Sentiment analysis; Sentiment analysis",Article,Scopus,2-s2.0-85124241998
"Casillo F., Deufemia V., Gravino C.","57442987800;6506220454;8931993900;","Detecting privacy requirements from User Stories with NLP transfer learning models",2022,"Information and Software Technology","146",,"106853","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124186150&doi=10.1016%2fj.infsof.2022.106853&partnerID=40&md5=08f1d5542ea14713408f3cd228bfdbc1","Context: To provide privacy-aware software systems, it is crucial to consider privacy from the very beginning of the development. However, developers do not have the expertise and the knowledge required to embed the legal and social requirements for data protection into software systems. Objective: We present an approach to decrease privacy risks during agile software development by automatically detecting privacy-related information in the context of user story requirements, a prominent notation in agile Requirement Engineering (RE). Methods: The proposed approach combines Natural Language Processing (NLP) and linguistic resources with deep learning algorithms to identify privacy aspects into User Stories. NLP technologies are used to extract information regarding the semantic and syntactic structure of the text. This information is then processed by a pre-trained convolutional neural network, which paved the way for the implementation of a Transfer Learning technique. We evaluate the proposed approach by performing an empirical study with a dataset of 1680 user stories. Results: The experimental results show that deep learning algorithms allow to obtain better predictions than those achieved with conventional (shallow) machine learning methods. Moreover, the application of Transfer Learning allows to considerably improve the accuracy of the predictions, ca. 10%. Conclusions: Our study contributes to encourage software engineering researchers in considering the opportunities to automate privacy detection in the early phase of design, by also exploiting transfer learning models. © 2022 Elsevier B.V.","Deep learning; Natural Language Processing; Transfer Learning; User Stories","Computer software; Data privacy; Deep learning; Learning algorithms; Natural language processing systems; Neural networks; Software design; Syntactics; Agile requirements; Agile software development; Deep learning; Learning models; Privacy aware; Privacy requirements; Privacy risks; Software-systems; Transfer learning; User stories; Semantics",Article,Scopus,2-s2.0-85124186150
"Guven Z.A., Unalir M.O.","57202999818;6506739590;","Natural language based analysis of SQuAD: An analytical approach for BERT",2022,"Expert Systems with Applications","195",,"116592","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124098052&doi=10.1016%2fj.eswa.2022.116592&partnerID=40&md5=19f7e0d064c956d4bebbcf2198e89e1a","In recent years, deep learning models have been used in the implementation of question answering systems. In this study, the performance of the question answering system was evaluated from the perspective of natural language processing using SQuAD, which was developed to measure the performance of deep learning language models. In line with the evaluations, in order to increase the performance, 3 natural language based methods, namely RNP, that can be used with pre-trained BERT language models have been proposed and they have increased the performance of the question answering system in which the pre-trained BERT models are used by 1.1% to 2.4%. As a result of the application of RNP methods with sentence selection, an increase in accuracy between 6.6% and 8.76% was achieved in answer detection. Since these methods don't require any training process, it has been shown that they can be used in question answering systems to increase the performance of any deep learning model. © 2022 Elsevier Ltd","BERT; Natural language processing; Question answering; SQuAD; Text analysis","Computational linguistics; Deep learning; Analytical approach; BERT; Language model; Learning languages; Learning models; Natural languages; Performance; Question Answering; Question answering systems; SQuAD; Natural language processing systems",Article,Scopus,2-s2.0-85124098052
"Pérez-Pérez M., Ferreira T., Igrejas G., Fdez-Riverola F.","56494562100;57346649200;6602313075;35580091100;","A deep learning relation extraction approach to support a biomedical semi-automatic curation task: The case of the gluten bibliome",2022,"Expert Systems with Applications","195",,"116616","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124098036&doi=10.1016%2fj.eswa.2022.116616&partnerID=40&md5=e6b3904aacde2ea3d8c2052aa70bda0f","Discover relevant biomedical interactions in the literature is crucial for enhancing biology research. This curation process has an essential role in studying the different processes and interactions reported that affect the biological process (e.g., genome, metabolome, and transcriptome). In this sense, the objective of this work is twofold: reduce the manual effort required to curate and review the existing biochemical interactions reported in the gluten-related bibliome, while proposing a novel vector-space integrated into a deep learning model to assists manual curators in a real curation task by learning from their previous decisions. With this objective, the present work proposes a novel vector-space that combine (i) high-level lexical and syntactic inference features as Wordnets and Health-related domain ontologies, (ii) unsupervised semantic resources as word embedding, (iii) semantic and syntactic sentence knowledge, (iv) abbreviation resolution support, (v) several state-of-the-art Named-entity recognition methods, and, finally, (vi) different feature construction and optimization techniques to support a semi-automatic curation workflow. Therefore, the application of the proposed workflow over a classified set of 2,451 relevant gluten-related documents produces a total of 8,349 relevant and 471,813 irrelevant relations distributed in thirteen domain health-related categories. Experimental results showed that the proposed workflow is a valuable approach for a semi-automatic relation extraction task. It was able to obtain satisfactory results in the early stages of a real-world curation task and saved manual annotation efforts by learning from the decisions made by manual curators in iterative annotation rounds. The average F.score for the proposed relation categories was 0.731, being the lowest F.score at 0.47 and the highest F.score at 0.929. The different resources used in this work as well as the manually curated corpus are public available on our GitHub repository. © 2022 The Author(s)","Deep learning; Gluten; Literature curation; Ontology-based methods; Relation extraction; Text mining","Automation; Deep learning; Extraction; Iterative methods; Ontology; Semantics; Vector spaces; Curation; Deep learning; F-score; Gluten; Learning relations; Literature curation; Ontology-based methods; Relation extraction; Semi-automatics; Work-flows; Syntactics",Article,Scopus,2-s2.0-85124098036
"Chae S.W., Lee S.H.","57205122127;57437125700;","Sharing emotion while spectating video game play: Exploring Twitch users' emotional change after the outbreak of the COVID-19 pandemic",2022,"Computers in Human Behavior","131",,"107211","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123955619&doi=10.1016%2fj.chb.2022.107211&partnerID=40&md5=5933852fb4f0fd14091837f2051e21f1","This paper examines how the COVID-19 pandemic associates with Twitch users' emotion, using natural language processing (NLP) as a method. Two comparable sets of text data were collected from Twitch internet relay chats (IRCs): one after the outbreak of the pandemic and another one before that. Positive emotion, negative emotion, and attitude to social interaction were tested by comparing the two text sets via a dictionary-based NLP program. Particularly regarding negative emotion, three negative emotions—anger, anxiety, and sadness—were measured given the nature of the pandemic. The results show that users' anger and anxiety significantly increased after the outbreak of the pandemic, while changes in sadness and positive emotion were not statistically significant. In terms of attitude to social interaction, users used significantly fewer “social” words after the outbreak of the pandemic than before. These findings were interpreted considering the nature of Twitch as a unique live mixed media platform, and how the COVID-19 pandemic is different from previous crisis events was discussed based on prior literature. © 2022 Elsevier Ltd","COVID-19; Emotion; Internet relay chat; Pandemic; Social interaction; Twitch","Human computer interaction; Relay control systems; Software testing; COVID-19; Emotion; Emotional change; Game plays; Internet relay chat; Pandemic; Positive emotions; Social interactions; Twitch; Video-games; Natural language processing systems",Article,Scopus,2-s2.0-85123955619
"He H., Yamanashi Y., Yoshikawa N.","57376017300;57225182280;57217277797;","Design of Discrete Hopfield Neural Network Using a Single Flux Quantum Circuit",2022,"IEEE Transactions on Applied Superconductivity","32","4",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121397750&doi=10.1109%2fTASC.2021.3132862&partnerID=40&md5=11bc1c73b56ffb21d6a7b35c514a022a","The superconductor single flux quantum (SFQ) logic family has been recognized as a promising candidate to resolve the energy consumption crisis in the post-Moore era, owing to its high switching speed and low power consumption. In the field of machine learning, where technology and computational requirements are growing rapidly (e.g., image recognition and natural language processing), there is great potential for the implementation of SFQ circuits. In this study, we investigate and implement a discrete Hopfield neural network (DHNN) using SFQ circuits. A DHNN is a binary neural network with less information than a standard full precision neural network; it also provides a higher processing speed. It is mainly used for pattern recognition and recovery. We designed the DHNN circuit with two patterns, each with eight elements. The circuit operates at the clock frequency of more than 50 GHz. © 2002-2011 IEEE.","Discrete hopfield neural network (DHNN); hopfield neural networks; neural computation; simulation; single flux quantum (SFQ)","Clocks; Energy utilization; Hopfield neural networks; Image recognition; Learning algorithms; Low power electronics; Natural language processing systems; Speech recognition; Biological neural networks; DH-HEMTs; Discrete hopfield neural network; Discrete Hopfield neural networks; Neural computations; Neural-networks; Simulation; Single flux quantum; Single-flux-quantum circuits; Computation theory",Article,Scopus,2-s2.0-85121397750
"Berk Wheelock L., Pachamanova D.A.","57374024100;6507593616;","Acceptable set topic modeling",2022,"European Journal of Operational Research","299","2",,"653","673",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121282041&doi=10.1016%2fj.ejor.2021.11.024&partnerID=40&md5=54fca425e367ff292473c9d51453b5c9","Topic modeling is a significant branch of natural language processing and machine learning focused on inferring the generative process of text. Traditionally, algorithms for estimating topic models have relied on Bayesian inference and Gibbs sampling. This paper proposes a novel “acceptable set” framework for formulating topic modeling problems inspired by ideas from discrete component analysis and data-driven robust optimization. Our approach not only simplifies the design and inference of topic models, but also allows for extensions and generalizations that are challenging to integrate into traditional approaches. Different restrictions (e.g., sparsity) and assumptions (e.g., alternative generative processes) can be easily incorporated into our formulations through additional or modified constraints. Our formulations also naturally control a widely used metric of solution quality, perplexity. We adapt state-of-the-art stochastic gradient methods to find good local optima for the optimization formulations. The algorithms are efficient, scaling to realistic problem sizes with runtimes comparable to existing methods. Through extensive computational experiments, we show that our methods have improved solution quality compared to baseline methods and reconstruct more reliably the underlying generative models. Our framework overcomes known vulnerabilities of traditional topic modeling algorithms: our methods are effective in low-data settings, register good out-of-sample performance, and perform well for a variety of initial assumptions on input parameter values. © 2021 Elsevier B.V.","Discrete component analysis; Hypothesis testing; Mirror descent; Robust optimization; Topic modeling","Bayesian networks; Gradient methods; Inference engines; Learning algorithms; Modeling languages; Optimization; Quality control; Stochastic models; Stochastic systems; Bayesian inference; Component analysis; Discrete component analyse; Discrete components; Generative process; Hypothesis testing; Mirror descent; Robust optimization; Solution quality; Topic Modeling; Natural language processing systems",Article,Scopus,2-s2.0-85121282041
"Gong J., Ye Z., Ma L.","57219692024;57226319817;8930473900;","Neighborhood co-occurrence modeling in 3D point cloud segmentation",2022,"Computational Visual Media","8","2",,"303","315",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120747713&doi=10.1007%2fs41095-021-0244-6&partnerID=40&md5=d8ae2247bdf0660b8f7172f6f5ed244c","A significant performance boost has been achieved in point cloud semantic segmentation by utilization of the encoder-decoder architecture and novel convolution operations for point clouds. However, co-occurrence relationships within a local region which can directly influence segmentation results are usually ignored by current works. In this paper, we propose a neighborhood co-occurrence matrix (NCM) to model local co-occurrence relationships in a point cloud. We generate target NCM and prediction NCM from semantic labels and a prediction map respectively. Then, Kullback-Leibler (KL) divergence is used to maximize the similarity between the target and prediction NCMs to learn the co-occurrence relationship. Moreover, for large scenes where the NCMs for a sampled point cloud and the whole scene differ greatly, we introduce a reverse form of KL divergence which can better handle the difference to supervise the prediction NCMs. We integrate our method into an existing backbone and conduct comprehensive experiments on three datasets: Semantic3D for outdoor space segmentation, and S3DIS and ScanNet v2 for indoor scene segmentation. Results indicate that our method can significantly improve upon the backbone and outperform many leading competitors. [Figure not available: see fulltext.]. © 2021, The Author(s).","3D vision; co-occurrence relation modeling; point cloud; semantic segmentation","3D modeling; Forecasting; Semantics; 3-D vision; Co-occurrence; Co-occurrence relation modeling; Co-occurrence relationships; Cooccurrence matrixes (COM); Kullback Leibler divergence; Neighbourhood; Point-clouds; Relation models; Semantic segmentation; Semantic Segmentation",Article,Scopus,2-s2.0-85120747713
"Xu Y., Xia B., Wan Y., Zhang F., Xu J., Ning H.","57348135700;57218580288;57348288100;57347506300;57222391300;57219175113;","CDCAT: A multi-language cross-document entity and event coreference annotation tool",2022,"Tsinghua Science and Technology","27","3",,"589","598",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119486499&doi=10.26599%2fTST.2020.9010060&partnerID=40&md5=edc0d9ce466400c0c8491834f562cf80","A tool for the manual annotation of cross-document entity and event coreferences that helps annotators to label mention coreference relations in text is essential for the annotation of coreference corpora. To the best of our knowledge, CROss-document Main Events and entities Recognition (CROMER) is the only open-source manual annotation tool available for cross-document entity and event coreferences. However, CROMER lacks multi-language support and extensibility. Moreover, to label cross-document mention coreference relations, CROMER requires the support of another intra-document coreference annotation tool known as Content Annotation Tool, which is now unavailable. To address these problems, we introduce Cross-Document Coreference Annotation Tool (CDCAT), a new multi-language open-source manual annotation tool for cross-document entity and event coreference, which can handle different input/output formats, preprocessing functions, languages, and annotation systems. Using this new tool, annotators can label a reference relation with only two mouse clicks. Best practice analyses reveal that annotators can reach an annotation speed of 0.025 coreference relations per second on a corpus with a coreference density of 0.076 coreference relations per word. As the first multi-language open-source cross-document entity and event coreference annotation tool, CDCAT can theoretically achieve higher annotation efficiency than CROMER. © 1996-2012 Tsinghua University Press.","entity coreference; event coreference; manual annotation tool; natural language processing","Linguistics; Open systems; Annotation tool; Coreference; Cross documents; Entity coreference; Event coreference; Event recognition; Main event; Manual annotation; Manual annotation tool; Natural language processing systems",Article,Scopus,2-s2.0-85119486499
"Sun Y., Li G., Du J., Ning B., Chen H.","56654480500;23005242300;57226382968;27867950600;57208694638;","A subgraph matching algorithm based on subgraph index for knowledge graph",2022,"Frontiers of Computer Science","16","3","163606","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098752822&doi=10.1007%2fs11704-020-0360-y&partnerID=40&md5=d0d680d993466c9ea5517ca008c8f787","The problem of subgraph matching is one fundamental issue in graph search, which is NP-Complete problem. Recently, subgraph matching has become a popular research topic in the field of knowledge graph analysis, which has a wide range of applications including question answering and semantic search. In this paper, we study the problem of subgraph matching on knowledge graph. Specifically, given a query graph q and a data graph G, the problem of subgraph matching is to conduct all possible subgraph isomorphic mappings of q on G. Knowledge graph is formed as a directed labeled multi-graph having multiple edges between a pair of vertices and it has more dense semantic and structural features than general graph. To accelerate subgraph matching on knowledge graph, we propose a novel subgraph matching algorithm based on subgraph index for knowledge graph, called as FGqT-Match. The subgraph matching algorithm consists of two key designs. One design is a subgraph index of matching-driven flow graph (FGqT), which reduces redundant calculations in advance. Another design is a multi-label weight matrix, which evaluates a near-optimal matching tree for minimizing the intermediate candidates. With the aid of these two key designs, all subgraph isomorphic mappings are quickly conducted only by traversing FGqT. Extensive empirical studies on real and synthetic graphs demonstrate that our techniques outperform the state-of-the-art algorithms. © 2022, Higher Education Press.","knowledge graph; matching tree; subgraph index; subgraph matching","Computational complexity; Flow graphs; Forestry; Mapping; Pattern matching; Semantics; Trees (mathematics); Graph search; Isomorphic mapping; Knowledge graphs; Matching algorithm; Matching tree; Matchings; Research topics; Subgraph index; Subgraph matching; Subgraphs; Knowledge graph",Article,Scopus,2-s2.0-85098752822
"Yang E., Liu M., Xiong D., Zhang Y., Meng Y., Xu J., Chen Y.","57225848111;57203789092;36562918900;45261676700;57219489464;54796280200;57114607100;","Improving generation diversity via syntax-controlled paraphrasing",2022,"Neurocomputing","485",,,"103","113",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125140623&doi=10.1016%2fj.neucom.2022.02.020&partnerID=40&md5=2ebf19b816c280f1c8f94678c9f50552","As the neural-based Seq2Seq model pushes the state-of-the-art in text generation, recent work has turned to controlling attributes of the text such models generate, where syntax-controlled text generation can be applied for the paraphrase generation task, i.e., given an input sentence and a syntactic control to generate a paraphrase. The main challenge is how to generate sentences that follow the given syntax while maintaining semantics during the decoding process. Previous approaches using constituency parse trees as syntactic control can achieve better performance, but still suffer from the problems of inaccurate utilization of syntactic information and original semantics loss. To this end, we propose a Syntax Attention-Guided Paraphrase (SAGP) generation model that can utilize the previously generated text to accurately select a syntactic node from the given constituency parse tree to guide the generation of paraphrases. The automatic and manual evaluation results on the public datasets of syntactically controlled paraphrase generation task show that SAGP achieves state-of-the-art results in both syntactic controllability and semantic consistency. In order to improve the semantic consistency, we further propose a coarse-grained syntactic control definition method, which first removes the part-of-speech node and then extracts higher-level subtrees as control, so that meaningful paraphrases can be generated within a loose constraint. The experimental results on the same evaluation set show that the coarse-grained syntactic control can significantly improve semantic consistency. © 2022 Elsevier B.V.","Coarse-grained Syntactic Control; Semantic consistency; Syntactic controllability; Syntax-controlled paraphrasing","Forestry; Semantics; Coarse-grained; Coarse-grained syntactic control; Constituency parse trees; Decoding process; Semantic consistency; State of the art; Syntactic control; Syntactic controllability; Syntax-controled paraphrasing; Text generations; Syntactics; article; attention; controlled study; human; human experiment; semantics; speech",Article,Scopus,2-s2.0-85125140623
"Hao Z., Chen J., Wen W., Wu B., Cai R.","7201992728;57456823500;55214667000;57210212134;55623871600;","Motif-based memory networks for complex-factoid question answering",2022,"Neurocomputing","485",,,"12","21",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124806949&doi=10.1016%2fj.neucom.2022.02.008&partnerID=40&md5=521dae7b707716535253fd5fee858234","Knowledge-based question answering (KBQA) is an interesting but challenging task in the field of natural language processing. And in recent years, there is increasing interest in introducing deep learning models for answering complex-factoid questions, which associate with multiple facts and require multi-hop inference. However, the complex-factoid question answering mainly faces two challenges: (1) multiple entities are involved in these questions, which bring multiple initial states for knowledge reasoning; (2) a number of complex-factoid questions require the intersection of multiple related sub-paths in knowledge bases, which demands repeated explorations of path reasoning, matching and assembling. To address the above challenges, we propose a motif-based Memory Network for answering complex-factoid questions, which introduces motifs as the basic constituents for semantic representation, and meanwhile includes a specific-designed memory network for knowledge reasoning and matching. Extensive experiments on real datasets demonstrate that our model significantly outperforms the state-of-the-art methods. © 2022 Elsevier B.V.","Embedding; Knowledge based question answering; Memory network; Motif","Complex networks; Deep learning; Natural language processing systems; Semantics; Embeddings; Factoid questions; Knowledge based; Knowledge based question answering; Knowledge reasoning; Learning models; Memory network; Motif; Multi-hops; Question Answering; Knowledge based systems; article; embedding; memory; reasoning",Article,Scopus,2-s2.0-85124806949
"Cardone B., Di Martino F., Senatore S.","57205073443;6603582981;7003512928;","A fuzzy partition-based method to classify social messages assessing their emotional relevance",2022,"Information Sciences","594",,,"60","75",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124885185&doi=10.1016%2fj.ins.2022.02.028&partnerID=40&md5=974821a1acf64e442349938e32179467","With the surge of the large volume of data availability, Machine Learning and mainly Deep Learning techniques are the leading solutions in classification and predictive tasks, targeted at data-efficient learning. These models learn by training on many diversified samples in a process that is computationally expensive or time-consuming. Moreover, in many real-world scenarios, the amount of available data for training is unsuitable, because it is unlabeled or covers only portions of the whole reference domain cases. This paper proposes an alternative approach for document classification that leverages the distribution of the data projected in the multi-dimensional feature space to assess the weight of features in the final classification. The approach does not rely on traditional iterative methods for classification but builds a relevance measure to assess the relevance/importance of the features describing the domain of interest. The idea is to harness this metric to select relevant features and then express the values calculated by these metrics in natural language by exploiting fuzzy variables and linguistic labels to make human comprehension more immediate. The approach has been employed for emotion extraction from social media messages. The novelty of this approach is twofold: first, the well-known TF-IDF measure was reinterpreted as a relevance measure of emotions discovered in text content. Then, the discovered emotion relevance was described by fuzzy linguistic labels, defined on an ad-hoc-designed fuzzy partition, to express the data classification in natural language, more suitable to human understanding. © 2022 Elsevier Inc.","Classification; Emotional categories; Fuzzy linguistic labels; Fuzzy partition; TF-IDF","Deep learning; Information retrieval systems; Iterative methods; Linguistics; Natural language processing systems; Data availability; Emotional category; Fuzzy linguistic label; Fuzzy linguistics; Fuzzy partition; Large volumes; Linguistic labels; Natural languages; Relevance measure; TF-IDF; Classification (of information)",Article,Scopus,2-s2.0-85124885185
"Ahmed S., Gentili M., Sierra-Sosa D., Elmaghraby A.S.","57217873442;7102664227;37076014100;7004230200;","Multi-layer data integration technique for combining heterogeneous crime data",2022,"Information Processing and Management","59","3","102879","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124516668&doi=10.1016%2fj.ipm.2022.102879&partnerID=40&md5=f8856f53f83404e2b71087b740d4a56f","Analysis of publicly available human and drug trafficking crime data faces the challenge of finding a comprehensive dataset that includes a sufficiently large number of crime incidents. Our proposed methodology attempts to address this challenge by using entity resolution techniques to merge multiple state-wide crime datasets and a county-wide incident report dataset to get a clearer picture of a category of criminal activity in a geographical area. This methodology combines incident reports, crime reports, and court records to close any gaps that may be present in a single data source. We apply this methodology to create a dataset that includes drug and human trafficking related crimes and incidents from three distinct sources (from Louisville Open Data Crime Reports, Federal Bureau of Investigation Kentucky Crime Incidents, and the Kentucky Online Offender Lookup website) to provide researchers data to study the link between drug and human trafficking related crimes. In a case study performed with the new merged dataset, an XGBoost classifier was able to label a 7-day sliding time window, within any given county, as containing a human trafficking related incident or not with a Matthews correlation coefficient of 0.86. © 2022 The Authors","Binary classification; Data integration; Entity resolution; Natural language processing; Trafficking crimes; XGBoost","Classification (of information); Data integration; Large dataset; Natural language processing systems; Open Data; Binary classification; Crime data; Drug trafficking; Entity resolutions; Human trafficking; Incident reports; Kentucky; Multi-layers; Trafficing crime; Xgboost; Crime",Article,Scopus,2-s2.0-85124516668
"Zhang B., Liu K., Wang H., Li M., Pan J.","57448981700;57217078270;57215896639;7405263968;55459047600;","Chinese named-entity recognition via self-attention mechanism and position-aware influence propagation embedding",2022,"Data and Knowledge Engineering","139",,"101983","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124476244&doi=10.1016%2fj.datak.2022.101983&partnerID=40&md5=6aeed0bcb797c768a4cbeb11f7831e8d","Chinese Named Entity Recognition (NER) has received extensive research attention in recent years. However, Chinese texts lack delimiters to divide the boundaries of words, and some existing approaches cannot capture the long-distance interdependent features. In this paper, we propose a novel end-to-end model for Chinese NER. A new global word boundary detection approach is designed to capture the semantic dependency via a self-attention mechanism to represent character embedding by assigning compatible weights for each character in a sentence. To improve the representation ability of Chinese named-entity boundaries, we introduce position-aware influence propagation with the Gaussian kernel for each character, which combines convergence propagation and radiation propagation. Convergence propagation mainly measures the influence of surrounding characters on the target character. The purpose of radiation propagation is to measure the range of influence of the target character on surrounding characters. The proposed method has been evaluated and shown to offer strong performance in two Chinese NER datasets: MSRA and PFR. © 2022 Elsevier B.V.","Chinese named-entity recognition; Gaussian kernel; Position-aware influence propagation; Self-attention","Natural language processing systems; Semantics; Attention mechanisms; Chinese named entity recognition; Chinese text; Delimiters; Embeddings; Gaussian kernels; Position-aware influence propagation; Radiation propagation; Self-attention; Target character; Embeddings",Article,Scopus,2-s2.0-85124476244
"Lu Y., Jin P., Pan X., Ding N.","57216705968;57205133645;57200443921;35387708500;","Delta-band neural activity primarily tracks sentences instead of semantic properties of words",2022,"NeuroImage","251",,"118979","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124426737&doi=10.1016%2fj.neuroimage.2022.118979&partnerID=40&md5=2b4c85f81c875b4e5e56a34f2a16462d","Human language is generally combinatorial: Words are combined into sentences to flexibly convey meaning. How the brain represents sentences, however, remains debated. Recently, it has been shown that delta-band cortical activity correlates with the sentential structure of speech. It remains debated, however, whether delta-band cortical tracking of sentences truly reflects mental representations of sentences or is caused by neural encoding of semantic properties of individual words. The current study investigates whether delta-band neural tracking of speech can be explained by semantic properties of individual words. Cortical activity is recorded using electroencephalography (EEG) when participants listen to sentences repeating at 1 Hz and word lists. The semantic properties of individual words, simulated using a word2vec model, predict a stronger 1 Hz response to word lists than to sentences. When listeners perform a word-monitoring task that does not require sentential processing, the 1 Hz response to word lists, however, is much weaker than the 1 Hz response to sentences, contradicting the prediction of the lexical semantics model. When listeners are explicitly asked to parse word lists into multi-word chunks, however, cortical activity can reliably track the multi-word chunks. Taken together, these results suggest that delta-band neural responses to speech cannot be fully explained by the semantic properties of single words and are potentially related to the neural representation of multi-word chunks. © 2022 The Author(s)","Chunk; EEG; Grouping; Language; Semantics; Speech","adult; article; electroencephalography; female; human; human experiment; language; male; mental representation; nerve potential; prediction; semantics; simulation; speech",Article,Scopus,2-s2.0-85124426737
"Bedi J., Toshniwal D.","57201504761;8683737500;","CitEnergy: A BERT based model to analyse Citizens’ Energy-Tweets",2022,"Sustainable Cities and Society","80",,"103706","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124185027&doi=10.1016%2fj.scs.2022.103706&partnerID=40&md5=e6c86000bb476b6c50cfb11e2f0ef1c6","Micro-blogging social site Twitter has emerged as a rich source of unstructured text information which could be processed and analysed to extract people's opinions about several topics and events including natural hazards, energy, sports, transportation, elections etc. The present research study adds a novel perspective in this dimension by extracting citizens’ opinions on several electricity-related issues. Recent research studies in this domain have employed Bag-of-Words (BoW) model for the numerical representation of tweets. However, the BoW model suffers from several issues such as incapable of handling the semantic relationship between words, sparse and high dimensional representation. The present research work overcomes all aforementioned shortcomings by integrating popular word-embeddings with deep learning models for the classification tasks. The study harnesses social media data for two classification tasks: Sentiment classification task and Complaints classification task. Firstly, a series of preprocessing steps are applied on tweets extracted from Twitter streaming API. Subsequently, different word-embeddings models are employed to generate a numerical representation of tweets while capturing the semantic relationship among words. Several deep learning-based sentiment classification models are then deployed on top of generated word-embeddings for identifying/classifying citizens’ sentiments from the tweets. Lastly, the tweets associated with negative sentiment class (identified by the sentiment classification model) are further processed and analysed for building the complaints classification model. The complaints classification model prioritize and assign negative sentiment tweets into one of the two target classes depending on the target issue raised in the tweets (Community level or recurring complaint and Individual level complaint). In addition to this, the current study also proposes Bi-directional Encoded Representations for Transformers (BERT) based Sentiment classification and Complaints classification models for achieving the improved classification accuracy. Experimental evaluation of the proposed BERT based models is done by comparing prediction results with several benchmark deep learning models. © 2022 Elsevier Ltd","BERT; Complaints classification; Deep learning; Energy; Sentiment analysis; Twitter","Deep learning; Embeddings; Information retrieval; Semantics; Social networking (online); Bi-directional; Bi-directional encoded representation for transformer; Classification models; Classification tasks; Complaint classification; Deep learning; Embeddings; Energy; Sentiment analysis; Sentiment classification; Sentiment analysis",Article,Scopus,2-s2.0-85124185027
"Zhang X., Cai F., Hu X., Zheng J., Chen H.","57440785200;57226808440;56069707600;57201618356;9043552700;","A Contrastive learning-based Task Adaptation model for few-shot intent recognition",2022,"Information Processing and Management","59","3","102863","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124067116&doi=10.1016%2fj.ipm.2021.102863&partnerID=40&md5=7fdffa343d0b65de14e5c39aefd4270c","Few-shot intent recognition aims to identify user's intent from the utterance with limited training data. A considerable number of existing methods mainly rely on the generic knowledge acquired on the base classes to identify the novel classes. Such methods typically ignore the characteristics of each meta task itself, resulting in the inability to make full use of limited given samples when classifying unseen classes. To deal with such issues, we propose a Contrastive learning-based Task Adaptation model (CTA) for few-shot intent recognition. In detail, we leverage contrastive learning to help achieve task adaptation and make full use of the limited samples of novel classes. First, a self-attention layer is employed in the task adaptation module, which aims to establish interactions between samples of different categories so that new representations are task-specific rather than relying entirely on the base classes. Then, the contrastive-based loss functions and the semantics of the label name are respectively used for reducing the similarity between sample representations in different categories while increasing it in the same categories. Experimental results on a public dataset OOS verify the effectiveness of our proposal by beating the competitive baselines in terms of accuracy. Besides, we conduct the cross-domain experiments on three datasets, i.e., OOS, SNIPS as well as ATIS. We find that CTA gains obvious improvements in terms of accuracy in all cross-domain experiments, indicating that it has a better generalization ability than other competitive baselines in both cross-domain and single-domain settings. © 2022 Elsevier Ltd","Contrastive learning; Few-shot learning; Intent recognition","Learning systems; Adaptation models; Adaptation module; Base class; Contrastive learning; Cross-domain; Few-shot learning; Intent recognition; Limited training data; Loss functions; Task adaptation; Semantics",Article,Scopus,2-s2.0-85124067116
"Li B., Wei Y., Sun X., Bo L., Chen D., Tao C.","57439738400;57204919030;24829988300;57204660270;57208146531;57439049500;","Towards the identification of bug entities and relations in bug reports",2022,"Automated Software Engineering","29","1","24","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124033388&doi=10.1007%2fs10515-022-00325-1&partnerID=40&md5=77b456ac54a5c5c13dbd1365ee2f9d0e","During the bug fixing process, developers usually analyze the historical relevant bug reports in bug repository to support various bug analysis and fixing activities. There are rich semantics and relationships in the bug reports, which can be helpful for bug retrieval, recommendation, and repair. In this paper, our purpose is to quickly extract effective knowledge of bug report from two perspectives: entity recognition and relation extraction to assist bug understanding and fixing. Meanwhile, we hope to strengthen the relevance of bug reports through the effective extraction of bug knowledge. In order to effectively extract the bug entities and relations in the bug report, we first define 8 types of relations between the bug entities and incorporate neural network Recurrent Neural Network (RNN) and RNN based on shortest dependency path (SDP-RNN) to automatically identify bug entities and their relations in bug reports. Results We evaluate the effectiveness of our method through four experimental questions. From the results, the bug knowledge extracted by our method can effectively represent the semantics and relations in the bug report, and obtain F1 scores of 79.32% and 63.8% in entity recognition and relation extraction, respectively. The proposed approach can efficiently extract the structured bug knowledge in the bug report, and further enhance the correlation between the bug reports and the effectiveness of the bug knowledge through the representation of these structured bug knowledge units. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Bug analysis; Bug entity recognition; Neural networks; Relation extraction","Data mining; Extraction; Program debugging; Semantics; Bug analyse; Bug entity recognition; Bug fixing process; Bug reports; Entity recognition; F1 scores; Network-based; Neural-networks; Relation extraction; Types of relations; Recurrent neural networks",Article,Scopus,2-s2.0-85124033388
"Gjurković M., Vukojević I., Šnajder J.","57219502527;57219741062;14020411700;","SIMPA: Statement-to-Item Matching Personality Assessment from text",2022,"Future Generation Computer Systems","130",,,"114","127",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123948307&doi=10.1016%2fj.future.2021.12.014&partnerID=40&md5=d006d2005ac57f513836c85d7cbfb237","Automated text-based personality assessment (ATBPA) methods can analyze large amounts of text data and identify nuanced linguistic personality cues. However, current approaches lack the interpretability, explainability, and validity offered by standard questionnaire instruments. To address these weaknesses, we propose an approach that combines questionnaire-based and text-based approaches to personality assessment. Our Statement-to-Item Matching Personality Assessment (SIMPA) framework uses natural language processing methods to detect self-referencing descriptions of personality in a target's text and utilizes these descriptions for personality assessment. The core of the framework is the notion of a trait-constrained semantic similarity between the target's freely expressed statements and questionnaire items. The conceptual basis is provided by the realistic accuracy model (RAM), which describes the process of accurate personality judgments and which we extend with a feedback loop mechanism to improve the accuracy of judgments. We present a simple proof-of-concept implementation of SIMPA for ATBPA on the social media site Reddit. We show how the framework can be used directly for unsupervised estimation of a target's Big 5 scores and indirectly to produce features for a supervised ATBPA model, demonstrating state-of-the-art results for the personality prediction task on Reddit. © 2021 The Author(s)","Natural language processing; Personality prediction; Realistic accuracy model; Social media text; Text analysis; Text-based personality assessment","Processing; Semantics; Social networking (online); Surveys; Accuracy model; Large amounts; Matchings; Personality assessments; Personality predictions; Realistic accuracy model; Social media; Social medium text; Text data; Text-based personality assessment; Natural language processing systems",Article,Scopus,2-s2.0-85123948307
"Guo Y., Xu W., Pradhan S., Bravo C., Ben-Tzvi P.","57219923360;57219924674;57226439283;18036697800;15753747100;","Personalized voice activated grasping system for a robotic exoskeleton glove",2022,"Mechatronics","83",,"102745","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123899205&doi=10.1016%2fj.mechatronics.2022.102745&partnerID=40&md5=fcff5089de388f11cc5464a0d2b30161","This paper proposes a novel human machine interface (HMI) and electronics system design to control a rehabilitation robotic exoskeleton glove. Such system can be activated with the user's voice, take voice commands as input, recognize the command and perform biometric authentication in real-time with limited computing power, and execute the command on the exoskeleton. The electronics design is a stand-alone plug-and-play modulated design independent of the exoskeleton design. This personalized voice activated grasping system achieves better wearability, lower latency, and improved security than any existing exoskeleton glove control system. © 2022 Elsevier Ltd","Electronics design; Exoskeleton glove; Natural language processing; SEA; Voice command system","Exoskeleton (Robotics); Command systems; Electronic design; Electronics system design; Exoskeleton glove; Human-machine interface system; Personalized voice; Robotic exoskeletons; SEA; Voice command; Voice command system; Natural language processing systems",Article,Scopus,2-s2.0-85123899205
"Wang J., Zhang X., Chen L., Xie X.","57206682159;56461460000;57189042207;35788279200;","Personalizing label prediction for GitHub issues",2022,"Information and Software Technology","145",,"106845","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123623135&doi=10.1016%2fj.infsof.2022.106845&partnerID=40&md5=106196288a028a2ce84192b0b869106c","Context: Automated label prediction tools can help developers manage and categorize issues on GitHub. However, different open-source projects use various forms of labels with the same meaning. Previous label prediction methods mainly solve the problem of the synonymous labels by manual preprocessing rules, but these preprocessing rules can only identify synonyms with the same prefix or suffix. Objective: These factors inspire us to propose a method to identify these synonymous labels automatically and recommend personalized labels for different open-source projects. Method: In this paper, we propose a Personalizing Label Prediction framework for Issues named PLPI. PLPI identifies labels with similar meanings by representing labels as semantic vectors and applying clustering methods. PLPI can predict personalized labels from the existing labels in the open-source project. Result: We conduct a comprehensive study to compare seven commonly adopted labeling models with our approach. The experimental results demonstrate the advantages of our approach. Finally, we show some representative examples and discuss the visualization results of synonyms clustering by dimension reduction. Conclusion: The experimental results show that our method PLPI can improve label prediction performance and provide personalized label recommendation results for different open-source projects. © 2022 Elsevier B.V.","Data analysis; Deep learning; Issue labeling; Language model","Deep learning; Forecasting; Clustering methods; Deep learning; Issue labeling; Label predictions; Labelings; Language model; Open source projects; Prediction methods; Prediction tools; Semantic vectors; Semantics",Article,Scopus,2-s2.0-85123623135
"Gu R., Shi J., Chen X., Wang Z., Che Y., Zhang K., Huang Y.","36997784300;57429676400;57430030700;56450325700;57398217800;57244534400;7501574686;","Octopus-DF: Unified DataFrame-based cross-platform data analytic system",2022,"Parallel Computing","110",,"102879","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123611971&doi=10.1016%2fj.parco.2021.102879&partnerID=40&md5=a06e15d3b1530a0bcbffafb2f9cf0efa","Nowadays, DataFrame serves as a core to model and implement numerous machine learning and data analytic algorithms. Traditional data analytic programming languages, such as Python, provide the DataFrame programming model natively. In the big data era, it is a natural demand to introduce the DataFrame model into distributed computing systems for convenient big data analysis. Therefore, various DataFrame libraries have been implemented on Spark and Dask. However, these distributed computing systems contain some parallelism semantics which are not very straightforward for data analysts. Also, a DataFrame-based algorithm may have quite different performance for various datasets over different platforms. And, it is difficult for data analysts to choose the optimal platforms that achieve the best performance for their programs. To address these problems, we build a unified DataFrame-based data analytic system Octopus-DF. Octopus-DF integrates Pandas, Dask, and Spark as the backend computing platforms and exposes the most widely used Pandas-style APIs to users. Then, as DataFrame computation performance plays a critical role in the computing efficiency of DataFrame-based data analytic algorithms, we designed a set of DataFrame computation optimizations which are divided into two parts: (1) multiple indexing and DAG optimizations, and (2) cross-platform scheduling strategy. Experimental results show that Octopus-DF outperformed the existing single platforms with 11.72× speedup on average. Compared with the existing platform combination strategies, Octopus-DF can achieve the optimal one. Moreover, the proposed optimizations can effectively speedup the execution workflow. © 2022 Elsevier B.V.","Cross-platform; DataFrame programming model; Distributed system; Scheduling","Application programming interfaces (API); Big data; Computational efficiency; Distributed computer systems; Machine learning; Scheduling; Semantics; Shellfish; Analytic algorithm; Analytics systems; Cross-platform; Data analysts; Data analytics; Dataframe programming model; Distributed computing systems; Distributed systems; Performance; Programming models; Molluscs",Article,Scopus,2-s2.0-85123611971
"Longo F., Braun M., Hutzler F., Richlan F.","57420195700;11339848200;6602801341;26028442400;","Impaired semantic categorization during transcranial direct current stimulation of the left and right inferior parietal lobule: Impaired semantic categorization",2022,"Journal of Neurolinguistics","62",,"101058","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123224429&doi=10.1016%2fj.jneuroling.2022.101058&partnerID=40&md5=2c2a9156b041d462adc50178a9aee642","We investigated whether semantic knowledge is organized according to domain- or feature-dimensions during a semantic categorization task. In addition, using transcranial direct current stimulation (tDCS), we assessed whether the left or right inferior parietal lobule is differentially engaged based on these dimensions. To this end, four different tDCS electrode montage groups were employed (anodal left, cathodal left, anodal right, cathodal right). Reaction times and accuracy were recorded in response to visually presented words (living and non-living concepts with a high or low number of features). In line with our expectations, living concepts elicited faster reaction times compared with non-living concepts and concepts with a high number of features elicited faster reaction times compared with concepts with a low number of features. In addition, a general, regionally and polarity-unspecific, deteriorating effect of tDCS emerged, with stimulation slowing down reaction times compared with sham. The results are discussed in the frameworks of major theories on the organization of semantic knowledge, including the Distributed Domain-Specific Hypothesis. © 2022 The Authors","Brain; Domain; Features; Inferior parietal lobule; Memory; Semantic knowledge; Transcranial direct current stimulation","adult; Article; clinical feature; controlled study; female; human; inferior parietal lobule; learning; major clinical study; male; measurement accuracy; memory; reaction time; semantics; stimulation; transcranial direct current stimulation",Article,Scopus,2-s2.0-85123224429
"Ahmed U., Srivastava G., Yun U., Lin J.C.-W.","57204392052;57202588447;57388239400;56449520400;","EANDC: An explainable attention network based deep adaptive clustering model for mental health treatment",2022,"Future Generation Computer Systems","130",,,"106","113",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123205972&doi=10.1016%2fj.future.2021.12.008&partnerID=40&md5=882812bc598513af1ff183614e962c52","Internet-delivered Psychological Treatment (IDPT) has been shown to be an effective method for improving psychological disorders. Natural language processing (NLP) requires an appropriate set of linguistic features for word representation and emotion segmentation. For psychological applications, models must be trained on extensive and diverse datasets to achieve expert-level performance. Labeling psychological texts authorized by patients is challenging because emotional biases can lead to incorrect segmentation of emotions and labeling emotional data is time consuming. In this paper, we propose an assistance tool for psychologists to explore the emotional aspects of mentally ill individuals. We first use an NLP-based method to create emotional lexicon embeddings, and then apply attention-based deep clustering. The learned representation is then used to visualize the emotional aspect of the text authorized by patients. We expand the patient authored text using synonymous semantic expansion. A latent semantic representation based on context is clustered using EANDC, which is a Explainable Attention Network-based Deep adaptive Clustering model. We use similarity metrics to select a subset of the text and then improve the explainability of learning using a curriculum-based optimization method. The experimental results show that synonym expansion based on the emotion lexicon increases accuracy without affecting the results. The attention method with bidirectional LSTM architecture achieved 0.81 ROC in a blind test. The self-learning based embedding then visualizes the weighted attention words and helps the psychiatrist to improve his explanatory power of the qualitative match for clinical notes and the remedy. The method helps in labeling text and improves the recognition rate of symptoms of mental disorders. © 2021 The Author(s)","Adaptive treatments; Explainable; Internet-delivered interventions; NLP; Text clustering; Word sense identification","Character recognition; Cluster analysis; Embeddings; Long short-term memory; Semantics; Adaptive clustering; Adaptive treatment; Clustering model; Explainable; Internet-delivered intervention; Labelings; Network-based; Text Clustering; Word sense; Word sense identification; Natural language processing systems",Article,Scopus,2-s2.0-85123205972
"Rodríguez A.J.C., Castro D.C., García S.H.","57416382900;57002595700;57224572919;","Noun-based attention mechanism for Fine-grained Named Entity Recognition",2022,"Expert Systems with Applications","193",,"116406","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123000519&doi=10.1016%2fj.eswa.2021.116406&partnerID=40&md5=b65162d246dc4e7c64bab08cc19f677a","Fine-grained Named Entity Recognition is a challenging Natural Language Processing problem as it requires classifying entity mentions into hundreds of types that can span across several domains and be organized in several hierarchy levels. This task can be divided into two subtasks: Fine-grained Named Entity Detection and Fine-grained Named Entity Typing. In this work, we propose solutions for both of these subtasks. For the former, we propose a system that uses a stack of Byte-Pair Encoded vectors in combination with Flair embeddings, followed by a BILSTM-CRF network, which allowed us to improve the current state of the art for the 1k-WFB-g dataset. In the second subtask, attention mechanisms have become a common component in most of the current architectures, where the patterns captured by these mechanisms are generic, so in theory, they could attend to any word in the text indistinctly, regardless of its syntactic type, often causing inexplicable errors. To overcome this limitation we propose an attention mechanism based specifically on the use of elements of the noun syntactic type. We have compared our results to those obtained with a generic attention mechanism, where our method presented better results. © 2021 Elsevier Ltd","Entity detection; Entity typing; Fine-grained Named Entity Recognition; Noun-based attention mechanism","Natural language processing systems; 'current; Attention mechanisms; Entity detection; Entity typing; Fine grained; Fine-grained named entity recognition; Named entities; Named entity recognition; Noun-based attention mechanism; Subtask; Syntactics",Article,Scopus,2-s2.0-85123000519
"Ciampi M., Coronato A., Naeem M., Silvestri S.","23396134400;8878672000;57188992600;56450341200;","An intelligent environment for preventing medication errors in home treatment",2022,"Expert Systems with Applications","193",,"116434","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122930277&doi=10.1016%2fj.eswa.2021.116434&partnerID=40&md5=602f92acebee7416d3cf3e30ead19f5c","This article presents an assistance system based on home sensors, Ambient Intelligence, and Artificial Intelligence, which helps the elderly during their medical treatment at home to reduce medication errors. The sort of medication errors we address are those due to medication omission, wrong dosage or timing, drug–drug interactions. Since the patient may have some physical and/or cognitive disabilities, the proposed solution provides advanced features of self-adaptation and exploits the most cutting edge Artificial Intelligence technologies such as Reinforcement Learning, Deep Learning and Natural Language Processing (NLP) to remind and monitor adherence to the prescribed treatment. In particular, the system offers functions for (i) personalised reminds; i.e. an intelligent agent – called Tutor – self-learns (via Reinforcement Learning) the best way to communicate with the patient; (ii) feedback about the medication the patient is going to take; i.e., another intelligent agent – called Checker- identifies the pillbox that the patient is handling before taking the pill (via Deep Neural Network, Optical Character Recognition, and Barcode Reading); and, (iii) alerts in case of known drug–drug interactions; i.e., an intelligent service – called Advisor – searches for the active principles of the medication (via NLP and Unified Medical Language System (UMLS) RxNorm resources) identified by the Checker and known interactions with other medications of the treatment. The final objective is to remind effectively when and what medication is to be taken, to check that the patient is going to take the correct medication, and to alert if possible drug–drug interactions are identified, remotely reporting about the adherence to the therapy or anomalies to the caregivers and/or doctors. Experimental evaluations show encouraging results in terms of drug recognition and drug–drug interactions identification. © 2022 Elsevier Ltd","Ambient intelligence; Artificial Intelligence; Deep Learning; Drug–drug interactions; Medication errors; Natural Language Processing; Reinforcement Learning","Deep neural networks; Drug interactions; Errors; Intelligent agents; Learning algorithms; Ambients; Assistance system; Cognitive disability; Deep learning; Drug-drug interactions; Home treatments; Intelligent environment; Medical treatment; Medication errors; Reinforcement learnings; Natural language processing systems",Article,Scopus,2-s2.0-85122930277
"García-Díaz J.A., Colomo-Palacios R., Valencia-García R.","57201498591;25653963200;55887649000;","Psychographic traits identification based on political ideology: An author analysis study on Spanish politicians’ tweets posted in 2020",2022,"Future Generation Computer Systems","130",,,"59","74",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122429680&doi=10.1016%2fj.future.2021.12.011&partnerID=40&md5=352373bbe3c712e291009ae2d82bd49d","In general, people are usually more reluctant to follow advice and directions from politicians who do not have their ideology. In extreme cases, people can be heavily biased in favour of a political party at the same time that they are in sharp disagreement with others, which may lead to irrational decision making and can put people's lives at risk by ignoring certain recommendations from the authorities. Therefore, considering political ideology as a psychographic trait can improve political micro-targeting by helping public authorities and local governments to adopt better communication policies during crises. In this work, we explore the reliability of determining psychographic traits concerning political ideology. Our contribution is twofold. On the one hand, we release the PoliCorpus-2020, a dataset composed by Spanish politicians’ tweets posted in 2020. On the other hand, we conduct two authorship analysis tasks with the aforementioned dataset: an author profiling task to extract demographic and psychographic traits, and an authorship attribution task to determine the author of an anonymous text in the political domain. Both experiments are evaluated with several neural network architectures grounded on explainable linguistic features, statistical features, and state-of-the-art transformers. In addition, we test whether the neural network models can be transferred to detect the political ideology of citizens. Our results indicate that the linguistic features are good indicators for identifying fine-grained political affiliation, they boost the performance of neural network models when combined with embedding-based features, and they preserve relevant information when the models are tested with ordinary citizens. Besides, we found that lexical and morphosyntactic features are more effective on author profiling, whereas stylometric features are more effective in authorship attribution. © 2021 The Author(s)","Author profiling; Authorship analysis; Authorship attribution; Linguistic features; Natural language processing","Behavioral research; Decision making; Electric grounding; Natural language processing systems; Network architecture; Neural networks; Author profiling; Authorship analysis; Authorship attribution; Irrational decision makings; Linguistic features; Micro targeting; Neural network model; Political ideologies; Political parties; Public authorities; Linguistics",Article,Scopus,2-s2.0-85122429680
"Carta S., Giuliani A., Piano L., Podda A.S., Reforgiato Recupero D.","7004254388;57222379357;57224174899;56875324700;57206674454;","VSTAR: Visual Semantic Thumbnails and tAgs Revitalization",2022,"Expert Systems with Applications","193",,"116375","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122324953&doi=10.1016%2fj.eswa.2021.116375&partnerID=40&md5=5840e64990535397f071672f9ec5c93c","Nowadays, video-sharing portals’ popularity has entailed massive growth in data uploads over the Internet. For several applications (e.g., browsing, retrieval, or recommendation of videos), dealing with vast data volumes has become a critical issue. In a video-sharing scenario, the devising of tools and infrastructures able to completely satisfy users’ interests and requests is becoming increasingly crucial to influence their online experiences. On the one hand, annotating a video with meaningful human-friendly words (i.e., tags) plays an essential role in matching users’ interests. On the other hand, providing a condensed and straightforward preview of the video content (i.e., thumbnails) is crucial to capture the user's attention immediately. In this context, we propose VSTAR (Visual Semantic Thumbnails and tAgs Revitalization), a novel approach in video optimization aimed at generating both suitable tags and thumbnails from a different perspective than classical approaches. The novelty lies in: (i) exploiting image captioning to extract visual and semantic information for generating both tags and thumbnails; (ii) identifying semantically related popular search queries (i.e., trends) to be suggested as new tags; (iii) giving the final user the control on a trade-off between quality and quantity of the generated items (tags and thumbnails); (iv) creating a proper dataset and making it publicly available. Experiments demonstrate the viability of our proposal. © 2021 Elsevier Ltd","Google trends; Machine learning; Semantic enrichment; Thumbnail enrichment; Video tagging","Economic and social effects; Semantics; Critical issues; Data volume; Google trends; Human-friendly; Semantic enrichment; Thumbnail enrichment; Users' interests; Video sharing; Video tagging; Visual semantics; Machine learning",Article,Scopus,2-s2.0-85122324953
"Srinivasarao U., Sharaff A.","57216695931;57120176100;","Email thread sentiment sequence identification using PLSA clustering algorithm",2022,"Expert Systems with Applications","193",,"116475","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122263896&doi=10.1016%2fj.eswa.2021.116475&partnerID=40&md5=a023d2dd1273d33b521a4b42be8b3507","Email messaging is the most common way of providing effective communication between internauts. Consequently, the total sent and received emails count will be increased. But, the internaut can't remember all such emails. Even though email thread identification approaches give satisfactory benefits to the internauts, but they may fail to alert them for a cause to identify the sentiments behind an email thread. To address, this issue Probabilistic Latent Semantic Analysis clustering algorithm has been used in this paper to identify the email sentiment thread sequence. The sentiment and the thread sequence within the emails have been discovered as clustering sentiment polarity and temporal categories with the help of PLSA clusters. At the initial stage, we used three feature extraction methods, latent semantic analysis (LDA), bag of words (BoW), TF-IDF and SentiWordNet (SWN) lexicon for generating sentiment features of email. Next, Probabilistic Latent Semantic Analysis algorithm is used to form email clusters based on sentiment features. Thus, it helps to identify thread sentiment and sequence of sentiment threads. Email threads give a mechanism by which any user will be able to find out the sequence in the thread on the basis of sentiment analysis of email related to a specific set of communication during a specific time period. Various parameters evaluation measures have been considered in this work to evaluate the proposed model such as accuracy, precision, recall and F-measure, and the proposed algorithm is compared with other standard algorithms. Furthermore, a statistical test has also been performed. © 2021 Elsevier Ltd","Probabilistic latent semantic analysis; Sentiment email clusters; Sentiment sequence of threads; SWN lexicon; Topic modeling","Clustering algorithms; Semantics; Sentiment analysis; Effective communication; Identification approach; Probabilistic latent semantic analysis; Sentiment email cluster; Sentiment features; Sentiment sequence of thread; SentiWordNet; Sentiwordnet lexicon; Sequence identification; Topic Modeling; Electronic mail",Article,Scopus,2-s2.0-85122263896
"Mustansir A., Shahzad K., Malik M.K.","57204064720;57222422198;36666694000;","Towards automatic business process redesign: an NLP based approach to extract redesign suggestions",2022,"Automated Software Engineering","29","1","12","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122214266&doi=10.1007%2fs10515-021-00316-8&partnerID=40&md5=2ec1977cda1ee04b04dac96de8259f18","Business process redesign (BPR) is widely recognized as a key phase of the business process management lifecycle. However, the existing studies have focused on proposing theoretical models, methodologies, and redesign patterns, whereas, the BPR activity remains dependent upon domain experts with little or no consideration to end-user feedback. To facilitate these experts, in this study, we have proposed a natural language processing (NLP) based approach to identify redesign suggestions from end-user feedback in natural language text. The proposed approach includes a novel set of annotation guidelines that can be used to generate computational resources for business process redesign. Secondly, to demonstrate the effectiveness of the proposed approach, we have generated computational resources which are composed of three real-world business processes and end-user feedback containing 8421 sentences. Finally, we have performed 270 experiments using six traditional and three deep learning techniques to evaluate their effectiveness for the identification of redesign suggestions from raw text. The classified suggestions can be used by domain experts to prioritize the redesign possibilities, without going through the details of the customer feedback. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Application of NLP in BPR; Business process redesign; Deep learning techniques; Software engineering; Supervised learning techniques","Application programs; Deep learning; Enterprise resource management; Life cycle; Natural language processing systems; Supervised learning; Application of natural language processing in business process redesign; Business process redesign; Computational resources; Deep learning technique; Domain experts; End-user feedback; Learning techniques; Modeling methodology; Theoretical modeling; Learning algorithms",Article,Scopus,2-s2.0-85122214266
"Zaki-Ismail A., Osama M., Abdelrazek M., Grundy J., Ibrahim A.","57220061393;57220054046;56080446200;7102156137;51461588300;","RCM-extractor: an automated NLP-based approach for extracting a semi formal representation model from natural language requirements",2022,"Automated Software Engineering","29","1","10","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122082212&doi=10.1007%2fs10515-021-00312-y&partnerID=40&md5=ccd54f3b7855e6bf1f8bce01037391fc","Most existing (semi-)automated requirements formalisation techniques assume requirements to be specified in predefined templates. They also employ template-specific transformation rules to provide the corresponding formal representation. Hence, such techniques have limited expressiveness and more importantly require system engineers to re-write their system requirements following defined templates for maintenance and evolution. In this paper, we introduce an automated requirements extraction technique (RCM-Extractor) to automatically extract the key constructs of a comprehensive and formalisable semi-formal representation model from textual requirements. This avoids the expressiveness issues affecting the existing requirement specification templates, and eliminates the need to rewriting the requirements to match the structure of such templates. We evaluated RCM-Extractor on a dataset of 162 requirements curated from several papers in the literature. RCM-Extractor achieved 87% precision, 98% recall, 92% F-measure, and 86% accuracy. In addition, we evaluated the capabilities of RCM-Extractor to extract requirements on a dataset of 15,000 automatically synthesised requirements that are constructed specifically to evaluate our approach. This dataset has a complete coverage of the possible structures and arrangements of the properties that can exist in system requirements. Our approach achieved 57%, 92% and 100% accuracy for un-corrected, partially-corrected and fully-corrected Stanford typed-dependencies representations of the synthesised requirements, respectively. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Natural-language extraction; Requirements extraction; Requirements formalization","Automation; Natural language processing systems; Requirements engineering; Natural language requirements; Natural languages; Natural-language extraction; Representation model; Requirement extraction; Requirements formalizations; Semi-formal representations; Synthesised; System requirements; Transformation rules; Extraction",Article,Scopus,2-s2.0-85122082212
"Moreno Schneider J., Rehm G., Montiel-Ponsoda E., Rodríguez-Doncel V., Martín-Chozas P., Navas-Loro M., Kaltenböck M., Revenko A., Karampatakis S., Sageder C., Gracia J., Maganza F., Kernerman I., Lonke D., Lagzdins A., Bosque Gil J., Verhoeven P., Gomez Diaz E., Boil Ballesteros P.","36606547100;28767949900;25654093800;35204031900;57210581382;57195404822;44461327800;55225811200;57191664371;57219630964;55392626700;57219628002;57193091389;57211836386;57219639642;57031866000;57209197843;57388356400;57388835000;","Lynx: A knowledge-based AI service platform for content processing, enrichment and analysis for the legal domain",2022,"Information Systems","106",,"101966","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121918359&doi=10.1016%2fj.is.2021.101966&partnerID=40&md5=1f7a75a77d6ec29ad54e02053f56a4fd","The EU-funded project Lynx focuses on the creation of a knowledge graph for the legal domain (Legal Knowledge Graph, LKG) and its use for the semantic processing, analysis and enrichment of documents from the legal domain. This article describes the use cases covered in the project, the entire developed platform and the semantic analysis services that operate on the documents. © 2022 The Authors","Applications; Knowledge discovery/representation; Systems; Text analytics; Tools","Data mining; Semantics; Content analysis; Content processing; Knowledge based; Knowledge discovery/representation; Knowledge graphs; Legal domains; Semantic analysis; Service platforms; System; Text analytics; Knowledge graph",Article,Scopus,2-s2.0-85121918359
"Fernandes W.P.D., Frajhof I.Z., de Almeida G.D.F.C.F., Rodrigues A.M.B., Barbosa S.D.J., Konder C.N., Nasser R.B., de Carvalho G.R., Lopes H.C.V.","57207843555;57209848460;57221571022;57202284338;57373577800;57209857377;56067886900;24553936100;7006023298;","Extracting value from Brazilian Court decisions",2022,"Information Systems","106",,"101965","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121267482&doi=10.1016%2fj.is.2021.101965&partnerID=40&md5=3759c9a50c0faa67e4b803f341675ef1","We propose a methodology to extract value from Brazilian Court decisions to support judges and lawyers in their decision-making. We instantiate our methodology in one information system we have developed. Such system (i) extracts plaintiff's legal claims and each specific provision on legal opinions enacted by lower and Appellate Courts, and (ii) connects each legal claim with the corresponding judicial provision. The information system presents the results through visualizations. Information Extraction for legal texts has been previously approached in the literature for different languages, using different methods. Our proposal is different from previous work, since our corpora comprise Brazilian lower and Appellate Court decisions, in which we look for a set of plaintiff's legal claims and judicial provisions commonly judged by the Court. We use the following methods to tackle the information extraction tasks: Bidirectional Long Short-Term Memory network; Conditional Random Fields; and a combination of Bidirectional Long Short-Term Memory network and Conditional Random Fields. In addition to the well-known distributed representation of words in word embeddings, we use character-level representation of words in character embeddings. We have built three corpora – Kauane Insurance Report, Kauane Insurance Lower, and Kauane Insurance Upper – to train and evaluate the system, using public data from the State Court of Rio de Janeiro. Our methods achieved good quality for Kauane Insurance Lower and Kauane Insurance Upper, and promising results for Kauane Insurance Report. © 2021 Elsevier Ltd","Conditional random fields; Deep learning; Information extraction; Law; Long short-term memory; Machine learning; Natural language processing; Recurrent neural networks","Brain; Embeddings; Information retrieval; Information systems; Information use; Insurance; Laws and legislation; Long short-term memory; Natural language processing systems; Random processes; Character level; Court decisions; Decisions makings; Deep learning; Distributed representation; Embeddings; Information extraction; Law; Legal texts; Memory network; Decision making",Article,Scopus,2-s2.0-85121267482
"El-Mashad S.Y., Hamed E.-H.S.","56201994100;57372096400;","Automatic creation of a 3D cartoon from natural language story",2022,"Ain Shams Engineering Journal","13","3","101641","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121137020&doi=10.1016%2fj.asej.2021.11.010&partnerID=40&md5=1108c3932225354ce8f08fe25eae66b1","The automatic creation of 3D animation from natural language text is used in many fields. The main target of this paper is to produce a 3D cartoon from a text input. Therefore, we need to analyze the input corpus to extract useful information by employing theories and tools from linguistics and natural language processing in addition to computer graphics for human language visualization. The system operates through two phases. The NLP phase, in which input text passes first through a coreference resolution solver in order to remove pronouns and substitute them with their corresponding nouns followed by a dependency parser in order to detect subject-action-object (SAO) relations in the resolved text. The sequence of SAOs resulting from the NLP phase is passed to the graphics phase. In the graphics phase a 3D animated video cartoon is generated by visualizing each SAO extracted in the NLP phase and Storytelling using the Unity game engine platform. The main contribution of this work is that the input does not have to be a screenplay. It is also demonstrated that performing coreference resolution before dependency parsing resulted in a more compact sequence of SAOs. © 2021","3D cartoon; Computer graphics; Natural language processing; Story visualization","Animation; Object detection; Three dimensional computer graphics; Visualization; 3D animation; 3d cartoon; Automatic creations; Coreference resolution; Human language; Language visualization; Natural languages; Natural languages texts; Story visualization; Text input; Natural language processing systems",Article,Scopus,2-s2.0-85121137020
"Sansone C., Sperlí G.","7004587033;55510822500;","Legal Information Retrieval systems: State-of-the-art and open issues",2022,"Information Systems","106",,"101967","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121096576&doi=10.1016%2fj.is.2021.101967&partnerID=40&md5=0e90d252dc65f4163838535c69c54654","In the last years, the legal domain has been revolutionized by the use of Information and Communication Technologies, producing large amount of digital information. Legal practitioners’ needs, then, in browsing these repositories has required to investigate more efficient retrieval methods, that assume more relevance because digital information is mostly unstructured. In this paper we analyze the state-of-the-art of artificial intelligence approaches for legal domain, focusing on Legal Information Retrieval systems based on Natural Language Processing, Machine Learning and Knowledge Extraction techniques. Finally, we also discuss challenges – mainly focusing on retrieving similar cases, statutes or paragraph for supporting latest cases’ analysis – and open issues about Legal Information Retrieval systems. © 2021 Elsevier Ltd","Artificial Intelligence; Legal Information Retrieval; Natural Processing Language; Ontology","Artificial intelligence; Information retrieval; Information retrieval systems; Information use; Learning algorithms; Search engines; Digital information; Information and Communication Technologies; Information-retrieval systems; Large amounts; Legal domains; Legal information retrieval; Ontology's; Retrieval methods; State of the art; System state; Natural language processing systems",Article,Scopus,2-s2.0-85121096576
"Rožanec J.M., Fortuna B., Mladenić D.","57217150010;55925609700;6602880697;","Knowledge graph-based rich and confidentiality preserving Explainable Artificial Intelligence (XAI)",2022,"Information Fusion","81",,,"91","102",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120902248&doi=10.1016%2fj.inffus.2021.11.015&partnerID=40&md5=f6322ebc397bca40317c74c57cfa3829","The paper proposes a novel architecture for explainable artificial intelligence based on semantic technologies and artificial intelligence. We tailor the architecture for the domain of demand forecasting and validate it on a real-world case study. The explanations provided result from knowledge fusion regarding concepts describing features relevant to a particular forecast, related media events, and metadata regarding external datasets of interest. The Knowledge Graph enhances the quality of explanations by informing concepts at a higher abstraction level rather than specific features. By doing so, explanations avoid exposing sensitive details regarding the demand forecasting models, thus preserving confidentiality. In addition, the Knowledge Graph enables linking domain knowledge, forecasted values, and forecast explanations while also providing insights into actionable aspects on which users can take action. The ontology and dataset we developed for this use case are publicly available for further research. © 2021 The Authors","Confidentiality; Demand forecasting; Explainable Artificial Intelligence; Knowledge Graph; Privacy; Smart manufacturing","Graphic methods; Knowledge graph; Semantics; Confidentiality; Demand forecasting; Explainable artificial intelligence; Graph-based; Knowledge graphs; Novel architecture; Privacy; Real-world; Semantic technologies; Smart manufacturing; Forecasting",Article,Scopus,2-s2.0-85120902248
"Ahanin Z., Ismail M.A.","57367227100;25825070200;","A multi-label emoji classification method using balanced pointwise mutual information-based feature selection",2022,"Computer Speech and Language","73",,"101330","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120885680&doi=10.1016%2fj.csl.2021.101330&partnerID=40&md5=31f75fd776d9dc2db076d99ffc6b9243","The availability of social media such as twitter allows users to express their feeling, emotions and opinions toward a topic. Emojis are graphic symbols that are regarded as the new generation of emoticons and an effective way of conveying feelings and emotions in social media. With the surging popularity of Emojis, the researchers in the area of Emotion Classification strive to understand the emotion correlated to each Emoji. Two of the most the successful approaches in emoji analysis rely on: 1) official Unicode description and 2) manually built emoji lexicons. Since the use of emoji is socially determined, the former approach is not aligned with intended semantic and usage, which leads researchers to opt for emoji lexicons. To overcome problem of lexicon-based approach, we proposed a method to classify emojis automatically. Therefore, we present a modified Pointwise Mutual Information (PMI) method, called Balanced Pointwise Mutual Information-Based (B-PMI), to develop a balanced weighted emoji classification based on the semantic similarity. Further, deep neural network is used to represent emoji in vector form (emoji embedding) to extend the pre-trained word embeddings. We carefully evaluated the proposed method in multiple twitter datasets that are employed in sentiment and emotion classification using machine learning (ML) and deep learning (DL) approaches. In both approaches, extending word embedding with the proposed emoji embedding improved results. The DL-based approach achieved the highest f1-score of 70.01% for sentiment classification, and accuracy score of 56.36% for emotion classification. ML-based approach obtained accuracy score of 52.17% in emotion classification. © 2021 Elsevier Ltd","Emoji classification; Emoji lexicon; Emotion classification; Multi-label classification; Natural Language Processing; Pointwise Mutual Information; Sentiment analysis","Deep neural networks; Embeddings; Semantics; Sentiment analysis; Social networking (online); Classification methods; Embeddings; Emoji classification; Emoji lexicon; Emotion classification; Multi-labels; Pointwise mutual information; Sentiment analysis; Sentiment classification; Social media; Classification (of information)",Article,Scopus,2-s2.0-85120885680
"Araujo A.F., Gôlo M.P.S., Marcacini R.M.","57296646200;57287395500;37013380700;","Opinion mining for app reviews: an analysis of textual representation and predictive models",2022,"Automated Software Engineering","29","1","5","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119969463&doi=10.1007%2fs10515-021-00301-1&partnerID=40&md5=5d40eb0d9084b03467f2ad98ed088a8e","Popular mobile applications receive millions of user reviews. These reviews contain relevant information for software maintenance, such as bug reports and improvement suggestions. The review’s information is a valuable knowledge source for software requirements engineering since the apps review analysis helps make strategic decisions to improve the app quality. However, due to the large volume of texts, the manual extraction of the relevant information is an impracticable task. Opinion mining is the field of study for analyzing people’s sentiments and emotions through opinions expressed on the web, such as social networks, forums, and community platforms for products and services recommendation. In this paper, we investigate opinion mining for app reviews. In particular, we compare textual representation techniques for classification, sentiment analysis, and utility prediction from app reviews. We discuss and evaluate different techniques for the textual representation of reviews, from traditional Bag-of-Words (BoW) to the most recent state-of-the-art Neural Language models (NLM). Our findings show that the traditional Bag-of-Words model, combined with a careful analysis of text pre-processing techniques, is still competitive. It obtains results close to the NLM in the classification, sentiment analysis and utility prediction tasks. However, NLM proved to be more advantageous since they achieved very competitive performance in all the predictive tasks covered in this work, provide significant dimensionality reduction, and deals more adequately with semantic proximity between the reviews’ texts. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Machine learning; Mobile applications; Opinion mining; Requirements engineering","Application programs; Data mining; Information retrieval; Machine learning; Mobile computing; Quality control; Requirements engineering; Semantics; Bug reports; Knowledge sources; Language model; Mobile applications; Predictive models; Representation model; Requirement engineering; Sentiment analysis; Textual representation; User reviews; Sentiment analysis",Article,Scopus,2-s2.0-85119969463
"Yang J., Yang L.T., Gao Y., Liu H., Xie X.","57226789161;57203323020;57113859700;36500836500;35729832500;","An Incremental Boolean Tensor Factorization for Knowledge Reasoning in Artificial Intelligence of Things",2022,"IEEE Transactions on Industrial Informatics","18","5",,"3367","3376",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112667809&doi=10.1109%2fTII.2021.3100978&partnerID=40&md5=35e137718a20fb594747e582a3ef5a5b","Human-oriented and machine-generated data in cyber-physical-social systems are often complicated graph-structured. Graph-powered learning methods are conducive to discovering valuable knowledge from large-scale graph data and improving decision-making processes. However, due to the neglect of diverse relations among things, most existing knowledge reasoning studies are inherently flawed and inefficient in processing the heterogeneous graphs with high-order connectivity. Tensor, as a powerful and effective tool to model high-level semantic interactions between various things, can provide high-order Internet of things graph with new perspectives and possibilities. Therefore, this article innovatively proposes a collaborative artificial intelligence of things data analysis and application framework based on Boolean tensors, which supports the expression and fusion of heterogeneous graph and ultimately promotes the AI processing. In this context, we focus on developing an incremental Boolean tensor factorization (IBTF) approach for efficient knowledge reasoning to meet the requirements of real-time and high-level quality demands for intelligent services. To the best of our knowledge, we are the first to do this work. More concretely, we present factors update and binary features merge algorithms for the integrated graph tensors to avoid numerous repeated calculations of historical data. Experimental results on general synthetic datasets demonstrate that the IBTF approach proposed in this article guarantees nearly equal approximate accuracy while reducing execution time by dozens and even more of times. Furthermore, experimental evaluations and interpretability analysis on real-world datasets verify the practicality of the proposed framework and approach. © 2005-2012 IEEE.","Artificial intelligence of things (AIoT); Boolean tensor factorization; cyber-physical-social systems (CPSS); knowledge reasoning and discovery; tensor","Artificial intelligence; Decision making; Factorization; Graph algorithms; Learning systems; Semantics; Tensors; Application frameworks; Decision making process; Experimental evaluation; Heterogeneous graph; High level semantics; Intelligent Services; Real-world datasets; Tensor factorization; Graph structures",Article,Scopus,2-s2.0-85112667809
"Zhu H., Tiwari P., Ghoneim A., Hossain M.S.","57226194256;57193601962;57006360900;24066717900;","A Collaborative AI-Enabled Pretrained Language Model for AIoT Domain Question Answering",2022,"IEEE Transactions on Industrial Informatics","18","5",,"3387","3396",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110904742&doi=10.1109%2fTII.2021.3097183&partnerID=40&md5=e831d5da82ba79418751d221f9493907","Large-scale knowledge in the artificial intelligence of things (AIoT) field urgently needs effective models to understand human language and automatically answer questions. Pretrained language models achieve state-of-the-art performance on some question answering (QA) datasets, but few models can answer questions on AIoT domain knowledge. Currently, the AIoT domain lacks sufficient QA datasets and large-scale pretraining corpora. In this article, we propose RoBERTa_ AIoT to address the problem of the lack of high-quality large-scale labeled AIoT QA datasets. We construct an AIoT corpus to further pretrain RoBERTa and BERT. RoBERTa_ AIoT and BERT_ AIoT leverage unsupervised pretraining on a large corpus composed of AIoT-oriented Wikipedia webpages to learn more domain-specific context and improve performance on the AIoT QA tasks. To fine-tune and evaluate the model, we construct three AIoT QA datasets based on the community QA websites. We evaluate our approach on these datasets, and the experimental results demonstrate the significant improvements of our approach. © 2005-2012 IEEE.","Artificial intelligence of things (AIoT); BERT; domain-specific; question answering (QA); RoBERTa","Artificial intelligence; Computational linguistics; Natural language processing systems; Websites; Domain knowledge; Domain specific; Human language; Improve performance; Language model; Large corpora; Question Answering; State-of-the-art performance; Large dataset",Article,Scopus,2-s2.0-85110904742
"Niu G., Li B., Zhang Y., Sheng Y., Shi C., Li J., Pu S.","57191197891;56092633500;34874069700;57190747968;55447999200;57219635578;57394040900;","Joint semantics and data-driven path representation for knowledge graph reasoning",2022,"Neurocomputing","483",,,"249","261",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124544397&doi=10.1016%2fj.neucom.2022.02.011&partnerID=40&md5=7db70a5ad4c5eaf54b95dc514185c90e","Reasoning on a large-scale knowledge graph (KG) is of great importance for KG applications like question answering. The path-based reasoning models can leverage much information over paths other than pure triples in the KG but face several challenges. Firstly, all the existing path-based methods are data-driven, lacking explainability, namely how the path representations and the reasoning results are obtained with human-understandable explanations. Besides, some approaches either consider only relational paths or ignore the heterogeneity between entities and relations both in paths, which cannot capture the rich semantics of paths well. To address the above challenges, in this work, we propose a novel joint semantics and data-driven path representation that balances explainability and generalization in the framework of KG embedding. Specifically, we inject horn rules to obtain the condensed paths through a transparent and explainable path composition procedure. The entity converter is designed to transform entities along paths into the representations in the semantic level similar to relations for reducing the heterogeneity between entities and relations. The KGs, both with and without type information, are considered. Our proposed model is evaluated on two classes of tasks: link prediction and path query answering. The experimental results show that our model obtains significant performance gains over several state-of-the-art baselines. © 2022 Elsevier B.V.","Entity converting; Horn rules; Joint semantics and data-driven; Knowledge graph reasoning; Path representation","Graph theory; Semantics; Data driven; Entity converting; Horn rule; Joint semantic and data-driven; Knowledge graph reasoning; Knowledge graphs; Large-scales; Path representation; Path-based; Question Answering; Knowledge graph; article; embedding; human; prediction; reasoning; semantics",Article,Scopus,2-s2.0-85124544397
"Hao J., Sun H., Ren P., Wang J., Qi Q., Liao J.","57200189469;57190948696;57202487971;55742764200;14058739100;7402988692;","Query-aware video encoder for video moment retrieval",2022,"Neurocomputing","483",,,"72","86",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124387048&doi=10.1016%2fj.neucom.2022.01.085&partnerID=40&md5=d83347b7c28a258fa8bbc0adc323fc31","Given an untrimmed video and a sentence query, video moment retrieval is to locate a target video moment that semantically corresponds to the query. It is a challenging task that requires a joint understanding of natural language queries and video contents. However, video contains complex contents, including query-related and query-irrelevant contents, which brings difficulty for the joint understanding. To this end, we propose a query-aware video encoder to capture the query-related visual contents. Specifically, we design a query-guided block following each encoder layer to recalibrate the encoded visual features according to the query semantics. The core of query-guided block is a channel-level attention gating mechanism, which could selectively emphasize query-related visual contents and suppress query-irrelevant ones. Besides, to fully match with different levels of contents in videos, we learn hierarchical and structural query clues to guide the visual content capturing. We disentangle sentence query into a semantics graph and capture the local contexts inside the graph via a trilinear model as query clues. Extensive experiments on Charades-STA and TACoS datasets demonstrate the effectiveness of our approach, and we achieve the state-of-the-art on the two datasets. © 2022 Elsevier B.V.","Temporal sentence grounding; Video and language; Video moment retrieval","Natural language processing systems; Signal encoding; Natural language queries; Query semantics; Query video; Temporal sentence grounding; Video and language; Video contents; Video encoder; Video moment retrieval; Visual content; Visual feature; Semantics; article; attention; human; human experiment; information retrieval; language; semantics; videorecording",Article,Scopus,2-s2.0-85124387048
"Sun B., Feng S., Li Y., Liu J., Li K.","57224706098;57219635952;57219491385;57271592700;8711010900;","THINK: A novel conversation model for generating grammatically correct and coherent responses",2022,"Knowledge-Based Systems","242",,"108376","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124947923&doi=10.1016%2fj.knosys.2022.108376&partnerID=40&md5=f2df74f372d2771d5ae3573d67364dbb","Many existing conversation models that are based on the encoder–decoder framework incorporate complex encoders. These powerful encoders serve to enrich the context vectors, so that the generated responses are more diverse and informative. However, these approaches face two potential challenges. First, the high complexity of the encoder means relative simplicity of the decoder. There is a danger that the decoder becomes too simple to effectively capture previously generated information. As a result, the decoder may produce duplicated and self-contradicting responses. Second, by having a complex encoder, the model may generate incoherent responses because the complex context vectors may deviate from the true semantics of context. In this work, we propose a conversation model named “THINK” (Teamwork generation Hover around Impressive Noticeable Keywords) that is equipped with a complex decoder to avoid generating duplicated and self-contradicting responses. The model also simplifies the context vectors and increases the coherence of generated responses in a reasonable way. For this model, we propose Teamwork generation framework and Semantics extractor. Compared with other baselines, both automatic and human evaluation showed the advantages of our model. © 2022","Conversation model; Open-domain dialogue generation; Semantics extractor; Teamwork generation framework","Decoding; Signal encoding; Automatic evaluation; Context vector; Conversation model; Dialogue generations; Encoder-decoder; High complexity; Open-domain dialog generation; Semantic extractor; Simple++; Teamwork generation framework; Semantics",Article,Scopus,2-s2.0-85124947923
"Jin M., Zhang H., Zhu L., Sun J., Liu L.","57450030400;56012965100;55710390400;12645161300;55648218000;","Coarse-to-fine dual-level attention for video-text cross modal retrieval",2022,"Knowledge-Based Systems","242",,"108354","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124524815&doi=10.1016%2fj.knosys.2022.108354&partnerID=40&md5=7078f35329e0f6d489e51646da38dd69","The effective representation of video features plays an important role in video vs. text cross-modal retrieval, and many researchers either use a single modal feature of the video or simply combine multi-modal features of the video. This makes the learned video features less robust. To enhance the robustness of video feature representation, we use coarse-fine-grained parallel attention model and feature fusion module to learn more effective video feature representation. Among them, coarse-grained attention learns the relationship between different feature blocks in the same modality feature and fine-grained attention applies attention to global features and strengthens the connection between points. Coarse-grained attention and Fine-grained attention complement each other. We integrate multi-head attention network into the model to expand the receptive field for features, and use the feature fusion module to further reduce the semantic gap between different video modalities. Our proposed model architecture not only strengthens the relationship between global features and local features, but also compensates the differences between different modality features in the video. Evaluation on three widely used datasets AcitivityNet-Captions, MSRVTT and LSMDC demonstrates its effectiveness. © 2022 Elsevier B.V.","Coarse-fine-grained parallel attention; Feature fusion; Multi-head attention; Video vs. text cross-modal retrieval","Coarse-fine-grained parallel attention; Cross-modal; Feature representation; Features fusions; Fine grained; Fusion modules; Multi-head attention; Text cross-modal retrieval; Video features; Video vs.; Semantics",Article,Scopus,2-s2.0-85124524815
"Zhang J., Zhang L., Hui B., Tian L.","57447260500;56404489100;16506742000;57447751800;","Improving complex knowledge base question answering via structural information learning",2022,"Knowledge-Based Systems","242",,"108252","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124388523&doi=10.1016%2fj.knosys.2022.108252&partnerID=40&md5=f1b939eb3dcc80bb55171c0a0eba8856","Responding to complex questions is one of the most difficult and valuable goals of KBQA. Current efforts mainly follow two approaches to extract the in-depth semantics of questions. Information retrieval-based methods tend to encode questions directly and ignore the explicit analysis of the question structure. Besides, although retaining the analysis ability of question structure, semantic parsing-based methods rely on the expensive query graph labels and suffer from sparse reward due to wrong explorations. To benefit from both sides, this paper proposes a novel semantic parsing model, structural information restraint (SIR) for KBQA. SIR applies structural information of questions for reinforcement-based path reasoning for the first time. Specifically, SIR synthesizes the dependency tree, constituency tree, and the first token to build a composited structural attention (StrucAtt) and realizes reasoning without expensive query graphs labels. Such an attention mechanism improves the efficiency of path reasoning by distinguishing different knowledge paths, based on the relevance between path features and question structure. In addition, we also design a type-assisted reward based on answer concepts (person, location, etc.) instead of simple variable types (string, number, etc.), which alleviates the sparse reward problem effectively. Besides, the experiment results clearly show that our model achieves SOTA on CWQ, CQ, and WQSP datasets. © 2022 Elsevier B.V.","Attention; Knowledge base; Knowledge graph; Question answering","Forestry; Semantics; Trees (mathematics); 'current; Attention; Complex questions; Explicit analysis; Knowledge graphs; Query graph; Question Answering; Semantic parsing; Structural information; Structure semantic; Knowledge graph",Article,Scopus,2-s2.0-85124388523
"Adhikari A., Dutta B., Dutta A.","57188661398;36933713500;56662313800;","Finding most informative common ancestor in cross-ontological semantic similarity assessment: An intrinsic information content-based approach",2022,"Expert Systems with Applications","192",,"116281","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121902330&doi=10.1016%2fj.eswa.2021.116281&partnerID=40&md5=14ce085158edc489a097582fb7eab568","Semantic Similarity (SS) has become a long-standing research domain in artificial intelligence and cognitive science for measuring the strength of the semantic relationship between entities (e.g., words, documents). Several ontology-based SS measures have been proposed in the recent time due to their ability of mimicking the cognitive process of humans. Among them, intrinsic information content (IC) based approaches have shown a significant correlation with human assessment. The design principle of the existing intrinsic IC-based SS measures constrain themselves to be applicable in a single ontology. However, such SS measures can be leveraged within two ontologies with the help of identifying the most informative common ancestors (MICA) across the ontologies. Existing IC-based MICA identification algorithms follow string matching of the labels of the concepts. In this paper, we propose a novel intrinsic IC-based MICA finding algorithm that exploits two domain-ontologies for finding SS without using string matching of the labels. The proposed approach has been evaluated using a widely used benchmark dataset of medical terms. The experimental results show that the proposed IC-based approach can be a stepping stone to a new direction in the process of finding MICA over two ontologies. © 2021 Elsevier Ltd","Information retrieval; Information theory; Knowledge based systems; Ontology; Semantic similarity","Information retrieval; Information theory; Integrated circuits; Knowledge based systems; Mica; Search engines; Semantics; Content-based; Content-based approach; Information contents; Knowledge-based systems; Ontology's; Research domains; Semantic similarity; Semantic similarity measures; Similarity assessment; String matching; Ontology",Article,Scopus,2-s2.0-85121902330
"Liang S., Zuo W., Shi Z., Wang S., Wang J., Zuo X.","57191054076;7005474153;57203552226;55919171700;57202342610;55523640700;","A multi-level neural network for implicit causality detection in web texts",2022,"Neurocomputing","481",,,"121","132",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123884563&doi=10.1016%2fj.neucom.2022.01.076&partnerID=40&md5=a6e3c190e361565024e7fa0228b885e0","Mining causality from text is a complex and crucial natural language understanding task corresponding to human cognition. Existing studies on this subject can be divided into two categories: feature engineering-based and neural model-based methods. In this paper, we find that the former has incomplete coverage and intrinsic errors but provides prior knowledge, whereas the latter leverages context information but has insufficient causal inference. To address the limitations, we propose a novel causality detection model named MCDN, which explicitly models the causal reasoning process, and exploits the advantages of both methods. Specifically, we adopt multi-head self-attention to acquire semantic features at the word level and develop the SCRN to infer causality at the segment level. To the best of our knowledge, this is the first time the Relation Network is applied with regard to the causality tasks. The experimental results demonstrate that: i) the proposed method outperforms the strong baselines on causality detection; ii) further analysis manifests the effectiveness and robustness of MCDN. © 2022 Elsevier B.V.","Causality Detection; Multi-level Neural Network; Relation Network; Transformer","Causality detection; Feature engineerings; Human cognition; Multi-level neural network; Multilevels; Natural language understanding; Neural-networks; Relation network; Transformer; Web texts; Semantics; article; attention; causal reasoning; human; human experiment",Article,Scopus,2-s2.0-85123884563
"Hu B., Liu Y., Chen N., Wang L., Liu N., Cao X.","57169311100;55960857900;57209551610;57219270559;57212011789;57222053076;","SEGCN-DCR: A syntax-enhanced event detection framework with decoupled classification rebalance",2022,"Neurocomputing","481",,,"55","66",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123788604&doi=10.1016%2fj.neucom.2022.01.069&partnerID=40&md5=be79aa997f52fc0150602d41ea3d1fec","Event Detection (ED) is a pivotal sub-task of Event Extraction(EE). It aims to locate triggers and categorize them into specific event types. Recent researches on ED have shown that graph convolutional neural networks with syntactic information can achieve advanced performance. However, these methods ignore the implicit importance score of tokens. This will weaken their ability of identifying trigger words. In addition, due to the long-tailed distribution in the corpus, previous methods perform poorly on sparsely labeled trigger words and are prone to overfitting on densely labeled ones. In this paper, we propose a Syntax-Enhanced GCN framework with a Decoupled Classification Rebalance mechanism (SEGCN-DCR) to address the above issues. Specifically, we exploit a tree-structured module based on dependency structure to reduce the noise by capturing global hierarchical syntactic information, and DCR mechanism to rescale the classifier weights, which makes classifier decision boundaries more reasonable. Experiments on benchmark ACE2005 show that the proposed method acquires state-of-the-art performance. © 2022","Deep Learning; Event Detection; GCN; Imbalanced Classification; LSTM; Natural Language Processing","Benchmarking; Classification (of information); Convolutional neural networks; Long short-term memory; Natural language processing systems; Deep learning; Detection framework; Event Types; Events detection; Events extractions; GCN; Imbalanced classification; LSTM; Subtask; Syntactic information; Syntactics; article; classifier; deep learning; natural language processing; noise",Article,Scopus,2-s2.0-85123788604
"Song X., Li J., Lei Q., Zhao W., Chen Y., Mian A.","57218316338;57196717102;57432041200;57221014709;7601428336;7006066881;","Bi-CLKT: Bi-Graph Contrastive Learning based Knowledge Tracing",2022,"Knowledge-Based Systems","241",,"108274","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124416078&doi=10.1016%2fj.knosys.2022.108274&partnerID=40&md5=a3971f963664dc69641b1ed80e9eaa7f","The goal of Knowledge Tracing (KT) is to estimate how well students have mastered a concept based on their historical learning of related exercises. The benefit of knowledge tracing is that students’ learning plans can be better organised and adjusted, and interventions can be made when necessary. With the recent rise of deep learning, Deep Knowledge Tracing (DKT) has utilised Recurrent Neural Networks (RNNs) to accomplish this task with some success. Other works have attempted to introduce Graph Neural Networks (GNNs) and redefine the task accordingly to achieve significant improvements. However, these efforts suffer from at least one of the following drawbacks: (1) they pay too much attention to details of the nodes rather than to high-level semantic information; (2) they struggle to effectively establish spatial associations and complex structures of the nodes; and (3) they represent either concepts or exercises only, without integrating them. Inspired by recent advances in self-supervised learning, we propose a Bi-Graph Contrastive Learning based Knowledge Tracing (Bi-CLKT) to address these limitations. Specifically, we design a two-layer comparative learning scheme based on an “exercise-to-exercise” (E2E) relational subgraph. It involves node-level contrastive learning of subgraphs to obtain discriminative representations of exercises, and graph-level contrastive learning to obtain discriminative representations of concepts. Moreover, we designed a joint contrastive loss to obtain better representations and hence better prediction performance. Also, we explored two different variants, using RNN and memory-augmented neural networks as the prediction layer for comparison to obtain better representations of exercises and concepts respectively. Extensive experiments on four real-world datasets show that the proposed Bi-CLKT and its variants outperform other baseline models. © 2022 Elsevier B.V.","Contrastive learning; Deep knowledge tracing; Graph neural network; Intelligent tutoring systems; Self-supervised learning","Computer aided instruction; Deep neural networks; Multilayer neural networks; Recurrent neural networks; Semantics; Supervised learning; Contrastive learning; Deep knowledge; Deep knowledge tracing; Graph neural networks; Intelligent tutoring; Intelligent tutoring system; Knowledge tracings; Self-supervised learning; Subgraphs; Tutoring system; Graph neural networks",Article,Scopus,2-s2.0-85124416078
"Zhang J., Zhang P., Guo D., Zhou Y., Wu Y., Yang B., Lin Y.","57195437554;57202889449;57213190082;57203099927;56097892900;57200878428;57191432858;","Automatic repetition instruction generation for air traffic control training using multi-task learning with an improved copy network",2022,"Knowledge-Based Systems","241",,"108232","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124023494&doi=10.1016%2fj.knosys.2022.108232&partnerID=40&md5=256c9e4268fac0af8369b65f835a79ad","To eliminate the need for human pseudo-pilots in air traffic controller training, a multi-task framework with a copy mechanism is proposed to automatically generate a repetition instruction, which can greatly reduce human and device resources. The proposed framework is implemented using a sequence-to-sequence architecture and is optimized through multi-task learning, including text instruction understanding (TIU) and repetition instruction generation (RIG). In the proposed framework, a shared encoder is designed to learn the representations from the input sequence. Two task-specific attention modules are proposed to extract task-specific features, and different decoders are applied to generate the TIU and RIG outputs. The TIU module provides both word- and sentence-level implicit semantic representations through slot filling and intent detection tasks. In the decoding procedure of the RIG task, the slot context vector is regarded as an additional input used to solve the problem of ambivalent words. Simultaneously, an improved copy network based on multi-task learning is proposed to consider the correlations between the input instruction and the output repetition in the RIG module. To avoid a loss imbalance for the TIU and RIG tasks, the gradient normalization algorithm is applied to learn the loss weights automatically by adjusting the loss gradient. Finally, the proposed framework was trained and evaluated using a real-world air traffic control corpus. The experiment results demonstrate that the proposed framework significantly outperforms other state-of-the-art methods for the RIG task, achieving a novel integrated metric of 97.19%. © 2022 Elsevier B.V.","Air traffic controller training; Automatic repetition instruction; Copy mechanism; Multi-task learning","Air navigation; Control towers; Controllers; Decoding; Learning systems; Semantics; Air traffic controller; Air traffic controller training; Air traffics; Automatic repetition instruction; Copy mechanism; Device resources; Instruction generations; Learn+; Multi tasks; Text instruction; Air traffic control",Article,Scopus,2-s2.0-85124023494
"Liu Y., Ng M.K.","57202718923;34571761900;","Deep neural network compression by Tucker decomposition with nonlinear response",2022,"Knowledge-Based Systems","241",,"108171","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123983849&doi=10.1016%2fj.knosys.2022.108171&partnerID=40&md5=50da20b529828241e34332172d87c6b6","Deep neural networks have shown impressive performance in many areas, including computer vision and natural language processing. Millions of parameters in deep neural network limit its deployment in low-end devices due to intensive memory and expensive computational cost. In the literature, several network compression techniques based on tensor decompositions have been proposed to compress deep neural networks. Existing techniques are designed in each network unit by approximating linear response or kernel tensor using various tensor decomposition methods. What is more, research has shown that there exists significant redundancy between different filters and feature channels of kernel tensor in each convolution layer. In this paper, we propose a new algorithm to compress deep neural network by considering both nonlinear response and the multilinear low-rank constraint in the kernel tensor. To overcome the resulted difficulty of nonconvex optimization, we propose a convex relaxation scheme such that it can be solved by alternating direction method of multipliers (ADMM) directly. Thus, the Tucker-2 rank and the feature matrix of Tucker decomposition can be determined simultaneously. The effectiveness of the proposed method is evaluated on CIFAR-10 and large-scale ILSVRC12 datasets for CNNs including ResNet-18, AlexNet and GoogleNet. According to our numerical computation, the proposed method is able to obtain highly reduction in model size with a small loss in accuracy. The compression performance of the proposed method is better than existing methods. © 2022 Elsevier B.V.","Convolution kernel; Deep neural network compression; Low rank approximation; Optimization; Tucker decomposition","Approximation theory; Convolution; Large dataset; Natural language processing systems; Numerical methods; Relaxation processes; Tensors; Convolution kernel; Deep neural network compression; End-devices; Low rank approximations; Network compression; Non-linear response; Optimisations; Performance; Tensor decomposition; Tucker decompositions; Deep neural networks",Article,Scopus,2-s2.0-85123983849
"Hagberg E., Hagerman D., Johansson R., Hosseini N., Liu J., Björnsson E., Alvén J., Hjelmgren O.","57203621728;57463735700;51863811000;6602661998;57463680600;57463906300;56733073400;54386645800;","Semi-supervised learning with natural language processing for right ventricle classification in echocardiography—a scalable approach",2022,"Computers in Biology and Medicine","143",,"105282","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125120147&doi=10.1016%2fj.compbiomed.2022.105282&partnerID=40&md5=e21e86122feb5563db5ea647eeca4d99","We created a deep learning model, trained on text classified by natural language processing (NLP), to assess right ventricular (RV) size and function from echocardiographic images. We included 12,684 examinations with corresponding written reports for text classification. After manual annotation of 1489 reports, we trained an NLP model to classify the remaining 10,651 reports. A view classifier was developed to select the 4-chamber or RV-focused view from an echocardiographic examination (n = 539). The final models were two image classification models trained on the predicted labels from the combined manual annotation and NLP models and the corresponding echocardiographic view to assess RV function (training set n = 11,008) and size (training set n = 9951. The text classifier identified impaired RV function with 99% sensitivity and 98% specificity and RV enlargement with 98% sensitivity and 98% specificity. The view classification model identified the 4-chamber view with 92% accuracy and the RV-focused view with 73% accuracy. The image classification models identified impaired RV function with 93% sensitivity and 72% specificity and an enlarged RV with 80% sensitivity and 85% specificity; agreement with the written reports was substantial (both κ = 0.65). Our findings show that models for automatic image assessment can be trained to classify RV size and function by using model-annotated data from written echocardiography reports. This pipeline for auto-annotation of the echocardiographic images, using a NLP model with medical reports as input, can be used to train an image-assessment model without manual annotation of images and enables fast and inexpensive expansion of the training dataset when needed. © 2022","Algorithm; Automation; Echocardiography; Machine learning; Natural language processing; Right ventricle; Scalable; Semi-supervised","Classification (of information); Deep learning; Echocardiography; Image annotation; Learning algorithms; Medical imaging; Text processing; Classification models; Echocardiographic images; Images classification; Manual annotation; Processing model; Right ventricle; Right ventricular; Right ventricular functions; Scalable; Semi-supervised; Natural language processing systems",Article,Scopus,2-s2.0-85125120147
"Kozik R., Kula S., Choraś M., Woźniak M.","26654240400;35145879300;55890809000;35410703700;","Technical solution to counter potential crime: Text analysis to detect fake news and disinformation",2022,"Journal of Computational Science","60",,"101576","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125017121&doi=10.1016%2fj.jocs.2022.101576&partnerID=40&md5=507052d174e9d803984487d8ee16175f","Fake news detection is a challenging and complex task. Yet, several approaches to deal with this problem have already been proposed. The majority of solutions employ the NLP-based approach, where various architectures of a deep artificial neural network are proposed. However, as the experiments show, different NLP-based solutions have great performance in a single domain, but transferring them to another one is tedious. Therefore, in this paper, we propose a hybrid approach to dealing with this problem. Instead of retraining one big model on different types of data, we bundle several smaller models using meta-learning techniques. This paper is an extension of our previous research presented in Kula et al. (2021). © 2022 Elsevier B.V.","Deep learning; Fake news detection; Natural Language Processing; SocialTruth; Transformers","Deep learning; Fake detection; Neural networks; Complex task; Deep learning; Fake news detection; Hybrid approach; Meta-learning techniques; Performance; Single domains; Socialtruth; Technical solutions; Transformer; Natural language processing systems",Article,Scopus,2-s2.0-85125017121
"Chen T., Zhou L., Wang N., Chen X.","57216901142;57188722398;15125132600;57461048700;","Joint entity and relation extraction with position-aware attention and relation embedding",2022,"Applied Soft Computing","119",,"108604","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125015035&doi=10.1016%2fj.asoc.2022.108604&partnerID=40&md5=a1c9d78033f5158fd832189b7f10c228","The joint extraction of entities and relations is an important task in natural language processing, which aims to obtain all relational triples in plain text. However, few existing methods excel in solving the overlapping triple problem. Moreover, most methods ignore the position and order of the words in the entity in the entity extraction process, which affects the performance of triples extraction. To solve these problems, a joint extraction model with position-aware attention and relation embedding is proposed, named PARE-Joint. The proposed model first recognizes the subjects, and then uses the subject and relation guided attention network to learn the enhanced sentence representation and determine the corresponding objects. In this way, the interaction between entities and relations is captured, and the overlapping triple problem can be better resolved. In addition, taking into account the important role of word order in the entity for triple extraction, the position-aware attention mechanism is used to extract the subjects and the objects in the sentences, respectively. The experimental results demonstrate that our model can solve the overlapping triple problem more effectively and outperform other baselines on four public datasets. © 2022 Elsevier B.V.","Attention mechanism; Entity recognition; Gate mechanism; Relation embedding; Relation extraction","Embeddings; Natural language processing systems; Attention mechanisms; Entity extractions; Entity recognition; Excel; Extraction process; Gate mechanism; Performance; Plain text; Relation embedding; Relation extraction; Extraction",Article,Scopus,2-s2.0-85125015035
"Cascallar-Fuentes A., Gallego-Fernández J., Ramos-Soto A., Saunders-Estévez A., Bugarín-Diz A.","57217384301;57457223000;55037491300;57457223100;57219228441;","Automatic generation of textual descriptions in data-to-text systems using a fuzzy temporal ontology: Application in air quality index data series",2022,"Applied Soft Computing","119",,"108612","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124802519&doi=10.1016%2fj.asoc.2022.108612&partnerID=40&md5=410da32aa38cc3caf6f89fe3e767241f","In this paper we present a model based on computational intelligence and natural language generation for the automatic generation of textual summaries from numerical data series, aiming to provide insights which help users to understand the relevant information hidden in the data. Our model includes a fuzzy temporal ontology with temporal references which addresses the problem of managing imprecise temporal knowledge, which is relevant in data series. We fully describe a real use case of application in the environmental information systems field, providing linguistic descriptions about the air quality index (AQI), which is a very well-known indicator provided by all meteorological agencies worldwide. We consider two different data sources of real AQI data provided by the official Galician (NW Spain) Meteorology Agency: (i) AQI distribution in the stations of the meteorological observation network and (ii) time series which describe the state and evolution of the AQI in each meteorological station. Both application models were evaluated following the current standards and good practices of manual human expert evaluation of the Natural Language Generation field. Assessment results by two experts meteorologists were very satisfactory, which empirically confirm that the proposed textual descriptions fit this type of data and service both in content and layout. © 2022 The Authors","Data to text systems; Fuzzy linguistic terms; Linguistic descriptions of data; Natural language generation","Air quality; Meteorology; Natural language processing systems; Ontology; Air quality indices; Automatic Generation; Data series; Data to text system; Fuzzy linguistic term; Fuzzy linguistics; Linguistic description of datum; Linguistic terms; Natural language generation; Textual description; Linguistics",Article,Scopus,2-s2.0-85124802519
"Zeng H., Cui X.","57445725000;57195974967;","SimCLRT: A Simple Framework for Contrastive Learning of Rumor Tracking",2022,"Engineering Applications of Artificial Intelligence","110",,"104757","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124791290&doi=10.1016%2fj.engappai.2022.104757&partnerID=40&md5=d070ceb19b47db5799874a7c71fbc8ae","As the second stage of the rumor defeat task pipeline, rumor tracking aims to filter tweets related to a specific event. However, due to the different levels of attention of different events, events related to fewer tweets may be masked by related to more tweets. In some research, the researchers give up detecting the events containing a small number of tweets for better results. To this end, we propose a Simple Framework for Contrastive Learning of Rumor Tracking (SimCLRT)-a novel rumor tracking framework that uses contrastive learning to alleviate the cover between tweets. SimCLRT contains three variants SimCLRT-CNN, SimCLRT-Linear, and SimCLRT-RNN. We conduct experiments on the two commonly used rumor tracking datasets PHEME and RumorEval. The results show that SimCLRT completely defeated baselines. Like the detection performance on the events that contain many tweets, SimCLRT can also effectively detect events containing a small number of tweets. Furthermore, we compare and analyze the performance of SimCLRT variants. SimCLRT-CNN is the model that performs best in our experiments. Although SimCLRT-Linear has a slight advantage on the RumorEval dataset, its robustness is weaker than SimCLRT-RNN and SimCLRT-CNN. If in a long text environment, we consider SimCLRT-RNN will perform more competitively. © 2022 The Author(s)","Contrastive learning; Deep learning; Natural language processing; Rumor tracking; Text classification","Classification (of information); Deep learning; Text processing; Compare and analyze; Contrastive learning; Deep learning; Detection performance; Rumor tracking; Simple++; Task pipelines; Text classification; Natural language processing systems",Article,Scopus,2-s2.0-85124791290
"Viscosi C., Fidelbo P., Benedetto A., Varvarà M., Ferrante M.","57207219096;57452530200;57371116800;57195772621;57452070700;","Selection of diagnosis with oncologic relevance information from histopathology free text reports: A machine learning approach",2022,"International Journal of Medical Informatics","160",,"104714","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124587523&doi=10.1016%2fj.ijmedinf.2022.104714&partnerID=40&md5=03081161798f97cfacb5878581e839d4","Histopathology reports are a primary data source for the case definition phase of a Cancer Registry. By reading the histopathology report, the operator that evaluates an oncology case can define the morphology and topography of cancer, and validate the case with the highest diagnosis base. The key problem of the Catania-Messina-Enna Integrated Cancer Registry (RTI) is that these reports are written in natural language and relevant information for cancer evaluation is only a little part of the total annual histopathological reports. In this population-based retrospective cohort study, we try to optimize the working time spent by the RTI operators in seeking and selecting the right information among the histopathology reports in the east Sicily population, by developing a binary classifier on a training set of labeled historical data and validating its outcome by a test set of labeled data created by the operators during the years. Using a machine learning algorithm we built a classification model that evaluates each free text report and returns a score that indicates the probability that it contains oncologic relevant information. The best performing algorithm, among the eight analyzed in this study, was the LightGBM that reached an F1-Score of 98.9%. Using the chosen classifier we shortened the time for case evaluation, improving the timeliness of cancer statistics. © 2022","Binary classification; Cancer registry; Machine learning; Natural language processing","Classification (of information); Computer aided diagnosis; Learning algorithms; Machine learning; Natural language processing systems; Personnel training; Population statistics; Text processing; Topography; Binary classification; Cancer registries; Cohort studies; Definition phase; Free texts; Machine learning approaches; Natural languages; Primary data source; Time-spent; Working time; Diseases; article; binary classification; cancer model; cancer registry; cancer statistics; classifier; cohort analysis; diagnosis; histopathology; human; human experiment; machine learning; natural language processing; probability; retrospective study; Sicily; timeliness; working time",Article,Scopus,2-s2.0-85124587523
"Ma J.W., Leite F.","57192367343;23097703400;","Performance boosting of conventional deep learning-based semantic segmentation leveraging unsupervised clustering",2022,"Automation in Construction","136",,"104167","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124537832&doi=10.1016%2fj.autcon.2022.104167&partnerID=40&md5=1add189518d2cbaac841dbed862fcf48","In Scan-to-BIM, semantically understanding 3D point clouds is an essential process that must precede 3D BIM elements generation. The main idea of this research stemmed from an assumption where recognizing a group of points would be simpler than assigning label per point from the machine's perspective when using deep learning classifiers. To validate our assumption, conventional point-wise classification problem was formulated as segment-wise classification problem leveraging unsupervised clustering. A single parameter executable hierarchical density-based algorithm and PointNet-based algorithms are chosen for segmentation and classification purposes. Using same baseline architecture, our segment-wise classification framework showed performance boosts of 1.12% and 7.66% for S3DIS and 3DFacilities compared to point-wise classification approach. Solving the nature of class imbalance problem and dataset augmentation ability are determined to be the contributing factors for showing superior results. Given semantic segmentation deep learning algorithm, our proposed framework provides an opportunity to improve the performance leveraging unsupervised method. © 2022 Elsevier B.V.","Deep learning; Hierarchical density-based spatial clustering of applications with noise; Point clouds; Scan-to-BIM; Semantic segmentation","Architectural design; Deep learning; Learning algorithms; Semantic Segmentation; 3D point cloud; Deep learning; Density-based spatial clustering of applications with noise; Hierarchical density-based spatial clustering of application with noise; Performance; Point wise; Point-clouds; Scan-to-BIM; Semantic segmentation; Unsupervised clustering; Semantics",Article,Scopus,2-s2.0-85124537832
"Barrows E., Martin K., Smith T.","57448993000;57449107900;57449332600;","Markup language for chemical process control and simulation",2022,"Computers and Chemical Engineering","160",,"107702","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124487386&doi=10.1016%2fj.compchemeng.2022.107702&partnerID=40&md5=d5389fd19a81f48a8edb1fea212a97de","Exothermic reactions can yield different products, depending upon the available heat energy, a consequence of the reaction rate. To control the yield of desired products, control of the rate of temperature rise is used. The acceptable range of temperature vs. time can be expressed in various ways, e. g., functions, individual temperature values. To afford humans this variety, and to facilitate the transfer of data among computational tools from multiple vendors, such as calorimetry and analysis tools, use of semantic labels, compatible with existing tools, has been implemented. An extension of XML to support expression of the range of acceptable temperature behavior vs. time as a reaction proceeds is described herein. A markup language organized to express the rich ontology of concepts used in process safety seems warranted, and though chemistry schema exist, none was found that specified the reactor temperature boundaries concept. We propose schema components for this concept. © 2022 Elsevier Ltd","Process composition; Safety; Semantic labels; Simulation; Synthesis automation; Temperature profile; XML","Computer simulation languages; Data transfer; Hypertext systems; Process control; XML; Chemical process control; Chemical process simulation; Heat energy; Process compositions; Product control; Reactions rates; Semantic labels; Simulation; Synthesis automation; Temperature profiles; Semantics",Article,Scopus,2-s2.0-85124487386
"Li Z., Kormilitzin A., Fernandes M., Vaci N., Liu Q., Newby D., Goodday S., Smith T., Nevado-Holgado A.J., Winchester L.","57438655500;21834209900;57222390668;56940878000;57192163000;55341862800;56685690400;57217434086;36490342200;7003849660;","Validation of UK Biobank data for mental health outcomes: A pilot study using secondary care electronic health records",2022,"International Journal of Medical Informatics","160",,"104704","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124408917&doi=10.1016%2fj.ijmedinf.2022.104704&partnerID=40&md5=503802b9382b60ad92b61499e01c6ddf","UK Biobank (UKB) is widely employed to investigate mental health disorders and related exposures; however, its applicability and relevance in a clinical setting and the assumptions required have not been sufficiently and systematically investigated. Here, we present the first validation study using secondary care mental health data with linkage to UKB from Oxford - Clinical Record Interactive Search (CRIS) focusing on comparison of demographic information, diagnostic outcome, medication record and cognitive test results, with missing data and the implied bias from both resources depicted. We applied a natural language processing model to extract information embedded in unstructured text from clinical notes and attachments. Using a contingency table we compared the demographic information recorded in UKB and CRIS. We calculated the positive predictive value (PPV, proportion of true positives cases detected) for mental health diagnosis and relevant medication. Amongst the cohort of 854 subjects, PPVs for any mental health diagnosis for dementia, depression, bipolar disorder and schizophrenia were 41.6%, and were 59.5%, 12.5%, 50.0% and 52.6%, respectively. Self-reported medication records in UKB had general PPV of 47.0%, with the prevalence of frequently prescribed medicines to each typical mental health disorder considerably different from the information provided by CRIS. UKB is highly multimodal, but with limited follow-up records, whereas CRIS offers a longitudinal high-resolution clinical picture with more than ten years of observations. The linkage of both datasets will reduce the self-report bias and synergistically augment diverse modalities into a unified resource to facilitate more robust research in mental health. © 2022 The Authors","Data resource; Linkage studies; Mental health; Neuro-epidemiology; UK Biobank; Validation study","Diseases; Population statistics; Biobanks; Clinical records; Data resources; Health disorders; Interactive search; Linkage study; Mental health; Neuro-epidemiology; UK biobank; Validation study; Natural language processing systems",Article,Scopus,2-s2.0-85124408917
"Hain D.S., Jurowetzki R., Buchmann T., Wolf P.","56436208900;56436805200;55824611800;57203328809;","A text-embedding-based approach to measuring patent-to-patent technological similarity",2022,"Technological Forecasting and Social Change","177",,"121559","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124302402&doi=10.1016%2fj.techfore.2022.121559&partnerID=40&md5=b96075a120f6812de74d7ff5e9a3d714","This paper describes an efficiently scaleable approach to measuring technological similarity between patents by combining embedding techniques from natural language processing with nearest-neighbor approximation. Using this methodology, we are able to compute similarities between all existing patents, which in turn enables us to represent the whole patent universe as a technological network. We validate both technological signature and similarity in various ways and, using the case of electric vehicle technologies, demonstrate their usefulness in measuring knowledge flows, mapping technological change, and creating patent quality indicators. This paper contributes to the growing literature on text-based indicators for patent analysis. We provide thorough documentation of our methods, including all code, and indicators at https://github.com/AI-Growth-Lab/patent_p2p_similarity_w2v). © 2022","Natural-language processing; Patent data; Patent landscaping; Patent quality; Technological similarity; Technology network","Data handling; Embeddings; Natural language processing systems; Roads and streets; Embedding technique; Embeddings; Nearest neighbor approximations; Patent datum; Patent landscaping; Patent quality; Scaleable; Technological networks; Technological similarity; Technology network; Patents and inventions; data processing; data quality; electric vehicle; nearest neighbor analysis; panel data; technology",Article,Scopus,2-s2.0-85124302402
"Song H., Wang S., Liu Y., Wang Y.","57444555700;36658014200;57243507800;57021610600;","Predicate-attention neural model for Chinese semantic role labeling",2022,"Computers and Electrical Engineering","99",,"107741","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124268191&doi=10.1016%2fj.compeleceng.2022.107741&partnerID=40&md5=4cc1914112127d57d33827c61e0b2bee","Semantic role labeling functions to convey the meaning of a sentence through forming a predicate-argument structure directed at the specific predicate. In recent years, end-to-end semantic role labeling methods associated with the deep neural network have received significant attention in the field of computational linguistics. Moreover, end-to-end semantic role labeling methods have demonstrated a beneficial capacity to reduce the incompleteness caused by handcrafted features, which is an observed short-coming of traditional Chinese role labeling methods. However, the critical focus of sentences are frequently lost as a result of existing semantic role labeling structures attributing equal importance to every single word, instead of the overall concept denoted by particular terms. Hence, the performance and function ability of deep neural network models is reduced. In this paper, we introduce a specific attention mechanism based on the established predicate. This mechanism would automatically calculate the weighted contributions of each word, and the corresponding Part-of-Speech, in order to accurately represent the general fundamental ideas of the sentence. In addition, we extended the Bidirectional LSTM using two different semantic role constraint methods, to effectively utilize the dependency and constraint relationships among different semantic role tags, hereby further improving the performance of the whole neural Chinese semantic role labeling model. Experimental results demonstrate the efficacy of our proposed model through providing a baseline that allows for meaningful comparisons, inferring that both weighted contributions of the predicate, and semantic role constraints can help significantly refine the overall model function. © 2022 Elsevier Ltd","Argument classification; Argument identification; Attention mechanism; Chinese semantic role labeling; Semantic parsing","Deep neural networks; Long short-term memory; Semantics; Syntactics; Argument classification; Argument identifications; Attention mechanisms; Chinese semantic role labeling; End to end; Labeling methods; Performance; Semantic parsing; Semantic role labeling; Semantic roles; Classification (of information)",Article,Scopus,2-s2.0-85124268191
"Mi C., Xie L., Zhang Y.","56437926000;35294300000;57280635900;","Improving data augmentation for low resource speech-to-text translation with diverse paraphrasing",2022,"Neural Networks","148",,,"194","205",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124238365&doi=10.1016%2fj.neunet.2022.01.016&partnerID=40&md5=ba54c24412ee431f8981d24dfb65cfba","High quality end-to-end speech translation model relies on a large scale of speech-to-text training data, which is usually scarce or even unavailable for some low-resource language pairs. To overcome this, we propose a target-side data augmentation method for low-resource language speech translation. In particular, we first generate large-scale target-side paraphrases based on a paraphrase generation model which incorporates several statistical machine translation (SMT) features and the commonly used recurrent neural network (RNN) feature. Then, a filtering model which consists of semantic similarity and speech–word pair co-occurrence was proposed to select the highest scoring source speech–target paraphrase pairs from candidates. Experimental results on English, Arabic, German, Latvian, Estonian, Slovenian and Swedish paraphrase generation show that the proposed method achieves significant and consistent improvements over several strong baseline models on PPDB datasets (http://paraphrase.org/). To introduce the results of paraphrase generation into the low-resource speech translation, we propose two strategies: audio–text pairs recombination and multiple references training. Experimental results show that the speech translation models trained on new audio–text datasets which combines the paraphrase generation results lead to substantial improvements over baselines, especially on low-resource languages. © 2022 Elsevier Ltd","Data augmentation; Paraphrasing; Speech translation","Computational linguistics; Computer aided language translation; Neural machine translation; Recurrent neural networks; Semantics; Speech transmission; Data augmentation; End to end; High quality; Large-scales; Low resource languages; Paraphrasing; Speech translation; Text training; Training data; Translation models; Speech; article; filtration; human; human experiment; language; recurrent neural network; speech",Article,Scopus,2-s2.0-85124238365
"Candaş A.B., Tokdemir O.B.","57443100400;6507059404;","Automated Identification of Vagueness in the FIDIC Silver Book Conditions of Contract",2022,"Journal of Construction Engineering and Management","148","4","04022007","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124196396&doi=10.1061%2f%28ASCE%29CO.1943-7862.0002254&partnerID=40&md5=f86042259ff7724cdbc7416889fce870","Contract conditions are crucial as they outline an agreement between different parties. The semantic terms in contract conditions need to be precisely designated. Where these conditions contain vague meanings, the interpretation of the conditions will vary, especially since the parties of the contract will be differently motivated to pursue their different expectations from it. The vague terms in contract conditions may thus cause a dispute and conflict among the parties that can jeopardize the eventual success of a construction project. The conventional practice of identifying vagueness in construction contract conditions is done manually, which is prone to error, time-consuming, and requires expert involvement. This study develops a methodology to automate the identification of vague terms in construction contract conditions with the sequential application of natural language processing (NLP) and machine learning (ML) techniques. Morphological and lexical analysis procedures are used to evaluate the corpus data obtained from a widely used typical construction contract published by International Federation of Consulting Engineers (FIDIC). Classifications of contract conditions in the corpus data are searched using several supervised ML techniques to determine the best performing classifier. The results show that the developed methodology reduces time spent on contract review, is reliable with a high level of accuracy in predicting the presence of vagueness, and removes dependence on expert participation in the contract review processes. © 2022 American Society of Civil Engineers.","Classification; Conditions of contract; Contract administration; Contract preparation; Machine learning (ML); Natural language processing (NLP); Rule-based; Supervised learning; Vagueness","Learning algorithms; Machine learning; Semantics; Silver; Condition; Condition of contract; Construction contract; Contract administration; Contract conditions; Contract preparation; Machine learning; Natural language processing; Rule based; Vagueness; Natural language processing systems",Article,Scopus,2-s2.0-85124196396
"Uronen L., Salanterä S., Hakala K., Hartiala J., Moen H.","57194025115;6701804966;55307771500;7005510609;25655389700;","Combining supervised and unsupervised named entity recognition to detect psychosocial risk factors in occupational health checks",2022,"International Journal of Medical Informatics","160",,"104695","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124195123&doi=10.1016%2fj.ijmedinf.2022.104695&partnerID=40&md5=5dc4fd77abea51e186599e295cbdff09","Introduction: In occupational health checks the information about psychosocial risk factors, which influence work ability, is documented in free text. Early detection of psychosocial risk factors helps occupational health care to choose the right and targeted interventions to maintain work capacity. In this study the aim was to evaluate if we can automate the recognition of these psychosocial risk factors in occupational health check electronic records with natural language processing (NLP). Materials and methods: We compared supervised and unsupervised named entity recognition (NER) to detect psychosocial risk factors from health checks’ documentation. Occupational health nurses have done these records. Results: Both methods found over 60% of psychosocial risk factors from the records. However, the combination of BERT-NER (supervised NER) and QExp (query expansion/paraphrasing) seems to be more suitable. In both methods the most (correct) risk factors were found in the work environment and equipment category. Conclusion: This study showed that it was possible to detect risk factors automatically from free-text documentation of health checks. It is possible to develop a text mining tool to automate the detection of psychosocial risk factors at an early stage. © 2022 The Author(s)","Health check; Occupational health; Psychosocial risk factors; Text mining, named entity recognition","Character recognition; Data mining; Industrial hygiene; Natural language processing systems; Occupational risks; Electronic records; Free texts; Health checks; Named entity recognition; Occupational health; Psychosocial risk factors; Risk factors; Text mining, named entity recognition; Work abilities; Work capacities; Health risks; article; documentation; human; mining; natural language processing; occupational health; occupational health nursing; risk assessment; risk factor; work environment",Article,Scopus,2-s2.0-85124195123
"Liang S., Luo Y., Meng Z.","55631770500;57211952520;56924375000;","Profiling Users for Question Answering Communities via Flow-Based Constrained Co-Embedding Model",2022,"ACM Transactions on Information Systems","40","2","3470565","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124098005&doi=10.1145%2f3470565&partnerID=40&md5=ecedf071dbffb2cbe73aecd57a7582fc","In this article, we study the task of user profiling in question answering communities (QACs). Previous user profiling algorithms suffer from a number of defects: they regard users and words as atomic units, leading to the mismatch between them; they are designed for other applications but not for QACs; and some semantic profiling algorithms do not co-embed users and words, leading to making the affinity measurement between them difficult. To improve the profiling performance, we propose a neural Flow-based Constrained Co-embedding Model, abbreviated as FCCM. FCCM jointly co-embeds the vector representations of both users and words in QACs such that the affinities between them can be semantically measured. Specifically, FCCM extends the standard variational auto-encoder model to enforce the inferred embeddings of users and words subject to the voting constraint, i.e., given a question and the users who answer this question in the community, representations of the users whose answers receive more votes are closer to the representations of the words associated with these answers, compared with representations of whose receiving fewer votes. In addition, FCCM integrates normalizing flow into the variational auto-encoder framework to avoid the assumption that the distributions of the embeddings are Gaussian, making the inferred embeddings fit the real distributions of the data better. Experimental results on a Chinese Zhihu question answering dataset demonstrate the effectiveness of our proposed FCCM model for the task of user profiling in QACs. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Co-embedding; question answering communities; user profiling; variational auto-encoder","Semantics; Signal encoding; User profile; Affinity measurement; Atomic units; Auto encoders; Co-embedding; Embeddings; Flow based; Performance; Question answering communities; User's profiling; Variational auto-encoder; Embeddings",Article,Scopus,2-s2.0-85124098005
"Pan Y., Liang S., Ren J., Meng Z., Zhang Q.","57441322300;55631770500;57211942506;56924375000;57208165743;","Personalized, Sequential, Attentive, Metric-Aware Product Search",2022,"ACM Transactions on Information Systems","40","2","3473337","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124094833&doi=10.1145%2f3473337&partnerID=40&md5=3dc05c92c69ceeec2a38cc63f42c281c","The task of personalized product search aims at retrieving a ranked list of products given a user's input query and his/her purchase history. To address this task, we propose the PSAM model, a Personalized, Sequential, Attentive and Metric-aware (PSAM) model, that learns the semantic representations of three different categories of entities, i.e., users, queries, and products, based on user sequential purchase historical data and the corresponding sequential queries. Specifically, a query-based attentive LSTM (QA-LSTM) model and an attention mechanism are designed to infer users dynamic embeddings, which is able to capture their short-term and long-term preferences. To obtain more fine-grained embeddings of the three categories of entities, a metric-aware objective is deployed in our model to force the inferred embeddings subject to the triangle inequality, which is a more realistic distance measurement for product search. Experiments conducted on four benchmark datasets show that our PSAM model significantly outperforms the state-of-the-art product search baselines in terms of effectiveness by up to 50.9% improvement under NDCG@20. Our visualization experiments further illustrate that the learned product embeddings are able to distinguish different types of products. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.","LSTM; metric learning; neural networks; personalized web search; Product search","Information retrieval; Long short-term memory; Semantics; Embeddings; Learn+; LSTM; Metric learning; Neural-networks; Personalized products; Personalized web searches; Product search; Semantic representation; User input; Embeddings",Article,Scopus,2-s2.0-85124094833
"Zhu N., Cao J., Lu X., Xiong H.","57191378660;57218980017;56017221800;7201935465;","Learning a Hierarchical Intent Model for Next-Item Recommendation",2022,"ACM Transactions on Information Systems","40","2","3473972","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124068013&doi=10.1145%2f3473972&partnerID=40&md5=17b3163263116156ad7932f8168b3cb4","A session-based recommender system (SBRS) captures users' evolving behaviors and recommends the next item by profiling users in terms of items in a session. User intent and user preference are two factors affecting his (her) decisions. Specifically, the former narrows the selection scope to some item types, while the latter helps to compare items of the same type. Most SBRSs assume one arbitrary user intent dominates a session when making a recommendation. However, this oversimplifies the reality that a session may involve multiple types of items conforming to different intents. In current SBRSs, items conforming to different user intents have cross-interference in profiling users for whom only one user intent is considered. Explicitly identifying and differentiating items conforming to various user intents can address this issue and model rich contextual information of a session. To this end, we design a framework modeling user intent and preference explicitly, which empowers the two factors to play their distinctive roles. Accordingly, we propose a key-array memory network (KA-MemNN) with a hierarchical intent tree to model coarse-to-fine user intents. The two-layer weighting unit (TLWU) in KA-MemNN detects user intents and generates intent-specific user profiles. Furthermore, the hierarchical semantic component (HSC) integrates multiple sets of intent-specific user profiles along with different user intent distributions to model a multi-intent user profile. The experimental results on real-world datasets demonstrate the superiority of KA-MemNN over selected state-of-the-art methods. © 2021 Association for Computing Machinery.","attention mechanism; intent modeling; memory network; representation learning; User modeling","Learning systems; Semantics; 'current; Attention mechanisms; Contextual information; Cross interference; Intent models; Memory network; Representation learning; User Modelling; User's preferences; User's profiles; User profile",Article,Scopus,2-s2.0-85124068013
"Da F., Kou G., Peng Y.","57437953700;57068290300;36022039700;","Deep learning based dual encoder retrieval model for citation recommendation",2022,"Technological Forecasting and Social Change","177",,"121545","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123954488&doi=10.1016%2fj.techfore.2022.121545&partnerID=40&md5=f857a135238c6a799d1542f34235ffb4","Citation recommendation recommends relevant documents to users based on their inputs and other information. Many traditional citation recommendation models use keywords to describe item attributes and ignore the semantics of sequences, which cause the relevance of the search results unsatisfactory. This paper proposes a deep-learning-based dual encoder retrieval (DER) model, which combines a text representation technique and a sentence pair matching approach, to improve the performance of citation recommendation. First, an input query and paper titles from publication databases are encoded to semantic vectors separately by two deep-learning-based encoders. Second, the semantic vector of the input query is matched with vectors that representing papers in the published databases by the multilayer perceptron approach to compute similarity scores. Finally, a list of documents, which are sorted in descending order of similarity scores, is generated. To validate the effectiveness of the proposed approach, it is compared with five baselines using a citation dataset. The results show that the proposed model achieves the best performance in terms of accuracy, recall, F1-measure, and AUC. In addition, we compare the DER (Glove) model with Google Scholar using a small example of twenty articles. The DER (Glove) model outperformed Google Scholar in seven recommendations, and tied in ten recommendations. © 2022","Citation recommendation; Deep learning retrieval approach; Dual encoder retrieval model","Deep learning; Semantics; Citation recommendation; Deep learning retrieval approach; Dual encoder retrieval model; Google scholar; Model use; Performance; Relevant documents; Retrieval models; Semantic vectors; Similarity scores; Signal encoding; data set; machine learning; modeling; vector",Article,Scopus,2-s2.0-85123954488
"Baskin I., Epshtein A., Ein-Eli Y.","7005310411;56492752500;7005503352;","Benchmarking machine learning methods for modeling physical properties of ionic liquids",2022,"Journal of Molecular Liquids","351",,"118616","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123852839&doi=10.1016%2fj.molliq.2022.118616&partnerID=40&md5=cb783bfb31f670fea9716f88ef1839f3","The great importance of the ability to quantitatively predict the properties of ionic liquids (ILs) using quantitative structure–property relationships (QSPR) models necessitates the understanding of which modern machine learning (ML) methods in combination with which types of molecular representations are preferable to use for this purpose. To address this problem, a large-scale benchmarking study of QSPR models built by combining three traditional ML methods and neural networks with seven different architectures with five types of molecular representations (in the form of either numerical molecular descriptors or SMILES text strings) to predict six important physical properties of ILs (density, electrical conductance, melting point, refractive index, surface tension, and viscosity) was carried out. The datasets include from 407 to 1204 diverse ILs composed of various organic and inorganic ions. QSPR models for predicting the properties of ILs at eight different temperatures were built using multi-task learning. The best combinations of ML methods and molecular representations were identified for each of the properties. A unified ranking system was introduced to rank and prioritize different ML methods and molecular representations. It was shown in this study that on average: (i) nonlinear ML methods perform much better than linear ones, (ii) neural networks perform better than traditional ML methods, (iii) Transformers, which are actively used in natural language processing (NLP), perform better than other types of neural networks due to the advanced ability to analyze chemical structures of ILs encoded into SMILES text strings. A special “component-wise” cross-validation scheme was applied to assess how much the predictive performance deteriorates for the ILs composed of cations and anions that are not present in the dataset. © 2022 Elsevier B.V.","Ionic liquids; Machine learning; Neural networks; OCHEM; QSPR","Benchmarking; Chemical analysis; Forecasting; Ionic liquids; Learning algorithms; Natural language processing systems; Neural networks; Numerical methods; Refractive index; Large-scales; Machine learning methods; Molecular descriptors; Molecular representations; Neural-networks; OCHEM; Property; Quantitative structure property relationships; Quantitative structure-property relationship models; Text string; Machine learning",Article,Scopus,2-s2.0-85123852839
"Papoutsoglou M., Rigas E.S., Kapitsaki G.M., Angelis L., Wachs J.","57128423800;55235701500;24801845800;6602528674;57195635739;","Online labour market analytics for the green economy: The case of electric vehicles",2022,"Technological Forecasting and Social Change","177",,"121517","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123788772&doi=10.1016%2fj.techfore.2022.121517&partnerID=40&md5=ae75106ac919d0e6706f8b20a3b94047","Since job characteristics in areas related to the green economy and Industry 4.0 are changing rapidly, combined methodologies to measure the labour demand and supply are needed. One substantial aspect of this emerging sector is the shift of the automotive industry towards the production of electric vehicles (EVs). The automotive sector is a major employer in Europe, directly employing over 2.8 million people. However, little is known about the effects this structural transformation of the automotive industry will have on labor markets, in particular in the area of information and communications technology (ICT). This prevents effective planning by educational institutions, who seek to prepare their students for future labor markets, and industry stakeholders aiming to assemble effective teams. In this paper, we develop a framework to analyze labor market trends using digital trace data, and apply it to the case of the EV industry. We track demand-side trends in the labor market using job advertisements from LinkedIn and supply-side trends using data from StackExchange and GitHub. Using natural language processing methods, we categorize the skills sought by EV industry employers on the demand side and topics of interest to individuals on the supply side. We also highlight those programming languages and frameworks most salient in the EV industry. © 2022 Elsevier Inc.","Electric vehicles; GitHub; Labour market; LinkedIn; Skills; Stack exchange","Commerce; Electric vehicles; Employment; Natural language processing systems; Demand-side; Github; Green economies; Job characteristics; Labour market; LinkedIn; Skill; Stack exchange; Supply sides; Vehicle industry; Automotive industry",Article,Scopus,2-s2.0-85123788772
"Hajek P., Prof., Sahut J.-M., Prof.","56894360000;6507098224;","Mining behavioural and sentiment-dependent linguistic patterns from restaurant reviews for fake review detection",2022,"Technological Forecasting and Social Change","177",,"121532","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123781663&doi=10.1016%2fj.techfore.2022.121532&partnerID=40&md5=ebca394336093968d22440608b2da06a","Online reviews are increasingly recognized as a key source of information influencing consumer behavior. This in turn implies that competitive advantage can be achieved by manipulating users’ perceptions about restaurants. The hospitality industry is particularly susceptible to this issue because products and services in this industry can only be rated upon consumption. Therefore, many efforts have recently been dedicated to developing automatic methods for detecting fake reviews based on data intelligence in this sector. Recent studies suggest that both the semantic meaning of consumer reviews and the sentiment conveyed may be useful indicators of fake reviews. However, the semantic meaning may be context-sensitive and may also disregard sentiment information. Moreover, the content analysis approach should be integrated with the reviewer's behavior to reveal their true intentions. To address these problems, we propose a review representation model based on behavioural and sentiment-dependent linguistic features that effectively exploit the domain context. Using a large dataset of Yelp restaurant reviews, we demonstrate that the proposed review representation model is more effective than existing approaches in terms of detection accuracy. It furthermore accurately estimates the average rating assigned by legitimate reviewers, which has significant managerial implications for the hospitality industry. © 2022","behaviour; data intelligence; detection; fake review; hospitality industry; machine learning model; restaurant; sentiment","Competition; Consumer behavior; Fake detection; Machine learning; Semantics; Behavior; Data intelligence; Detection; Fake review; Hospitality industry; Machine learning models; Representation model; Restaurant; Restaurant reviews; Sentiment; Large dataset",Article,Scopus,2-s2.0-85123781663
"Somyanonthanakul R., Theeramunkong T.","57095005900;56006507400;","Scenario-based analysis for discovering relations among interestingness measures",2022,"Information Sciences","590",,,"346","385",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123761465&doi=10.1016%2fj.ins.2021.12.121&partnerID=40&md5=6db736783eea5b63d78bf8deb0e98a84","Many interestingness measures have been proposed for mining meaningful association rules among two events in the form of A→B, but their characteristics and semantic similarity relations have not been comprehensively investigated. This paper presents a scenario-based approach for characterizing sixty-one commonly used measures and revealing their relationships in three steps. The first step generates a set of 969 three-probability scenarios, S={s|s=(p(A),p(B),p(A,B))∧p(A),p(B),p(A|B)∈[0,1]∧p(A,B)⩽min(p(A),p(B))}, in consideration of all possible situations in the range of 0.0 to 1.0 with a step of 0.05, excluding infinity and not-a-number cases. In the second step, 937,992 pairs of scenarios are enumerated, and for each scenario pair s1 and s2, the values of a measure (M) of s1 and s2, i.e., M(s1) and M(s2), are compared with the result of greater-than (M(s1)&gt;M(s2)), smaller-than (M(s1)&lt;M(s2)), or equal-to (M(s1)=M(s2)) for characterizing the measure. The final step is based on three types of relations: (1) behavior-based, (2) correlation-based, and (3) association-based similarity relations. The behavior of measures is depicted using nine common algebraic/statistical properties and four special condition properties, i.e., zero, min–max, infinity, and not-a-number of the measures. Similarities among the measures can be examined by grouping measures based on their properties. With three correlation functions, i.e., correlation coefficient, joint entropy, and mutual information, a correlation analysis was performed to discover relations among interestingness measures in the form of dendrograms and clusters with thresholding. Finally, the details of the relations among these interestingness measures are explored with association rule mining. Besides support, confidence, and lift, we propose five types of rules, i.e. same-directionrule (S-rule), opposite-direction rule (O-rule), equal-both rule (E-rule), equal-left rule (EL-rule),and equal-right rule (ER-rule) for a five-gradient comparison of any two measures to outline their similarities and dissimilarities in five directions. © 2022 Elsevier Inc.","Association rule mining; Behavior characterization; Interestingness measures; Scenario-based analysis","Data mining; Semantics; Behavior-based; Behaviour characterization; Interestingness measures; Not a numbers; Property; Scenario-based; Scenario-based analysis; Semantic similarity; Similarity relations; Types of relations; Association rules",Article,Scopus,2-s2.0-85123761465
"Collodi L., Bacciu D., Bianchi M., Averta G.","57431144000;23395838700;57202798446;57195416332;","Learning With Few Examples the Semantic Description of Novel Human-Inspired Grasp Strategies From RGB Data",2022,"IEEE Robotics and Automation Letters","7","2",,"2573","2580",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123729841&doi=10.1109%2fLRA.2022.3144520&partnerID=40&md5=2081c87dcf31434685e0d9a4d5f54098","Data-driven approaches and human inspiration are fundamental to endow robotic manipulators with advanced autonomous grasping capabilities. However, to capitalize upon these two pillars, several aspects need to be considered, which include the number of human examples used for training; the need for having in advance all the required information for classification (hardly feasible in unstructured environments); the trade-off between the task performance and the processing cost. In this letter, we propose a RGB-based pipeline that can identify the object to be grasped and guide the actual execution of the grasping primitive selected through a combination of Convolutional and Gated Graph Neural Networks. We consider a set of human-inspired grasp strategies, which are afforded by the geometrical properties of the objects and identified from a human grasping taxonomy, and propose to learn new grasping skills with only a few examples. We test our framework with a manipulator endowed with an under-actuated soft robotic hand. Even though we use only 2D information to reduce the footprint of the network, we achieve 90#x0025; of successful identifications of the most appropriate human-inspired grasping strategy over ten different classes, of which three were few-shot learned, outperforming an ideal model trained with all the classes, in sample-scarce conditions. 2377-3766 © 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.","Feature extraction; Grasping; Robot kinematics; Robots; Task analysis; Taxonomy; Training","Economic and social effects; Graph neural networks; Job analysis; Manipulators; Personnel training; Robotics; Semantics; Data-driven approach; Features extraction; Grasping; Processing costs; Robot kinematics; Semantic descriptions; Task analysis; Task performance; Trade off; Unstructured environments; Taxonomies",Article,Scopus,2-s2.0-85123729841
"Ivani A.S., Giubergia A., Santos L., Geminiani A., Annunziata S., Caglio A., Olivieri I., Pedrocchi A.","57431004500;57431004600;57192921674;57038761400;55642229800;57431801200;55198600300;55912237900;","A gesture recognition algorithm in a robot therapy for ASD children",2022,"Biomedical Signal Processing and Control","74",,"103512","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123722154&doi=10.1016%2fj.bspc.2022.103512&partnerID=40&md5=419a3c94b1a26818c0d946aa47f12d08","Children with Autism Spectrum Disorders (ASDs) exhibit significant impairments in gesture imitation. Newest interventions are based on Human-Robot Interaction (HRI) since children with ASD cope well with stylized, rule-based and predictable systems. These collaborative approaches encompass therapy games based on joint exercises, imitation and interaction between robots and children. This paper's aim was to implement an algorithm to automatically recognize small and similar gestures within a humanoid-robot therapy called IOGIOCO for ASD children. IOGIOCO is a multi-level HRI therapy meant to teach 19 meaningful gestures in a semantic framework based on a feedback interaction. Gestures were tracked as 3D coordinates of body keypoints captured by a Kinect. A Residual Neural Network was implemented and trained on a segmented Dataset acquired within this study to generate the offline model which was then exploited in a real-time classification using a sliding window. Feedback as sound stimuli from NAO robot was provided based on the automatic evaluation of each performance. Clinical acquisitions were carried out on 4 ASD children within the IOGIOCO therapy. Offline recognition was successful: exploiting Artificial Neural Networks we reached 95% of test accuracy for 19 gestures. A real-time recognition on healthy subjects reached 94% accuracy. Clinical applications were evaluated through the F1 score that achieved 79% value. These outcomes were encouraging considering the wide gesture set and all the challenges the therapy raises. This kind of automatic algorithm was able to decrease the therapist workload and increase the robustness of the therapy and engagement of the child. © 2022","Artificial neural networks classification; ASD; Gesture recognition; Human robot interaction; Real-time classification","Anthropomorphic robots; Classification (of information); Feedback; Gesture recognition; Human robot interaction; Man machine systems; Semantics; Artificial neural network classification; Autism spectrum disorders; Children with autisms; Gesture recognition algorithm; Gestures recognition; Humans-robot interactions; Neural network classification; Real- time; Real-time classification; Robot therapy; Neural networks",Article,Scopus,2-s2.0-85123722154
"Pan X., Song S., Chen Y., Wang L., Huang G.","57201677892;13310063000;57190884452;16833826600;7403425368;","PLAM: A plug-in module for flexible graph attention learning",2022,"Neurocomputing","480",,,"76","88",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123693945&doi=10.1016%2fj.neucom.2022.01.045&partnerID=40&md5=1c29cae02bebfd81d6c2802999a84078","Graph Convolutional Networks (GCNs) are general deep representation learning models for graph-structured data. In this paper, we propose a simple Plug-in Attention Module (PLAM) to improve the representation power of GCNs, inspired by the recent success of the query-key mechanism in computer vision and natural language processing. With this module, our network is able to adaptively learn the weights from a node towards its neighbors. Different from existing attention-based GCNs, the proposed PLAM has several important properties. First, the parameter space for the attention module is isolated from that for feature learning. This ensures that the proposed approach can be conveniently applied to existing GCNs as a plug-in module. Second, the anchor node and neighbor nodes are treated separately when learning the attention weights, which further enhances the flexibility of our structure. Third, our attention module extracts higher-level information by computing the inner product of the features between the anchor node and neighbor nodes, leading to significantly increased representation power. Last, we take a step forward and propose a novel structural encoding technique for the graph attention module to inject local and global structure information. Although being simple, our PLAM models have achieved state-of-the-art performances on graph-structured datasets under both the transductive and inductive settings. Additionally, experiments on image and point cloud datasets show potential applications of PLAM on several computer vision tasks. © 2022","Graph convolutional networks; Graph-based learning; Self-attention; Semi-supervised node classification","Computer vision; Convolution; Convolutional neural networks; Deep learning; Graph theory; Natural language processing systems; Anchor nodes; Convolutional networks; Graph convolutional network; Graph-based learning; Plug-ins; Representation power; Self-attention; Semi-supervised; Semi-supervised node classification; Simple++; Graphic methods; article; attention; computer vision; human; learning; natural language processing",Article,Scopus,2-s2.0-85123693945
"Xiao L., Xu P., Jing L., Akujuobi U., Zhang X.","57216695192;57226312231;8893085600;57200216363;9238032200;","Semantic guide for semi-supervised few-shot multi-label node classification",2022,"Information Sciences","591",,,"235","250",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123682331&doi=10.1016%2fj.ins.2021.12.130&partnerID=40&md5=feb85495cdb6843e8b8a333674d9c15c","We study a new research problem named semi-supervised few-shot multi-label node classification which has the following characteristics: 1) the extreme imbalance between the number of labeled and unlabeled nodes that are connected on graphs (handled by semi-supervised node learning); 2) the few labeled nodes per label (few-shot learning); and 3) the semantical correlations among labels for they share the same subsets of nodes (multi-label classification). In this paper, we propose a Label-Aware Representation Network (LARN) model to tackle this problem, by taking advantage of the semantic knowledge of labels to characterize nodes and their neighbors. Such a label-aware feature learning process allows a node to prepare its representation by knowing how it will be classified. The learned rich representations so can combat the scarcity of labeled training nodes. A label correlation scanner is then proposed to adaptively capture the label correlation and extract the useful information to generate the final node representation. Experimental results demonstrate that LARN consistently outperforms the state-of-the-art methods with significant margins, especially when only a few-shot labeled nodes are available. © 2022 Elsevier Inc.","Few-shot learning; Graph neural network; Label semantic; Multi-label classification; Semi-supervised learning","Classification (of information); Learning systems; Semantic Web; Semantics; Feature learning; Few-shot learning; Graph neural networks; Label correlations; Label semantics; Multi-labels; Network models; Research problems; Semantics knowledge; Semi-supervised; Graph neural networks",Article,Scopus,2-s2.0-85123682331
"Loureiro D., Mário Jorge A., Camacho-Collados J.","57216622545;57201384518;56899042300;","LMMS reloaded: Transformer-based sense embeddings for disambiguation and beyond",2022,"Artificial Intelligence","305",,"103661","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123582341&doi=10.1016%2fj.artint.2022.103661&partnerID=40&md5=5d60529c76cdd64ae2d3ad4d261866a6","Distributional semantics based on neural approaches is a cornerstone of Natural Language Processing, with surprising connections to human meaning representation as well. Recent Transformer-based Language Models have proven capable of producing contextual word representations that reliably convey sense-specific information, simply as a product of self-supervision. Prior work has shown that these contextual representations can be used to accurately represent large sense inventories as sense embeddings, to the extent that a distance-based solution to Word Sense Disambiguation (WSD) tasks outperforms models trained specifically for the task. Still, there remains much to understand on how to use these Neural Language Models (NLMs) to produce sense embeddings that can better harness each NLM's meaning representation abilities. In this work we introduce a more principled approach to leverage information from all layers of NLMs, informed by a probing analysis on 14 NLM variants. We also emphasize the versatility of these sense embeddings in contrast to task-specific models, applying them on several sense-related tasks, besides WSD, while demonstrating improved performance using our proposed approach over prior work focused on sense embeddings. Finally, we discuss unexpected findings regarding layer and model performance variations, and potential applications for downstream tasks. © 2022 Elsevier B.V.","Neural language models; Semantic representations","Computational linguistics; Natural language processing systems; Semantics; Contextual words; Distributional semantics; Embeddings; Language model; Neural language model; Semantic representation; Sense inventories; Specific information; Word representations; Word Sense Disambiguation; Embeddings",Article,Scopus,2-s2.0-85123582341
"Tian L., Chen B., Ren J., Zhang H., Wu Z., Han N., Chen Y., Liu H.","57216438023;56387823300;57211311893;57210071079;57222246439;57416541700;57416426200;57279681600;","Multi-scale visual attention for attribute disambiguation in zero-shot learning",2022,"Signal Processing: Image Communication","103",,"116614","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123008676&doi=10.1016%2fj.image.2021.116614&partnerID=40&md5=c4736821e00aac74c1c2e0cba0da6901","Observing the phenomenon that the discriminative visual features and unambiguous attribute descriptions are important in zero-shot learning (ZSL), we propose a Multi-scale Visual Attention for Attribute Disambiguation (MVAAD). MVAAD contains a Multi-Scale Visual Attention Network (MSVAN) to realize attentions on image regions, which helps MVAAD to learn more discriminative visual features. Based on the multi-scale visual features in MSVAN, we also develop a Coarse-to-fine Visual-guided Attribute Selection Module (CVASM) to use the multi-scale visual attentive features for attribute disambiguation. Both of MSVAN and CVASM can be jointly trained in an end-to-end manner by minimizing the visual-semantic classification loss and the latent-visual contrastive triplet loss. Experimental results on four popular ZSL benchmarks, AwA2, CUB, SUN and FLO, illustrate that MVAAD is able to not only achieve the state-of-the-art performance, but also give meaningful and explainable visualizations on the visual attention and the attribute selection. © 2021","Attribute disambiguation; Visual attention; Zero-shot image recognition","Behavioral research; Benchmarking; Semantics; Attribute disambiguation; Attribute selection; Coarse to fine; End to end; Image regions; Learn+; Multi-scales; Visual Attention; Visual feature; Zero-shot image recognition; Image recognition",Article,Scopus,2-s2.0-85123008676
"Sang H., Jiang R., Wang Z., Zhou Y., He B.","57221051056;57207412423;57299527900;57413380300;57299711600;","A Novel Neural Multi-Store Memory Network for Autonomous Visual Navigation in Unknown Environment",2022,"IEEE Robotics and Automation Letters","7","2",,"2039","2046",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122872444&doi=10.1109%2fLRA.2022.3140795&partnerID=40&md5=0c7bb111d52b250cecfe778ffdccb4c1","Learning to achieve a user-specified objective from a random position in unseen environments is challenging for image-guided navigation agents. The abilities of long-horizon reasoning and semantic understanding are still lacking. Inspired by the human memory mechanism, we introduce a neural multi-store memory network to the reinforcement learning framework for target-driven visual navigation. The proposed memory network utilizes three temporal stages of memory to build time dependency for better scene understanding. Sensory memory encodes observations and embeds transient information into working memory, which is short-term and realized by a gated recurrent neural network (RNN). Then, the long-term memory stores the latent state from each step of the RNN into a single slot. Finally, a self-attention reading mechanism is designed to retrieve goal-related information from long-term memory. In addition, to improve the scene generalization capability of the agent, we facilitate training of the visual representation with a self-supervised auxiliary task and image augmentation. This method can navigate agents in unknown visual-realistic environments using only egocentric observations, without the need for any position sensors or pretrained models. The evaluation results on the Matterport3D dataset through the Habitat simulator demonstrate that our method outperforms the state-of-the-art approaches. 2377-3766 © 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.","Navigation; Recurrent neural networks; Reinforcement learning; Semantics; Task analysis; Training; Visualization","Image enhancement; Job analysis; Navigation; Recurrent neural networks; Reinforcement learning; Semantic Web; Cognitive science; Embodied cognitive science; Long term memory; Memory network; Random position; Reinforcement learnings; Task analysis; Unknown environments; Vision based navigation; Visual Navigation; Semantics",Article,Scopus,2-s2.0-85122872444
"Ezaldeen H., Misra R., Bisoy S.K., Alatrash R., Priyadarshini R.","57219917328;56463501600;55899630000;57219914958;57195339620;","A hybrid E-learning recommendation integrating adaptive profiling and sentiment analysis",2022,"Journal of Web Semantics","72",,"100700","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122815302&doi=10.1016%2fj.websem.2021.100700&partnerID=40&md5=dacb7a6711106292daf58266d5f26308","This research proposes a novel framework named Enhanced e-Learning Hybrid Recommender System (ELHRS) that provides an appropriate e-content with the highest predicted ratings corresponding to the learner's particular needs. To accomplish this, a new model is developed to deduce the Semantic Learner Profile automatically. It adaptively associates the learning patterns and rules depending on the learner's behavior and the semantic relations computed in the semantic matrix that mutually links e-learning materials and terms. Here, a semantic-based approach for term expansion is introduced using DBpedia and WordNet ontologies. Further, various sentiment analysis models are proposed and incorporated as a part of the recommender system to predict ratings of e-learning resources from posted text reviews utilizing fine-grained sentiment classification on five discrete classes. Qualitative Natural Language Processing (NLP) methods with tailored-made Convolutional Neural Network (CNN) are developed and evaluated on our customized dataset collected for a specific domain and a public dataset. Two improved language models are introduced depending on Skip-Gram (S-G) and Continuous Bag of Words (CBOW) techniques. In addition, a robust language model based on hybridization of these couple of methods is developed to derive better vocabulary representation, yielding better accuracy 89.1% for the CNN-Three-Channel-Concatenation model. The suggested recommendation methodology depends on the learner's preferences, other similar learners’ experience and background, deriving their opinions from the reviews towards the best learning resources. This assists the learners in finding the desired e-content at the proper time. © 2021 Elsevier B.V.","Adaptive profiling; Convolutional Neural Network; Fine-grained sentiment analysis; Hybrid E-learning recommendation; Semantic learner profile; Word embeddings","Computational linguistics; Convolution; Convolutional neural networks; E-learning; Learning systems; Ontology; Recommender systems; Semantic Web; Semantics; Adaptive profiling; Convolutional neural network; E - learning; Embeddings; Fine grained; Fine-grained sentiment analyse; Hybrid E-learning recommendation; Learner profiles; Semantic learner profile; Sentiment analysis; Word embedding; Sentiment analysis",Article,Scopus,2-s2.0-85122815302
"Nguyen C.V., Le K.H., Tran A.M., Pham Q.H., Nguyen B.T.","57205467835;57219436768;57219434291;55402671300;57219109528;","Learning for amalgamation: A multi-source transfer learning framework for sentiment classification",2022,"Information Sciences","590",,,"1","14",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122805619&doi=10.1016%2fj.ins.2021.12.059&partnerID=40&md5=7bd0ed80179eb6e1b4dc1f951309d58b","Transfer learning plays an essential role in Deep Learning, which can remarkably improve the performance of the target domain, whose training data is not sufficient. Our work explores beyond the common practice of transfer learning with a single pre-trained model. We focus on the task of Vietnamese sentiment classification and propose LIFA, a framework to learn a unified embedding from several pre-trained models. We further propose two more LIFA variants that encourage the pre-trained models to either cooperate or compete with one another. Studying these variants sheds light on the success of LIFA by showing that sharing knowledge among the models is more beneficial for transfer learning. Moreover, we construct the AISIA-VN-Review-F dataset, the first large-scale Vietnamese sentiment classification database. We conduct extensive experiments on the AISIA-VN-Review-F and existing benchmarks to demonstrate the efficacy of LIFA compared to other techniques. To contribute to the Vietnamese NLP research, we publish our source code and datasets to the research community upon acceptance. © 2021 Elsevier Inc.","LIFA; Low-resource NLP; Mixture of experts; Sentiment classification; Transfer learning","Classification (of information); Deep learning; Large dataset; Metals; Learning frameworks; LIFA; Low-resource NLP; Mixture of experts; Multi-Sources; Performance; Sentiment classification; Source Transfer; Transfer learning; Vietnamese; Natural language processing systems",Article,Scopus,2-s2.0-85122805619
"Phan H.T., Nguyen N.T., Hwang D.","57201079477;7403180310;35118943000;","Convolutional attention neural network over graph structures for improving the performance of aspect-level sentiment analysis",2022,"Information Sciences","589",,,"416","439",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122637110&doi=10.1016%2fj.ins.2021.12.127&partnerID=40&md5=0caddee511e5c44a213c09a84306b563","Recently, aspect-level sentiment analysis methods using graph convolutional network (GCN)-based structures with fairly good performance have been introduced. However, previous GCN-based methods often experience one of the following limitations. First, GCNs usually use edges with binary weights. However, binary weights are not helpful in many tasks. Second, these GCNs only focus on extracting node features from some single words or phrases and ignore their context in the entire sentence or paragraph or only consider the information of independent phrases when determining the relation between two graph edges overlooking the semantic relation among these phrases. Finally, no studies simultaneously use the information on the context, the semantic relation, and the sentiment knowledge among words or phrases to build GCNs for aspect-level sentiment analysis. Therefore, to resolve these limitations, in this study, we propose a new method, the CANN-SSCG model, as follows. First, we built three separate heterogeneous graphs, namely, syntax-based, semantic-based, and context-based graphs. Second, we constructed a general heterogeneous graph (SSC graph) by combining the three constructed graphs. We then converted the nodes of the SSC graph into sentence vectors using a GCN with two layers (creating an SSC-GCN). Finally, we used a convolutional neural network algorithm with attention to position embeddings (CANN) on the output of the SSC-GCN model for aspect-level sentiment analysis. The experiments, which used three different datasets, including reviews and tweets, showed that the proposed method yields promising results based on the F1score. © 2022 The Author(s)","Aspect-level sentiment analysis; CANN-SSCG model; Context-based graph; Semantic-based graph; SSC graph; Syntax-based graph","Convolution; Convolutional neural networks; Graph neural networks; Sentiment analysis; Syntactics; Aspect-level sentiment analyse; CANN-SSCG model; Context-based; Context-based graph; Convolutional networks; Performance; Semantic-based graph; Sentiment analysis; SSC graph; Syntax-based graph; Semantics",Article,Scopus,2-s2.0-85122637110
"Rajpathak D., Peranandam P.M., Ramesh S.","13405607700;8386687200;7103210987;","Automatic development of requirement linking matrix based on semantic similarity for robust software development",2022,"Journal of Systems and Software","186",,"111211","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122634295&doi=10.1016%2fj.jss.2021.111211&partnerID=40&md5=9f727a56152276ec54686db48522f4d4","With growing complexity of modern software, it is important that the relevant textual requirements are correctly linked into a ‘requirement liking matrix’ during early system development stages. The resulting requirement linking matrix highlights direct and indirect interactions between different requirements, thus facilitating improved design, development, and testing of complex software systems, e.g. automotive software, electrical/electronic architectures. The sheer volume of textual requirements collected in real-life coupled with data noises makes the task of automatic requirement linking a non-trivial exercise. In this paper, we propose a novel semantic similarity model for automatically linking different requirements to organize them into a requirement linking matrix. The model computes the similarity in terms of term-to-term, tuple-to-tuple, and text-to-text scores. The scores are ranked to determine whether the links are having “High”, “Low”, or “No” relationship with each other in a requirement linking matrix. The model is deployed as a prototype tool and its performance is validated by using the real-life data. We also compare our approach with the alternative approaches proposed in literature. The system achieved the average F1 score of 0.93 in correctly linking the heterogeneous requirements. © 2022 Elsevier Inc.","Automotive; Decision support; Requirement engineering; Requirement linking; Semantic similarity","Matrix algebra; Semantics; Software design; Software testing; Automotives; Decision supports; Development stages; Direct interactions; Indirect interactions; matrix; Requirement engineering; Requirement linking; Semantic similarity; System development; Decision support systems",Article,Scopus,2-s2.0-85122634295
"Sun Y., Li P., Cheng G., Qu Y.","57219587250;57394089600;36103954800;8400208900;","Skeleton parsing for complex question answering over knowledge bases",2022,"Journal of Web Semantics","72",,"100698","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122137365&doi=10.1016%2fj.websem.2021.100698&partnerID=40&md5=af8c07275f1cb2510b2d8ac76dac5b9d","Answering complex questions involving multiple relations over knowledge bases is a challenging task. Many previous works rely on dependency parsing. However, errors in dependency parsing would influence their performance, in particular for long complex questions. In this paper, we propose a novel skeleton grammar to represent the high-level structure of a complex question. This lightweight formalism and its BERT-based parsing algorithm help to improve the downstream dependency parsing. To show the effectiveness of skeleton, we develop two question answering approaches: skeleton-based semantic parsing (called SSP) and skeleton-based information retrieval (called SIR). In SSP, skeleton helps to improve structured query generation. In SIR, skeleton helps to improve path ranking. Experimental results show that, thanks to skeletons, our approaches achieve state-of-the-art results on three datasets: LC-QuAD 1.0, GraphQuestions, and ComplexWebQuestions 1.1. © 2021 Elsevier B.V.","Complex question answering; Dependency parsing; KBQA; Question decomposition; Skeleton parsing","Formal languages; Semantics; Complex question answering; Complex questions; Dependency parsing; High-level structure; KBQA; Parsing algorithm; Performance; Question Answering; Question decomposition; Skeleton parsing; Musculoskeletal system",Article,Scopus,2-s2.0-85122137365
"Adhikari S., Thapa S., Naseem U., Singh P., Huo H., Bharathy G., Prasad M.","57218940241;57218940311;57212385431;57212313896;57211270770;13905892400;57217990284;","Exploiting linguistic information from Nepali transcripts for early detection of Alzheimer's disease using natural language processing and machine learning techniques",2022,"International Journal of Human Computer Studies","160",,"102761","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121967746&doi=10.1016%2fj.ijhcs.2021.102761&partnerID=40&md5=3d02cc89c5c2d1521c8e28c2d192692b","Alzheimer's disease (AD) is considered as progressing brain disease, which can be slowed down with the early detection and proper treatment by identifying the early symptoms. Language change serves as an early sign that a patient's cognitive functions have been impacted, potentially leading to early detection. The effects of language changes are being studied thoroughly in the English language to analyze the linguistic patterns in AD patients using Natural Language Processing (NLP). However, it has not been much explored in local languages and low-resourced languages like Nepali. In this paper, we have created a novel dataset on low resources language, i.e., Nepali, consisting of transcripts of the AD patients and control normal subjects. We have also presented baselines by applying various machine learning (ML) and deep learning (DL) algorithms on a novel dataset for the early detection of AD. The proposed work incorporates the speech decline of AD patients in order to classify them as control subjects or AD patients. This study makes an effective conclusion that the difficulty in processing information of AD patients reflects in their speech narratives of patients while describing a picture. The dataset is made publicly available. © 2021 Elsevier Ltd","Alzheimer's disease; Deep learning; Low resourced language; Machine learning; Natural language processing; Nepali language","Deep learning; Learning algorithms; Linguistics; Natural language processing systems; Alzheimers disease; Brain disease; Cognitive functions; Deep learning; English languages; Linguistic information; Linguistic patterns; Low resourced language; Machine learning techniques; Nepali language; Neurodegenerative diseases",Article,Scopus,2-s2.0-85121967746
"Han S., Lee M.K.","57220155513;57388625000;","FAQ chatbot and inclusive learning in massive open online courses",2022,"Computers and Education","179",,"104395","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121914848&doi=10.1016%2fj.compedu.2021.104395&partnerID=40&md5=4939a4ee0dab0b936ae05a268a2c319a","Recognizing the research gap involving the lack of equity considerations in new technology implementation, this study compares students' learning experiences when using an FAQ chatbot with using an FAQ webpage. We trained a natural language processing–based chatbot utilizing content from an FAQ webpage and deployed it in two journalism massive open online courses (MOOCs) with 46 students and compared their experiences with 74 students' experiences with the FAQ webpage as a baseline. There were equal numbers of male and female students, their ages ranged from 18 to 65+, and they hailed from 45 unique countries. Considering the importance of supporting students with an inclusive Q&A experience before implementing any new technology into real-world operation, this study investigates students' disparate Q&A experiences by measuring their intention to use the interface as well as perceived Q&A service quality, enjoyment, and barriers utilizing a between-subjects online experiment. The results indicate that the students preferred an FAQ webpage over an FAQ chatbot, and the chatbot users experienced a higher magnitude of barriers compared to the webpage users. For the chatbot users, we found that region and native language factors influenced their Q&A experiences significantly. We discussed the meaning of the students’ disparate experiences from multiple perspectives—namely, human-computer interaction, MOOC context, and technologies as social practice aspects. Lastly, we determined how feasible it is to provide an inclusive learning experience for the MOOC population with the FAQ chatbot, based on the contextualized meaning of MOOC inclusiveness in current literature. This study suggests multi-faceted aspects to consider when adopting new technologies in MOOCs to provide an inclusive learning experience, and underscores the need for more active research in chatbot use to serve diverse student needs in MOOCs. © 2021 Elsevier Ltd","Accessibility; Chatbot; Human and computer interaction; Inclusive learning environment; Massive open online courses","Computer aided instruction; Curricula; E-learning; Human computer interaction; Natural language processing systems; Websites; Chatbots; Computer interaction; Humaninteraction; Inclusive learning environment; Learning environments; Learning experiences; Massive open online course; New technology implementations; Research gaps; Web-page; Students",Article,Scopus,2-s2.0-85121914848
"Liu J., Li X., Zhang S., Yan D., Wang C.","57190127140;57373276300;12445682700;55376442600;57213521588;","Broad-spectrum noise-like pulse and Q-switched noise-like pulse in a Tm-doped fiber laser",2022,"Optics and Laser Technology","148",,"107716","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121257224&doi=10.1016%2fj.optlastec.2021.107716&partnerID=40&md5=9c4715789d03677777016acb06e6494e","We experimentally and numerically study the broad-spectrum noise-like pulse (NLP) in a Tm-doped fiber laser (TDFL). The 3-dB spectral width of the NLP is 126 nm, which is, to the best of our knowledge, the broadest spectrum that has been directly obtained from negative dispersion TDFLs. The NLP can transform into another dynamic pulse operation, a Q-switched noise-like pulse (QSNLP). Since the QSNLP showed a noise-like structure within a Q-switched envelope, compared to conventional Q-switched mode-locked pulse, it is break-free during amplification. It offers excellent potential for obtaining high-energy pulses. The numerical studies demonstrate the existence of the NLPs and the rationality of the existence of QSNLP based on the experiment condition. Our work also enriches the operation of bunching structure pulses in fiber lasers. © 2021 Elsevier Ltd","Noise-like pulse; Q-switched noise-like pulse; Tm-doped fiber laser","Fiber lasers; Fibers; Natural language processing systems; Broad spectrum; Mode-locked pulse; Noise-like pulse; Pulse operation; Q-switched; Q-switched envelopes; Q-switched noise-like pulse; Spectral widths; Switched-mode; Tm-doped fiber laser; Q switching",Article,Scopus,2-s2.0-85121257224
"Asprino L., Ciancarini P., Nuzzolese A.G., Presutti V., Russo A.","57191745706;7004218817;42862074000;55885160000;57211808245;","A reference architecture for social robots",2022,"Journal of Web Semantics","72",,"100683","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120943585&doi=10.1016%2fj.websem.2021.100683&partnerID=40&md5=c79ced1ab31aba9efa7111bb4d724dba","Social robotics poses tough challenges to software designers who are required to take care of difficult architectural drivers like acceptability, trust of robots as well as to guarantee that robots establish a personalized interaction with their users. Moreover, in this context recurrent software design issues such as ensuring interoperability, improving reusability and customizability of software components also arise. Designing and implementing social robotic software architectures is a time-intensive activity requiring multi-disciplinary expertise: this makes it difficult to rapidly develop, customize, and personalize robotic solutions. These challenges may be mitigated at design time by choosing certain architectural styles, implementing specific architectural patterns and using particular technologies. Leveraging on our experience in the MARIO project, in this paper we propose a series of principles that social robots may benefit from. These principles lay also the foundations for the design of a reference software architecture for social robots. The goal of this work is twofold: (i) Establishing a reference architecture whose components are unambiguously characterized by an ontology thus allowing to easily reuse them in order to implement and personalize social robots; (ii) Introducing a series of standardized software components for social robots architecture (mostly relying on ontologies and semantic technologies) to enhance interoperability, to improve explainability, and to favor rapid prototyping. © 2021 Elsevier B.V.","Architectural patterns; Interoperability; Linked open data; Ontologies; Social robots design; Software architectures","Architectural design; Computer software reusability; Linked data; Machine design; Ontology; Reusability; Robotics; Robots; Semantics; Software architecture; Software design; Architectural pattern; Linked open datum; Ontology's; Reference architecture; Robot designs; Social robot design; Social robotics; Social robots; Software designers; Software-component; Interoperability",Article,Scopus,2-s2.0-85120943585
"Zhou H., Qi L., Huang H., Yang X., Wan Z., Wen X.","57202096570;57201703499;56804044100;57015778200;57208213665;57365848100;","CANet: Co-attention network for RGB-D semantic segmentation",2022,"Pattern Recognition","124",,"108468","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120830159&doi=10.1016%2fj.patcog.2021.108468&partnerID=40&md5=ec1bd21b495d8ecad61591f2a63cfc57","Incorporating the depth (D) information to RGB images has proven the effectiveness and robustness in semantic segmentation. However, the fusion between them is not trivial due to their inherent physical meaning discrepancy, in which RGB represents RGB information but D depth information. In this paper, we propose a co-attention network (CANet) to build sound interaction between RGB and depth features. The key part in the CANet is the co-attention fusion part. It includes three modules. Specifically, the position and channel co-attention fusion modules adaptively fuse RGB and depth features in spatial and channel dimensions. An additional fusion co-attention module further integrates the outputs of the position and channel co-attention fusion modules to obtain a more representative feature which is used for the final semantic segmentation. Extensive experiments witness the effectiveness of the CANet in fusing RGB and depth features, achieving state-of-the-art performance on two challenging RGB-D semantic segmentation datasets, i.e., NYUDv2 and SUN-RGBD. © 2021","Co-attention; Multi-modal fusion; RGB-D; Semantic segmentation","Semantic Web; Semantics; Co-attention; Depth features; Depth information; Fusion modules; Multi-modal fusion; Physical meanings; RGB images; RGB-D; Semantic segmentation; Sound interactions; Semantic Segmentation",Article,Scopus,2-s2.0-85120830159
"Zhang J., Li Q., Geng Y.-A., Wang W., Sun W., Shi C., Ding Z.","57221657592;56111060800;57366166500;56915076100;57279002400;55447999200;36633871400;","A zero-shot learning framework via cluster-prototype matching",2022,"Pattern Recognition","124",,"108469","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120770885&doi=10.1016%2fj.patcog.2021.108469&partnerID=40&md5=a484fa01040bac52063b00ac18b9d113","Given the descriptions of classes, Zero-Shot Learning (ZSL) aims to recognize unseen samples by learning a projection between the visual features of samples and the semantic descriptions (prototypes) of classes from seen data. However, due to the inherent distribution gap between seen and unseen domains, the learned projection is generally biased to seen classes and may produce misleading relationships between unseen samples and prototypes (sample-prototype relationship). To tackle this problem, we propose a Cluster-Prototype Matching (CPM) framework which exploits the distribution information of samples to explore the cluster structure of samples and then use the robust cluster-prototype relationship to correct the biased sample-prototype relationship. Specifically, we first use an iterative cluster generation module to identify the underlying cluster structure of samples based on their embedding features, which are acquired via a basic ZSL model. Then each identified cluster will be matched with a specific class prototype through the Kuhn-Munkres algorithm, based on which we can export a sharp cluster-prototype similarity. Finally, the cluster-prototype similarity is combined with the sample-prototype similarity to determine the class labels of test samples. We apply CPM to five well-established ZSL methods and the experimental results show that CPM can significantly improve the performance of basic models and enable them achieve or beyond the state-of-the-art. © 2021 Elsevier Ltd","Cluster-prototype matching; Domain shift; Image classification; Zero-shot learning","Iterative methods; Semantics; Cluster prototype; Cluster structure; Cluster-prototype matching; Domain shift; Images classification; Learning frameworks; Matchings; Semantic descriptions; Visual feature; Zero-shot learning; Image classification",Article,Scopus,2-s2.0-85120770885
"Hu W., Liu L., Sun Y., Wu Y., Liu Z., Zhang R., Peng T.","57362800600;57363239200;57363239300;57363239400;57363239500;57362800700;57224624384;","NLIRE: A Natural Language Inference method for Relation Extraction",2022,"Journal of Web Semantics","72",,"100686","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120641970&doi=10.1016%2fj.websem.2021.100686&partnerID=40&md5=a0ac8cc0e72c0a01040692aa9acaf735","Relation extraction task aims at detecting the semantic relation between a pair of entities in a given target sentence. However, previous methods lack the description of the relation definition, thus needing to model the implication of relations during training. To tackle this issue, we propose a natural language inference method for relation extraction. Given a premise and a hypothesis, the natural language inference task refers to predicting whether the facts in the premise necessarily imply the facts in the hypothesis. Specifically, for each relation type, we construct a relation description. These relation descriptions are the definition of relation, containing prior knowledge that helps model understand the meaning of relation. The given target sentence is viewed as the premise, and these descriptions are viewed as the hypotheses. Then model infers whether these hypotheses can be concluded from the premise. Based on the inference results, our model selects the relation corresponding to the most confident hypothesis as the prediction. Substantial experiments on SemEval2010 Task8 dataset demonstrate that the proposed method achieves state-of-the-art performance. © 2021 Elsevier B.V.","Knowledge enhancing; Natural language inference; Relation extraction","Semantics; HELP model; Inference methods; Knowledge enhancing; Language inference; Natural language inference; Natural languages; Prior-knowledge; Relation extraction; Semantic relations; State-of-the-art performance; Extraction",Article,Scopus,2-s2.0-85120641970
"Ding J., Hu W., Yu X., Qu Y.","56335584100;57191221527;57360086000;8400208900;","An empirical study of representing adjectives over knowledge bases: Approach, lexicon and application",2022,"Journal of Web Semantics","72",,"100681","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120506292&doi=10.1016%2fj.websem.2021.100681&partnerID=40&md5=396d19b59a4283bfd33d73774cd7e71a","Adjectives are common in natural language, and their usage and semantics have been studied broadly. In recent years, with the rapid growth of knowledge bases (KBs), many knowledge-based question answering (KBQA) systems are developed to answer users’ natural language questions over KBs. A fundamental task of such systems is to transform natural language questions into structural queries, e.g., SPARQL queries. Thus, such systems require knowledge about how natural language expressions are represented in KBs, including adjectives. In this paper, we specifically address the problem of representing adjectives over KBs. We propose a novel approach, called Adj2SP, to represent adjectives as SPARQL query patterns. Adj2SP contains a statistic-based approach and a neural network-based approach, both of them can effectively reduce the search space for adjective representations and overcome the lexical gap between input adjectives and their target representations. Two adjective representation datasets are built for evaluation, with adjectives used in QALD and Yahoo! Answers, as well as their representations over DBpedia. Experimental results show that Adj2SP can generate representations of high quality and significantly outperform several alternative approaches in F1-score. Furthermore, we publish Lark, a lexicon for adjective representations over KBs. Current KBQA systems show an improvement of over 24% in F1-score by integrating Adj2SP. © 2021 Elsevier B.V.","Adjective representation; Knowledge base; Question answering; SPARQL","Natural language processing systems; Petroleum reservoir evaluation; Semantics; Adjective representation; Empirical studies; F1 scores; Knowledge based; Natural language questions; Natural languages; Question Answering; Question answering systems; Rapid growth; SPARQL; Knowledge based systems",Article,Scopus,2-s2.0-85120506292
"Silva Rosa L., Soares Silva T., Fantinato M., Heloisa Thom L.","57357367300;57357367400;15019013300;25121848500;","A visual approach for identification and annotation of business process elements in process descriptions",2022,"Computer Standards and Interfaces","81",,"103601","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120338461&doi=10.1016%2fj.csi.2021.103601&partnerID=40&md5=3b045933c1f41ff077f216d3792c4af9","Business process management (BPM) has been proven to provide several benefits for organizations (e.g., efficiency, agility, governance). However, the effort required for adopting a process-centered approach can be a challenge in different aspects, including financial concerns, organizational changes and time consumption. To achieve this goal, many companies use different approaches, such as document analysis, to be able to discover and understand their business processes. In light of this, in this paper, we propose a user-interactive visual approach to support the process comprehension by identifying and annotating core BPMN 2.0 elements in process descriptions. Specifically, our approach is able to detect sequences of words that indicate the presence of a process element, create a consistent data structure, and expose it as a consumable web service. To evaluate our approach, we conducted a survey experiment, showing promising results in every category evaluated, for which 88% of the users indicated positive results concerning the usefulness of the approach to assist the process modeling phase. Additionally, a process modeling case study shows a designed process model with a precision of 77% of process elements in comparison to its original process model. © 2021 Elsevier B.V.","Business process management; Natural language processing; Visually interactive process description","Administrative data processing; Natural language processing systems; Process engineering; Visual languages; Web services; Business Process; Identification and annotation; In-process; Interactive process; Organizational change; Process descriptions; Process elements; Process-models; Time consumption; Visually interactive process description; Enterprise resource management",Article,Scopus,2-s2.0-85120338461
"Pan F., Li S., Ao X., He Q.","57271408600;57211409102;57197191379;57271675100;","Relation Reconstructive Binarization of word embeddings",2022,"Frontiers of Computer Science","16","2","162307","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115683869&doi=10.1007%2fs11704-021-0108-3&partnerID=40&md5=8f8995546515730a595f4850cb325768","Word-embedding acts as one of the backbones of modern natural language processing (NLP). Recently, with the need for deploying NLP models to low-resource devices, there has been a surge of interest to compress word embeddings into hash codes or binary vectors so as to save the storage and memory consumption. Typically, existing work learns to encode an embedding into a compressed representation from which the original embedding can be reconstructed. Although these methods aim to preserve most information of every individual word, they often fail to retain the relation between words, thus can yield large loss on certain tasks. To this end, this paper presents Relation Reconstructive Binarization (R2B) to transform word embeddings into binary codes that can preserve the relation between words. At its heart, R2B trains an auto-encoder to generate binary codes that allow reconstructing the word-by-word relations in the original embedding space. Experiments showed that our method achieved significant improvements over previous methods on a number of tasks along with a space-saving of up to 98.4%. Specifically, our method reached even better results on word similarity evaluation than the uncompressed pre-trained embeddings, and was significantly better than previous compression methods that do not consider word relations. © 2022, Higher Education Press.","binary word embedding; embedding compression; variational auto-encoder","Binary codes; Hash functions; Natural language processing systems; Signal encoding; Auto encoders; Binarizations; Binary word embedding; Binary words; Embedding compression; Embeddings; Low resource devices; Processing model; Variational auto-encoder; Embeddings",Article,Scopus,2-s2.0-85115683869
"Zhang H., Wong R.K., Chu V.W.","57211169997;7402126320;55776088300;","Hybrid Variational Autoencoder for Recommender Systems",2022,"ACM Transactions on Knowledge Discovery from Data","16","2","37","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114997398&doi=10.1145%2f3470659&partnerID=40&md5=90e6720675fb24c0a135372c742f3f62","E-commerce platforms heavily rely on automatic personalized recommender systems, e.g., collaborative filtering models, to improve customer experience. Some hybrid models have been proposed recently to address the deficiency of existing models. However, their performances drop significantly when the dataset is sparse. Most of the recent works failed to fully address this shortcoming. At most, some of them only tried to alleviate the problem by considering either user side or item side content information. In this article, we propose a novel recommender model called Hybrid Variational Autoencoder (HVAE) to improve the performance on sparse datasets. Different from the existing approaches, we encode both user and item information into a latent space for semantic relevance measurement. In parallel, we utilize collaborative filtering to find the implicit factors of users and items, and combine their outputs to deliver a hybrid solution. In addition, we compare the performance of Gaussian distribution and multinomial distribution in learning the representations of the textual data. Our experiment results show that HVAE is able to significantly outperform state-of-the-art models with robust performance. © 2021 Association for Computing Machinery.","hybrid filtering; Recommender systems","Electronic commerce; Learning systems; Recommender systems; Semantics; Content information; Customer experience; Hybrid solution; Multinomial distributions; Personalized recommender systems; Robust performance; Semantic relevance; State of the art; Collaborative filtering",Article,Scopus,2-s2.0-85114997398
"Cantini R., Marozzo F., Bruno G., Trunfio P.","57215871062;37097646500;57260610100;6602968036;","Learning Sentence-to-Hashtags Semantic Mapping for Hashtag Recommendation on Microblogs",2022,"ACM Transactions on Knowledge Discovery from Data","16","2","32","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114988746&doi=10.1145%2f3466876&partnerID=40&md5=a402694ecfd00ce39fa4f688514f43da","The growing use of microblogging platforms is generating a huge amount of posts that need effective methods to be classified and searched. In Twitter and other social media platforms, hashtags are exploited by users to facilitate the search, categorization, and spread of posts. Choosing the appropriate hashtags for a post is not always easy for users, and therefore posts are often published without hashtags or with hashtags not well defined. To deal with this issue, we propose a new model, called HASHET (HAshtag recommendation using Sentence-to-Hashtag Embedding Translation), aimed at suggesting a relevant set of hashtags for a given post. HASHET is based on two independent latent spaces for embedding the text of a post and the hashtags it contains. A mapping process based on a multi-layer perceptron is then used for learning a translation from the semantic features of the text to the latent representation of its hashtags. We evaluated the effectiveness of two language representation models for sentence embedding and tested different search strategies for semantic expansion, finding out that the combined use of BERT (Bidirectional Encoder Representation from Transformer) and a global expansion strategy leads to the best recommendation results. HASHET has been evaluated on two real-world case studies related to the 2016 United States presidential election and COVID-19 pandemic. The results reveal the effectiveness of HASHET in predicting one or more correct hashtags, with an average F-score up to 0.82 and a recommendation hit-rate up to 0.92. Our approach has been compared to the most relevant techniques used in the literature (generative models, unsupervised models, and attention-based supervised models) by achieving up to 15% improvement in F-score for the hashtag recommendation task and 9% for the topic discovery task. © 2021 Association for Computing Machinery.","Deep neural networks; hashtag recommendation; sentence embedding; social media; word embedding","Embeddings; Mapping; Multilayer neural networks; Semantics; Social networking (online); Micro-blogging platforms; Multi layer perceptron; Presidential election; Representation model; Search strategies; Semantic expansion; Semantic features; Social media platforms; Translation (languages)",Article,Scopus,2-s2.0-85114988746
"Tran Q., Tran L., Hai L.C., Linh N.V., Than K.","57226606950;57226612243;57226591320;57226539722;55369225600;","From implicit to explicit feedback: A deep neural network for modeling sequential behaviours and long-short term preferences of online users",2022,"Neurocomputing","479",,,"89","105",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123607406&doi=10.1016%2fj.neucom.2022.01.023&partnerID=40&md5=5f759366281e5731b32a13a51c1eb190","In this work, we examine the advantages of using multiple types of behaviours in recommendation systems. Intuitively, each user often takes some implicit actions (e.g., click) before making an explicit decision (e.g., purchase). Previous studies show that implicit and explicit feedback has different roles for a useful recommendation. However, these studies either exploit implicit and explicit behaviours separately or ignore the semantics of sequential interactions between users and items. In addition, we go from the hypothesis that a user's preferences at a time are combinations of long-term and short-term interests. In this paper, we propose some Deep Learning architectures. The first one is Implicit to Explicit (ITE), to exploit users’ interests through the sequence of their actions. The second and third ones are two versions of ITE with Bidirectional Encoder Representations from Transformers based (BERT-based) architecture called BERT-ITE and BERT-ITE-Si, which combine users’ long- and short-term preferences without and with side information to enhance users’ representations. The experimental results show that our models outperform previous state-of-the-art ones and also demonstrate our views on the effectiveness of exploiting the implicit to explicit order as well as combining long- and short-term preferences in three large-scale datasets. The source code of our paper is available at: https://github.com/tranquyenbk173/BERT_ITE. © 2022 Elsevier B.V.","Collaborative filtering; Deep learning; Explicit feedback; Implicit feedback; Long-term preference; Recommendation systems; Short-term preference","Deep neural networks; Large dataset; Network architecture; Online systems; Recommender systems; Semantics; User profile; Deep learning; Explicit feedback; Implicit feedback; Long-term preference; Online users; Sequential interactions; Short term; Short-term interests; Short-term preference; User's preferences; Collaborative filtering; article; deep learning; deep neural network; filtration; semantics",Article,Scopus,2-s2.0-85123607406
"Markov I., Nastase V., Strapparava C.","57225431345;8681915400;6602773549;","Exploiting native language interference for native language identification",2022,"Natural Language Engineering","28","2",,"167","197",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096898655&doi=10.1017%2fS1351324920000595&partnerID=40&md5=84b27789269d829a7b005d3ce16303b7","Native language identification (NLI) - the task of automatically identifying the native language (L1) of persons based on their writings in the second language (L2) - is based on the hypothesis that characteristics of L1 will surface and interfere in the production of texts in L2 to the extent that L1 is identifiable. We present an in-depth investigation of features that model a variety of linguistic phenomena potentially involved in native language interference in the context of the NLI task: the languages' structuring of information through punctuation usage, emotion expression in language, and similarities of form with the L1 vocabulary through the use of anglicized words, cognates, and other misspellings. The results of experiments with different combinations of features in a variety of settings allow us to quantify the native language interference value of these linguistic phenomena and show how robust they are in cross-corpus experiments and with respect to proficiency in L2. These experiments provide a deeper insight into the NLI task, showing how native language interference explains the gap between baseline, corpus-independent features, and the state of the art that relies on features/representations that cover (indiscriminately) a variety of linguistic phenomena. ©","cognates; emotions; native language identification; Native language interference; punctuation","Linguistics; Emotion expression; Linguistic phenomena; Native language; Second language; State of the art; Natural language processing systems",Article,Scopus,2-s2.0-85096898655
"Salloum W., Habash N.","35797495500;6602991728;","Unsupervised Arabic dialect segmentation for machine translation",2022,"Natural Language Engineering","28","2",,"223","248",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095428265&doi=10.1017%2fS1351324920000455&partnerID=40&md5=12a6f69c335c68f39d448d5c757dd186","Resource-limited and morphologically rich languages pose many challenges to natural language processing tasks. Their highly inflected surface forms inflate the vocabulary size and increase sparsity in an already scarce data situation. In this article, we present an unsupervised learning approach to vocabulary reduction through morphological segmentation. We demonstrate its value in the context of machine translation for dialectal Arabic (DA), the primarily spoken, orthographically unstandardized, morphologically rich and yet resource poor variants of Standard Arabic. Our approach exploits the existence of monolingual and parallel data. We show comparable performance to state-of-the-art supervised methods for DA segmentation. ©","Arabic dialects; Machine translation; Morphology; Unsupervised learning","Computer aided language translation; Natural language processing systems; Arabic dialects; Dialectal arabics; Machine translations; Morphological segmentation; NAtural language processing; Standard arabics; State of the art; Supervised methods; Computational linguistics",Article,Scopus,2-s2.0-85095428265
"Zhu Y., Wu O.","57430934400;24081688000;","Elementary discourse units with sparse attention for multi-label emotion classification",2022,"Knowledge-Based Systems","240",,"108114","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123726483&doi=10.1016%2fj.knosys.2021.108114&partnerID=40&md5=f89743869af77c1717aeb25550a0ba1a","Emotion classification is an important task in natural language processing. Existing studies usually regard it as a multi-label classification task. However, they fail to effectively capture clause information and highlight weak (low-content) emotions that tend to be overwhelmed in co-existing emotions. To tackle these limitations, we propose a novel network “EduEmo”, which contains three parts: BERT-based encoder, Word-level attention layer, and RealFormer-based encoder. Specifically, BERT-based encoder models the associations between labels and words; Word-level attention layer captures the elementary discourse units (EDUs) representations that commonly contain single-emotion; RealFormer-based encoder leverages sparse attention to highlight the weak emotions and model the associations between labels and EDUs. In addition, we propose auxiliary-adversarial training algorithm, which adds perturbations to hard samples along the direction of gradient descent opposite to standard adversarial training. Experimental results on two benchmark datasets show that the proposed model outperforms favorably previous state-of-the-art methods. Experimental results on auxiliary-adversarial training indicate that the proposed training algorithm can further improve the generalization performance of adversarial training on emotion classification. © 2022 Elsevier B.V.","Adversarial training; Attention; Elementary discourse units; Emotion classification; Multi-label classification","Classification (of information); Gradient methods; Learning algorithms; Natural language processing systems; Adversarial training; Attention; Benchmark datasets; Co-existing; Elementary discourse unit; Emotion classification; Gradient-descent; Multi-labels; Training algorithms; Word level; Signal encoding",Article,Scopus,2-s2.0-85123726483
"Rassil A., Chougrad H., Zouaki H.","57217100435;57188701294;6507986820;","Holistic Graph Neural Networks based on a global-based attention mechanism",2022,"Knowledge-Based Systems","240",,"108105","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123032243&doi=10.1016%2fj.knosys.2021.108105&partnerID=40&md5=ef869037afe89da5c0408b8d73d6d780","Graph Neural Networks (GNNs) have become increasingly popular due to their impressive capacity to perform classification or regression on high-dimensional graph-structured data. However, standard message passing GNNs typically define nodes embeddings through a recursive neighborhood aggregation process which updates the representation vector of each node with reference to its neighborhood only. In this paper, we propose the Holistic Graph Neural Network (HGNN), a two-fold architecture which introduces a global-based attention mechanism for learning and generating nodes embeddings. The global features we inject, summarize the overall global behavior of the graph in addition to the local semantic and structural information. These global features will make each individual node aware of the global behavior of the graph outside the borders of the local neighborhood. We further propose a variant of the HGNN, we call HGNNα based on a more sophisticated hierarchical global-feature extraction mechanism. We explore diverse global pooling strategies to derive highly expressive global features. We also show that state-of-the-art GNNs can significantly benefit from the addition of the global-based attention introduced. Furthermore, we prove the efficiency of the HGNN model theoretically and adapt it to support graph data which carries edge attributes for example the Molecular datasets from the Open Graph Benchmark. Experiments on Bioinformatics datasets, Social Networks and Molecular datasets demonstrate that our proposed models achieve much better performance than state-of-the-art methods, for instance we achieved improvements of +11% on COLLAB and +13% on IMDB-BINARY datasets. © 2022 Elsevier B.V.","Global pooling; Graph classification; Graph Neural Networks; Node representation","Embeddings; Graph theory; Message passing; Semantics; Attention mechanisms; Embeddings; Global behaviors; Global feature; Global pooling; Graph classification; Graph neural networks; Neighbourhood; Network-based; Node representation; Graph neural networks",Article,Scopus,2-s2.0-85123032243
"Xu J., Zhou L., Zhao W., Fan Y., Ding X., Yuan X.","37071627700;57285126600;57286052600;53870130700;55347107200;57204117133;","Zero-shot learning for compound fault diagnosis of bearings",2022,"Expert Systems with Applications","190",,"116197","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119600112&doi=10.1016%2fj.eswa.2021.116197&partnerID=40&md5=ee9ba27b2f5079f1f40ab0f923c6bed5","Due to the concurrency and coupling of various types of faults, and the number of possible fault modes grows exponentially, thereby compound fault diagnosis is a difficult problem in bearing fault diagnosis. The existing deep learning models can extract fault features when there are a large number of labeled compound fault samples. In industrial scenarios, collecting and labeling sufficient compound fault samples are unpractical. Using the model trained on single fault samples to identify unknown compound faults is challenging and innovative. To address this problem, we propose a Zero-shot Learning Compound Fault Diagnosis Model of bearings (ZLCFDM). We design an encoding method to express the semantics of single faults and compound faults according to the fault characteristics. A convolutional neural network is developed to extract the time–frequency features of the compound fault signal. Then we embed the semantic feature of the fault into the visual space of the fault data. The Euclidean distance is used to measure the distance between the signal features and the semantic features of the compound faults to identify the categories of unknown compound faults. To validate the proposed method, we conduct experiments on a self-built testbed. The results demonstrate that the accuracy of identifying compound fault reached 77.73% when the model was trained without any compound fault samples. © 2021 Elsevier Ltd","Compound fault diagnosis; Fault diagnosis; Semantics; Zero-shot Learning","Bearings (machine parts); Convolutional neural networks; Deep learning; Fault detection; Semantics; Bearing fault diagnosis; Compound fault diagnosis; Compound faults; Fault modes; Fault sample; Faults diagnosis; Learning models; Semantic features; Single fault; Zero-shot learning; Failure analysis",Article,Scopus,2-s2.0-85119600112
"Agarwal R., Chatterjee N.","57266000400;56212078200;","Improvements in Multi-Document Abstractive Summarization using Multi Sentence Compression with Word Graph and Node Alignment",2022,"Expert Systems with Applications","190",,"116154","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119268273&doi=10.1016%2fj.eswa.2021.116154&partnerID=40&md5=0b2963cf9ae8a478c6aae9d021508b16","The present work proposes a scheme for multi-document abstractive text summarization using node-aligned Word Graph based representation of clustered sentences. In the first step, the proposed scheme uses SBERT embedding for representing the sentences as fixed-size vectors. The sentences belonging to the same cluster are then represented using Word Graph, in which words of different sentences are aligned based on their semantic and syntactic similarities. The advantage of the above representation is that it utilizes alignment information of words between pairs of similar sentences to merge nodes in the Word Graph, and thereby facilitating the generation of sentences with multiple chunks of information. A sentence scoring function assisted by an intensification function is used to measure the grammaticality and informativeness of the generated sentences. Integer Linear Programming has been used to make the final selection of the scored sentences for generating the abstract. Experiments conducted for the task of sentence fusion and multi-document summarization demonstrate superior performance in comparison with the state-of-the-art techniques available in the literature. © 2021 Elsevier Ltd","Multi sentence compression; Multi-document abstraction; Node alignment; Sentence fusion; Word Graph","Graph theory; Graphic methods; Integer programming; Semantics; Embeddings; Graph-based representations; Multi sentence compression; Multi-document abstraction; Multidocuments; Node alignment; Sentence compression; Sentence fusions; Text Summarisation; Word graphs; Abstracting",Article,Scopus,2-s2.0-85119268273
"Zhou D., Peng X., Li L., Han J.-M.","35782606000;57219551995;56111424200;57218091388;","Cross-lingual embeddings with auxiliary topic models",2022,"Expert Systems with Applications","190",,"116194","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119249832&doi=10.1016%2fj.eswa.2021.116194&partnerID=40&md5=29c85c73141c59a7e9f1e17f431cd045","Projection-based methods for generating high-quality Cross-Lingual Embeddings (CLEs) have shown state-of-the-art performance in many multilingual applications. Supervised methods that rely on character-level information or unsupervised methods that need only monolingual information are both popular and have their pros and cons. However, there are still problems in terms of the quality of monolingual word embedding spaces and the generation of the seed dictionaries. In this work, we aim to generate effective CLEs with auxiliary Topic Models. We utilize both monolingual and bilingual topic models in the procedure of generating monolingual embedding spaces and seed dictionaries for projection. We present a comprehensive evaluation of our proposed model through the means of bilingual lexicon extraction, cross-lingual semantic word similarity and cross-lingual document classification tasks. We show that our proposed model outperforms existing supervised and unsupervised CLE models built on basic monolingual embedding spaces and seed dictionaries. It also exceeds CLE models generated from representative monolingual topical word embeddings. © 2021 Elsevier Ltd","Cross-lingual embeddings; Projection-based methods; Seed dictionaries; Topical models; Word embedding models","Information retrieval systems; Semantics; Cross-lingual; Cross-lingual embedding; Embeddings; High quality; Projection-based method; Seed dictionary; Topic Modeling; Topical model; Word embedding model; Embeddings",Article,Scopus,2-s2.0-85119249832
"Sharma H., Singh Jalal A.","57211687211;57202751422;","A framework for visual question answering with the integration of scene-text using PHOCs and fisher vectors",2022,"Expert Systems with Applications","190",,"116159","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118877951&doi=10.1016%2fj.eswa.2021.116159&partnerID=40&md5=e5add948322bce6ef246b56d19666e81","Text contained in an image gives useful information about that image. Consider a warning signboard with text “high voltage”; it indicates the hazard or risk involved in the image. Thus, this semantic textual information can be very useful for better understanding of images, which is not utilized by the existing visual question answering (VQA) models. However, the presence of this textual information in images can strongly guide the VQA task. This work deal with the task of visual question answering by exploiting these textual cues together with the visual content to boost the accuracy of VQA models. In the work, a novel VQA model is proposed based on the PHOC and fisher vector based representation. Based on the PHOCs of the scene-text, we have constructed a powerful descriptor by using a Fisher Vectors. Also, the proposed model uses transformer model together with dynamic pointer networks for answer decoding process. Thus, the proposed model uses a sequence of decoding steps for answer generation instead of just assuming answer prediction as a classification problem as considered by previous works. We have shown the qualitative and quantitative results on three popular datasets: VQA 2.0, TextVQA and ST-VQA. The results show the effectiveness of the proposed model over the existing models. © 2021","Computer vision; Dynamic pointer networks; Fisher vector; PHOC; Visual Question Answering (VQA)","Decoding; Semantics; Vectors; Dynamic pointer network; Fisher vectors; High-voltages; Model use; PHOC; Question Answering; Question Answering Task; Scene Text; Textual information; Visual question answering; Computer vision",Article,Scopus,2-s2.0-85118877951
"Xu M., Zeng B., Yang H., Chi J., Chen J., Liu H.","57223128725;57196723371;57210557953;57416264000;57415625500;57415873500;","Combining dynamic local context focus and dependency cluster attention for aspect-level sentiment classification",2022,"Neurocomputing","478",,,"49","69",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122993231&doi=10.1016%2fj.neucom.2021.12.084&partnerID=40&md5=0f4f550c1e9ddf4271364a98268f304a","Aspect-level sentiment classification (ASC), as a subtask of Aspect-based sentiment analysis (ABSA), aims to analyze the sentiment polarity of different aspect terms in a sentence. Although the existing methods have been proven to be effective, they fail to effectively identify the range of local context and fully leverage the nonequivalent property of dependency relation. Hence, we propose a concept of dependency cluster and design two modules named Dynamic Local Context Focus (DLCF) and Dependency Cluster Attention (DCA) respectively. The DLCF can dynamically capture the range of local context based on the different max distance from the target aspect term to its context words and the DCA allows the model to pay more attention to the cluster which is more critical for sentiment classification. Together with the two modules, we then propose the DLCF-DCA model, in which the DLCF is equipped before DCA. Considering the DLCF has masked or weighted down the less-semantic-relative words, the semantic information can therefore be better extracted in DCA. Experiments conducted on six benchmark datasets demonstrate that DLCF-DCA achieves the state-of-the-art results. Moreover, the ablation experiment results also verify the effectiveness of each part in DLCF-DCA. © 2021","Aspect-level sentiment classification; Dependency cluster attention; Dynamic local context focus","Semantics; Aspect-level sentiment classification; Context-based; Dependency cluster attention; Dependency relation; Dynamic local context focus; Local contexts; Property; Sentiment analysis; Sentiment classification; Subtask; Sentiment analysis; article; attention; controlled study",Article,Scopus,2-s2.0-85122993231
"Gao J., Zhang Z., Cao P., Huang W., Li F.","57218701156;15041087700;57376503400;56647137000;56318889800;","Citation entity recognition method using multi-feature semantic fusion based on deep learning",2022,"Concurrency and Computation: Practice and Experience","34","6","e6770","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121354385&doi=10.1002%2fcpe.6770&partnerID=40&md5=f63b4e1e538be30cf8b1825bde8a351a","The effective entity recognition method can quickly and accurately identify the citation entity to facilitate citation comparison, thereby reducing the occurrence of academic fraud and other behaviors. But there is no very effective way to solve this problem till now. In recent years, neural network models for named entity recognition (NER) have shown better performances on general domain datasets. After the multi-feature citation dataset is created, the article proposes contextual multi-feature embedding (CMFE) method for word embedding which use multi-feature to enhance semantic and use CNN to get multi-level feature. Based on CMFE, a multi-feature semantic fusion model (MFSFM) is proposed. It designs the multi-convolution kernel mixed residual CNN module to obtain local attention information and enhance the sensitivity of the entity boundary information. The BiLSTM and LSTM is used for timing learning. The experimental results of Chinese citation datasets and Chinese–English mixed citation datasets show that CMFE can better represent semantics, and MFSFM can perform citation entity recognition well. Finally, the experimental results of CONLL2003 dataset show that it is general on NER. © 2021 John Wiley & Sons Ltd.","citation entity recognition; CNN; contextual multi-feature embedding; CRF; LSTM; multi-feature semantic fusion","Long short-term memory; Semantics; Citation entity recognition; CNN; Contextual multi-feature embedding; CRF; Entity recognition; Feature embedding; LSTM; Multi-feature semantic fusion; Multifeatures; Semantic fusion; Embeddings",Article,Scopus,2-s2.0-85121354385
"Jena B., Nayak G.K., Saxena S.","57216221471;57197141333;57217377039;","Convolutional neural network and its pretrained models for image classification and object detection: A survey",2022,"Concurrency and Computation: Practice and Experience","34","6","e6767","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120947550&doi=10.1002%2fcpe.6767&partnerID=40&md5=0b53adb132ecb93465b9f5bd4d398d47","At present, in the age of computers and automation of services, deep learning (DL) technology, mainly the subset of machine learning (ML) and artificial intelligence (AI), is expressively used in innumerable domains of computer vision such as data analysis, image recognition, classification, natural language processing, and many more. It has become the foremost choice of researchers as of its effectiveness in producing decent results. This paper presents detailed and analytical literature starting from the very elementary level to the recent trends of this trending technology while focusing on the most used DL model, that is, convolutional neural network and its pretrained models for image classification and object detection. It also reviews diverse existing current literature based on this. Further, a brief introduction of AI, ML, and DL has also been presented, making the foundation for the readers. As pretrained models continuously give an upper edge to DL over ML and other technologies, 23 most popular pretrained models with their architectural diagrams have also been presented. This paper aims to summarize and analyze all the concepts used to formulate DL and its models. Also, we have emphasized more on the GoogleNet models and the entire Inception modules in detail. Finally, the fascinating applications and discussion on integral components of DL have been presented. This paper will definitely draw the attention of the students and researchers working in the area of DL and its models. © 2021 John Wiley & Sons Ltd.","convolutional; deep learning; machine learning; neural network; neural network; pretrained models","Convolutional neural networks; Deep learning; Engineering education; Image classification; Image recognition; Learning algorithms; Natural language processing systems; Object detection; Object recognition; Convolutional neural network; Deep learning; Elementary levels; Images classification; Learning models; Learning technology; Neural-networks; Pretrained model; Recent trends; Convolution",Article,Scopus,2-s2.0-85120947550
"Liu Q., He X., Zhang M., Teng Q., Li B., Qing L.","57211107381;9237988800;57390633700;7005503530;57199786691;23971364800;","Feature separation and double causal comparison loss for visible and infrared person re-identification",2022,"Knowledge-Based Systems","239",,"108042","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122621432&doi=10.1016%2fj.knosys.2021.108042&partnerID=40&md5=c8149ae74f15d051cd31a17dba68a830","Visible and infrared cross-modal person re-identification (VI-ReID) is the task of retrieving person images from visible images and infrared images. In the past, most VI-ReID algorithms focused only on learning common representations of different modes. In contrast, we extract identity-related features from different modalities and filter out identity-independent interference, and we let our network learn the domain unchanged as a more effective feature representation. In this paper, a novel end-to-end feature separation and double causal comparison loss framework for VI-ReID (FSDCC) is proposed to address cross-modality ReID tasks. We first separate the features using the feature separation module (FSM) to obtain strong identity-related and weak identity-related features. Then, double causal comparison loss is used to guide the model training. This process effectively reduces the influence of identity-irrelevant information such as occlusion and background, and finally achieves enhanced expression of identity-relevant features. Simultaneously, we combine identity loss and weighted regularization TriHard loss in a progressive joint training manner. Additionally, to enhance the CNN's ability to extract global semantic information and better establish the connection between two pixels with a certain distance on the image, we propose CNS non-local neural network (CNS non-local), finally improving VI-ReID accuracy. Extensive experiments on two cross-modality datasets demonstrate that the proposed method outperforms current state-of-the-art by a large margin, achieving rank-1/mAP accuracy 87.18%/79.10% on the RegDB dataset, and 68.79%/65.72% on the SYSU-MM01 dataset. © 2021 Elsevier B.V.","Cross-modality; Double causal comparison; Feature separation; Visible-infrared retrieval","Infrared imaging; Large dataset; Semantics; And filters; Cross modality; Cross-modal; Double causal comparison; Feature separation; Identification algorithms; Nonlocal; Person re identifications; Visible image; Visible-infrared retrieval; Image enhancement",Article,Scopus,2-s2.0-85122621432
"Zhang H., Yang S., Zhu H.","56098272800;57222109843;8538289200;","CJE-TIG: Zero-shot cross-lingual text-to-image generation by Corpora-based Joint Encoding",2022,"Knowledge-Based Systems","239",,"108006","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122219654&doi=10.1016%2fj.knosys.2021.108006&partnerID=40&md5=e58ae4c11aaf751ba267a0d5dc55ce96","Recently, text-to-Image (T2I) generation has been well developed by improving synthesis authenticity, text-consistency and generation diversity. However, large amount of pairwise image–text data required restricts generalization of synthesis models only to its pre-trained language. In this paper, a cross-lingual pre-training method is proposed to adapt target low-resource language to pre-trained generative models. As far as we known, this is the first time that arbitrary input languages could access T2I generation. This joint encoding scheme fulfills both universal and visual semantic alignment. With any prepared GAN-based T2I framework, pre-trained source encoder model could be easily fine-tuned to construct target encoder model and hence entirely enable transfer of T2I synthesis ability between languages. After that, a semantic-level alignment independent of source T2I structure is established to guarantee optimal text consistency and detail generation. Different from monolingual T2I methods that apply discriminator to enhance generation quality, we use an adversarial training scheme that optimizes the sentence-level alignment along with the word-level alignment with a self-attention mechanism. Considering of training for low-resource languages lack of parallel texts in practice, target input embedding is designed available for zero-shot learning. Experimental results prove robustness of the proposed cross-lingual T2I pre-training on multiple downstream generative models and target languages applied. © 2021 Elsevier B.V.","Cross-lingual pre-training; Joint adversarial training; Semantic alignment; Text-to-image synthesis; Universal contextual word vector space","Alignment; Encoding (symbols); Image enhancement; Semantics; Structural optimization; Vector spaces; Contextual words; Cross-lingual; Cross-lingual pre-training; Images synthesis; Joint adversarial training; Pre-training; Semantic alignments; Text-to-image synthesis; Universal contextual word vector space; Word vectors; Signal encoding",Article,Scopus,2-s2.0-85122219654
"Al-Muhammed M.J., Lonsdale D.W.","14041270700;9279039800;","Ontology-aware dynamically adaptable free-form natural language agent interface for querying databases",2022,"Knowledge-Based Systems","239",,"108012","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122197563&doi=10.1016%2fj.knosys.2021.108012&partnerID=40&md5=945d3aff3b9d24f250021e02dc89e2b2","Studying the literature, one can see a large number of systems that provide natural language interfaces to databases. Despite their importance, these interfaces address only one part of the problem: transforming natural language queries to SQL queries and perhaps executing them against the underlying database. To truly handle the problem, it should be possible to develop dynamically adaptable database interfaces that can (1) adjust their functional behavior on the fly using only domain knowledge and (2) dynamically bind themselves to arbitrary databases and interface with them with no (or very little) human intervention. In particular, the interfaces should have a fixed process (algorithms and codes) and rely only on domain knowledge for adapting their functionality to any arbitrary database. This paper addresses this problem by offering a free-form natural language interface agent that, given domain ontologies, can bind itself to a target database and provide a natural language interface to it. The agent system uses its ontologies to establish mappings on-the-fly between a specific domain ontology and the underlying database's metadata, and then to transform free-form natural language queries to formal SQL queries that can execute against the underlying database. The preliminary simulations using our proof-of-concept prototype showed that our system successfully attached itself to databases and achieved high recall and precision in transforming natural language queries to formal ones. © 2021 Elsevier B.V.","Database interfaces; Dynamic natural language interfaces; Dynamically adaptable interface; On-the-fly ontology-metadata association; Ontology-based agent interface; Ontology-based process auto-configuration","Domain Knowledge; Metadata; Natural language processing systems; Query languages; Query processing; Adaptable interfaces; Auto-configuration; Database interfaces; Dynamic natural language interface; Dynamically adaptable interface; Natural language interfaces; On-the-fly ontology-metadatum association; Ontology's; Ontology-based; Ontology-based agent interface; Ontology-based process auto-configuration; Ontology",Article,Scopus,2-s2.0-85122197563
"Zhang L., He M.","57196128153;57218618570;","Prediction of solar cell materials via unsupervised literature learning",2022,"Journal of Physics Condensed Matter","34","9","095902","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122631305&doi=10.1088%2f1361-648X%2fac3e1e&partnerID=40&md5=89c0d4e66d53f1f59c52c63cc98ac0be","Despite the significant advancement of the data-driven studies for physical science, the textual data that are numerous in the literature are not fully embraced by the physics and materials community. In this manuscript, we successfully employ the natural language processing (NLP) technique to unsupervisedly predict the existence of solar cell types including the dye-sensitized solar cells and the perovskite solar cells based on literatures published prior to their first discovery without human annotation. Enlightened by this, we further identify possible solar cell material candidates via NLP starting with a comprehensive training database of 3.2 million paper abstracts published before 2021. The NLP model effectively predicts the existing solar cell materials, while an uncommon solar cell material namely PtSe2 is suggested as an appropriate candidate for the future solar cells. Its optoelectronic properties are comprehensive investigated via first-principles calculations to reveal the decent stability and optoelectronic performance of the NLP-predicted candidate. This study demonstrates the viability of the textual data for the data-driven materials prediction and highlights the NLP method as a powerful tool to reliably predict the solar cell materials. © 2021 IOP Publishing Ltd.","machine learning; natural language processing; NLP; perovskite; solar cell","Calculations; Dye-sensitized solar cells; Forecasting; Learning algorithms; Natural language processing systems; Perovskite solar cells; Platinum compounds; Selenium compounds; Cell-based; Data driven; Dye- sensitized solar cells; Human annotations; Language processing techniques; Material candidate; Physical science; Solar cell materials; Textual data; Training database; Perovskite",Article,Scopus,2-s2.0-85122631305
"Shu T., Wang Z., Lin L., Jia H., Zhou J.","57456122500;57465222000;57465222100;14015792800;57455336400;","Customer Perceived Risk Measurement with NLP Method in Electric Vehicles Consumption Market: Empirical Study from China",2022,"Energies","15","5","1637","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125182787&doi=10.3390%2fen15051637&partnerID=40&md5=60542838ae394f2d953ba0f1b2fbda18","In recent years, as people’s awareness of energy conservation, environmental protection, and sustainable development has increased, discussions related to electric vehicles (EVs) have aroused public debate on social media. At some point, most consumers face the possible risks of EVs—a critical psychological perception that invariably affects sales of EVs in the consumption market. This paper chooses to deconstruct customers’ perceived risk from third-party comment data in social media, which has better coverage and objectivity than questionnaire surveys. In order to analyze a large amount of unstructured text comment data, the natural language processing (NLP) method based on machine learning was applied in this paper. The measurement results show 15 abstracts in five consumer perceived risks to EVs. Among them, the largest number of comments is that of “Technology Maturity” (A13) which reached 25,329, and which belongs to the “Performance Risk” (PR1) dimension, indicating that customers are most concerned about the performance risk of EVs. Then, in the “Social Risk” (PR5) dimension, the abstract “Social Needs” (A51) received only 3224 comments and “Preference and Trust Rank” (A52) reached 22,324 comments; this noticeable gap indicated the changes in how consumers perceived EVs social risks. Moreover, each dimension’s emotion analysis results showed that negative emotions are more than 40%, exceeding neutral or positive emotions. Importantly, customers have the strongest negative emotions about the “Time Risk” (PR4), accounting for 54%. On a finer scale, the top three negative emotions are “Charging Time” (A42), “EV Charging Facilities” (A41), and “Maintenance of Value” (A33). Another interesting result is that “Social Needs” (A51)’s positive emotional comments were larger than negative emotional comments. The paper provides substantial evidence for perceived risk theory research by new data and methods. It can provide a novel tool for multi-dimensional and fine-granular capture customers’ perceived risks and negative emotions. Thus, it has the potential to help government and enterprises to adjust promotional strategies in a timely manner to reduce higher perceived risks and emotions, accelerating the sustainable development of EVs’ consumption market in China. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Consumption market; Electric vehicles; Emotion analysis; EVs; Natural language processing; NLP; Perceived risk; Social media comment","Abstracting; Behavioral research; Commerce; Electric vehicles; Environmental protection; Hazards; Learning algorithms; Natural language processing systems; Planning; Risk assessment; Risk perception; Sales; Surveys; Sustainable development; Consumption market; Emotion analysis; Perceived risk; Performance risk; Processing method; Risk measurement; Social media; Social medium comment; Social needs; Social risks; Social networking (online)",Article,Scopus,2-s2.0-85125182787
"Huang J., Zhou K., Xiong A., Li D.","55742338500;57462474400;14030855900;57462803600;","Smart Contract Vulnerability Detection Model Based on Multi‐Task Learning",2022,"Sensors","22","5","1829","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125103145&doi=10.3390%2fs22051829&partnerID=40&md5=e125281a4a78bddba9b2aaf2ade3c79d","The key issue in the field of smart contract security is efficient and rapid vulnerability detection in smart contracts. Most of the existing detection methods can only detect the presence of vulnerabilities in the contract and can hardly identify their type. Furthermore, they have poor scalability. To resolve these issues, in this study, we developed a smart contract vulnerability detection model based on multi‐task learning. By setting auxiliary tasks to learn more directional vulnerability features, the detection capability of the model was improved to realize the detection and recognition of vulnerabilities. The model is based on a hard‐sharing design, which consists of two parts. First, the bottom sharing layer is mainly used to learn the semantic information of the input contract. The text representation is first transformed into a new vector by word and positional embedding, and then the neural network, based on an attention mechanism, is used to learn and extract the feature vector of the contract. Second, the task‐specific layer is mainly employed to realize the functions of each task. A classical convolutional neural network was used to construct a classification model for each task that learns and extracts features from the shared layer for training to achieve their respective task objectives. The experimental results show that the model can better identify the types of vulnerabilities after adding the auxiliary vulnerability detection task. This model realizes the detection of vulnerabilities and recognizes three types of vulnerabilities. The multi‐task model was observed to perform better and is less expensive than a single‐task model in terms of time, computation, and storage. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Multi‐task learning; Security; Smart contract; Vulnerability detection","Convolutional neural networks; Learning systems; Multilayer neural networks; Semantics; Detection capability; Detection methods; Detection models; Key Issues; Learn+; Model-based OPC; Multitask learning; Security; Semantics Information; Vulnerability detection; Smart contract",Article,Scopus,2-s2.0-85125103145
"Kuznetsov M., Novikova E., Kotenko I., Doynikova E.","55350527900;55415626100;15925268000;37097097100;","Privacy Policies of IoT Devices: Collection and Analysis",2022,"Sensors","22","5","1838","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125075599&doi=10.3390%2fs22051838&partnerID=40&md5=6fa26ca5a7b7c77129cb4981e86a23d9","Currently, personal data collection and processing are widely used while providing digital services within mobile sensing networks for their operation, personalization, and improvement. Personal data are any data that identifiably describe a person. Legislative and regulatory documents adopted in recent years define the key requirements for the processing of personal data. They are based on the principles of lawfulness, fairness, and transparency of personal data processing. Privacy policies are the only legitimate way to provide information on how the personal data of service and device users is collected, processed, and stored. Therefore, the problem of making privacy policies clear and transparent is extremely important as its solution would allow end users to comprehend the risks associated with personal data processing. Currently, a number of approaches for analyzing privacy policies written in natural language have been proposed. Most of them require a large training dataset of privacy policies. In the paper, we examine the existing corpora of privacy policies available for training, discuss their features and conclude on the need for a new dataset of privacy policies for devices and services of the Internet of Things as a part of mobile sensing networks. The authors develop a new technique for collecting and cleaning such privacy policies. The proposed technique differs from existing ones by the usage of e-commerce platforms as a starting point for document search and enables more targeted collection of the URLs to the IoT device manufacturers’ privacy policies. The software tool implementing this technique was used to collect a new corpus of documents in English containing 592 unique privacy policies. The collected corpus contains mainly privacy policies that are developed for the Internet of Things and reflect the latest legislative requirements. The paper also presents the results of the statistical and semantic analysis of the collected privacy policies. These results could be further used by the researchers when elaborating techniques for analysis of the privacy policies written in natural language targeted to enhance their transparency for the end user. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Data collection; Dataset; IoT; Latent Dirichlet allocation; Natural language processing; Privacy policies; Privacy policy corpus","Data acquisition; Data privacy; Frequency domain analysis; Information services; Large dataset; Natural language processing systems; Semantics; Statistics; Transparency; Data collection; Dataset; Digital services; End-users; Latent Dirichlet allocation; Mobile sensing networks; Natural languages; Privacy policies; Privacy policy corpus; Internet of things",Article,Scopus,2-s2.0-85125075599
"Nguyen V., Rybinski M., Karimi S., Xing Z.","57203492979;56888667200;35302172500;57234963500;","Search like an expert: Reducing expertise disparity using a hybrid neural index for COVID-19 queries",2022,"Journal of Biomedical Informatics","127",,"104005","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125009207&doi=10.1016%2fj.jbi.2022.104005&partnerID=40&md5=85b61f4818bd3a80180da2dacac444a4","Consumers from non-medical backgrounds often look for information regarding a specific medical information need; however, they are limited by their lack of medical knowledge and may not be able to find reputable resources. As a case study, we investigate reducing this knowledge barrier to allow consumers to achieve search effectiveness comparable to that of an expert, or a medical professional, for COVID-19 related questions. We introduce and evaluate a hybrid index model that allows a consumer to formulate queries using consumer language to find relevant answers to COVID-19 questions. Our aim is to reduce performance degradation between medical professional queries and those of a consumer. We use a universal sentence embedding model to project consumer queries into the same semantic space as professional queries. We then incorporate sentence embeddings into a search framework alongside an inverted index. Documents from this index are retrieved using a novel scoring function that considers sentence embeddings and BM25 scoring. We find that our framework alleviates the expertise disparity, which we validate using an additional set of crowdsourced—consumer—queries even in an unsupervised setting. We also propose an extension of our method, where the sentence encoder is optimised in a supervised setup. Our framework allows for a consumer to search using consumer queries to match the search performance with that of a professional. © 2022 Elsevier Inc.","Biomedical search; COVID-19; Dense retrieval; Information retrieval; Medical misinformation; Natural language processing; Neural index; Universal sentence embeddings","Embeddings; Information retrieval; Professional aspects; Semantics; Biomedical search; COVID-19; Dense retrieval; Embeddings; Medical information needs; Medical knowledge; Medical misinformation; Medical professionals; Neural index; Universal sentence embedding; Natural language processing systems",Article,Scopus,2-s2.0-85125009207
"Mishra R., Gupta A., Gupta H.P., Dutta T.","57222116400;57214417148;55083157000;15135757900;","A Sensors Based Deep Learning Model for Unseen Locomotion Mode Identification using Multiple Semantic Matrices",2022,"IEEE Transactions on Mobile Computing","21","3",,"799","810",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124618413&doi=10.1109%2fTMC.2020.3015546&partnerID=40&md5=08f0eaaf649bcd9b6fc384e56924c28f","With the availability of various sensors in the smartphone, identifying a locomotion mode becomes convenient and effortless in recent years. Information about locomotion mode helps to improve journey planning, travel time estimation, and traffic management. Though there exists a significant amount of work towards locomotion mode recognition, the performance of these work is not pertinent and heavily depends on the labeled training instances. As it is impractical to gather a prior information (labeled instances) about all types of locomotion modes, the recognition model should be able to identify a new or unseen locomotion mode without having any corresponding training instance. This paper proposes a sensors based deep learning model to identify a locomotion mode by using labeled training instances. The approach also incorporates a concept of Zero-Shot learning to identify an unseen locomotion mode. The model obtains an attribute matrix based on the fusion of three semantic matrices. It also constructs a feature matrix by extracting the deep learning and hand-crafted features from the training instances. Later, the model builds a classifier by learning a mapping between attribute and feature matrices. Finally, this work evaluates the performance of the approach on collected and existing datasets using accuracy and F1 score. © 2002-2012 IEEE.","Deep learning; identification; locomotion mode; sensors","Deep learning; Matrix algebra; Traffic control; Travel time; Deep learning; Feature matrices; Identification; Learning models; Locomotion mode; matrix; Mode identification; Performance; Sensor; Smart phones; Semantics",Article,Scopus,2-s2.0-85124618413
"Guo X., Lu S., Tang Z., Bai Z., Diao L., Zhou H., Li L.","57208496804;57201324762;57219837582;57226144976;57219836195;57208495249;57189531507;","CG-ANER: Enhanced contextual embeddings and glyph features-based agricultural named entity recognition",2022,"Computers and Electronics in Agriculture","194",,"106776","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124189483&doi=10.1016%2fj.compag.2022.106776&partnerID=40&md5=c61ccb46fdf472a4bd3bf86a9bdfe643","In recent years, deep learning has greatly improved the performance of named entity recognition models in various fields, especially in the agricultural domain. However, most existing works only utilize word embedding models to generate the context-independent embeddings, which is limited in modeling polysemous words. Moreover, the abundant morphological information in agricultural texts has not been fully utilized. Besides, the local context information needs to be further extracted. To solve the aforementioned issues, a novel enhanced contextual embeddings and glyph features-based model was proposed. First, the contextual embeddings were dynamically generated by the fine-tuned Bidirectional Encoder Representation from Transformers (BERT) on the domain-specific corpus (e.g., agricultural texts), and then the multi-granularity information was obtained from the layers of BERT. Thus, the contextual embeddings not only contain domain-specific knowledge but also include multi-grained semantic information. Second, a novel 3-dimension convolutional neural network-based framework was designed to capture the contextual glyph features for each character from the image perspective. Third, a channel-wised fusion architecture was also introduced to further improve the ability of the convolutional neural network layer to capture local context features. Experimental results showed that our proposed model achieved the best F1-scores of 95.02% and 96.51% on AgCNER and Resume datasets, which indicated the effectiveness and generalization of our model to identify the entities in cross-domain texts. The ablation study in many aspects also demonstrated the better performance of the proposed model. © 2022 Elsevier B.V.","3-dimension convolutional neural network; Agricultural named entity recognition; Bi-directional long short-term memory network; Fine-tuning language model; Glyph features","Agriculture; Convolution; Convolutional neural networks; Deep learning; Domain Knowledge; Embeddings; Multilayer neural networks; Semantics; 3-dimension; 3-dimension convolutional neural network; Agricultural named entity recognition; Bi-directional; Bi-directional long short-term memory network; Convolutional neural network; Fine tuning; Fine-tuning language model; Glyph feature; Language model; Memory network; Named entity recognition; Network layers; ablation; agricultural ecosystem; architecture; recognition",Article,Scopus,2-s2.0-85124189483
"Shin J., Moon J.","57223303449;55362466700;","Learning to combine the modalities of language and video for temporal moment localization",2022,"Computer Vision and Image Understanding","217",,"103375","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124081780&doi=10.1016%2fj.cviu.2022.103375&partnerID=40&md5=6a979854c281c56e8c6cd094c67c7ca5","Temporal moment localization aims to retrieve the best video segment matching a moment specified by a query. The existing methods generate the visual and semantic embeddings independently and fuse them without full consideration of the long-term temporal relationship between them. To address these shortcomings, we introduce a novel recurrent unit, cross-modal long short-term memory (CM-LSTM), by mimicking the human cognitive process of localizing temporal moments that focuses on the part of a video segment related to the part of a query, and accumulates the contextual information across the entire video recurrently. In addition, we devise a two-stream attention mechanism for both attended and unattended video features by the input query to prevent necessary visual information from being neglected. To obtain more precise boundaries, we propose a two-stream attentive cross-modal interaction network (TACI) that generates two 2D proposal maps obtained globally from the integrated contextual features, which are generated by using CM-LSTM, and locally from boundary score sequences and then combines them into a final 2D map in an end-to-end manner. On the TML benchmark dataset, ActivityNet-Captions, the TACI outperforms state-of-the-art TML methods with R@1 of 45.50% and 27.23% for IoU@0.5 and IoU@0.7, respectively. In addition, we show that the revised state-of-the-arts methods by replacing original LSTM with our CM-LSTM achieves performance gains. © 2022 The Author(s)","Boundary alignment; Cross-modal integration; Temporal moment localization; Temporal video grounding; Temporal video localization","Arts computing; Semantic Segmentation; Semantics; Boundary alignment; Cross-modal; Cross-modal integration; Localisation; Temporal moment localization; Temporal moments; Temporal video; Temporal video grounding; Temporal video localization; Video localization; Long short-term memory",Article,Scopus,2-s2.0-85124081780
"Trappey A.J.C., Trappey C.V., Chao M.-H., Wu C.-T.","7003314683;6603075752;36767086300;57224450597;","VR-enabled engineering consultation chatbot for integrated and intelligent manufacturing services",2022,"Journal of Industrial Information Integration","26",,"100331","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124080870&doi=10.1016%2fj.jii.2022.100331&partnerID=40&md5=850065d421f71680ce4287ac1320f480","Industrial equipment manufacturing, such as electric power transformers, require highly customized design, assembly, installation and maintenance services for customers. Engineering consultation is an essential manufacturing service which requires deep domain knowledge. This research develops and implements the framework and the prototype of a Virtual Reality (VR) enabled intelligent engineering consultation chatbot system with natural language communication capabilities. The consultation chatbot case focuses on the power transformer knowledge domain. The system is constructed using a comprehensive knowledge base of Frequently Asked Questions (FAQs) and answers from a power transformer manufacturer. A total of 490,000 technical articles (with more than 1.1 billion words) from Wikipedia and a comprehensive domain-specific document set are used to train a word embedding model for retrieving questions and answers (QA) from the FAQ knowledge base. Immersive VR technology provides a highly interactive QA chatbot with realistic graphical views for informative engineering counselling. The case example demonstrates the accuracy of the proposed VR-enabled chatbot for transformer QAs exceeds 91%. © 2022 Elsevier Inc.","Chatbot; Manufacturing service; Natural language processing; Power transformer; Product-service system; Virtual reality","Domain Knowledge; Manufacture; Power transformers; Virtual reality; Chatbots; Design services; Electric power transformers; Equipment manufacturing; Frequently asked questions; Industrial equipment; Integrated manufacturing; Intelligent Manufacturing; Manufacturing service; Product-service systems; Natural language processing systems",Article,Scopus,2-s2.0-85124080870
"Chen T., Lin L., Chen R., Hui X., Wu H.","57037321300;15061363400;57203873194;55454512500;50662424500;","Knowledge-Guided Multi-Label Few-Shot Learning for General Image Recognition",2022,"IEEE Transactions on Pattern Analysis and Machine Intelligence","44","3",,"1371","1384",,4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124055151&doi=10.1109%2fTPAMI.2020.3025814&partnerID=40&md5=e275377a02caf77694d338540d3e6637","Recognizing multiple labels of an image is a practical yet challenging task, and remarkable progress has been achieved by searching for semantic regions and exploiting label dependencies. However, current works utilize RNN/LSTM to implicitly capture sequential region/label dependencies, which cannot fully explore mutual interactions among the semantic regions/labels and do not explicitly integrate label co-occurrences. In addition, these works require large amounts of training samples for each category, and they are unable to generalize to novel categories with limited samples. To address these issues, we propose a knowledge-guided graph routing (KGGR) framework, which unifies prior knowledge of statistical label correlations with deep neural networks. The framework exploits prior knowledge to guide adaptive information propagation among different categories to facilitate multi-label analysis and reduce the dependency of training samples. Specifically, it first builds a structured knowledge graph to correlate different labels based on statistical label co-occurrence. Then, it introduces the label semantics to guide learning semantic-specific features to initialize the graph, and it exploits a graph propagation network to explore graph node interactions, enabling learning contextualized image feature representations. Moreover, we initialize each graph node with the classifier weights for the corresponding label and apply another propagation network to transfer node messages through the graph. In this way, it can facilitate exploiting the information of correlated labels to help train better classifiers, especially for labels with limited training samples. We conduct extensive experiments on the traditional multi-label image recognition (MLR) and multi-label few-shot learning (ML-FSL) tasks and show that our KGGR framework outperforms the current state-of-the-art methods by sizable margins on the public benchmarks. © 1979-2012 IEEE.","few-shot learning; graph reasoning; Image recognition; knowledge graph; multi-label learning","Backpropagation; Deep neural networks; Graph theory; Image recognition; Knowledge graph; Sampling; Semantics; 'current; Co-occurrence; Few-shot learning; Graph reasoning; Knowledge graphs; Label dependencies; Multi-label learning; Multi-labels; Routing frameworks; Training sample; Classification (of information); article; deep neural network; learning; semantics",Article,Scopus,2-s2.0-85124055151
"Rong X., Yi C., Tian Y.","56727669200;36192382300;16556710700;","Unambiguous Text Localization, Retrieval, and Recognition for Cluttered Scenes",2022,"IEEE Transactions on Pattern Analysis and Machine Intelligence","44","3",,"1638","1652",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124052101&doi=10.1109%2fTPAMI.2020.3018491&partnerID=40&md5=5019392bd639064106cc295113c5ba7f","Text instance as one category of self-described objects provides valuable information for understanding and describing cluttered scenes. The rich and precise high-level semantics embodied in the text could drastically benefit the understanding of the world around us. While most recent visual phrase grounding approaches focus on general objects, this paper explores extracting designated texts and predicting unambiguous scene text information, i.e., to accurately localize and recognize a specific targeted text instance in a cluttered image from natural language descriptions (referring expressions). To address this issue, first a novel recurrent dense text localization network (DTLN) is proposed to sequentially decode the intermediate convolutional representations of a cluttered scene image into a set of distinct text instance detections. Our approach avoids repeated text detections at multiple scales by recurrently memorizing previous detections, and effectively tackles crowded text instances in close proximity. Second, we propose a context reasoning text retrieval (CRTR) model, which jointly encodes text instances and their context information through a recurrent network, and ranks localized text bounding boxes by a scoring function of context compatibility. Third, a recurrent text recognition module is introduced to extend the applicability of aforementioned DTLN and CRTR models, via text verification or transcription. Quantitative evaluations on standard scene text extraction benchmarks and a newly collected scene text retrieval dataset demonstrate the effectiveness and advantages of our models for the joint scene text localization, retrieval, and recognition task. © 1979-2012 IEEE.","deep neural network; Natural language description; referring expression; text detection; text recognition; text retrieval","Character recognition; Semantics; Cluttered scenes; Language description; Natural language description; Natural languages; Referring expressions; Scene Text; Text detection; Text localization; Text recognition; Text retrieval; Deep neural networks; article; extraction; genetic transcription; human; human experiment; information retrieval; language; quantitative analysis; reasoning",Article,Scopus,2-s2.0-85124052101
"Hinz T., Heinrich S., Wermter S.","57191251410;36903266000;7003826680;","Semantic Object Accuracy for Generative Text-to-Image Synthesis",2022,"IEEE Transactions on Pattern Analysis and Machine Intelligence","44","3",,"1552","1565",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124051702&doi=10.1109%2fTPAMI.2020.3021209&partnerID=40&md5=130bfa3f42e2e9874f988b198615f0f3","Generative adversarial networks conditioned on textual image descriptions are capable of generating realistic-looking images. However, current methods still struggle to generate images based on complex image captions from a heterogeneous domain. Furthermore, quantitatively evaluating these text-to-image models is challenging, as most evaluation metrics only judge image quality but not the conformity between the image and its caption. To address these challenges we introduce a new model that explicitly models individual objects within an image and a new evaluation metric called Semantic Object Accuracy (SOA) that specifically evaluates images given an image caption. The SOA uses a pre-trained object detector to evaluate if a generated image contains objects that are mentioned in the image caption, e.g., whether an image generated from 'a car driving down the street' contains a car. We perform a user study comparing several text-to-image models and show that our SOA metric ranks the models the same way as humans, whereas other metrics such as the Inception Score do not. Our evaluation also shows that models which explicitly model objects outperform models which only model global image characteristics. © 1979-2012 IEEE.","evaluation of generative models; generative adversarial network (GAN); generative models; Text-to-image synthesis","Object detection; Quality control; Semantics; Evaluation metrics; Evaluation of generative model; Generative adversarial network; Generative model; Image caption; Image modeling; Images synthesis; Semantic objects; Text-to-image synthesis; Generative adversarial networks; article; car driving; controlled study; diagnostic test accuracy study; human; synthesis",Article,Scopus,2-s2.0-85124051702
"Weinan E., Zhou Y.","57194935723;57440206400;","A Mathematical Model for Universal Semantics",2022,"IEEE Transactions on Pattern Analysis and Machine Intelligence","44","3",,"1124","1132",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124051475&doi=10.1109%2fTPAMI.2020.3022533&partnerID=40&md5=4aab4f3e2ed5a643948de7c84b037af8","We characterize the meaning of words with language-independent numerical fingerprints, through a mathematical analysis of recurring patterns in texts. Approximating texts by Markov processes on a long-range time scale, we are able to extract topics, discover synonyms, and sketch semantic fields from a particular document of moderate length, without consulting external knowledge-base or thesaurus. Our Markov semantic model allows us to represent each topical concept by a low-dimensional vector, interpretable as algebraic invariants in succinct statistical operations on the document, targeting local environments of individual words. These language-independent semantic representations enable a robot reader to both understand short texts in a given language (automated question-answering) and match medium-length texts across different languages (automated word translation). Our semantic fingerprints quantify local meaning of words in 14 representative languages across five major language families, suggesting a universal and cost-effective mechanism by which human languages are processed at the semantic level. Our protocols and source codes are publicly available on https://github.com/yajun-zhou/linguae-naturalis-principia-mathematica. © 1979-2012 IEEE.","hitting time; question answering; recurrence time; Recurring patterns in texts; semantic model; word translation","Cost effectiveness; Data mining; Knowledge based systems; Markov processes; Natural language processing systems; Translation (languages); Hitting time; Language independents; Mathematical analysis; Question Answering; Recurrence time; Recurring pattern in text; Semantic fields; Semantic modelling; Time-scales; Word translation; Semantics; article; controlled study; human; human experiment; knowledge base; linguistics; Markov chain; mathematical analysis; reading; robotics; semantics; topical drug administration",Article,Scopus,2-s2.0-85124051475
"Izadi M., Akbari K., Heydarnoori A.","57212699639;57221910561;14055787200;","Predicting the objective and priority of issue reports in software repositories",2022,"Empirical Software Engineering","27","2","50","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123988141&doi=10.1007%2fs10664-021-10085-3&partnerID=40&md5=89f291da12a6d15be97029f2bd0a03be","Software repositories such as GitHub host a large number of software entities. Developers collaboratively discuss, implement, use, and share these entities. Proper documentation plays an important role in successful software management and maintenance. Users exploit Issue Tracking Systems, a facility of software repositories, to keep track of issue reports, to manage the workload and processes, and finally, to document the highlight of their team’s effort. An issue report is a rich source of collaboratively-curated software knowledge, and can contain a reported problem, a request for new features, or merely a question about the software product. As the number of these issues increases, it becomes harder to manage them manually. GitHub provides labels for tagging issues, as a means of issue management. However, about half of the issues in GitHub’s top 1000 repositories do not have any labels. In this work, we aim at automating the process of managing issue reports for software teams. We propose a two-stage approach to predict both the objective behind opening an issue and its priority level using feature engineering methods and state-of-the-art text classifiers. To the best of our knowledge, we are the first to fine-tune a Transformer for issue classification. We train and evaluate our models in both project-based and cross-project settings. The latter approach provides a generic prediction model applicable for any unseen software project or projects with little historical data. Our proposed approach can successfully predict the objective and priority level of issue reports with 82 % (fine-tuned RoBERTa) and 75 % (Random Forest) accuracy, respectively. Moreover, we conducted human labeling and evaluation on unlabeled issues from six unseen GitHub projects to assess the performance of the cross-project model on new data. The model achieves 90 % accuracy on the sample set. We measure inter-rater reliability and obtain an average Percent Agreement of 85.3 % and Randolph’s free-marginal Kappa of 0.71 that translate to a substantial agreement among labelers. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Classification; Issue reports; Machine learning; Mining software repositories; Natural language processing; Prioritization; Software evolution and maintenance","Classification (of information); Computer software maintenance; Decision trees; Forecasting; Learning algorithms; Machine learning; Text processing; Issue report; Issue Tracking; Mining software; Mining software repository; Prioritization; Priority levels; Software entities; Software evolution and maintenances; Software management; Software repositories; Natural language processing systems",Article,Scopus,2-s2.0-85123988141
"Lin J.-W., Chang R.-G.","57195518797;7403713245;","Chinese story generation of sentence format control based on multi-channel word embedding and novel data format",2022,"Soft Computing","26","5",,"2179","2196",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123889918&doi=10.1007%2fs00500-021-06548-w&partnerID=40&md5=b86a18c1f69033f49772da5fbdb513c0","It is very difficult to generate stories in Chinese language. So far, there is no effective method to generate smooth articles. Here proposed a novel approach to improve the generation of Chinese stories in artificial intelligence, in order that it can effectively control the part-of-speech structure in sentence generation to imitate the writer’s writing style. The main proposal consists of three parts. First, the pre-processing of the sentence discards the input as the summary and the output as the text. It uses the format containing < SOS > < MOS > < EOS > for processing and the detailed method is defined in session 4. The second part is for vectorization. Traditional vectorization methods include Word2vec, Fasttext, LexVec and Glove; the different vectorization methods can help data semantic or grammatical understanding. Combining different vectorization methods improves the information of the input data. Therefore, this paper proposes the multi-channel word embedding and the details defined in session 5. The last part contains the optimization of the model architecture and how to control the process of sentence generation effectively. It also rewrites the Bert model proposed by Google to be the proposed model architecture. In addition, the Softmax function had been optimizing to reduce the search time during training and increase the training speed in the model. To make the model have better performance, the necessary training of the generative adversarial network was carried out, and the GAN architecture was revised for the data set, and the detail is defined in session 6. After the model is trained, to effectively control the structure of the generated sentence. This paper proposes a complete generation flowchart. In the process, based on the concept of FP-Tree, all sentences in the data set are built into a tree structure, and the part-of-speech structure of the next sentence is restricted through model generation combined with FP-Tree and the detail is defined in session 7. In addition, the experimental results show that our proposed method can effectively control the results of Chinese story generation and generate sentences with better performance and the detail is defined in session 8. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Chinese story generation method; Data format; FP-Tree; GAN; Multi-channel word embedding; Transformer model","Forestry; Generative adversarial networks; Network architecture; Process control; Semantics; Trees (mathematics); Chinese story generation method; Data format; Embeddings; FP tree; GAN; Generation method; Multi channel; Multi-channel word embedding; Story generations; Transformer modeling; Embeddings",Article,Scopus,2-s2.0-85123889918
"Cruz R.M.O., de Sousa W.V., Cavalcanti G.D.C.","41261179000;57427413600;7004509486;","Selecting and combining complementary feature representations and classifiers for hate speech detection",2022,"Online Social Networks and Media","28",,"100194","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123832213&doi=10.1016%2fj.osnem.2021.100194&partnerID=40&md5=e6736feaace1362b32381e5e05757817","Hate speech is a major issue in social networks due to the high volume of data generated daily. Recent works demonstrate the usefulness of machine learning (ML) in dealing with the nuances required to distinguish between hateful posts from just sarcasm or offensive language. Many ML solutions for hate speech detection have been proposed by either changing how features are extracted from the text or the classification algorithm employed. However, most works consider only one type of feature extraction and classification algorithm. This work argues that a combination of multiple feature extraction techniques and different classification models is needed. We propose a framework to analyze the relationship between multiple feature extraction and classification techniques to understand how they complement each other. The framework is used to select a subset of complementary techniques to compose a robust multiple classifiers system (MCS) for hate speech detection. The experimental study considering four hate speech classification datasets demonstrates that the proposed framework is a promising methodology for analyzing and designing high-performing MCS for this task. MCS system obtained using the proposed framework significantly outperforms the combination of all models and the homogeneous and heterogeneous selection heuristics, demonstrating the importance of having a proper selection scheme. Source code, figures and dataset splits can be found in the GitHub repository: https://github.com/Menelau/Hate-Speech-MCS. © 2022 Elsevier B.V.","Hate speech; Machine learning; Multiple classifiers system; Natural language processing; Text classification","Classification (of information); Extraction; Feature extraction; Learning algorithms; Natural language processing systems; Speech; Speech recognition; Text processing; Classification algorithm; Complementary features; Feature classifiers; Feature extraction and classification; Feature extraction techniques; Hate speech; Multiple classifier systems; Multiple features; Speech detection; Text classification; Machine learning",Article,Scopus,2-s2.0-85123832213
"Alharbi A.I., Smith P., Lee M.","57217703843;57212190478;56143088500;","Integrating Character-level and Word-level Representation for Affect in Arabic Tweets",2022,"Data and Knowledge Engineering","138",,"101973","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123786220&doi=10.1016%2fj.datak.2021.101973&partnerID=40&md5=cdbcae6349b7f6586b995e7bf7ea43c2","Affect tasks, which range from sentiment polarity classification to finer grained sentiment strength and emotional intensity detection, have become of increasing interest due to the vast amount of user-generated content and advanced learning models. Word representation models have been leveraged effectively within a variety of natural language processing tasks. However, these models are not always effective in the context of social media. When dealing with social media posts in Arabic, the use of Arabic dialects needs to be considered. Although using informal text to train word-level models can lead to the identification of words that convey the same meaning, these models are unable to capture the full extent of the words that are used in the real world due to out-of-vocabulary (OOV) words. The inability to identify such words is one of the main limitations of word-level models. One approach of overcoming OOV is through the use of character-level embeddings as they can effectively learn the vectors of word parts or character n-grams. This study uses a combination of character-level and word-level models to identify the most effective methods by which affective Arabic words in tweets can be represented semantically and morphologically. We evaluate our generated models and the proposed method by integrating them in a supervised learning framework that was used for a range of affect tasks and other related tasks. Our findings reveal that the developed models surpassed the performance of state-of-the-art Arabic pre-trained word embeddings over eight datasets. In addition, our models enhance previous state-of-the-art outcomes on tasks involving Arabic emotion intensity, outperforming the top-systems that used advanced ensemble learning models and several additional features. © 2022 Elsevier B.V.","Affect tasks; Arabic tweets; Character-level embeddings; Word-level embeddings","Natural language processing systems; Social networking (online); Affect task; Arabic tweet; Character level; Character-level embedding; Embeddings; Learning models; Level model; Word level; Word-level embedding; Embeddings",Article,Scopus,2-s2.0-85123786220
"Sheshmani A., You Y.-Z.","23092286300;36641103500;","Categorical representation learning: morphism is all you need",2022,"Machine Learning: Science and Technology","3","1","015016","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123690940&doi=10.1088%2f2632-2153%2fac2c5d&partnerID=40&md5=c565d171d115f2e9b335fd6a6b68d542","We provide a construction for categorical representation learning and introduce the foundations of ‘categorifier’. The central theme in representation learning is the idea of everything to vector. Every object in a dataset S can be represented as a vector in Rn by an encoding map E : Obj(S) → Rn. More importantly, every morphism can be represented as a matrix E : Hom(S) → Rnn. The encoding map E is generally modeled by a deep neural network. The goal of representation learning is to design appropriate tasks on the dataset to train the encoding map (assuming that an encoding is optimal if it universally optimizes the performance on various tasks). However, the latter is still a set-theoretic approach. The goal of the current article is to promote the representation learning to a new level via a category-theoretic approach. As a proof of concept, we provide an example of a text translator equipped with our technology, showing that our categorical learning model outperforms the current deep learning models by 17 times. The content of the current article is part of a US provisional patent application filed by QGNai, Inc. © 2021 The Author(s). Published by IOP Publishing Ltd","Categorical representation learning; Category theory; Natural language processing (NLP)","Deep neural networks; Encoding (symbols); Natural language processing systems; 'current; Categorical representation learning; Category theory; Category-theoretic approach; Learning models; matrix; Morphisms; Natural language processing; Performance; Set-theoretic approach; Signal encoding",Article,Scopus,2-s2.0-85123690940
"Zhang J., Wang C., Muthu A., Varatharaju V.M.","57429348800;57428638200;57428990900;57428991000;","Computer multimedia assisted language and literature teaching using Heuristic hidden Markov model and statistical language model",2022,"Computers and Electrical Engineering","98",,"107715","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123588536&doi=10.1016%2fj.compeleceng.2022.107715&partnerID=40&md5=bda7cdaa24965bb83723cab90080f656","Computer technology has been used for decades in secondary education and foreign language preparation. Still, attempts to incorporate technology have presented educators with various challenges due to rapid progress in technology and occasional changes in language education methods. One way of improving student engagement is to provide connectivity throughout the teaching and learning process in a classroom, allowing students to improve their English language skills. The multimedia classroom allows students to interact with various texts, giving them a solid background intasks and material of mainstream college classes. This paper introduces a method to recognise large unconstrained handwritten text vocabulary using Heuristic Hidden Markov Model and Statistical Language model (HHMM-SLM).This provides statistical language models to be applied to enhance our system's performance. Numerous experiments with single and multiple writer data have been conducted. Language models have been used to improve system accuracy. The variable size Lexica is used (between 10,000 and 50,000 words). Further, a lexicon for English with a very precise width has been developed, which makes its contribution. Our approach is comprehensive and compared to other literature approaches to deal with the same issue. © 2022","Ai-assisted language model; Computer multimedia; Hidden Markov model; Language teaching; Statistical language Model","Character recognition; Computational linguistics; Computer aided instruction; E-learning; Education computing; Engineering education; Heuristic methods; Learning systems; Natural language processing systems; Statistics; Students; Ai-assisted language model; Computer multimedias; Computer technology; Education methods; Foreign language; Hidden-Markov models; Language and literatures; Language education; Language model; Statistical language modelling; Hidden Markov models",Article,Scopus,2-s2.0-85123588536
"Jiang J., Xiao T., Xu J., Wen D., Gao L., Dou Y.","35221569200;57146453700;56734690800;57232206900;57429085600;15131095400;","A low-latency LSTM accelerator using balanced sparsity based on FPGA",2022,"Microprocessors and Microsystems","89",,"104417","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123587604&doi=10.1016%2fj.micpro.2021.104417&partnerID=40&md5=7e04f12624964586ab42415ad2464080","Long Short-Term Memory (LSTM) has been widely used in the fields of Automatic Speech Recognition (ASR) and Natural Language Processing (NLP). In order to accelerate LSTM inference, previous works have proposed various compression methods for weight pruning. Bank-Balanced Sparsity (BBS) is an efficient compression method that has a balanced distribution of non-zero elements and negligible precision degradation. However, it costs considerable additional memory overhead to store the indices, which limits the compression ratio and poses a challenge for FPGAs with limited on-chip resources. A Shared Index Bank-Balanced Sparsity (SIBBS) compression method is presented in this paper. The rows of a weight matrix are divided into multiple bank clusters to balance the non-zero weight distribution. The banks in one cluster share the indices. The overall cost of indices is reduced by 2x–8x compared with BBS. A coarse-grained inputs similarity skipping scheme is proposed at the same time to utilize the SIBBS pruning balance, which achieves 10% LSTM operation reduction with little accuracy degradation and negligible overhead. A sparse matrix and vector multiplication architecture for the SIBBS method is proposed and the customized accelerator is implemented on the Xilinx XCKU115 FPGA. Compared with the state-of-the-art LSTM accelerators based on FPGA, our accelerator achieved a 1.47x–79.5x reduction in latency with little degradation in accuracy. © 2022 The Authors","Accelerator; FPGA; LSTM; Model compression","Acceleration; Cost reduction; Long short-term memory; Natural language processing systems; Speech recognition; % reductions; Accelerator; Automatic speech recognition; Compression methods; Low latency; Memory overheads; Model compression; On chips; Speech recognition languages; Weight matrices; Field programmable gate arrays (FPGA)",Article,Scopus,2-s2.0-85123587604
"Othman N., Faiz R., Smaïli K.","57190130617;55920074400;6506309427;","Learning English and Arabic question similarity with Siamese Neural Networks in community question answering services",2022,"Data and Knowledge Engineering","138",,"101962","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123258433&doi=10.1016%2fj.datak.2021.101962&partnerID=40&md5=f37a4f4dd20153687f59e4f306357c78","In this paper, we tackle the task of similar question retrieval (QR) which is essential for Community Question Answering (cQA) and aims to retrieve historical questions that are semantically equivalent to the new queries. Over time, with the sharp increase of community archives and the accumulation of duplicated questions, the QR problem has become increasingly challenging due to the shortness of the community questions as well as the word mismatch problem as users can formulate the same query using different wording. Although many efforts have been devoted to address this problem, existing methods mostly relied on supervised models which significantly depend on massive training data sets and manual feature engineering. Such methods are chiefly constrained by their specificities that ignore the word order and do not capture enough syntactic and semantic information in questions. In this paper, we rely on Neural Networks (NNs) which use a deep analysis of words and questions to take into consideration the semantics as well as the structure of questions to predict the semantic text similarity. We propose a deep learning approach based on a Siamese architecture with Long Short-Term Memory (LSTM) networks, augmented with an attention mechanism to let the model give different words different attention while modeling questions. We also explore the use of Convolutional Neural Networks (CNN) nested within the Siamese architecture to retrieve relevant questions. Different similarity measures were tested to predict the semantic similarity between the pairs of questions. To evaluate the proposed approach, we conducted experiments on large-scale datasets in English and Arabic. © 2021 Elsevier B.V.","CNN; Community question answering; LSTM; Question retrieval; Siamese","Convolutional neural networks; Large dataset; Network architecture; Semantics; Community question answering; Convolutional neural network; Learning English; Mismatch problems; Neural-networks; Question retrieval; Question-answering services; Sharp increase; Siamese; Training dataset; Long short-term memory",Article,Scopus,2-s2.0-85123258433
"Zhao Y., Komachi M., Kajiwara T., Chu C.","57216846748;24168747300;56903744700;57417197100;","Region-attentive multimodal neural machine translation",2022,"Neurocomputing","476",,,"1","13",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123240221&doi=10.1016%2fj.neucom.2021.12.076&partnerID=40&md5=5a0712e8fe1a45178efb1f2fa343ea4d","We propose a multimodal neural machine translation (MNMT) method with semantic image regions called region-attentive multimodal neural machine translation (RA-NMT). Existing studies on MNMT have mainly focused on employing global visual features or equally sized grid local visual features extracted by convolutional neural networks (CNNs) to improve translation performance. However, they neglect the effect of semantic information captured inside the visual features. This study utilizes semantic image regions extracted by object detection for MNMT and integrates visual and textual features using two modality-dependent attention mechanisms. The proposed method was implemented and verified on two neural architectures of neural machine translation (NMT): recurrent neural network (RNN) and self-attention network (SAN). Experimental results on different language pairs of Multi30k dataset show that our proposed method improves over baselines and outperforms most of the state-of-the-art MNMT methods. Further analysis demonstrates that the proposed method can achieve better translation performance because of its better visual feature use. © 2022 The Authors","Multimodal neural machine translation; Object detection; Recurrent neural network; Self-attention network; Semantic image regions","Computational linguistics; Computer aided language translation; Convolutional neural networks; Object detection; Object recognition; Recurrent neural networks; Semantic Web; Semantics; Global visual features; Image regions; Machine translation methods; Multi-modal; Multimodal neural machine translation; Performance; Self-attention network; Semantic image region; Semantic images; Visual feature; Neural machine translation; article; attention network; human; human experiment; language; recurrent neural network",Article,Scopus,2-s2.0-85123240221
"Lundin N.B., Jones M.N., Myers E.J., Breier A., Minor K.S.","57212324672;57212372917;57216903573;35433257500;15053470500;","Semantic and phonetic similarity of verbal fluency responses in early-stage psychosis",2022,"Psychiatry Research","309",,"114404","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123101878&doi=10.1016%2fj.psychres.2022.114404&partnerID=40&md5=8c0a3a89ac96638c171fedc6af81c6ec","Linguistic abnormalities can emerge early in the course of psychotic illness. Computational tools that quantify similarity of responses in standardized language-based tasks such as the verbal fluency test could efficiently characterize the nature and functional correlates of these disturbances. Participants with early-stage psychosis (n=20) and demographically matched controls without a psychiatric diagnosis (n=20) performed category and letter verbal fluency. Semantic similarity was measured via predicted context co-occurrence in a large text corpus using Word2Vec. Phonetic similarity was measured via edit distance using the VFClust tool. Responses were designated as clusters (related items) or switches (transitions to less related items) using similarity-based thresholds. Results revealed that participants with early-stage psychosis compared to controls had lower fluency scores, lower cluster-related semantic similarity, and fewer switches; mean cluster size and phonetic similarity did not differ by group. Lower fluency semantic similarity was correlated with greater speech disorganization (Communication Disturbances Index), although more strongly in controls, and correlated with poorer social functioning (Global Functioning: Social), primarily in the psychosis group. Findings suggest that search for semantically related words may be impaired soon after psychosis onset. Future work is warranted to investigate the impact of language disturbances on social functioning over the course of psychotic illness. © 2022","Computational linguistics; Early psychosis; Phonetic similarity; Semantic coherence","adult; Article; clinical article; computational linguistics; controlled study; data analysis; demography; disorientation; female; human; linguistics; male; outpatient; phonetic similarity; phonetics; psychosis; semantic coherence; semantics; social interaction; speech; verbal fluency",Article,Scopus,2-s2.0-85123101878
"Liu R., Jia C., Wei J., Xu G., Vosoughi S.","57217248068;57220033204;57224667137;57221690079;25655603900;","Quantifying and alleviating political bias in language models",2022,"Artificial Intelligence","304",,"103654","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123059107&doi=10.1016%2fj.artint.2021.103654&partnerID=40&md5=8e440a117680dd4d51da44f944ef6bdd","Current large-scale language models can be politically biased as a result of the data they are trained on, potentially causing serious problems when they are deployed in real-world settings. In this paper, we first describe metrics for measuring political bias in GPT-2 generation, and discuss several interesting takeaways: 1) The generation of vanilla GPT-2 model is mostly liberal-leaning, 2) Such political bias depends on the sensitive attributes mentioned in the context, and 3) Priming the generation with a explicit political identifier, the extent of political bias is imbalanced (between liberal and conservative). We then propose a reinforcement learning (RL) framework for mitigating such political biases in generated text: By using rewards from word embeddings or a classifier, our RL framework guides debiased generation without having access to the training data or requiring the model to be retrained. In empirical experiments on three attributes sensitive to political bias (gender, location, and topic), our methods reduced bias according to both our metrics and human evaluation, while maintaining readability and semantic coherence. © 2021 Elsevier B.V.","Bias in language models; Measuring bias; Mitigating bias; Natural language generation; Political bias","Classification (of information); Computational linguistics; Natural language processing systems; Semantics; 'current; Bias in language model; Language model; Large-scales; Measuring bias; Mitigating bias; Natural language generation; Political bias; Real world setting; Sensitive attribute; Reinforcement learning",Article,Scopus,2-s2.0-85123059107
"Zhu C., Motohashi K.","57408666700;22934746600;","Identifying the technology convergence using patent text information: A graph convolutional networks (GCN)-based approach",2022,"Technological Forecasting and Social Change","176",,"121477","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122686325&doi=10.1016%2fj.techfore.2022.121477&partnerID=40&md5=0b3718d9335c65abfbe60621ee1c044e","The potential for new values and products created by technology convergence to disruptively transform existing industries and markets is high. In this regard, it has been crucial for companies to understand and identify potential convergence patterns as early as possible to make timely strategic plans. This study proposes a new semantic method by showing how a graph convolutional network model can be used to monitor technology convergence. In particular, the model is trained to generate patents and technology keyword vectors from which new indicators are derived. We validate these new indicators and show that the proposed method outperforms existing studies using information regarding cross-citations and co-occurrence of international patent classification classes. Furthermore, we presented the usefulness of the proposed method to monitor technology convergence using a case study of the convergence between artificial intelligence (AI) and distributed ledger technology (DLT). The results show that convergence between AI and DLT is driven mainly by employing AI for DLT, and the role of each keyword (sub-domain) in the convergence process is also presented. © 2022","Artificial intelligence; Distributed ledger technology; Graph convolution networks; Patent analysis; Technology convergence","Artificial intelligence; Classification (of information); Distributed ledger; Information use; Patents and inventions; Semantics; Co-occurrence; Convolutional networks; Distributed ledg technology; Graph convolution network; Network models; Network-based approach; Patent analysis; Strategic plan; Technology convergence; Text information; Convolution; artificial intelligence; artificial neural network; information; technology",Article,Scopus,2-s2.0-85122686325
"Lu H., Hu H., Lin X.","57309415900;57219788619;56760471400;","DensE: An enhanced non-commutative representation for knowledge graph embedding with adaptive semantic hierarchy",2022,"Neurocomputing","476",,,"115","125",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122648705&doi=10.1016%2fj.neucom.2021.12.079&partnerID=40&md5=0e4d99cb172ed6923b141b3c7c5468fe","Capturing the composition patterns of relations is a vital task in knowledge graph completion. It also serves as a fundamental step towards multi-hop reasoning over learned knowledge. Previously, several rotation-based translational methods have been developed to model composite relations using the product of a series of complex-valued diagonal matrices. However, these methods tend to make several oversimplified assumptions on the composite relations, e.g., forcing them to be commutative, independent from entities and lacking semantic hierarchy. To systematically tackle these problems, we have developed a novel knowledge graph embedding method, named DensE, to provide an improved modeling scheme for the complex composition patterns of relations. In particular, our method decomposes each relation into an SO(3) group-based rotation operator and a scaling operator in the three dimensional (3-D) Euclidean space. This design principle leads to several advantages of our method: (1) For composite relations, the corresponding diagonal relation matrices can be non-commutative, reflecting a predominant scenario in real world applications; (2) Our model preserves the natural interaction between relational operations and entity embeddings; (3) The scaling operation provides the modeling power for the intrinsic semantic hierarchical structure of entities; (4) The enhanced expressiveness of DensE is achieved with high computational efficiency in terms of both parameter size and training time; and (5) Modeling entities in Euclidean space instead of quaternion space keeps the direct geometrical interpretations of relational patterns. Experimental results on multiple benchmark knowledge graphs show that DensE is comparable to the current state-of-the-art models for missing link prediction, especially on composite relations. In addition, the interpretations generated by DensE also reveal how relations with distinct patterns (i.e., symmetry/anti-symmetry, inversion and composition) are modeled, which suggests several important directions of future studies. © 2022 Elsevier B.V.","Geometric interpretation; Knowledge graph embedding; Link prediction; Non-commutative composite relations; Semantic hierarchy","Computational efficiency; Geometry; Graph embeddings; Semantics; Composition patterns; Geometric interpretation; Graph embeddings; Knowledge graph embedding; Knowledge graphs; Link prediction; Non-commutative; Non-commutative composite relation; Scalings; Semantic hierarchies; Knowledge graph; article; decomposition; embedding; human; prediction; quaternion; rotation",Article,Scopus,2-s2.0-85122648705
"Ji W., Wang R., Tian Y., Wang X.","57197800973;55825442900;57193621959;35171979500;","An attention based dual learning approach for video captioning",2022,"Applied Soft Computing","117",,"108332","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122645859&doi=10.1016%2fj.asoc.2021.108332&partnerID=40&md5=dd9e9c1d56d75ebabf2dd0e0224af9c8","Video captioning aims to generate sentences/captions to describe video contents. It is one of the key tasks in the field of multimedia processing. However, most of the current video captioning approaches utilize only the visual information of a video to generate captions. Recently, a new encoder–decoder–reconstructorarchitecture was developed for video captioning, which can capture the information in both raw videos and the generated captions through dual learning. Based on this architecture, this paper proposes a novel attention based dual learning approach (ADL) for video captioning. Specifically, ADL is composed of a caption generation module and a video reconstruction module. The caption generation module builds a translatable mapping between raw video frames and the generated video captions, i.e., using the visual features extracted from videos by an Inception-V4 network to produce video captions. Then the video reconstruction module reproduces raw video frames using the generated video captions, i.e., using the hidden states of the decoder in the caption generation module to reproduce/synthesize raw visual features. A multi-head attention mechanism is adopted to help the two modules focus on the most effective information in videos and captions, and a dual learning mechanism is adopted to fine-tune the performance of the two modules to generate final video captions. Therefore, ADL can minimize the semantic gap between raw videos and the generated captions by minimizing the differences between the reproduced and the raw videos, thereby improving the quality of the generated video captions. Experimental results demonstrate that ADL is superior to the state-of-the-art video captioning approaches on benchmark datasets. © 2021","Attention mechanism; Deep neural network; Dual learning; Encoder–decoder; Video captioning","Decoding; Image reconstruction; Network coding; Semantics; Attention mechanisms; Dual learning; Encoder-decoder; Learning approach; Video captioning; Video captions; Video contents; Video frame; Video reconstruction; Visual feature; Deep neural networks",Article,Scopus,2-s2.0-85122645859
"Zhong B., Wu H., Xiang R., Guo J.","23975246400;57205511865;57224466677;57245460600;","Automatic Information Extraction from Construction Quality Inspection Regulations: A Knowledge Pattern-Based Ontological Method",2022,"Journal of Construction Engineering and Management","148","3","04021207","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122359623&doi=10.1061%2f%28ASCE%29CO.1943-7862.0002240&partnerID=40&md5=dc829dc9d8c2dbb6ad9333f6136fbccb","Quality compliance checking is essential to ensure construction quality, the prerequisite for which is information extraction from construction quality inspection regulations (CQIRs). Due to the inclusion of multiple qualitative constraints, complex syntax, semantic structures, and exceptions, extracting constraint information from CQIR automatically is difficult. To address the research gap, a knowledge pattern-based ontological method was developed to extract constraint information automatically from CQIR. The entire study process was guided by design science. To begin, knowledge patterns of three typical types of construction quality constraints were investigated to identify constraint elements and their semantic relationships, namely construction procedure constraints, product quality attribute constraints, and resource selection constraints. Then an ontology model was developed to represent these knowledge patterns by defining concepts and properties based on identified constraint elements and semantic relations. Based on the proposed ontology model, Java Annotation Patterns Engine (JAPE) rules were encoded to extract constraint information from CQIR. Finally, a prototype system was created to validate the proposed method, using text data from five mandatory regulations of groundwork and foundation construction. Experimental results demonstrated the theoretical feasibility of the presented method in automatically extracting constraints from CQIR. © 2021 American Society of Civil Engineers.","Construction quality; Information extraction; Knowledge pattern modelling; Ontology; Quality compliance checking","Compliance control; Data mining; Information retrieval; Quality control; Semantics; Automatic information extraction; Constraint information; Construction quality; Information extraction; Knowledge pattern modeling; Knowledge patterns; Ontology model; Ontology's; Quality compliance checking; Quality inspection; Ontology",Article,Scopus,2-s2.0-85122359623
"Gui H., Tseng B., Hu W., Wang S.Y.","57396332300;57223001365;57372118000;55163328800;","Looking for low vision: Predicting visual prognosis by fusing structured and free-text data from electronic health records",2022,"International Journal of Medical Informatics","159",,"104678","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122245006&doi=10.1016%2fj.ijmedinf.2021.104678&partnerID=40&md5=d331118d56226f0418f4d7bcb7748ccc","Introduction: Low vision rehabilitation improves quality-of-life for visually impaired patients, but referral rates fall short of national guidelines. Automatically identifying, from electronic health records (EHR), patients with poor visual prognosis could allow targeted referrals to low vision services. The purpose of this study was to build and evaluate deep learning models that integrate EHR data that is both structured and free-text to predict visual prognosis. Methods: We identified 5547 patients with low vision (defined as best documented visual acuity (VA) less than 20/40) on ≥ 1 encounter from EHR from 2009 to 2018, with ≥ 1 year of follow-up from the earliest date of low vision, who did not improve to greater than 20/40 over 1 year. Ophthalmology notes on or prior to the index date were extracted. Structured data available from the EHR included demographics, billing and procedure codes, medications, and exam findings including VA, intraocular pressure, corneal thickness, and refraction. To predict whether low vision patients would still have low vision a year later, we developed and compared deep learning models that used structured inputs and free-text progress notes. We compared three different representations of progress notes, including 1) using previously developed ophthalmology domain-specific word embeddings, and representing medical concepts from notes as 2) named entities represented by one-hot vectors and 3) named entities represented as embeddings. Standard performance metrics including area under the receiver operating curve (AUROC) and F1 score were evaluated on a held-out test set. Results: Among the 5547 low vision patients in our cohort, 40.7% (N = 2258) never improved to better than 20/40 over one year of follow-up. Our single-modality deep learning model based on structured inputs was able to predict low vision prognosis with AUROC of 80% and F1 score of 70%. Deep learning models utilizing named entity recognition achieved an AUROC of 79% and F1 score of 63%. Deep learning models further augmented with free-text inputs using domain-specific word embeddings, were able to achieve AUROC of 82% and F1 score of 69%, outperforming all single- and multiple-modality models representing text with biomedical concepts extracted through named entity recognition pipelines. Discussion: Free text progress notes within the EHR provide valuable information relevant to predicting patients’ visual prognosis. We observed that representing free-text using domain-specific word embeddings led to better performance than representing free-text using extracted named entities. The incorporation of domain-specific embeddings improved the performance over structured models, suggesting that domain-specific text representations may be especially important to the performance of predictive models in highly subspecialized fields such as ophthalmology. © 2021 Elsevier B.V.","Deep learning; Named entity recognition; Natural language processing; Ophthalmology; Vision, low","Deep learning; Diagnosis; Embeddings; Forecasting; Ophthalmology; Patient rehabilitation; Records management; Visual languages; Deep learning; Domain specific; Embeddings; F1 scores; Free texts; Learning models; Low vision; Named entity recognition; Receiver operating curves; Vision, low; Natural language processing systems",Article,Scopus,2-s2.0-85122245006
"Chen Y., Hu D., Li M., Duan H., Lu X.","57310570300;57191379260;57210809597;57394757700;55687460600;","Automatic SNOMED CT coding of Chinese clinical terms via attention-based semantic matching",2022,"International Journal of Medical Informatics","159",,"104676","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122145848&doi=10.1016%2fj.ijmedinf.2021.104676&partnerID=40&md5=54bcde39b16123608ebb455598b04a52","Background: A considerable amount of meaningful information is routinely recorded in Chinese clinical data in text format, referred to as Chinese clinical terms. The lack of coding is a major difficulty hindering the application of clinical terms. SNOMED CT is a widely used and comprehensive clinical health care terminology collection because of its coverage, granularity, clinical orientation, and logical underpinning. It is useful and efficient for automatically assigning SNOMED CT codes to Chinese clinical terms, but it still faces several problems. Current cross-language clinical term matching studies rely on external resources, such as machine translation and rule-based methods. Semantic matching methods have achieved strong performance on text matching, but few studies have been done on cross-language clinical term matching. We present an effective attention-based semantic matching algorithm to automatically cross-language code Chinese clinical terms with SNOMED CT. Method: Firstly, BERT was used to turn the input into word embedding. Then, the word embeddings were encoded through a BiLSTM with self-attention to focus on capturing distant relationships among words with different weights depending on their contribution to semantic matching. Then, decomposable attention was used to make semantic matching trivially parallelizable to speed up calculation. Finally, fully connected layers and a sigmoid were utilized to output matching results. Results: The 29,960 manually coded Chinese clinical terms, 30,040 unmatched Chinese clinical terms and SNOMED CT codes were collected to evaluate the proposed method. Compared with the existing semantic matching method, the proposed approach achieves state-of-the-art results demonstrating the effectiveness of the method with an accuracy of 0.905, a precision of 0.856, a recall of 0.518, and an F-measure of 0.645. The proposed Chinese-English bilingual term mapping, Chinese character-level and word-level encoder, English word-level encoder, BERT model, and attention mechanism performed better than other methods. Conclusion: The proposed automatic SNOMED CT coding approach of Chinese clinical terms via attention-based semantic matching can improve the performance of automated SNOMED CT code assignment for Chinese clinical terms and improve the efficiency of the code assignment. © 2021","Automatic coding; Decomposble attention; Semantic matching; SNOMED CT","Semantics; Signal encoding; Terminology; Automatic coding; Clinical terms; Cross languages; Decomposble attention; Embeddings; Matching methods; Matchings; Performance; Semantic matching; SNOMED-CT; Embeddings; algorithm; article; attention; calculation; Chinese script; embedding; human; human experiment; language; recall; sigmoid; Systematized Nomenclature of Medicine; velocity",Article,Scopus,2-s2.0-85122145848
"Chen X., Zhang F., Zhou F., Bonsangue M.","57201797585;7404970636;57191032864;6701786379;","Multi-scale graph capsule with influence attention for information cascades prediction",2022,"International Journal of Intelligent Systems","37","3",,"2584","2611",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122038323&doi=10.1002%2fint.22786&partnerID=40&md5=8cc3ddf0503822a1cde0161c213f399f","Information cascade size prediction is one of the primary challenges for understanding the diffusion of information. Traditional feature-based methods heavily rely on the quality of handcrafted features, requiring extensive domain knowledge and hard to generalize to new domains. Recently, inspired by the success of deep learning in computer vision and natural language processing, researchers have developed neural network-based approaches for tackling this problem. However, existing deep learning-based methods either focused on modeling the temporal characteristics of cascades but ignored the structural information or failed to take the order-scale and position-scale into consideration in modeling structures of information propagation. This paper proposed a novel graph neural network-based model, called MUCas, to learn the latent representations of cascade graphs from a multi-scale perspective, which can make full use of the direction-scale, high-order-scale, position-scale, and dynamic-scale of cascades via a newly designed MUlti-scale Graph Capsule Network (MUG-Caps) and the influence-attention mechanism. Extensive experiments conducted on two real-world data sets demonstrate that our MUCas significantly outperforms the state-of-the-art approaches. © 2021 Wiley Periodicals LLC",,"Backpropagation; Deep learning; Graph neural networks; Natural language processing systems; Cascade sizes; Domain knowledge; Feature-based method; Information cascades; Learning-based methods; Multi-scales; Network-based approach; Neural-networks; Size predictions; Temporal characteristics; Domain Knowledge",Article,Scopus,2-s2.0-85122038323
"Yang K., Hu X., Zhang Q., Wei J., Liu W.","57274764300;15755952500;36623064200;51462248900;23467299400;","VAEPass: A lightweight passwords guessing model based on variational auto-encoder",2022,"Computers and Security","114",,"102587","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121965172&doi=10.1016%2fj.cose.2021.102587&partnerID=40&md5=29fffa5717670a4fcfad991a3a1b0981","Password guessing has attracted considerable attention in recent years. With the successful application of deep learning (DL) methods in natural language processing, password guessing models that leverage deep learning techniques by treating passwords as short texts, e.g., Recurrent Neural Network-based model and PassGAN model, have been confirmed to be more effective in terms of generalizability. However, the architectures of these existing DL-based password guessing models are typically extremely complex, which makes the process of training and password generation time-consuming. It is desirable to build a lightweight password guessing model that can reduce the time required for model training while maintaining the password guessing effect. In this study, we propose VAEPass, a lightweight password guessing model based on a Variational Auto-Encoder (VAE), which comprises of an encoder and a decoder established using Gated Convolutional Neural Network (GCNN). Furthermore, we improve the proposed VAEPass model to treat common character combinations summarized from training passwords as tokens and guess passwords, known as VAEPasstoken, at a token-level. Experiments demonstrate that the matching rate of the proposed VAEPasstoken is 2.7%∼9.3% higher than that of PassGAN method in the one-site test. Moreover, compared with the state-of-the-art PassGAN model (i.e., a DL-based model), the parameters in VAEPass are approximately 32% of that in PassGAN and the training time required by VAEPass is approximately 11% of the time required by PassGAN. © 2021 Elsevier Ltd","Gated convolutional neural network; Information security; Password authentication; Password guessing attack; Variational auto-encoder","Convolution; Convolutional neural networks; Natural language processing systems; Network coding; Network security; Recurrent neural networks; Auto encoders; Convolutional neural network; Gated convolutional neural network; Learning methods; Learning techniques; Model-based OPC; Password guessing; Password-authentication; Password-guessing attacks; Variational auto-encoder; Authentication",Article,Scopus,2-s2.0-85121965172
"Shao T., Cai F., Chen W., Chen H.","57201615567;57226808440;57195605079;9043552700;","Self-supervised clarification question generation for ambiguous multi-turn conversation",2022,"Information Sciences","587",,,"626","641",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121925778&doi=10.1016%2fj.ins.2021.12.040&partnerID=40&md5=7ed9ea2d87e44fb0ae3b4c4d0777a62a","Clarification Question Generation (CQG) aims to automatically generate clarification questions to avoid misunderstanding. In this paper, we focus on generating clarification questions in the scenario of ambiguous multi-turn conversation, which can be well applied to the interactive systems, e.g., dialogue systems and conversational recommendation systems. As a novel direction, limited manual-annotated samples are available for CQG. Moreover, existing approaches mainly ignore the representation of ambiguous semantics and cannot deal with the Out-of-Vocabulary (OOV) problem in a good manner. To address the above issues, we propose a Self-supervised Hierarchical Pointer-generator model (SHiP) for this task. In detail, similar to the backbone Coarse-to-fine process of CQG, we first formulate two self-supervised learning pretext tasks, i.e., Dialogue History Prediction and Entity Name Prediction. Then, we incorporate a hierarchical Transformer mechanism and a pointer-generator mechanism to understand the ambiguous multi-turn conversations and solve the OOV problem. Finally, we propose an end-to-end co-training paradigm to train the pretext tasks and downstream tasks. We quantify the improvements of SHiP against the competitive baselines on a publicly available dataset CLAQUA, showing a general improvement of 6.75% and 3.91% over state-of-the-art baseline in terms of BLEU and ROUGE-L, respectively. © 2021 Elsevier Inc.","clarification question; neural network; question generation; self-supervised learning","Clarifiers; Ships; Speech processing; Supervised learning; Clarification question; Coarse to fine; Conversational recommendations; Dialogue systems; Generator modelling; Interactive system; Multi-turn; Neural-networks; Question generation; Self-supervised learning; Semantics",Article,Scopus,2-s2.0-85121925778
"Karthikeyan A., Priyakumar U.D.","57221249629;6602129643;","Artificial intelligence: machine learning for chemical sciences",2022,"Journal of Chemical Sciences","134","1","2","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121540965&doi=10.1007%2fs12039-021-01995-2&partnerID=40&md5=ccdf850b523de74ded931475dbf736e2","Research in molecular sciences witnessed the rise and fall of Artificial Intelligence (AI)/ Machine Learning (ML) methods, especially artificial neural networks, few decades ago. However, we see a major resurgence in the use of modern ML methods in scientific research during the last few years. These methods have had phenomenal success in the areas of computer vision, speech recognition, natural language processing (NLP), etc. This has inspired chemists and biologists to apply these algorithms to problems in natural sciences. Availability of high performance Graphics Processing Unit (GPU) accelerators, large datasets, new algorithms, and libraries has enabled this surge. ML algorithms have successfully been applied to various domains in molecular sciences by providing much faster and sometimes more accurate solutions compared to traditional methods like Quantum Mechanical (QM) calculations, Density Functional Theory (DFT) or Molecular Mechanics (MM) based methods, etc. Some of the areas where the potential of ML methods are shown to be effective are in drug design, prediction of high–level quantum mechanical energies, molecular design, molecular dynamics materials, and retrosynthesis of organic compounds, etc. This article intends to conceptually introduce various modern ML methods and their relevance and applications in computational natural sciences. Graphical abstract: [Figure not available: see fulltext.] Synopsis Recent surge in the application of machine learning (ML) methods in fundamental sciences has led to a perspective that these methods may become important tools in chemical science. This perspective provides an overview of the modern ML methods and their successful applications in chemistry during the last few years. © 2021, Indian Academy of Sciences.","computational chemistry; computational materials; Deep learning; drug design; machine learning; molecular design; neural networks","Abstracting; Computation theory; Computer graphics; Deep learning; Density functional theory; Design for testability; Graphics processing unit; Large dataset; Learning algorithms; Molecular dynamics; Natural language processing systems; Neural networks; Program processors; Quantum theory; Speech recognition; Chemical science; Computational materials; Deep learning; Drug Design; Machine learning methods; Modern machines; Molecular design; Molecular science; Neural-networks; Scientific researches; Computational chemistry",Article,Scopus,2-s2.0-85121540965
"Segundo Díaz R.L., Rovelo Ruiz G., Bouzouita M., Coninx K.","57375006600;57201032763;57375225200;6602251819;","Building blocks for creating enjoyable games—A systematic literature review",2022,"International Journal of Human Computer Studies","159",,"102758","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121297030&doi=10.1016%2fj.ijhcs.2021.102758&partnerID=40&md5=65d626605c7a6adab13bf6f7cf6cf92d","Designing serious games that engage lots of players is still a challenge, especially for domains that introduce complex, specialised, and tedious tasks that are difficult to represent in a game in terms of entertainment. Therefore, researchers have investigated ways to motivate players, including enjoyment. Enjoyment is tied to emotional experience and is associated with positive player reactions throughout a gameplay session. However, an inventory with concrete elements (including descriptions and empirical proofs) producing that experience is missing. While researchers have investigated enjoyment and its relationship with game design elements (GDE), the efforts remain dispersed and isolated across different areas. Besides, there is no guideline describing this relationship that assists designers in their creation process. Therefore, this paper presents a systematic literature review to provide a detailed understanding of GDE, player enjoyment, and instruments for evaluation. Additionally, an analysis of two successful cases of Games With a Purpose (GWAP, a subset of serious games) for linguistics is presented to highlight the impact of the GDE and providing relationships with the GDE mentioned in this literature review. We found 33 GDE, from which 28 positively affect player enjoyment, and they can be used as building blocks to design enjoyable GWAP (or other serious games). Further, we create a list of instruments that provide an ample understanding of the constructs of player enjoyment, namely enjoyment, immersion, flow, positive affect, and presence. The listed instruments can give researchers higher confidence as they will allow replication and comparison of studies. These two components are critical in the process of design and evaluation of games. Furthermore, the GWAP analysis shows that effectively the GDE are used in GWAP to enhance interaction and player enjoyment. Finally, conclusions and practical suggestions for future work are given. © 2021","Game design elements; Games with a purpose; Instruments to evaluate player enjoyment; Natural language processing; Player enjoyment","Architectural design; Natural language processing systems; Serious games; Building blockes; Design elements; Emotional experiences; Game design; Game design element; Game with a purpose; Instrument to evaluate player enjoyment; Player's enjoyments; Systematic literature review; Game design",Article,Scopus,2-s2.0-85121297030
"Nazar N., Aleti A., Zheng Y.","57140861200;35092219900;57226183539;","Feature-based software design pattern detection",2022,"Journal of Systems and Software","185",,"111179","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121212481&doi=10.1016%2fj.jss.2021.111179&partnerID=40&md5=ce9a29a5caa9b86b62190eb575e815d4","Software design patterns are standard solutions to common problems in software design and architecture. Knowing that a particular module implements a design pattern is a shortcut to design comprehension. Manually detecting design patterns is a time consuming and challenging task, therefore, researchers have proposed automatic design pattern detection techniques. However, these techniques show low performance for certain design patterns. In this work, we introduce a design pattern detection approach, DPDF that improves the performance over the state-of-the-art by using code features with machine learning classifiers to automatically train a design pattern detector. DPDF creates a semantic representation of Java source code using the code features and the call graph, and applies the Word2Vec algorithm on the semantic representation to construct the word-space geometric model of the Java source code. DPDF then builds a machine learning classifier trained on a labelled dataset and identifies software design patterns with over 80% Precision and over 79% Recall. Additionally, we have compared DPDF with two existing design pattern detection techniques namely FeatureMaps &amp; MARPLE-DPD. Empirical results demonstrate that our approach outperforms the existing approaches by approximately 35% and 15% respectively in terms of Precision. The run-time performance also supports the practical applicability of our classifier. © 2021 Elsevier Inc.","Code features; Machine learning; Software design patterns; Word-space-model","Classification (of information); Feature extraction; Java programming language; Semantics; Software design; Code feature; Design pattern detections; Design Patterns; Java source codes; Performance; Semantic representation; Software design patterns; Space models; Word spaces; Word-space-model; Machine learning",Article,Scopus,2-s2.0-85121212481
"Smith E., Papadopoulos D., Braschler M., Stockinger K.","57224807287;57195630441;6603066610;6603748288;","LILLIE: Information extraction and database integration using linguistics and learning-based algorithms",2022,"Information Systems","105",,"101938","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120964492&doi=10.1016%2fj.is.2021.101938&partnerID=40&md5=5a89b2ea4efbc5e9f8167be1abcf3f9c","Querying both structured and unstructured data via a single common query interface such as SQL or natural language has been a long standing research goal. Moreover, as methods for extracting information from unstructured data become ever more powerful, the desire to integrate the output of such extraction processes with “clean”, structured data grows. We are convinced that for successful integration into databases, such extracted information in the form of “triples” needs to be both (1) of high quality and (2) have the necessary generality to link up with varying forms of structured data. It is the combination of both these aspects, which heretofore have been usually treated in isolation, where our approach breaks new ground. The cornerstone of our work is a novel, generic method for extracting open information triples from unstructured text, using a combination of linguistics and learning-based extraction methods, thus uniquely balancing both precision and recall. Our system called LILLIE (LInked Linguistics and Learning-Based Information Extractor) uses dependency tree modification rules to refine triples from a high-recall learning-based engine, and combines them with syntactic triples from a high-precision engine to increase effectiveness. In addition, our system features several augmentations, which modify the generality and the degree of granularity of the output triples. Even though our focus is on addressing both quality and generality simultaneously, our new method substantially outperforms current state-of-the-art systems on the two widely-used CaRB and Re-OIE16 benchmark sets for information extraction. We have made our code publicly available to facilitate further research. © 2021 The Authors","Data integration; Information extraction; Machine learning for database systems","Balancing; Benchmarking; Data mining; Database systems; Information retrieval; Information use; Linguistics; Machine learning; Natural language processing systems; Query processing; Search engines; Databases integrations; Information extraction; Learning-based algorithms; Machine learning for database system; Natural languages; Query interfaces; Research goals; SQL languages; Structured data; Unstructured data; Data integration",Article,Scopus,2-s2.0-85120964492
"Zeng J., Liu T., Jia W., Zhou J.","57225026342;57215717029;26643193700;55938080200;","Relation construction for aspect-level sentiment classification",2022,"Information Sciences","586",,,"209","223",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120964385&doi=10.1016%2fj.ins.2021.11.081&partnerID=40&md5=e430496b883eeb98090afe8f6ec1df79","Aspect-level sentiment classification aims to obtain fine-grained sentiment polarities of different aspects in one sentence. Most existing approaches handle the classification by acquiring the importance of context words towards each given aspect individually, and ignore the benefits brought by aspect relations. Since the sentiment of one aspect can be deduced through their relationship according to other aspects, in this paper, we propose a novel relation construction multi-task learning network (RMN), which is the first attempt to extract aspect relations as an auxiliary classification task. RMN generates aspect representations through graph convolution networks with a semantic dependency graph and utilizes the bi-attention mechanism to capture the relevance between the aspect and the context. Unlike conventional multi-task learning methods that need extra datasets, we construct an auxiliary relation-level classification task that extracts aspect relations from the original dataset with shared parameters. Extensive experiments on five public datasets from SemEval 14, 15, 16 and MAMS show that our RMN improves about 0.09% to 0.8% on accuracy and about 0.04% to 1.19% on F1 score, compared to several comparative baselines. © 2021 Elsevier Inc.","Aspect relations; Aspect-level; Graph convolutional network; Sentiment analysis","Classification (of information); Convolution; Convolutional neural networks; Learning systems; Semantics; Aspect relation; Aspect-level; Classification tasks; Context-word; Convolutional networks; Fine grained; Graph convolutional network; Semantic dependency; Sentiment analysis; Sentiment classification; Sentiment analysis",Article,Scopus,2-s2.0-85120964385
"González-Peña P., Coventry K.R., Bayliss A.P., Doherty M.J.","57218464106;57207525464;7005185834;7203078266;","The extended development of mapping spatial demonstratives onto space",2022,"Journal of Experimental Child Psychology","215",,"105336","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120941051&doi=10.1016%2fj.jecp.2021.105336&partnerID=40&md5=38ddec13f9d8798d41145f48fcd7d7f3","Spatial demonstratives (this and that in English) convey distance relative to speaker (within reach vs. out of reach) and object characteristics such as ownership. Previous studies indicate that object characteristics affect adult demonstrative choice, for example, greater use of this for owned objects. Here, production of spatial demonstratives was studied developmentally to identify when demonstrative production is sensitive to both distance and ownership. In two experiments, 7-year-olds, 11-year-olds, and adults completed an object location memory task, and a language task eliciting this or that to indicate an object. Results indicate that adult-like demonstrative production starts around 7 years of age and continues to develop beyond 11 years. Nonlinguistic spatial memory did not vary significantly across age groups. Spatial demonstratives encode both semantic and spatial object characteristics throughout development, revealing the fundamental importance of semantic factors for demonstrative production. © 2021 Elsevier Inc.","Language development; Object knowledge; Ownership; Spatial demonstratives; Spatial memory; Spatial representation","adult; article; child; female; groups by age; human; human experiment; language development; language test; male; school child; spatial memory; depth perception; language; semantics; spatial memory; Adult; Humans; Language; Semantics; Space Perception; Spatial Memory",Article,Scopus,2-s2.0-85120941051
"Fernandez-Fernandez R., Victores J.G., Gago J.J., Estevez D., Balaguer C.","57195303930;36716896800;57205486919;57136692300;6701864168;","Neural Policy Style Transfer",2022,"Cognitive Systems Research","72",,,"23","32",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120895759&doi=10.1016%2fj.cogsys.2021.11.003&partnerID=40&md5=483b25b250e76065aef183685a708222","Style Transfer has been proposed in a number of fields: fine arts, natural language processing, and fixed trajectories. We scale this concept up to control policies within a Deep Reinforcement Learning infrastructure. Each network is trained to maximize the expected reward, which typically encodes the goal of an action, and can be described as the content. The expressive power of deep neural networks enables encoding a secondary task, which can be described as the style. The Neural Policy Style Transfer (NPST)1 algorithm is proposed to transfer the style of one policy to another, while maintaining the content of the latter. Different policies are defined via Deep Q-Network architectures. These models are trained using demonstrations through Inverse Reinforcement Learning. Two different sets of user demonstrations are performed, one for content and other for style. Different styles are encoded as defined by user demonstrations. The generated policy is the result of feeding a content policy and a style policy to the NPST algorithm. Experiments are performed in a catch-ball game inspired by the Deep Reinforcement Learning classical Atari games; and a real-world painting scenario with a full-sized humanoid robot, based on previous works of the authors. The implementation of three different Q-Network architectures (Shallow, Deep and Deep Recurrent Q-Network) to encode the policies within the NPST framework is proposed and the results obtained in the experiments with each of these architectures compared. © 2021 Elsevier B.V.","Deep learning; Deep reinforcement learning; Robotics; Style Transfer","Anthropomorphic robots; Deep neural networks; Demonstrations; Encoding (symbols); Learning algorithms; Natural language processing systems; Network architecture; Recurrent neural networks; Control policy; Deep learning; Expressive power; Fine arts; Humanoid robot; Inverse reinforcement learning; Real-world; Secondary tasks; Style transfer; Reinforcement learning; analytic method; architecture; Article; artificial neural network; back propagation; controlled study; deep learning; deep neural network; human; information processing; neural policy style transfer; painting; reinforcement (psychology)",Article,Scopus,2-s2.0-85120895759
"Malik S., Shoaib U., Bukhari S.A.C., El Sayed H., Khan M.A.","57214873519;38362474300;57196354764;10739584700;35190505200;","A hybrid query expansion framework for the optimal retrieval of the biomedical literature",2022,"Smart Health","23",,"100247","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120893066&doi=10.1016%2fj.smhl.2021.100247&partnerID=40&md5=68c304e7a7990c81f2d2b6a9c522a49a","With the proliferation of biomedical literature, it is quite challenging for biomedical scientists to keep them updated with the new advancements. In biomedical literature retrieval systems, the keywords in the user-defined queries are often defined with various lexical variants consequently leading to the vocabulary mismatch (VM). One possible way to cope with these issues is to introduce a query expansion (QE) framework to enrich the original queries with the auxiliary semantically similar terms for each keyword mentioned in a query. In this research, we propose a biomedical QE framework to alleviate the VM. The proposed approach combines the clinical diagnosis information (CDI) and word embeddings (WEs) simultaneously to retrieve the relevant biomedical literature. The process of embeddings vocabulary terms as real-valued and low dimensional vectors referred to as word embedding has garnered significant attention by potentially capturing the implicit semantics. We have exploited threefold word embeddings (Domain-Specific, Domain-Agnostic, and Hybrid) and integrated the embeddings outcomes with the CDI to get the best query combination for the efficient retrieval of biomedical literature. Experimental results procured for the Text REtrieval Conference dataset showed that CDI, when used with the hybrid word embeddings surpassed the WEs trained for the domain-specific and domain-agnostic data. The results demonstrate that the utilization of this unique setup of merging two techniques is a valuable addition to the QE process leading to significantly improved precision rate and VM in biomedical literature retrieval. We hope that our approach would assist investigators to use this query combination to retrieve relevant articles. © 2021 Elsevier Inc.","Markov random field; Query expansion; Semantics; Vocabulary mismatch; Word embeddings","Agnostic; article; attention; diagnosis; embedding; human; information retrieval; Markov random field; semantics; vocabulary",Article,Scopus,2-s2.0-85120893066
"Cha P., Ginsparg P., Wu F., Carrasquilla J., McMahon P.L., Kim E.-A.","57218424046;6602339160;57211776387;24278799300;35792524800;57365353700;","Attention-based quantum tomography",2022,"Machine Learning: Science and Technology","3","1","01LT01","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120753676&doi=10.1088%2f2632-2153%2fac362b&partnerID=40&md5=58a81249e7f0a3c3b5fc8bb873984c91","With rapid progress across platforms for quantum systems, the problem of many-body quantum state reconstruction for noisy quantum states becomes an important challenge. There has been a growing interest in approaching the problem of quantum state reconstruction using generative neural network models. Here we propose the ‘attention-based quantum tomography’ (AQT), a quantum state reconstruction using an attention mechanism-based generative network that learns the mixed state density matrix of a noisy quantum state. AQT is based on the model proposed in ‘Attention is all you need’ by Vaswani et al (2017 NIPS) that is designed to learn long-range correlations in natural language sentences and thereby outperform previous natural language processing (NLP) models. We demonstrate not only that AQT outperforms earlier neural-network-based quantum state reconstruction on identical tasks but that AQT can accurately reconstruct the density matrix associated with a noisy quantum state experimentally realized in an IBMQ quantum computer. We speculate the success of the AQT stems from its ability to model quantum entanglement across the entire quantum system much as the attention model for NLP captures the correlations among words in a sentence. © 2021 The Author(s). Published by IOP Publishing Ltd","Deep learning; IBMQ; Quantum state tomography; Transformer","Deep learning; Natural language processing systems; Quantum computers; Quantum entanglement; Tomography; Deep learning; Density matrix; IBMQ; Learn+; Quantum state; Quantum state reconstruction; Quantum state tomography; Quantum system; Quantum tomography; Transformer; Quantum optics",Article,Scopus,2-s2.0-85120753676
"Das B., Majumder M., Sekh A.A., Phadikar S.","57195314718;54788156300;57220090017;26638877000;","Automatic question generation and answer assessment for subjective examination",2022,"Cognitive Systems Research","72",,,"14","22",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120697514&doi=10.1016%2fj.cogsys.2021.11.002&partnerID=40&md5=de2120bc7e6332e88564a66d5d39da66","Automatic generation of questions and evaluating their answers is a highly challenging task in natural language processing and educational technology. This work focuses on generating subjective questions and also an evaluation system is suggested for assessing the answers. For generating the questionnaires, key-phrases are extracted from the course curriculum (syllabus). Next, based on the key-phrases, different types of subjective questions are generated. Finally, the evaluation of student's responses is achieved using a multi-criteria-decision-making approach. It uses a set of model answers taken from different textbooks and subject experts to evaluate the answers. Multiple measures are used to assess the answers by comparing them with this model set. The results of the profound system reveal that the automated appraisal process can reduce the manual effort of the human. © 2021 Elsevier B.V.","Automatic answer evaluation; Keyword extraction; Multi-criteria-decision making; Question generation; Subjective examination","Decision making; Surveys; Automatic answer evaluation; Automatic Generation; Key-phrase; Keywords extraction; Multi criteria decision-making; Multicriteria decision-making; Multicriterion decision makings; Question generation; Student response; Subjective examination; Natural language processing systems; article; curriculum; extraction; human; human experiment; multicriteria decision analysis; questionnaire",Article,Scopus,2-s2.0-85120697514
"Bashath S., Perera N., Tripathi S., Manjang K., Dehmer M., Streib F.E.","57202098975;56898704900;57197323715;57219339880;13404645900;57364567000;","A data-centric review of deep transfer learning with applications to text data",2022,"Information Sciences","585",,,"498","528",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120668947&doi=10.1016%2fj.ins.2021.11.061&partnerID=40&md5=66cbc6a406237f85ed9c52849369d645","In recent years, many applications are using various forms of deep learning models. Such methods are usually based on traditional learning paradigms requiring the consistency of properties among the feature spaces of the training and test data and also the availability of large amounts of training data, e.g., for performing supervised learning tasks. However, many real-world data do not adhere to such assumptions. In such situations transfer learning can provide feasible solutions, e.g., by simultaneously learning from data-rich source data and data-sparse target data to transfer information for learning a target task. In this paper, we survey deep transfer learning models with a focus on applications to text data. First, we review the terminology used in the literature and introduce a new nomenclature allowing the unequivocal description of a transfer learning model. Second, we introduce a visual taxonomy of deep learning approaches that provides a systematic structure to the many diverse models introduced until now. Furthermore, we provide comprehensive information about text data that have been used for studying such models because only by the application of methods to data, performance measures can be estimated and models assessed. © 2021 The Author(s)","Deep learning; Domain adaptation; Machine learning; Natural language processing; Transfer learning","Deep learning; Learning algorithms; Terminology; Data centric; Deep learning; Domain adaptation; Learning models; Learning paradigms; Property; Text data; Traditional learning; Training data; Transfer learning; Natural language processing systems",Article,Scopus,2-s2.0-85120668947
"Nguyen Q.N., Sidorova A., Torres R.","57193231466;8889923500;55711254400;","User interactions with chatbot interfaces vs. Menu-based interfaces: An empirical study",2022,"Computers in Human Behavior","128",,"107093","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120502436&doi=10.1016%2fj.chb.2021.107093&partnerID=40&md5=e6fad766acae1aa35a39422a9c143189","Rapid advances in Natural Language Processing (NLP) are transforming customer service by making it possible to create chatbot applications that can understand users’ intents and response in a human-like manner. Chatbots promise to enhance customer experiences by creating more personal customer interactions than those afforded by traditional menu-based web applications. But are chatbots always superior to more traditional user interfaces (UI)? This study seeks to understand the differences in user satisfaction with a chatbot system vis-a-vis a menu-based interface system, and identify factors that influence user satisfaction. Grounded in the self-determination theory, the research model proposed here focuses on the effect of chatbot use on perceived autonomy, perceived competence, cognitive load, performance satisfaction, and system satisfaction. An experimental study was conducted, and data were analyzed using Partial Least Square Structural Equation Modeling. The findings indicate that chatbot systems lead to a lower level of perceived autonomy and higher cognitive load, compared with menu-based interface systems, resulting in a lower degree of user satisfaction. Implications of these findings for research and practice are discussed. © 2021 Elsevier Ltd","Chatbot; Conversational agent; Human-computer interaction; Self-determination theory; User satisfaction","Cognitive systems; Interactive computer systems; Natural language processing systems; Sales; User interfaces; Chatbots; Cognitive loads; Conversational agents; Customer-service; Empirical studies; Human like; Interface system; Self-determination theories; User interaction; Users' satisfactions; Human computer interaction; article; controlled study; empiricism; experimental study; human; human computer interaction; human experiment; least square analysis; satisfaction; structural equation modeling; theoretical study",Article,Scopus,2-s2.0-85120502436
"Makedonski P., Grabowski J.","35226436100;22334240800;","Facilitating the co-evolution of semantic descriptions in standards and models",2022,"Information and Software Technology","143",,"106763","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120324331&doi=10.1016%2fj.infsof.2021.106763&partnerID=40&md5=188950390609d6364df7e9f04b9db602","Context: Standardised specifications for sophisticated technologies are subdivided in multiple documents maintained by different working groups, typically accompanied by models and other formalised artefacts. As the specifications and the models evolve, ensuring their consistency at scale becomes challenging. Objective: While previous work developed a methodology for facilitating the co-evolution of models and standards, based on the Network Function Virtualisation (NFV) Information Model (IM) and models extracted from the related standardised specifications, the methodology focused on structural aspects only. This article refines the methodology, enabling the alignment of semantic descriptions of information elements and attributes, both across specifications and across information elements. Method: To enable the alignment of semantic descriptions, we extend the methodology by using statistical and visual analyses of terms used in the specifications. The underlying meta-model for the information extracted from the specifications is extended to accommodate the capturing of additional semantic information. Results: We report on our experiences with the application of a prototypical implementation of the methodology during the continued alignment and maintenance of the IM and the related standardised specifications. More than 400 potential inconsistencies were identified, leading to more than 100 contributions, some of which addressed multiple findings. Feedback from the working group provided insights on how to refine the methodology further. Conclusions: Models shall play a more central role and be better integrated throughout the specification development and implementation processes, helping to ensure and maintain consistency among specifications. Our experiences may provide useful insights into ongoing and future initiatives where similar challenges are faced. © 2021 Elsevier B.V.","Duplication; Maintenance; Modelling; Semantics; Standards; Traceability","Alignment; Network function virtualization; Specifications; Co-evolution; Duplication; Information elements; Information Modeling; Modeling; Multiple documents; Semantic descriptions; Structural aspects; Traceability; Working groups; Semantics",Article,Scopus,2-s2.0-85120324331
"Lapeña R., Pérez F., Cetina C., Pastor Ó.","57190120492;36091753500;19933413800;6701438651;","Leveraging BPMN particularities to improve traceability links recovery among requirements and BPMN models",2022,"Requirements Engineering","27","1",,"135","160",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119673826&doi=10.1007%2fs00766-021-00365-1&partnerID=40&md5=e8ced7da71494ba3ccc1f216170e101e","Traceability links recovery (TLR) has been a topic of interest for many years. However, TLR approaches are based on the latent semantics of the software artifacts, and are not equipped to deal with software artifacts that lack those inherent semantics, such as BPMN models. The aim of this work is to enhance TLR approaches in BPMN models by incorporating the linguistic particularities of BPMN models into the TLR process. Our approach runs through a threefold contribution: (1) we identify the particularities of BPMN models; (2) we describe how to leverage the particularities; and (3) we build three variants of the best exploratory TLR approach which specifically cater to BPMN models. The approach is evaluated through both an academic case study and a real-world industrial case study. The results show that incorporating the particularities of BPMN into the TLR process leads the specific approach to improve the traceability results obtained by generalist approaches, maintaining precision levels and improving recall. The novel findings of this paper suggest that there is a benefit in researching and taking in account the particularities of the different kinds of models in order to optimize the results of TLR between requirements and models, instead of relying on generalist approaches. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Model-driven software engineering; Requirements engineering; Traceability links recovery","Recovery; Requirements engineering; Software engineering; Case-studies; Latent semantics; Link recoveries; Model-driven software engineerings; Real-world; Recovery process; Requirement engineering; Software artefacts; Traceability link recovery; Traceability links; Semantics",Article,Scopus,2-s2.0-85119673826
"Jiang X.-F., Xiong L., Cen T., Bai L., Zhao N., Zhang J., Zheng C.-J., Jiang T.-Y.","56451931200;57209131640;57349347200;57211559870;57226623414;57222715144;57348927900;57349632300;","Analyst sentiment and earning forecast bias in financial markets",2022,"Physica A: Statistical Mechanics and its Applications","589",,"126601","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119601699&doi=10.1016%2fj.physa.2021.126601&partnerID=40&md5=c347cc186681371c2448e799e26dd266","With the natural language processing technique, we quantitatively measure the sentiment expressed in analyst reports of stock markets. Both the analyst sentiment and the earning forecast biases decrease with the fiscal month. The temporal variation of the earning forecast biases displays an asymmetric behavior, i.e., underestimated initial biases may be corrected by the wisdom of crowds, while overestimated ones could not. The Pearson correlation between the analyst sentiment and the earning forecast errors becomes positive, when approaching the end of the fiscal year. In addition, the analyst sentiment is different from the common investor sentiment. © 2021 Elsevier B.V.","Analyst report; Analyst sentiment; Earning forecast; Financial market","Commerce; Correlation methods; Financial markets; Investments; Natural language processing systems; Analyst report; Analyst sentiment; Asymmetric behaviors; Earnings forecast; Earnings forecast bias; Financial market; Language processing techniques; Pearson correlation; Temporal variation; Wisdom of crowds; Forecasting",Article,Scopus,2-s2.0-85119601699
"Azzolini D., Bellodi E., Ferilli S., Riguzzi F., Zese R.","57204126073;36607390600;35502407200;56232243200;55813375100;","Abduction with probabilistic logic programming under the distribution semantics",2022,"International Journal of Approximate Reasoning","142",,,"41","63",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119493622&doi=10.1016%2fj.ijar.2021.11.003&partnerID=40&md5=4f8086aee8d54713b235f22623a06ebb","In Probabilistic Abductive Logic Programming we are given a probabilistic logic program, a set of abducible facts, and a set of constraints. Inference in probabilistic abductive logic programs aims to find a subset of the abducible facts that is compatible with the constraints and that maximizes the joint probability of the query and the constraints. In this paper, we extend the PITA reasoner with an algorithm to perform abduction on probabilistic abductive logic programs exploiting Binary Decision Diagrams. Tests on several synthetic datasets show the effectiveness of our approach. © 2021 Elsevier Inc.","Abduction; Distribution semantics; Probabilistic logic programming; Statistical relational artificial intelligence","Binary decision diagrams; Computer circuits; Probabilistic logics; Probability distributions; Semantics; Abduction; Abductive logic programming; Abductive logic programs; Distribution semantics; Joint probability; Probabilistic logic programming; Probabilistic logic programs; Probabilistics; Reasoners; Statistical relational artificial intelligence; Logic programming",Article,Scopus,2-s2.0-85119493622
"Wu C., Yao Y., Wu Q., Yang Y., Wu Z., Yang Y., Tian J., Xu K.","57215263147;7403567882;57202392792;57195445741;57254076200;57340296000;57340397200;57226141162;","Different types of noise-like pulse in a nonlinear multimodal interference based mode-locked fiber laser",2022,"Optics and Laser Technology","147",,"107681","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119208522&doi=10.1016%2fj.optlastec.2021.107681&partnerID=40&md5=ab3dddde9369fc312516ee30d8a7212b","In this paper, we employ a symmetrical graded index multimode fiber-step index multimode fiber-graded index multimode fiber (GIMF-SIMF-GIMF) structure as artificial saturable absorber to achieve mode-locked operation in an Erbium-doped fiber laser. After optimizing the cavity length, pump power and orientation of polarization controller, noise-like pulse (NLP) is built in anomalous dispersion region experimentally. It is worth noting that except for the traditional NLP, a special NLP with unique features is obtained whose temporal and spectral properties are different from the known case on NLP reported before. According to the experimental results, we also acquire and compare properties of subpulse within NLP envelope with the change of pump power. Last but not least, harmonic of the peculiar NLP is found under appropriate cavity conditions. © 2021 Elsevier Ltd","Mode-locked fiber laser; Noise-like pulse; Nonlinearity","Fiber lasers; Mode-locked fiber lasers; Multimode fibers; Natural language processing systems; Optical pumping; Cavity length; Erbium-doped fiber lasers; Fibre structure; Graded-Index Multimode fibers; Mode locked operations; Multi-modal; Noise-like pulse; Nonlinearity; Polarization controllers; Pump power; Saturable absorbers",Article,Scopus,2-s2.0-85119208522
"Lin D., Tang J., Li X., Pang K., Li S., Wang T.","57211990431;8701005000;56457212300;56457638600;36634432600;55925347300;","BERT-SMAP: Paying attention to Essential Terms in passage ranking beyond BERT",2022,"Information Processing and Management","59","2","102788","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119203816&doi=10.1016%2fj.ipm.2021.102788&partnerID=40&md5=50850ce52398582c50819124cb80e3df","Passage ranking has attracted considerable attention due to its importance in information retrieval (IR) and question answering (QA). Prior works have shown that pre-trained language models (e.g. BERT) can improve ranking performance. However, these simple BERT-based methods tend to focus on passage terms that exactly match the question, which makes them easily fooled by the overlapping but irrelevant (distracting) passages. To solve this problem, we propose a self-matching attention-pooling mechanism (SMAP) to highlight the Essential Terms in the question-passage pairs. Further, we propose a hybrid passage ranking architecture, called BERT-SMAP, which combines SMAP with BERT to more effectively identify distracting passages and downplay their influence. BERT-SMAP uses the representations obtained through SMAP to enhance BERT's classification mechanism as an interaction-focused neural ranker, and as the inputs of a matching function. Experimental results on three evaluation datasets show that our model outperforms the previous best BERTbase-based approaches, and is comparable to the state-of-the-art method that utilizes a much stronger pre-trained language model. © 2021 Elsevier Ltd","Attention mechanism; Information retrieval; Passage ranking; Pre-trained model; Question answering","Computational linguistics; Natural language processing systems; Attention mechanisms; Classification mechanism; Language model; Matching functions; Matchings; Passage ranking; Pre-trained model; Question Answering; Ranking performance; Simple++; Information retrieval",Article,Scopus,2-s2.0-85119203816
"Navarro-Almanza R., Sanchez M.A., Castro J.R., Mendoza O., Licea G.","57194059186;56757648300;56002749300;22635166300;57195132906;","Interpretable Mamdani neuro-fuzzy model through context awareness and linguistic adaptation",2022,"Expert Systems with Applications","189",,"116098","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118692078&doi=10.1016%2fj.eswa.2021.116098&partnerID=40&md5=356155650afdaa54b276648b9b0a5b72","Interpretable machine learning is trending as it aims to build a human-understandable decision process. There are two main types of machine learning systems: white-box and black-box models. White-box models are inherently interpretable but commonly suffer from under-fitting phenomena; on the other hand, black-box models perform quite well in a wide range of application domain problems, but their reasoning behind a decision is hard or even impossible to understand. In the soft-computing area, fuzzy inference systems are rule-based systems that use fuzzy reasoning, bringing human perception modeling and computing with word capability. These rule-based systems are designed either manually or automatically but are commonly optimized to fit better some phenomena’ data (in a supervised learning task). After the optimization process, the initial semantic meaning of fuzzy sets is modified (slightly, in the best cases), creating a gray-box model. The principal objective of the proposed methodology in this paper is to extract a high-quality rule in terms of comprehensibility, accuracy and fidelity. This is accomplished by using a fuzzy linguistic interpretable model from an optimized neuro-fuzzy model, considering the initial knowledge context with which it was built. A grammar-guided genetic algorithm is used as the optimization process to find the interpretable description of the model. A collection of 16 datasets for classification tasks were used to evaluate our proposal, obtaining an f1-score of 0.814 with 0.026 standard deviation in the optimized model; the obtained fidelity, in terms of similarity from the interpretable model to the optimized one, was 0.93 of mean with 0.018 standard deviation. Obtained results show that neuro-fuzzy systems could play an important role in interpretable machine learning, providing natural language explanations from previous knowledge. © 2021 Elsevier Ltd","Automatic fuzzy rule generation; Fuzzy knowledge base; Grammar-Guide Genetic Algorithms; Interpretable machine learning","Classification (of information); Fuzzy inference; Fuzzy neural networks; Fuzzy systems; Learning algorithms; Machine learning; Semantics; Soft computing; Statistics; Automatic fuzzy rule generation; Black box modelling; Fuzzy knowledge base; Grammar-guide genetic algorithm; Interpretable machine learning; Neuro-fuzzy modeling; Optimisations; Rule generation; Rules based systems; Standard deviation; Genetic algorithms",Article,Scopus,2-s2.0-85118692078
"De Masellis R., Di Francescomarino C., Ghidini C., Tessaris S.","36442172800;25928227700;6701842843;55935585200;","Solving reachability problems on data-aware workflows",2022,"Expert Systems with Applications","189",,"116059","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118496385&doi=10.1016%2fj.eswa.2021.116059&partnerID=40&md5=6487e2ef8ac9439e54c00d3bef8edc66","Recent advances in the field of Business Process Management (BPM) have brought about several suites able to model data objects along with the traditional control flow perspective. Nonetheless, when it comes to formal verification there is still a lack of effective verification tools on imperative data-aware process models and executions: the data perspective is often abstracted away and verification tools are often missing. Automated Planning is one of the core areas of Artificial Intelligence where theoretical investigations and concrete and robust tools have made possible the reasoning about dynamic systems and domains. Moreover planning techniques are gaining popularity in the context of BPM. Starting from these observations, we provide here a concrete framework for formal verification of reachability properties on an expressive, yet empirically tractable class of data-aware process models, an extension of Workflow Nets. Then we provide a rigorous mapping between the semantics of such models and that of three important Automated Planning paradigms: Action Languages, Classical Planning, and Model-Checking. Finally, we perform a comprehensive assessment of the performance of three popular tools supporting the above paradigms in solving reachability problems for imperative data-aware business processes, which paves the way for a theoretically well founded and practically viable exploitation of planning-based techniques on data-aware business processes. © 2021 Elsevier Ltd","Data-aware business process management; Data-aware workflow nets; Formal verification; Model-checking; Planning; Reachability","Concretes; Enterprise resource management; Model checking; Semantics; Automated planning; Business Process; Data-aware business process management; Data-aware workflow nets; Models checking; Process-models; Reachability; Reachability problem; Verification tools; Work-flows; Formal verification",Article,Scopus,2-s2.0-85118496385
"Delmondes Neto J.P., Paraboni I.","57318495200;14027249800;","Multi-source BERT stack ensemble for cross-domain author profiling",2022,"Expert Systems","39","3","e12869","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118296486&doi=10.1111%2fexsy.12869&partnerID=40&md5=b9bc377dd20ae0220f2a076eda754e12","Author profiling is the computational task of inferring an author's demographics (e.g., gender, age etc.) based on text samples written by them. As in other text classification tasks, optimal results are usually obtained by using training data taken from the same text genre as the target application, in so-called in-domain settings. On the other hand, when training data in the required text genre is unavailable, a possible alternative is to perform cross-domain author profiling, that is, building a model from a source domain (e.g., Facebook posts), and then using it to classify text in a different target domain (e.g., e-mails.) Methods of this kind may however suffer from cross-domain vocabulary discrepancies and other difficulties. As a means to ameliorate these, the present work discusses a particular strategy for cross-domain author profiling in which multiple source domains are combined in a stack ensemble architecture of pre-trained language models. Results from this approach are shown to compare favourably against standard single-source cross-domain author profiling, and are found to reduce overall accuracy loss in comparison with optimal in-domain gender and age classification. © 2021 John Wiley & Sons Ltd.","author profiling; cross-domain classification; natural language processing; text classification","Natural language processing systems; Text processing; Author profiling; Computational task; Cross-domain; Cross-domain classification; Facebook; Multi-Sources; Optimal results; Target application; Text genre; Training data; Classification (of information)",Article,Scopus,2-s2.0-85118296486
"Cunliffe D., Vlachidis A., Williams D., Tudhope D.","7004500447;36440453700;57211046033;6603885821;","Natural language processing for under-resourced languages: Developing a Welsh natural language toolkit",2022,"Computer Speech and Language","72",,"101311","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117612913&doi=10.1016%2fj.csl.2021.101311&partnerID=40&md5=a0740ece6a15be82fcf1bb0cea51bd3f","Language technology is becoming increasingly important across a variety of application domains which have become common place in large, well-resourced languages. However, there is a danger that small, under-resourced languages are being increasingly pushed to the technological margins. Under-resourced languages face significant challenges in delivering the underlying language resources necessary to support such applications. This paper describes the development of a natural language processing toolkit for an under-resourced language, Cymraeg (Welsh). Rather than creating the Welsh Natural Language Toolkit (WNLT) from scratch, the approach involved adapting and enhancing the language processing functionality provided for other languages within an existing framework and making use of external language resources where available. This paper begins by introducing the GATE NLP framework, which was used as the development platform for the WNLT. It then describes each of the core modules of the WNLT in turn, detailing the extensions and adaptations required for Welsh language processing. An evaluation of the WNLT is then reported. Following this, two demonstration applications are presented. The first is a simple text mining application that analyses wedding announcements. The second describes the development of a Twitter NLP application, which extends the core WNLT pipeline. As a relatively small-scale project, the WNLT makes use of existing external language resources where possible, rather than creating new resources. This approach of adaptation and reuse can provide a practical and achievable route to developing language resources for under-resourced languages. © 2021 Elsevier Ltd","Cymraeg; Language technology; Natural language processing; Under-resourced languages; Welsh","Applications domains; Cymraeg; Language processing; Language resources; Language technology; Natural languages; Processing functionality; Under-resourced languages; Underlying language; Welsh; Natural language processing systems",Article,Scopus,2-s2.0-85117612913
"Qi L., Zhang Y., Yin Q., Liu T.","57204179688;55949765600;56979802000;57199476645;","MS-Transformer: Introduce multiple structural priors into a unified transformer for encoding sentences",2022,"Computer Speech and Language","72",,"101304","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116574977&doi=10.1016%2fj.csl.2021.101304&partnerID=40&md5=5114dee9162854042503e76db9796042","Transformers have been widely utilized in recent NLP studies. Unlike CNNs or RNNs, the vanilla Transformer is position-insensitive, and thus is incapable of capturing the structural priors between sequences of words. Existing studies commonly apply one single mask strategy on Transformers for incorporating structural priors while failing at modeling more abundant structural information of texts. In this paper, we aim at introducing multiple types of structural priors into Transformers, proposing the Multiple Structural Priors Guided Transformer (MS-Transformer) that transforms different structural priors into different attention heads by using a novel multi-mask based multi-head attention mechanism. In particular, we integrate two categories of structural priors, including the sequential order and the relative position of words. For the purpose of capturing the latent hierarchical structure of the texts, we extract these information not only from the word contexts but also from the dependency syntax trees. Experimental results on three tasks show that MS-Transformer achieves significant improvements against other strong baselines. © 2021 Elsevier Ltd","Natural language processing; Sentence representation; Transformer","Signal encoding; Trees (mathematics); Attention mechanisms; Hierarchical structures; Relative positions; Sentence representation; Sequential ordering; Single-mask; Structural information; Syntax tree; Transformer; Word contexts; Natural language processing systems",Article,Scopus,2-s2.0-85116574977
"Wang Y.-C., Chuang C.-M., Wu C.-K., Pan C.-L., Tsai R.T.-H.","57221509568;57191262968;55998452900;57260892700;25824124300;","Cross-language article linking with deep neural network based paragraph encoding",2022,"Computer Speech and Language","72",,"101279","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115025465&doi=10.1016%2fj.csl.2021.101279&partnerID=40&md5=b9c5fde5298e8ce4a1a7dee823269fe8","Cross-language article linking (CLAL), the task of generating links between articles in different languages from different encyclopedias, is critical for facilitating sharing among online knowledge bases. Some previous CLAL research has been done on creating links among Wikipedia wikis, but much of this work depends heavily on simple language patterns and encyclopedia format or metadata. In this paper, we propose a new CLAL method based on deep learning paragraph embeddings to link English Wikipedia articles with articles in Baidu Baike, the most popular online encyclopedia in mainland China. To measure article similarity for link prediction, we employ several neural networks with attention mechanisms, such as CNN and LSTM, to train paragraph encoders that create vector representations of the articles’ semantics based only on article text, rather than link structure, as input data. Using our “Deep CLAL” method, we compile a data set consisting of Baidu Baike entries and corresponding English Wikipedia entries. Our approach does not rely on linguistic or structural features and can be easily applied to other language pairs by using pre-trained word embeddings, regardless of whether the two languages are on the same encyclopedia platform. © 2021","Convolutional neural network; Cross-language article linking; Deep learning; Link discovery; Long short-term memory; Paragraph encoding","Deep learning; Deep neural networks; Embeddings; Long short-term memory; Network coding; Semantics; Attention mechanisms; Knowledge basis; Language patterns; Mainland chinas; Online encyclopedia; Structural feature; Vector representations; Wikipedia articles; Electronic document exchange",Article,Scopus,2-s2.0-85115025465
"Pradhan A., Senapati M.R., Sahu P.K.","57221426574;6701820053;57225770426;","Improving sentiment analysis with learning concepts from concept, patterns lexicons and negations: Improving sentiment analysis with learning concepts",2022,"Ain Shams Engineering Journal","13","2","101559","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113605363&doi=10.1016%2fj.asej.2021.08.004&partnerID=40&md5=865e3edafa63efb219e1d558429050d6","The way of expressing sentiment (−ve/+ve) in the form of textual information depends on the way of thinking of human beings. Identifying aspect extraction and sentiment polarity from written texts is a crucial task. Mainly, a multi-level learning approach for aspect extraction from statistical methods, pattern-based methods, and rule-based methods. This work proposes the application of two probabilistic graphical Latent Dirichlet Allocation (LDA) and Probabilistic Latent Semantic Analysis (PLSA) algorithms to generate latent topic terms as possible aspects. Then frequency-based and Concept lexicons are used to retrieve unigram to multi-word phrases with associated opinion words. Polarity shift is a significant issue that reverses the polarity of the aspects that affect the sentiment classification of the system. Therefore, to improve the performance of the machine learning classification algorithm in ABSA a hybrid approach comprising rule-based methods and a graph-theoretic model is applied to deal with the explicit and implicit polarity shift. The performance of the proposed method is measured using Naive Bayes, a machine learning classification algorithm on two datasets, SemEval 2014 Restaurant and SemEval 2014 Laptop dataset. Experimental result shows that the method for aspect extraction outperforms baseline methods by 86.32% and 82.64% for Restaurant, Laptop dataset, respectively. Similarly, for aspect-based sentiment classification, the accuracy and F1 measure on Restaurant domain 84.73%, 81.28% and 82.06% and 80.71% on the laptop domain. © 2021","Aspect Based Sentiment Analysis; Aspect Term Extraction; Concept Lexicon; Feature Selection; Pattern Lexicon; SenticNet 4","Classification (of information); Extraction; Graph algorithms; Graph theory; Laptop computers; Machine learning; Semantics; Sentiment analysis; Statistics; Turing machines; Graph theoretic modeling; Latent dirichlet allocations; Learning approach; Machine learning classification; Pattern based method; Probabilistic latent semantic analysis; Sentiment classification; Textual information; Learning algorithms",Article,Scopus,2-s2.0-85113605363
"Zhao S., Li F., Chen X., Guan X., Jiang J., Huang D., Qing Y., Wang S., Wang P., Zhang G., Li C., Luo P., Cui H.","57203346375;57218844150;57196261666;57226793167;57196257978;57226802400;57226790862;57216638843;57198929340;57198462638;57204858110;57203040853;54415524700;","VPipe: A Virtualized Acceleration System for Achieving Efficient and Scalable Pipeline Parallel DNN Training",2022,"IEEE Transactions on Parallel and Distributed Systems","33","3","9472938","489","506",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112712312&doi=10.1109%2fTPDS.2021.3094364&partnerID=40&md5=bfa589009d37ddaedffa02312b008546","The increasing computational complexity of DNNs achieved unprecedented successes in various areas such as machine vision and natural language processing (NLP), e.g., the recent advanced Transformer has billions of parameters. However, as large-scale DNNs significantly exceed GPU's physical memory limit, they cannot be trained by conventional methods such as data parallelism. Pipeline parallelism that partitions a large DNN into small subnets and trains them on different GPUs is a plausible solution. Unfortunately, the layer partitioning and memory management in existing pipeline parallel systems are fixed during training, making them easily impeded by out-of-memory errors and the GPU under-utilization. These drawbacks amplify when performing neural architecture search (NAS) such as the evolved Transformer, where different network architectures of Transformer needed to be trained repeatedly. vPipe is the first system that transparently provides dynamic layer partitioning and memory management for pipeline parallelism. vPipe has two unique contributions, including (1) an online algorithm for searching a near-optimal layer partitioning and memory management plan, and (2) a live layer migration protocol for re-balancing the layer distribution across a training pipeline. vPipe improved the training throughput of two notable baselines (Pipedream and GPipe) by 61.4-463.4 percent and 24.8-291.3 percent on various large DNNs and training settings. © 1990-2012 IEEE.","distributed artificial intelligence; distributed systems; Machine learning; memory management; parallel systems; pipeline","Balancing; Computational linguistics; E-learning; Graphics processing unit; Memory architecture; Natural language processing systems; Network architecture; Program processors; Acceleration systems; Conventional methods; Migration protocols; NAtural language processing; Neural architectures; Out-of-memory errors; Pipeline parallelisms; Training throughputs; Pipelines",Article,Scopus,2-s2.0-85112712312
"Alsanie W., Alkanhal M.I., Alhamadi M., Alqabbany A.O.","56685811200;57218372981;57218500300;57226133419;","Automatic scoring of arabic essays over three linguistic levels",2022,"Progress in Artificial Intelligence","11","1",,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111710783&doi=10.1007%2fs13748-021-00257-z&partnerID=40&md5=9eb8bbf737e7329cd8416da89f59a57c","The importance of open questions requiring argumentative answers to assess student’s competence, along with the increasing number of people applying to colleges, have increased the demand to have systems which automatically score written essays. Developing such a system faces two main challenges. The first is, in many cases, scoring a free answer is largely subjective and does not have well-defined criteria. The second is scoring free answers requires deep language understanding. In this paper, we present an automatic scoring system for Arabic with these two challenges being considered. We only consider the essays of learners of Arabic as a second language in the beginning and intermediate levels. We omit essays of students at advanced levels as these essays might pose different challenges that require deep language understanding. The essays are scored by extracting specific features from the three linguistic levels, lexical, syntax and semantics. Syntactic level scoring is based on the sentence structure. Each level is scored independently and then the final score of the essay is a combination of these scores. We present different experiments with linear and non-linear combination methods on a real dataset. The results obtained from our experiments show that the trained models with respect to a human rater achieve accuracies and quadratic weighted kappa values similar to the agreement between two human raters. It is evident from our results that, with some realistic assumptions, a decision support Arabic scoring system can be achieved. © 2021, Springer-Verlag GmbH Germany, part of Springer Nature.","Artificial intelligence for education; Automatic essay scoring; Automatic grading systems; Language processing for education","Decision support systems; Semantics; Syntactics; Automatic scoring; Decision supports; Intermediate level; Language understanding; Number of peoples; Scoring systems; Second language; Sentence structures; Students",Article,Scopus,2-s2.0-85111710783
"Cui F., Di H., Shen L., Ouchi K., Liu Z., Xu J.","57216620218;57192311399;57204502124;7203010806;55553993700;54796280200;","Modeling semantic and emotional relationship in multi-turn emotional conversations using multi-task learning",2022,"Applied Intelligence","52","4",,"4663","4673",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111677698&doi=10.1007%2fs10489-021-02683-x&partnerID=40&md5=73ee4af3c1d8f0216faf2a5d146e1e68","Recognition and expression of emotion are key factors to the success of multi-turn conversations. Emotion recognition that can help model the relationship between query and response is used to be employed in single-turn conversation models. However, little work focuses on infusing the emotional factor in multi-turn conversation generation so far. To alleviate these problems, we propose Multi-turn Emotional Conversation Model (MECM) by using multi-task learning, which improves the ability to represent emotions in multi-turn conversations. MECM is based on hierarchical latent variable model, that utilizes context hidden to sharing the common information. Besides it also contains an emotion classifier to help the model recognize the emotion in the conversation, and a conversation generator to maintain consistency of content and transformation of emotion. Experimental results show that our model significantly improves the quality of responses in terms of diversity and empathy, and keeps better performance on semantic similarity compared with baseline methods. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Emotion; Multi-task learning; Multi-turn conversation; Variational autoencoder","Multi-task learning; Semantics; Baseline methods; Emotion recognition; Emotional factors; HELP model; Latent variable modeling; Model semantics; Multi-turn; Semantic similarity; Learning systems",Article,Scopus,2-s2.0-85111677698
"Zhao J., Fan M., Feng M.","39362593000;26424940800;57436059100;","ChartSeer: Interactive Steering Exploratory Visual Analysis With Machine Intelligence",2022,"IEEE Transactions on Visualization and Computer Graphics","28","3",,"1500","1513",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105113603&doi=10.1109%2fTVCG.2020.3018724&partnerID=40&md5=29a8b097580e47bbe267a26f0b7a733a","During exploratory visual analysis (EVA), analysts need to continually determine which subsequent activities to perform, such as which data variables to explore or how to present data variables visually. Due to the vast combinations of data variables and visual encodings that are possible, it is often challenging to make such decisions. Further, while performing local explorations, analysts often fail to attend to the holistic picture that is emerging from their analysis, leading them to improperly steer their EVA. These issues become even more impactful in the real world analysis scenarios where EVA occurs in multiple asynchronous sessions that could be completed by one or more analysts. To address these challenges, this work proposes ChartSeer, a system that uses machine intelligence to enable analysts to visually monitor the current state of an EVA and effectively identify future activities to perform. ChartSeer utilizes deep learning techniques to characterize analyst-created data charts to generate visual summaries and recommend appropriate charts for further exploration based on user interactions. A case study was first conducted to demonstrate the usage of ChartSeer in practice, followed by a controlled study to compare ChartSeers performance with a baseline during EVA tasks. The results demonstrated that ChartSeer enables analysts to adequately understand current EVA status and advance their analysis by creating charts with increased coverage and visual encoding diversity. © 2020 IEEE.","Collaboration; Data visualization; Encoding; Machine learning; Semantics; Task analysis; Visualization","Deep learning; Encoding (symbols); Job analysis; Semantics; Signal encoding; Visualization; 'current; Collaboration; Data variables; Encodings; Learning techniques; Machine intelligence; Real-world; Task analysis; Visual analysis; Visual encodings; Data visualization; article; artificial intelligence; deep learning; exploratory research",Article,Scopus,2-s2.0-85105113603
"Peng H., Yang R., Wang Z., Li J., He L., Yu P.S., Zomaya A.Y., Ranjan R.","57190014707;56147403700;35111811300;55720560100;55141799000;7402366049;7005128430;57197711780;","Lime: Low-Cost and Incremental Learning for Dynamic Heterogeneous Information Networks",2022,"IEEE Transactions on Computers","71","3",,"628","642",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100861037&doi=10.1109%2fTC.2021.3057082&partnerID=40&md5=b085fa211f429294ede2793afcb99667","Understanding the interconnected relationships of large-scale information networks like social, scholar and Internet of Things networks is vital for tasks like recommendation and fraud detection. The vast majority of the real-world networks are inherently heterogeneous and dynamic, containing many different types of nodes and edges and can change drastically over time. The dynamicity and heterogeneity make it extremely challenging to reason about the network structure. Unfortunately, existing approaches are inadequate in modeling real-life dynamical networks as they either have strong assumption of a given stochastic process or fail to capture the heterogeneity of network structure, and they all require extensive computational resources. We introduce Lime, a better approach for modeling dynamic and heterogeneous information networks. Lime is designed to extract high-quality network representation with significantly lower memory resources and computational time over the state-of-the-arts. Unlike prior work that uses a vector to encode each network node, we exploit the semantic relationships among network nodes to encode multiple nodes with similar semantics in shared vectors. By using many fewer node vectors, our approach significantly reduces the required memory space for encoding large-scale networks. To effectively trade information sharing for reduced memory footprint, we employ the recursive neural network (RsNN) with carefully designed optimization strategies to explore the node semantics in a novel cuboid space. We then go further by showing, for the first time, how an effective incremental learning approach can be developed - with the help of RsNN, our cuboid structure, and a set of novel optimization techniques - to allow a learning framework to quickly and efficiently adapt to a constantly evolving network. We evaluate Lime by applying it to three representative network-based tasks, node classification, node clustering and anomaly detection, performing on three large-scale datasets. We compare Lime against eleven prior state-of-the-art approaches for learning network representation. Our extensive experiments demonstrate that Lime not only reduces the memory footprint by over 80 percent and the processing time over 2x when learning network representation but also delivers comparable performance for downstream processing tasks. We show that our incremental learning method can boost the learning time by up to 20x without compromising the quality of the learned network representation. © 1968-2012 IEEE.","Heterogeneous information networks; Incremental learning; Memory optimization; Network representation learning","Anomaly detection; Costs; Encoding (symbols); Information services; Large dataset; Lime; Semantics; Computational resources; Downstream-processing; Heterogeneous information; Incremental learning; Information networks; Large-scale datasets; Network representation; Semantic relationships; Classification (of information)",Article,Scopus,2-s2.0-85100861037
"Zhang W., Fang Y., Liu Z., Wu M., Zhang X.","57204108927;55469295200;57191689277;57221315294;55715219100;","Mg2vec: Learning Relationship-Preserving Heterogeneous Graph Representations via Metagraph Embedding",2022,"IEEE Transactions on Knowledge and Data Engineering","34","3",,"1317","1329",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095511815&doi=10.1109%2fTKDE.2020.2992500&partnerID=40&md5=ad7946d81add6669b465a1b01fe0911f","Given that heterogeneous information networks (HIN) encompass nodes and edges belonging to different semantic types, they can model complex data in real-world scenarios. Thus, HIN embedding has received increasing attention, which aims to learn node representations in a low-dimensional space, in order to preserve the structural and semantic information on the HIN. In this regard, metagraphs, which model common and recurring patterns on HINs, emerge as a powerful tool to capture semantic-rich and often latent relationships on HINs. Although metagraphs have been employed to address several specific data mining tasks, they have not been thoroughly explored for the more general HIN embedding. In this paper, we leverage metagraphs to learn relationship-preserving HIN embedding in a self-supervised setting, to support various relationship mining tasks. In particular, we observe that most of the current approaches often under-utilize metagraphs, which are only applied in a pre-processing step and do not actively guide representation learning afterwards. Thus, we propose the novel framework of mg2vec, which learns the embeddings for metagraphs and nodes jointly. That is, metagraphs actively participates in the learning process by mapping themselves to the same embedding space as the nodes do. Moreover, metagraphs guide the learning through both first- and second-order constraints on node embeddings, to model not only latent relationships between a pair of nodes, but also individual preferences of each node. Finally, we conduct extensive experiments on three public datasets. Results show that mg2vec significantly outperforms a suite of state-of-the-art baselines in relationship mining tasks including relationship prediction, search and visualization. © 2020 IEEE.","Heterogeneous information networks; Network embedding; Relationship mining","Graph structures; Information services; Semantics; Heterogeneous graph; Heterogeneous information; Learning process; Low-dimensional spaces; Pre-processing step; Real-world scenario; Semantic information; State of the art; Embeddings",Article,Scopus,2-s2.0-85095511815
"Kong J., Wang J., Zhang X.","57221157168;56802804300;24802813900;","Hierarchical BERT with an adaptive fine-tuning strategy for document classification",2022,"Knowledge-Based Systems","238",,"107872","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121786022&doi=10.1016%2fj.knosys.2021.107872&partnerID=40&md5=77b9c27bf54af8fcf55d7f8402c3c90b","Pretrained language models (PLMs) have achieved impressive results and have become vital tools for various natural language processing (NLP) tasks. However, there is a limitation that applying these PLMs to document classification when the document length exceeds the maximum acceptable length of the PLM since the excess portion is truncated in these models. If the keywords are in the truncated part, then the performance of the model declines. To address this problem, this paper proposes a hierarchical BERT with an adaptive fine-tuning strategy (HAdaBERT). It consists of a BERT-based model as the local encoder and an attention-based gated memory network as the global encoder. In contrast to existing PLMs that directly truncate documents, the proposed model uses a part of the document as a region, dividing input document into several containers. This allows the useful information in each container to be extracted by a local encoder and composed by a global encoder according to its contribution to the classification. To further improve the performance of the model, this paper proposes an adaptive fine-tuning strategy, which dynamically decides the layers of BERT to be fine-tuned instead of fine-tuning all layers for each input text. Experimental results on different corpora indicated that this method outperformed existing neural networks for document classification. © 2021 Elsevier B.V.","Adaptive fine-tuning strategy; Document classification; Hierarchical BERT; Pretrained language model","Classification (of information); Computational linguistics; Containers; Information retrieval systems; Signal encoding; Adaptive fine-tuning; Adaptive fine-tuning strategy; Document Classification; Document length; Hierarchical BERT; Language model; Memory network; Performance; Pretrained language model; Tuning strategy; Natural language processing systems",Article,Scopus,2-s2.0-85121786022
"Dong X., Zhang Y., Pang K., Chen F., Lu M.","57382886800;37762522900;57383974300;57209804198;55366701400;","Heterogeneous graph neural networks with denoising for graph embeddings",2022,"Knowledge-Based Systems","238",,"107899","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121652921&doi=10.1016%2fj.knosys.2021.107899&partnerID=40&md5=74edca26ad24fb9f6ffee161609a2cf0","With the increasing popularity of graph structures, Graph embedding, Which aims to project nodes into low dimensional space while preserving the topological structure information of graphs and the information of nodes themselves, Has attracted an increased amount of attention in recent years. most of the embedding methods based on heterogeneous graphs use a meta-path guided random walk to capture the semantic and structural correlation between different types of nodes in the graph. despite the success of the meta-path-guided heterogeneous graph embedding method, The choice of meta-path is still an open and challenging problem. the design of the meta-path scheme largely depends on domain knowledge. in this paper, We propose a heterogeneous graph neural network with denoising (HGNND) to handle the issue. considering that there are different types of nodes in heterogeneous graphs, And their features are usually distributed in different spaces, The HGNND projects features of different types of nodes into a common vector space. then, The whole heterogeneous graph is input into the graph neural network to aggregate the neighbor node information and capture the structure information of the heterogeneous graph. finally, The noise nodes that may affect the performance of the whole model are filtered out by the denoising operation. extensive experiments on three real-world datasets demonstrate that our proposed model achieves state-of-the-art performance, It further proves that the model can still effectively aggregate semantic information without using meta-paths. © 2021 Elsevier B.V.","Denoising; Graph embedding; Graph neural networks; Heterogeneous graph","Aggregates; Domain Knowledge; Graph embeddings; Graph theory; Semantics; Vector spaces; De-noising; Embedding method; Graph embeddings; Graph neural networks; Heterogeneous graph; Low-dimensional spaces; Random Walk; Structural correlation; Structure information; Topological structure; Graph neural networks",Article,Scopus,2-s2.0-85121652921
"Xiong P., Liang L., Zhu Y., Zhu T.","55108139800;57271798400;57272076300;9737124100;","PriTxt: A privacy risk assessment method for text data based on semantic correlation learning",2022,"Concurrency and Computation: Practice and Experience","34","5","e6680","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118491466&doi=10.1002%2fcpe.6680&partnerID=40&md5=462c202c13e4491d6c0c648322c4976c","Privacy risk assessment plays a fundamental role in privacy preservation, as it determines the extent to which subsequent processing (such as generalization and obfuscation), should be applied to the sensitive data. However, most existing works on privacy risk assessment have focused on structured data, while unstructured text data remain relatively underexplored due to the complexity of natural language. In this article, we propose a novel method, PriTxt, for evaluating the privacy risk associated with text data by exploiting the semantic correlation. Using definitions derived from the General Data Protection Regulation (GDPR), a de facto standard of privacy preservation in practice, PriTxt first defines the private features that related to individual privacy in order to locate the sensitive words. By using the word2vec algorithm, a word-embedding model is further constructed to identify the quasi-sensitive words that are semantically correlated to the private features. The privacy risk of a given text is finally evaluated by aggregating the weighted risks of the sensitive and the quasi-sensitive words in the text. Experiments on real-world datasets demonstrate that the proposed PriTxt is effective for conducting risk assessment on text data and further outperforms the traditional methods. © 2021 John Wiley & Sons Ltd.",,"Data privacy; Natural language processing systems; Semantics; General data protection regulations; Generalisation; Privacy preservation; Privacy risks; Risk assessment methods; Risks assessments; Sensitive datas; Text data; Unstructured text data; Unstructured texts; Risk assessment",Article,Scopus,2-s2.0-85118491466
"Wilch J., Fischer J., Langer N., Felger M., Bengel M., Vogel-Heuser B.","57216614525;57216102484;57224204251;57224187580;22333614200;6603480302;","Erste Schritte einer automatischen Funktionalitätssemantik zur Verbesserung von SPS Softwaremodularität [Towards automatic generation of functionality semantics to improve PLC software modularization]",2022,"At-Automatisierungstechnik","70","2",,"181","191",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124675907&doi=10.1515%2fauto-2021-0138&partnerID=40&md5=b3966aaa27a1c952aae158f73486bc5c","Functions of automated Production Systems (aPS) can be realized by control software (SW), whose high quality and short development time are, therefore, vital. To achieve both, SW should be modular and, thereby, reusable. Static code analysis can help improve the modularization of existing software, e. g., by automatically analyzing control and information flow. However, manual code reviews are still typically required because planning a SW's modularization requires a semantic understanding of its functionality. This paper presents an approach to, instead, identify SW functionality automatically and evaluates it with SW from three aPS manufacturers. © 2022 Wilch et al., published by De Gruyter.","automated Production Systems; control software; functionality; semantics; static code analysis",,Article,Scopus,2-s2.0-85124675907
"Xu X., Geng G., Cao X., Li K., Zhou M.","57263209600;57397967000;57220759588;57397405300;57398346400;","TDNet: transformer-based network for point cloud denoising",2022,"Applied Optics","61","6",,"C80","C88",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122314873&doi=10.1364%2fAO.438396&partnerID=40&md5=a3ecf5500b4a2c42cb21175e2fd5131d","This study proposes a novel, to the best of our knowledge, transformer-based end-to-end network (TDNet) for point cloud denoising based on encoder–decoder architecture. The encoder is based on the structure of a transformer in natural language processing (NLP). Even though points and sentences are different types of data, the NLP transformer can be improved to be suitable for a point cloud because the point can be regarded as a word. The improved model facilitates point cloud feature extraction and transformation of the input point cloud into the underlying high-dimensional space, which can characterize the semantic relevance between points. Subsequently, the decoder learns the latent manifold of each sampled point from the high-dimensional features obtained by the encoder, finally achieving a clean point cloud. An adaptive sampling approach is introduced during denoising to select points closer to the clean point cloud to reconstruct the surface. This is based on the view that a 3D object is essentially a 2D manifold. Extensive experiments demonstrate that the proposed network is superior in terms of quantitative and qualitative results for synthetic data sets and real-world terracotta warrior fragments. © 2021 Optical Society of America",,"Decoding; Semantics; Signal encoding; De-noising; Encoder-decoder architecture; End-to-end network; Feature transformations; Features extraction; High dimensional spaces; Knowledge transformers; Learn+; Point-clouds; Semantic relevance; Natural language processing systems; article; feature extraction; natural language processing",Article,Scopus,2-s2.0-85122314873
"Baydogan C., Alatas B.","57203224994;10341297500;","Deep-Cov19-Hate: A Textual-Based Novel Approach for Automatic Detection of Hate Speech in Online Social Networks throughout COVID-19 with Shallow and Deep Learning Models",2022,"Tehnicki Vjesnik","29","1",,"149","156",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122139625&doi=10.17559%2fTV-20210708143535&partnerID=40&md5=f613b024941f75bf8181f28386b30242","The use of various online social media platforms rising day by day caused an increase in the correct or incorrect information shared by users, especially during COVID-19. The introduction of COVID-19 on the world agenda gave rise to an overall bad reaction against East Asia (esp. China) in online social media platforms. The social media users who spread degrading, racist, disrespectful, abusive, discriminatory, critical, abuse, harsh, offensive, etc. posts accused the Asian people of being responsible for the outbreak of COVID-19. For this reason, the development of the Hate Speech Detection (HSD) system was necessary in order to prevent the spread of these posts about COVID-19. In this article, a textual-based study on COVID-19-related hate speech (HS) sharing in online social networks was carried out with Shallow Learning (SL) and Deep Learning (DL) methods. In the first step of this study, typical Natural Language Processing (NLP) pipeline was applied for gathered two different datasets. This NLP pipeline was performed using bag of words, term frequency, document matrix, etc. techniques for features extraction representing datasets. Then, ten different SL and DL models were fine-tuned for HS datasets related to COVID-19. Accuracy, precision, sensitivity, and F-score performance measurement criteria were calculated to compare the performance of the SL and DL algorithms for the problem of HSD. The RNN, one of the models proposed for the first and second dataset in HSD, prevailed with the highest accuracy values of 78.7% and 90.3%, respectively. Due to the promising results of all approaches operated in the HSD, they are forecasted to be chosen in the solution of many other social media and network problems related to COVID-19. © 2022, Strojarski Facultet. All rights reserved.","COVID-19; Hate speech detection; Shallow and deep learning models; Social media analysis; Social network problems","Deep learning; Natural language processing systems; Pipelines; Speech recognition; COVID-19; Hate speech detection; Learning models; Network problems; Online social medias; Shallow and deep learning model; Social media analysis; Social media platforms; Social network problem; Speech detection; Social networking (online)",Article,Scopus,2-s2.0-85122139625
"Liu L., Wang M., He X., Qing L., Chen H.","57197869418;57217013613;9237988800;23971364800;57377211900;","Fact-based visual question answering via dual-process system",2022,"Knowledge-Based Systems","237",,"107650","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121430804&doi=10.1016%2fj.knosys.2021.107650&partnerID=40&md5=12e749c160ef80fb6d2c3d81022875fd","Fact-based visual question answering (FVQA) requires the model to answer questions based on the observed images and external knowledge. The key is to enable the agent to understand questions and images and then reason on the knowledge base to find the correct answer. Founded on the dual-process theory in cognitive science, an effective framework for the FVQA is proposed in this study by coordinating a perception module (System 1) and an explicit reasoning module (System 2). When a question and an image are given, System 1 first learns the joint representation of them, and then System 2 predicts the answer via reasoning on a fact graph and a semantic graph. Specifically, System 1 is implemented by a two-parallel BERT-style model, while System 2 by a graph neural network (GNN) with a dual-level attention mechanism. Experiments on two public datasets, i.e., FVQA and OK-VQA datasets, show that our model outperforms other baselines. Moreover, the proposed model also provides the interpretation of the reasoning process in addition to a correct answer to the question. © 2021 Elsevier B.V.","Dual-level attention; Dual-process theory; Fact-based VQA; Graph reasoning; Multimodal transformer","Graph neural networks; Semantics; Dual process; Dual-level attention; Dual-process theories; Fact-based VQA; Graph reasoning; Module systems; Multi-modal; Multimodal transformer; Process system; Question Answering; Knowledge based systems",Article,Scopus,2-s2.0-85121430804
"Matchin W., Basilakos A., Ouden D.-B.D., Stark B.C., Hickok G., Fridriksson J.","36473887900;40461090900;55991948100;57185150000;7004718457;8338967200;","Functional differentiation in the language network revealed by lesion-symptom mapping: Functional Differentiation in the Language Network",2022,"NeuroImage","247",,"118778","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121244024&doi=10.1016%2fj.neuroimage.2021.118778&partnerID=40&md5=c40a6ac54682d1922cda643a191f8733","Theories of language organization in the brain commonly posit that different regions underlie distinct linguistic mechanisms. However, such theories have been criticized on the grounds that many neuroimaging studies of language processing find similar effects across regions. Moreover, condition by region interaction effects, which provide the strongest evidence of functional differentiation between regions, have rarely been offered in support of these theories. Here we address this by using lesion-symptom mapping in three large, partially-overlapping groups of aphasia patients with left hemisphere brain damage due to stroke (N = 121, N = 92, N = 218). We identified multiple measure by region interaction effects, associating damage to the posterior middle temporal gyrus with syntactic comprehension deficits, damage to posterior inferior frontal gyrus with expressive agrammatism, and damage to inferior angular gyrus with semantic category word fluency deficits. Our results are inconsistent with recent hypotheses that regions of the language network are undifferentiated with respect to high-level linguistic processing. © 2021","Agrammatism; Aphasia; Lesion-symptom mapping; Neurobiology of language; Syntax; Word fluency","adult; angular gyrus; aphasia; Article; brain damage; brain function; brain lesion symptom mapping; brain mapping; cerebrovascular accident; disease association; expressive agrammatism; female; fluency disorder; grammar; human; inferior frontal gyrus; language disability; language network; left hemisphere; major clinical study; male; middle temporal gyrus; neuroimaging; semantics; speech discrimination; syntactic comprehension",Article,Scopus,2-s2.0-85121244024
"Wu J., Hu J.","57218546282;7406417389;","Improved prior selection using semantics in maximum a posteriori for few-shot learning",2022,"Knowledge-Based Systems","237",,"107688","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119919988&doi=10.1016%2fj.knosys.2021.107688&partnerID=40&md5=89269c6a8c5a446be416664863912c07","Few-shot learning is to recognize novel concepts with few labeled samples. Recently, significant progress has been made to address the overfitting caused by data scarcity, especially those on modeling the distribution of novel categories given a single point. However, they often deeply rely on the prior knowledge from base set, which is generally hard to define, and its selection can easily bias the learning. A popular pipeline is to pretrain a feature extractor with base set and generate statistics from them as prior information. Since pretrained feature extractor cannot extract accurate representations for categories have never seen, and there is only 1 or 5 support images from novel categories, making it hard to acquire accurate priors, especially when they are far away from the class center. To address these issues, in this paper, we base our network on Maximum a posteriori (MAP), proposing a strategy for better prior selection from base set. We specially introduce semantic information, which are learned from unsupervised text corpora and easily available, to alleviate biases caused by unrepresentative support samples. Our intuition is that when the support from visual information is biased, semantics can provide strong prior knowledge to assist learning. Experimental results on four few-shot benchmarks also show that it outperforms the state-of-the-art methods by a large margin, improves around 2.08%∼12.68% than the best results in each dataset on both 1- and 5-shot tasks. © 2021 Elsevier B.V.","Few-shot learning; Maximum a posteriori; Prior selection; Semantics","Large dataset; Basis sets; Data scarcity; Feature extractor; Few-shot learning; Maximum a posteriori; Novel concept; Overfitting; Prior selection; Prior-knowledge; Single point; Semantics",Article,Scopus,2-s2.0-85119919988
"Yuan X., Ma X., Zhang L., Fang Y., Wu D.","56342340600;57215562285;57221584556;57223950380;57223953029;","Beyond Class-Level Privacy Leakage: Breaking Record-Level Privacy in Federated Learning",2022,"IEEE Internet of Things Journal","9","4",,"2555","2565",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112202658&doi=10.1109%2fJIOT.2021.3089713&partnerID=40&md5=13c2a30e7ccc828075b0c6a1a1528a1e","Federated learning (FL) enables multiple clients to collaboratively build a global learning model without sharing their own raw data for privacy protection. Unfortunately, recent research still found privacy leakage in FL, especially on image classification tasks, such as the reconstruction of class representatives. Nevertheless, such analysis on image classification tasks is not applicable to uncover the privacy threats against natural language processing (NLP) tasks, whose records composed of sequential texts cannot be grouped as class representatives. The finer (record-level) granularity in NLP tasks not only makes it more challenging to extract individual text records, but also exposes more serious threats. This article presents the first attempt to explore the record-level privacy leakage against NLP tasks in FL. We propose a framework to investigate the exposure of the records of interest in federated aggregations by leveraging the perplexity of language modeling. Through monitoring the exposure patterns, we propose two correlation attacks to identify the corresponding clients when extracting their specific records. Extensive experimental results demonstrate the effectiveness of the proposed attacks. We have also examined several countermeasures and shown that they are ineffective to mitigate such attacks, and hence further research is expected. © 2014 IEEE.","Federated learning (FL); language modeling; natural language processing; neural networks; privacy","Data privacy; Image classification; Modeling languages; Correlation attack; Global learning; Multiple clients; NAtural language processing; Privacy leakages; Privacy protection; Privacy threats; Recent researches; Natural language processing systems",Article,Scopus,2-s2.0-85112202658
"Zhang L., He M.","57196128153;57218618570;","Unsupervised machine learning for solar cell materials from the literature",2022,"Journal of Applied Physics","131","6","064902","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124684281&doi=10.1063%2f5.0064875&partnerID=40&md5=cdb227100b8e6330477dc4406706b29c","Machine learning and data-driven methods have been adopted for material science research in recent years; yet, the textual data are not fully embraced by the materials and physics community. In this work, we aim to make the computers unsupervisedly learn the latent information on the solar cell materials based on the textual data with minimal human intervention and perform solar cell materials predictions. An unsupervised machine learning model is constructed by automatically extracting the information from the materials literature database using word embeddings, which successfully establishes the hidden relationships between the materials formulas and their photovoltaic applications. Uncommon solar cell materials predicted by the natural language processing (NLP)-based machine learning method are further evaluated via the first-principles methods to reveal the optoelectronic properties of the predicted candidate, demonstrating the validity of the NLP-assisted machine learning model. This study highlights the text-based machine learning methods for solar cell materials and calls for a wide deployment of the NLP methods for the materials research. © 2022 Author(s).",,"Learning algorithms; Machine learning; Solar cells; Data-driven methods; Learn+; Machine learning methods; Machine learning models; Material science; Physics community; Science research; Solar cell materials; Textual data; Unsupervised machine learning; Natural language processing systems",Article,Scopus,2-s2.0-85124684281
"Li X., Zhang S., Liu J., Yang Z.","47161324100;12445682700;57190127140;55716607000;","Efficient method to improve the distribution probability of dissipative soliton and noise-like pulse in all-normal-dispersion fiber lasers",2022,"Optics Express","30","4",,"6161","6175",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124474203&doi=10.1364%2fOE.452919&partnerID=40&md5=e5a6a7acd5123a8a30daf2a24462686e","Inspired by the chirped pulse amplification technique, herein, we show an efficient method to improve the distribution probability of dissipative soliton and noise-like pulse in all-normal-dispersion fiber lasers by using an intracavity pulse power editing (PPE) technique for the first time. The dissipative-soliton fiber laser is thus simplified into three parts: a PPE link, a saturable absorber (SA), and a spectral filter. Pulse with different peak powers can be edited in the PPE link, then undergo the positive- or reverse-saturable absorption of the SA, and finally pass through the filter. Further, just by assigning the length of single-mode fiber (SMF) at different positions in the PPE link with a fixed cavity length, four pulse patterns, including dissipative soliton (DS), DS molecules, a bound pattern of DS and noise-like pulse (NLP), and pure NLP, can be controllably produced in fiber lasers. The observed bound pattern of DS and NLP is a new addition to the pulse dynamic pattern family. It is found that the longer the SMF after the gain fiber is, the pulse will be severely broadened. This pulse can easily enter the positive-saturable absorption region of most saturated absorption curves, which will increase the probability of DS radiation; if the SMF behind the gain fiber is shorter, the pulse is not severely broadened. The pulse has a high probability of entering the reverse-saturable absorption range of most saturated absorption curves, resulting in a higher likelihood of generating NLP. In experiments, it is only necessary to increase the SMF length between the gain fiber and the isolator to build a DS fiber laser; however, to construct an NLP fiber laser, only the SMF length between the gain fiber and the isolator needs to be shortened. The experimental results agree well with the numerical predictions. The results significantly broaden the design possibilities for pulse lasers, making them much more accessible to produce specific pulse patterns. © 2022 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement",,"Natural language processing systems; Probability distributions; Saturable absorbers; Single mode fibers; Solitons; Absorption curves; All-normal dispersions; Dissipative solitons; Distribution probability; Gain fibers; Pulse pattern; Pulse power; Reverse saturable absorption; Saturated absorptions; Single-mode fibers; Fiber lasers",Article,Scopus,2-s2.0-85124474203
"Li J., Wang C., Wang P.","57283335000;15521387700;56801146200;","Tunable noise-like pulse and Q-switched erbium-doped fiber laser",2022,"Optics Express","30","4",,"4768","4781",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124043732&doi=10.1364%2fOE.450372&partnerID=40&md5=7864d3258d60201b7b782b27f7ea6915","A switchable, widely wavelength-tunable noise-like pulse (NLP) and Q-switched Er-doped fiber (EDF) laser with a linear cavity structure is proposed and experimentally demonstrated in this work. The net-normal-dispersion mode-locked NLP operation based on a semiconductor saturable mirror (SESAM) is realized in a 57 nm continuous tuning range from 1528 to 1585 nm by using a tunable filter (TF). When the pump power is 500 mW, the NLPs produce a maximum average output power of about 16 mW with a 3-dB spectral bandwidth of about 17 nm at the central wavelength of 1555 nm, while the average peak power is about 58.8 W. The measured characteristics of the output NLPs at 1555 nm are consistent with the numerical results under the condition of ∆β2, net = 0.095 ps2, and Esat = 0.77 nJ. In addition, stable Q-switched pulses with a 67 nm wavelength tuning range from 1518 to 1585 nm are obtained by adjusting the central wavelength of the filter. The maximum pulse energy reaches 231.4 nJ at the center wavelength of 1555 nm, corresponding to a peak power of about 278.8 mW. The proposed wavelength-tunable fiber laser is simple and versatile, demonstrating significant potential for numerous practical applications. © 2022 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement",,"Erbium; Fiber lasers; Natural language processing systems; Semiconductor lasers; Cavity structure; Central wavelength; Er-doped fiber laser; Erbium-doped fiber lasers; Linear cavity; Peak power; Q-switched; Switchable; Tunables; Wavelength tunable; Q switching",Article,Scopus,2-s2.0-85124043732
"Liu J., Xu X., Shi Y., Deng C., Shi M.","57343297100;57384509800;57384747100;57208019993;57214753178;","RELAXNet: Residual efficient learning and attention expected fusion network for real-time semantic segmentation",2022,"Neurocomputing","474",,,"115","127",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121703501&doi=10.1016%2fj.neucom.2021.12.003&partnerID=40&md5=e6d89906f5c3c7bce0ca01d3b0c20414","As a dense prediction problem, semantic segmentation consumes extensive memory and computational resources. However, the application of semantic segmentation requires the model to perform real-time analyses in portable devices, thus it is crucial to seek a trade-off between segmentation accuracy and inference speed. In this paper, we propose a lightweight semantic segmentation method based on attention mechanism to address this problem. First, we use novel Efficient Bottleneck Residual (EBR) Module and Efficient Asymmetric Bottleneck Residual (EABR) Module to extract both local and contextual information,which adopt a well-designed combination of depth-wise convolution, dilated convolution and factorized convolution, with channel shuffle to boost information interaction. Second, we introduce attention mechanism into skip connection between the encoder and decoder to promote reasonable fusion of high-level and low-level features, which furtherly enhance the accuracy. With only 1.9 M parameters, our model obtains 74.8% mIoU and 64 FPS running speed on Cityscapes dataset and 71.2% mIoU and 79 FPS running speed on Camvid dataset. Experiments demonstrate that our model achieves competitive results in terms of segmentation accuracy and running speed while controlling parameters. © 2021","Attention mechanism; Real-time analysis; Semantic segmentation","Economic and social effects; Semantic Segmentation; Semantics; Attention mechanisms; Efficient learning; Extensive memory; Memory resources; Prediction problem; Real time analysis; Real-time semantics; Running speed; Segmentation accuracy; Semantic segmentation; Convolution; article; attention; learning; running; velocity",Article,Scopus,2-s2.0-85121703501
"Khadilkar K., KhudaBukhsh A.R., Mitchell T.M.","57208585469;36835705200;57415385100;","Gender bias, social bias, and representation: 70 years of BHollywood",2022,"Patterns","3","2","100409","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122925602&doi=10.1016%2fj.patter.2021.100409&partnerID=40&md5=13dec6806a8bdc69554a83e5137af512","We use a suite of cutting-edge natural language processing methods to quantify and characterize societal and gender biases in popular movie content. Our data set consists of English subtitles of popular movies from Bollywood–the Mumbai film industry—spanning 7 decades (700 movies). In addition, we include movies from Hollywood and movies nominated for the Academy Awards for contrastive purposes. Our findings indicate that while the overall portrayal of women has improved over time in popular movie dialogues from both Bollywood and Hollywood, modern films still exhibit considerable gender bias and are yet to achieve equal representation among genders. We also observe a strong bias favoring fair skin color in Bollywood content that occurred consistently across all time periods we considered. While our geographic representation analysis indicates improved inclusion over time for several Indian states, it also reveals a long-standing under-representation of many northeastern Indian states. © 2021 The Authors","Bollywood; DSML 2: Proof-of-concept: Data science output has been formulated, implemented, and tested for one domain/problem; gender bias; Hollywood; social bias","Cutting; Natural language processing systems; Bollywood; Cutting edges; Data set; Domain problems; DSML 2: proof-of-concept: data science output have been formulated, implemented, and tested for one domain/problem; Gender bias; Hollywood; Processing method; Proof of concept; Social bias; Motion pictures",Article,Scopus,2-s2.0-85122925602
"Wang Z., Tian J., Fang H., Chen L., Qin J.","15052608600;57283466900;57283910700;57283685000;54684947200;","LightLog: A lightweight temporal convolutional network for log anomaly detection on the edge",2022,"Computer Networks","203",,"108616","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119961419&doi=10.1016%2fj.comnet.2021.108616&partnerID=40&md5=c949c9671054617493d52edb0a90c551","Log anomaly detection on edge devices is the key to enhance edge security when deploying IoT systems. Despite the success of many newly proposed deep learning based log anomaly detection methods, handling large-scale logs on edge devices is still a bottleneck due to the limited computational power on these devices to fulfil the real-time processing requirement for accurate anomaly detection. In this work, we propose a novel lightweight log anomaly detection algorithm, named LightLog, to tackle this research gap. In specific, we achieve real-time processing speed on the task via two aspects: (i) creation of a low-dimensional semantic vector space based on word2vec and post-processing algorithms (PPA); and (ii) design of a lightweight temporal convolutional network (TCN) for the detection. These two components significantly reduce the number of parameters and computations of a standard TCN while improving the detection performance. Experimental results show that our LightLog outperforms several benchmarking methods, namely DeepLog, LogAnomaly and RobustLog, by achieving 97.0 F1 score on HDFS Dataset and 97.2 F1 score on BGL with smallest model size. This effective yet efficient method paves the way to the deployment of log anomaly detection on the edge. Our source code and datasets are freely available on https://github.com/Aquariuaa/LightLog. © 2021 Elsevier B.V.","Edge computing; Global average pooling; Log anomaly detection; Pointwise-convolution; Temporal convolutional network","Anomaly detection; Convolutional neural networks; Deep learning; Edge computing; Semantics; Vector spaces; Anomaly detection; Anomaly detection methods; Convolutional networks; Edge computing; F1 scores; Global average pooling; Log anomaly detection; Point wise; Pointwise-convolution; Temporal convolutional network; Convolution",Article,Scopus,2-s2.0-85119961419
"Parrish A., Pylkkänen L.","57207579162;24344534400;","Conceptual Combination in the LATL With and Without Syntactic Composition",2022,"Neurobiology of Language","3","1",,"46","66",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124846231&doi=10.1162%2fnol_a_00048&partnerID=40&md5=12d82d81bbc0c4ec3bf6c1bae7cc2536","The relationship among syntactic, semantic, and conceptual processes in language comprehension is a central question to the neurobiology of language. Several studies have suggested that conceptual combination in particular can be localized to the left anterior temporal lobe (LATL), while syntactic processes are more often associated with the posterior temporal lobe or inferior frontal gyrus. However, LATL activity can also correlate with syntactic computations, particularly in narrative comprehension. Here we investigated the degree to which LATL conceptual combination is dependent on syntax, specifically asking whether rapid (~200 ms) magnetoencephalography effects of conceptual combination in the LATL can occur in the absence of licit syntactic phrase closure and in the absence of a semantically plausible output for the composition. We find that such effects do occur: LATL effects of conceptual combination were observed even when there was no syntactic phrase closure or plausible meaning. But syntactic closure did have an additive effect such that LATL signals were the highest for expressions that composed both conceptually and syntactically. Our findings conform to an account in which LATL conceptual composition is influenced by local syntactic composition but is also able to operate without it. © 2021 Massachusetts Institute of Technology Published under a Creative Commons Attribution 4.0 International (CC BY 4.0) license.","Conceptual combination; Magnetoencephalography; Semantics; Syntax",,Article,Scopus,2-s2.0-85124846231
"Fernandino L., Tong J.-Q., Conant L.L., Humphries C.J., Binder J.R.","16315573600;57231167300;6603732482;7004942680;57440517900;","Decoding the information structure underlying the neural representation of concepts",2022,"Proceedings of the National Academy of Sciences of the United States of America","119","6","e2108091119","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124059714&doi=10.1073%2fpnas.2108091119&partnerID=40&md5=69a112e92e12e87cfad3fda048727ff6","The nature of the representational code underlying conceptual knowledge remains a major unsolved problem in cognitive neuroscience. We assessed the extent to which different representational systems contribute to the instantiation of lexical concepts in high-level, heteromodal cortical areas previously associated with semantic cognition. We found that lexical semantic information can be reliably decoded from a wide range of heteromodal cortical areas in the frontal, parietal, and temporal cortex. In most of these areas, we found a striking advantage for experience-based representational structures (i.e., encoding information about sensory-motor, affective, and other features of phenomenal experience), with little evidence for independent taxonomic or distributional organization. These results were found independently for object and event concepts. Our findings indicate that concept representations in the heteromodal cortex are based, at least in part, on experiential information. They also reveal that, in most heteromodal areas, event concepts have more heterogeneous representations (i.e., they are more easily decodable) than object concepts and that other areas beyond the traditional “semantic hubs” contribute to semantic cognition, particularly the posterior cingulate gyrus and the precuneus. © This article is distributed under Creative Commons Attribution-NonCommercialNoDerivatives License 4.0 (CC BY-NC-ND).","Concept representation; Embodied semantics; Lexical semantics; Representational similarity analysis; Semantic memory","article; brain cortex; cognition; frontal cortex; human; human experiment; parietal cortex; posterior cingulate; precuneus; semantic memory; semantics; temporal cortex",Article,Scopus,2-s2.0-85124059714
"Paula A.J., Ferreira O.P., Souza Filho A.G., Filho F.N., Andrade C.E., Faria A.F.","37003074600;7004915311;57219510737;57437109600;57437820900;26633795900;","Machine Learning and Natural Language Processing Enable a Data-Oriented Experimental Design Approach for Producing Biochar and Hydrochar from Biomass",2022,"Chemistry of Materials","34","3",,"979","990",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123949182&doi=10.1021%2facs.chemmater.1c02961&partnerID=40&md5=62bfe5a3be1f5ec336d529f81ea3b053","Carbon functional materials (CFMs) such as biochar and hydrochar can be obtained from hundreds of biomass precursors varying from urban sludge to agriculture wastes. They can be produced through tens of synthesis methods and postsynthesis processing steps tuned at specific conditions (e.g., temperature, time, and chemical concentrations). To achieve a ""rational design""platform for a system with a high dimensional parameter space such as CFMs, we processed 10,975 scientific articles (from years 2000 to 2020) related to the subject with automatic reading-interpreting-extracting computational routines (namely, the a.RIX engine). The a.RIX engine automatically recognized more than a hundred precursors, among which wheat straw, rice husk, and rice straw were the most studied for CFM synthesis and application in agriculture (e.g., as an amendment), as fuel (energy generation), and as an adsorbent. Parameters related to the CFMs' synthesis conditions, such as carbonization temperature and time, and parameters related to CFMs' properties, such as surface area and heavy metals adsorption capacity, can also be extracted from the articles. Correlations between the CFM precursors and synthesis conditions indicated very little statistical difference between the carbonization temperature and time used for the CFMs' synthesis from different precursors. Essentially, precursors are carbonized at temperatures varying from 100 to 900 °C for 30 min to 6 h using pyrolysis, hydrothermal carbonization, and gasification. When focusing the analysis on just CFMs produced by pyrolysis (biochar), we observed that peanut shells can produce materials with higher surface areas than other precursors (P < 0.05). When performing correlations between biochar synthesis conditions and their properties, general trends can be confirmed: (i) the higher the carbonization temperature, the lower the H/C and O/C ratios, and (ii) the increase in the surface area can be achieved by preserving a high aromatic degree (low H/C ratio) and a low oxidation level (low O/C ratio). However, a deeper understanding of the relation between CFM synthesis/postsynthesis methods and the resulting properties can only be achieved using clustering algorithms (e.g., k-means) and complex network analysis. The a.RIX engine groups articles describing optimized synthesis conditions and CFM properties (e.g., low carbonization temperature, low carbonization time, and large surface area and adsorption capacity) and automatically recognizes the synthesis/postsynthesis steps used for these groups. The program efficiently recognized that precursors such as peanut shells can be converted into highly porous biochar by using experimental routes such as ""pyrolysis""→ ""activation""→ ""drying""→ ""ashing""→ ""washing""→ ""filtration.""With this approach, we show that a noncomputational review of scientific articles for materials with a huge parameter space such as CFMs is largely obsolete. Finally, taken together, the results provide a powerful platform for data-oriented experimental design of CFMs produced from biomass. ©",,"Agricultural wastes; Engines; Hybrid materials; K-means clustering; Learning algorithms; Machine learning; Natural language processing systems; Pyrolysis; Adsorption capacities; Biochar; Carbonization temperatures; Materials synthesis; Parameter spaces; Peanut shells; Postsynthesis; Scientific articles; Surface area; Synthesis conditions; Heavy metals",Article,Scopus,2-s2.0-85123949182
"McCrae J.P., Fransen T., Ahmadi S., Buitelaar P., Goswami K.","36666801700;57219794613;57210119212;14041096000;57219787194;","Toward an Integrative Approach for Making Sense Distinctions",2022,"Frontiers in Artificial Intelligence","5",,"745626","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124947635&doi=10.3389%2ffrai.2022.745626&partnerID=40&md5=a3da9df83fa0218394551c5c83bcdf16","Word senses are the fundamental unit of description in lexicography, yet it is rarely the case that different dictionaries reach any agreement on the number and definition of senses in a language. With the recent rise in natural language processing and other computational approaches there is an increasing demand for quantitatively validated sense catalogues of words, yet no consensus methodology exists. In this paper, we look at four main approaches to making sense distinctions: formal, cognitive, distributional, and intercultural and examine the strengths and weaknesses of each approach. We then consider how these may be combined into a single sound methodology. We illustrate this by examining two English words, “wing” and “fish,” using existing resources for each of these four approaches and illustrate the weaknesses of each. We then look at the impact of such an integrated method and provide some future perspectives on the research that is necessary to reach a principled method for making sense distinctions. Copyright © 2022 McCrae, Fransen, Ahmadi, Buitelaar and Goswami.","cognitive semantics; distributional semantics; generative lexicon; lexicography; multilinguality; semantics; word senses; wordnets",,Article,Scopus,2-s2.0-85124947635
"Zhang Z., Schomaker L.","57219361101;19640514000;","DiverGAN: An Efficient and Effective Single-Stage Framework for Diverse Text-to-Image Generation",2022,"Neurocomputing","473",,,"182","198",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122483394&doi=10.1016%2fj.neucom.2021.12.005&partnerID=40&md5=c455f2d4c9d3954ce57b9935e80b01b1","In this paper, we concentrate on the text-to-image synthesis task that aims at automatically producing perceptually realistic pictures from text descriptions. Recently, several single-stage methods have been proposed to deal with the problems of a more complicated multi-stage modular architecture. However, they often suffer from the lack-of-diversity issue, yielding similar outputs given a single textual sequence. To this end, we present an efficient and effective single-stage framework (DiverGAN) to generate diverse, plausible and semantically consistent images according to a natural-language description. DiverGAN adopts two novel word-level attention modules, i.e., a channel-attention module (CAM) and a pixel-attention module (PAM), which model the importance of each word in the given sentence while allowing the network to assign larger weights to the significant channels and pixels semantically aligning with the salient words. After that, Conditional Adaptive Instance-Layer Normalization (CAdaILN) is introduced to enable the linguistic cues from the sentence embedding to flexibly manipulate the amount of change in shape and texture, further improving visual-semantic representation and helping stabilize the training. Also, a dual-residual structure is developed to preserve more original visual features while allowing for deeper networks, resulting in faster convergence speed and more vivid details. Furthermore, we propose to plug a fully-connected layer into the pipeline to address the lack-of-diversity problem, since we observe that a dense layer will remarkably enhance the generative capability of the network, balancing the trade-off between a low-dimensional random latent code contributing to variants and modulation modules that use high-dimensional and textual contexts to strength feature maps. Inserting a linear layer after the second residual block achieves the best variety and quality. Both qualitative and quantitative results on benchmark data sets demonstrate the superiority of our DiverGAN for realizing diversity, without harming quality and semantic consistency. © 2021 The Authors","attention mechanism; generative adversarial network; lack-of-diversity issue; single-stage framework; text-to-image generation","Benchmarking; Economic and social effects; Generative adversarial networks; Semantics; Textures; Attention mechanisms; Diversity issues; Image generations; Images synthesis; Lack-of-diversity issue; Modular architectures; Multi-stages; Single stage; Single-stage framework; Text-to-image generation; Pixels; article; attention; embedding; human; human experiment; language; pipeline; synthesis; velocity",Article,Scopus,2-s2.0-85122483394
"Liu P., Guo Y., Wang F., Li G.","57226046999;55901264100;56949334800;56183721800;","Chinese named entity recognition: The state of the art",2022,"Neurocomputing","473",,,"37","53",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121220398&doi=10.1016%2fj.neucom.2021.10.101&partnerID=40&md5=db171b5be121771daff5b8c13b8c99da","Named Entity Recognition(NER), one of the most fundamental problems in natural language processing, seeks to identify the boundaries and types of entities with specific meanings in natural language text. As an important international language, Chinese has uniqueness in many aspects, and Chinese NER (CNER) is receiving increasing attention. In this paper, we give a comprehensive survey of recent advances in CNER. We first introduce some preliminary knowledge, including the common datasets, tag schemes, evaluation metrics and difficulties of CNER. Then, we separately describe recent advances in traditional research and deep learning research of CNER, in which the CNER with deep learning is our focus. We summarize related works in a basic three-layer architecture, including character representation, context encoder, and context encoder and tag decoder. Meanwhile, the attention mechanism and adversarial-transfer learning methods based on this architecture are introduced. Finally, we present the future research trends and challenges of CNER. © 2021 The Authors","Adversarial transfer learning; Attention mechanism; Character representation; CNER; Context encoder; Tag decoder","Character recognition; Decoding; Deep learning; Natural language processing systems; Adversarial transfer learning; Attention mechanisms; Character representations; Chinese named entity recognition; Chinese NER; Context encoder; Named entity recognition; State of the art; Tag decoder; Transfer learning; Signal encoding; article; attention; deep learning; human; human experiment; transfer of learning",Article,Scopus,2-s2.0-85121220398
"Chen Z., Wu Y., Feng Y., Zhao D.","57346704200;57211752642;55387599700;55387416000;","Integrating Manifold Knowledge for Global Entity Linking with Heterogeneous Graphs",2022,"Data Intelligence","4","1",,"20","40",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124146665&doi=10.1162%2fdint_a_00116&partnerID=40&md5=f06d0714cbeda83b80d9b139b949cef0","Entity Linking (EL) aims to automatically link the mentions in unstructured documents to corresponding entities in a knowledge base (KB), which has recently been dominated by global models. Although many global EL methods attempt to model the topical coherence among all linked entities, most of them failed in exploiting the correlations among manifold knowledge helpful for linking, such as the semantics of mentions and their candidates, the neighborhood information of candidate entities in KB and the fine-grained type information of entities. As we will show in the paper, interactions among these types of information are very useful for better characterizing the topic features of entities and more accurately estimating the topical coherence among all the referred entities within the same document. In this paper, we present a novel HEterogeneous Graph-based Entity Linker (HEGEL) for global entity linking, which builds an informative heterogeneous graph for every document to collect various linking clues. Then HEGEL utilizes a novel heterogeneous graph neural network (HGNN) to integrate the different types of manifold information and model the interactions among them. Experiments on the standard benchmark datasets demonstrate that HEGEL can well capture the global coherence and outperforms the prior state-of-the-art EL methods. © 2022 Chinese Academy of Sciences. Published under a Creative Commons Attribution 4.0 International (CC BY 4.0) license.","Entity disambiguation; Entity linking; Graph neural network; Heterogeneous graph; Knowledge base","Graph neural networks; Knowledge graph; Semantics; Entity disambiguation; Entity linking; Fine grained; Global models; Graph neural networks; Graph-based; Heterogeneous graph; Neighborhood information; Type information; Unstructured documents; Graphic methods",Article,Scopus,2-s2.0-85124146665
"Baietto A., Boubin J., Farr P., Bihl T.J., Jones A.M., Stewart C.","57442408400;57211265166;57218824795;22956785400;56874817400;57191586287;","Lean Neural Networks for Autonomous Radar Waveform Design",2022,"Sensors","22","4","1317","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124118763&doi=10.3390%2fs22041317&partnerID=40&md5=c64fa38bef6116a5f94ec02562ec4c05","In recent years, neural networks have exploded in popularity, revolutionizing the domains of computer vision, natural language processing, and autonomous systems. This is due to neural networks ability to approximate complex non-linear functions. Despite their effectiveness, they generally require large labeled data sets and considerable processing power for both training and prediction. Some of these bottlenecks have been mitigated by recent increased availability of high-quality data sets, improvements in neural network development software, and greater hardware support. Due to algorithmic bloat, neural network inference times and imprecision make them undesirable for some problems where fast classical algorithm solutions already exist, other classes of algorithms, such as convex optimization, with non-trivial execution times could be reduced using neural solutions. These algorithms could be replaced with light-weight neural networks, benefiting from their high degree of parallelization and high accuracy when properly trained. Previous work has explored how low size, weight, and power (low SWaP) neural networks and neuromorphic computing can be used to improve autonomous radar waveform design techniques that currently rely on convex optimization. Autonomous radar waveform design helps meet the need for interference mitigation caused by an ever-growing number of consumer and commercial technologies which pollute the radio frequency (RF) spectrum. Spectral notching, a radar waveform design technique, augments transmitted radar waveforms to avoid frequencies with excessive interference while maintaining the integrity of the waveform. In this paper, we extend that work, demonstrating that lean neural networks and specialized hardware can improve inference time for waveform design without sacrificing accuracy. Our lean neural solution incorporates problem-specific information into the layer structures and loss functions to decrease network size and improve accuracy. We provide model outcomes implemented on radio frequency system on a chip (RFSoC) hardware that support our simulation results. Our neural network solution decreases inference time on traditional CPU hardware by 1057x and on GPU hardware accelerators by 883x while maintaining 99% cosine similarity. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Neural network; Optimization; Radar","Availability; Computer hardware; Convex optimization; Design; Functions; Heuristic algorithms; Natural language processing systems; Radar; Radio waves; System-on-chip; Waveform analysis; Autonomous radar; Convex optimisation; Data set; Design technique; Labeled data; Neural-networks; Nonlinear functions; Optimisations; Radar waveforms; Waveform designs; Inference engines",Article,Scopus,2-s2.0-85124118763
"Davis C., Aid G.","55447665500;56527418500;","Machine learning-assisted industrial symbiosis: Testing the ability of word vectors to estimate similarity for material substitutions",2022,"Journal of Industrial Ecology","26","1",,"27","43",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125063075&doi=10.1111%2fjiec.13245&partnerID=40&md5=7c6d8673203d294a55d0d97813aaa215","A challenge of facilitating industrial symbiosis involves identifying novel uses of waste streams that can satisfy the demands of other industries. For these efforts, a variety of characteristics must often be considered. A mine of relevant knowledge has been gathered in resources such as academic journals and patent databases. However, in looking to harness the potential of such data to support facilitation, compiling information on expansive ranges of material properties and technical requirements from a variety of unstructured sources can pose a significant manual effort. To ameliorate this, we demonstrate and evaluate an automated system that, given a large collection of patents and academic articles related to waste valorization, is able to assist with the process of identifying which waste streams could potentially be used as substitute feedstocks. Instead of aiming to measure (potentially thousands of) material properties directly, we use word correlations as a proxy to reflect “common knowledge.” Novel in furthering this approach is the application of word vectors, which have emerged as a promising natural language processing tool. The process employs a machine learning approach where words are represented as high-dimensional vectors which encode latent features related to words that often appear around it. When this approach is assessed by comparing its suggestions to documented cases, the use of vectors shows potential to incorporate latent information in data-based explorations. Further research into how this approach compares, and could be integrated with, established symbiosis development practices will be key to understanding its full potential and drawbacks. © 2022 The Authors. Journal of Industrial Ecology published by Wiley Periodicals LLC on behalf of the International Society for Industrial Ecology","industrial ecology; industrial symbiosis; machine learning; resource recovery; tacit knowledge; word vectors","Automation; Ecology; Learning algorithms; Machine learning; Natural language processing systems; Vectors; Academic journal; Academic patents; Industrial ecology; Industrial symbiosis; International society; Materials substitutions; Resource recovery; Tacit knowledge; Waste stream; Word vectors; Patents and inventions; industrial ecology; machine learning; mine; symbiosis",Article,Scopus,2-s2.0-85125063075
"Sun H., Guo X., Yang Z., Chu X., Liu X., He L.","15754838300;57461213300;57212528506;57461213400;36782771600;57194046350;","Predicting Future Locations with Semantic Trajectories",2022,"ACM Transactions on Intelligent Systems and Technology","13","1","7","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125011972&doi=10.1145%2f3465060&partnerID=40&md5=dcd957b7a1d981cb01824f0af236fc68","Location prediction has attracted much attention due to its important role in many location-based services, including taxi services, route navigation, traffic planning, and location-based advertisements. Traditional methods only use spatial-temporal trajectory data to predict where a user will go next. The divorce of semantic knowledge from the spatial-temporal one inhibits our better understanding of users' activities. Inspired by the architecture of Long Short Term Memory (LSTM), we design ST-LSTM, which draws on semantic trajectories to predict future locations. Semantic data add a new dimension to our study, increasing the accuracy of prediction. Since semantic trajectories are sparser than the spatial-temporal ones, we propose a strategic filling algorithm to solve this problem. In addition, as the prediction is based on the historical trajectories of users, the cold-start problem arises. We build a new virtual social network for users to resolve the issue. Experiments on two real-world datasets show that the performance of our method is superior to those of the baselines. © 2022 Association for Computing Machinery.","cold-start; data sparsity; Location prediction; semantic information; trajectory pattern mining","Location; Location based services; Long short-term memory; Semantics; Taxicabs; Telecommunication services; Trajectories; Cold-start; Data sparsity; Location prediction; Location-based services; Pattern mining; Semantic trajectories; Semantics Information; Spatial temporals; Trajectory pattern; Trajectory pattern mining; Forecasting",Article,Scopus,2-s2.0-85125011972
"Moholkar K., Patil S.H.","57202970293;36160499300;","Lioness Adapted GWO-Based Deep Belief Network Enabled with Multiple Features for a Novel Question Answering System",2022,"International Journal of Uncertainty, Fuzziness and Knowlege-Based Systems","30","1",,"93","114",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124989727&doi=10.1142%2fS0218488522500052&partnerID=40&md5=b3d895bbf3477e93379657674e72f229","Recently, the researches on Question Answering (QA) systems attract progressive attention with the enlargement of data and the advances on machine learning. Selection of answers from QA system is a significant task for enhancing the automatic QA systems. However, the major complexity relies in the designing of contextual factors and semantic matching. Motivation: Question Answering is a specialized form of Information Retrieval which seeks knowledge. We are not only interested in getting the relevant pages but we are interested in getting specific answer to queries. Question Answering is in itself intersection of Natural Language Processing, Information Retrieval, Machine Learning, Knowledge Representation, Logic and Inference and Semantic Search. Contribution: Feature extraction plays a major role for accurate classification, where the learned features get extracted for enhancing the capability of sequence learning. Optimized Deep Belief network model is adopted for the precise question answering system, which could handle both objective and subjective questions. A new hybrid optimization algorithm known as Lioness Adapted GWO (LA-GWO) algorithm is introduced, which mainly concentrates on high reliability and convergence rate. This paper intends to formulate a novel QA system, and the process starts with word embedding. From the embedded results, some of the features get extracted, and subsequently, the classification is carried out using the hybrid optimization enabled Deep Belief Network (DBN). Specifically, the hidden neurons in DBN will be optimally tuned using a new Lioness Adapted GWO (LA-GWO) algorithm, which is the hybridization of both Lion Algorithm (LA) and Grey Wolf optimization (GWO) models. Finally, the performance of proposed work is compared over other conventional methods with respect to accuracy, sensitivity, specificity, and precision, respectively. © 2022 World Scientific Publishing Company.","Dbn classifier; Embedding; Feature extraction; La-gwo model; Qa systems","Classification (of information); Deep learning; Embeddings; Extraction; Information retrieval; Knowledge representation; Natural language processing systems; Query processing; Semantics; Dbn classifier; Deep belief networks; Embeddings; Features extraction; Gray wolves; La-gwo model; Optimization algorithms; QA system; Question Answering; Question answering systems; Feature extraction",Article,Scopus,2-s2.0-85124989727
"Crivellari A., Resch B., Shi Y.","57207740391;24528725900;7404964007;","TraceBERT—A Feasibility Study on Reconstructing Spatial–Temporal Gaps from Incomplete Motion Trajectories via BERT Training Process on Discrete Location Sequences",2022,"Sensors","22","4","1682","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124891796&doi=10.3390%2fs22041682&partnerID=40&md5=0bba45c59b924ad2ae7d06230afa2d3d","Trajectory data represent an essential source of information on travel behaviors and human mobility patterns, assuming a central role in a wide range of services related to transportation planning, personalized recommendation strategies, and resource management plans. The main issue when dealing with trajectory recordings, however, is characterized by temporary losses in the data collection, causing possible spatial–temporal gaps and missing trajectory segments. This is especially critical in those use cases based on non-repetitive individual motion traces, when the user’s missing information cannot be directly reconstructed due to the absence of historical individual repetitive routes. Inserted in the context of location-based trajectory modeling, we tackle the problem by proposing a technical parallelism with the natural language processing domain. Specifically, we introduce the use of the Bidirectional Encoder Representations from Transformers (BERT), a state-of-the-art language representation model, into the trajectory processing research field. By training deep bidirectional representations from unlabeled location sequences, jointly conditioned on both left and right context, we derive an explicit predicted estimation of the missing locations along the trace. The proposed framework, named TraceBERT, was tested on a real-world large-scale trajectory dataset of short-term tourists, exploring an effective attempt of adapting advanced language modeling approaches into mobility-based applications and demonstrating a prominent potential on trajectory reconstruction over traditional statistical approaches. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","BERT; Human mobility; Neural networks; Spatial–temporal gaps; Trajectories","Behavioral research; Information management; Large dataset; Location; Modeling languages; Trajectories; Bidirectional encoder representation from transformer; Discrete location; Feasibility studies; Human mobility; Motion trajectories; Neural-networks; Spatial temporals; Spatial–temporal gap; Temporal gaps; Training process; Natural language processing systems",Article,Scopus,2-s2.0-85124891796
"Liu M., Hu H., Li L., Yu Y., Guan W.","7406299692;55511518500;57212197142;57200411014;57206938789;","Chinese Image Caption Generation via Visual Attention and Topic Modeling",2022,"IEEE transactions on cybernetics","52","2",,"1247","1257",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124805860&doi=10.1109%2fTCYB.2020.2997034&partnerID=40&md5=d3331c478734f26d93861df25808e8ac","Automatic image captioning is to conduct the cross-modal conversion from image visual content to natural language text. Involving computer vision (CV) and natural language processing (NLP), it has become one of the most sophisticated research issues in the artificial-intelligence area. Based on the deep neural network, the neural image caption (NIC) model has achieved remarkable performance in image captioning, yet there still remain some essential challenges, such as the deviation between descriptive sentences generated by the model and the intrinsic content expressed by the image, the low accuracy of the image scene description, and the monotony of generated sentences. In addition, most of the current datasets and methods for image captioning are in English. However, considering the distinction between Chinese and English in syntax and semantics, it is necessary to develop specialized Chinese image caption generation methods to accommodate the difference. To solve the aforementioned problems, we design the NICVATP2L model via visual attention and topic modeling, in which the visual attention mechanism reduces the deviation and the topic model improves the accuracy and diversity of generated sentences. Specifically, in the encoding phase, convolutional neural network (CNN) and topic model are used to extract visual and topic features of the input images, respectively. In the decoding phase, an attention mechanism is applied to processing image visual features for obtaining image visual region features. Finally, the topic features and the visual region features are combined to guide the two-layer long short-term memory (LSTM) network for generating Chinese image captions. To justify our model, we have conducted experiments over the Chinese AIC-ICC image dataset. The experimental results show that our model can automatically generate more informative and descriptive captions in Chinese in a more natural way, and it outperforms the existing image captioning NIC model.",,"China; language; natural language processing; semantics; China; Language; Natural Language Processing; Neural Networks, Computer; Semantics",Article,Scopus,2-s2.0-85124805860
"Lai T., Cheng L., Wang D., Ye H., Zhang W.","57454857900;7403337601;57220177645;57225128514;57216740863;","RMAN: Relational multi-head attention neural network for joint extraction of entities and relations",2022,"Applied Intelligence","52","3",,"3132","3142",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124699269&doi=10.1007%2fs10489-021-02600-2&partnerID=40&md5=5fda5fb381b41d980f0177405dde87c3","The task of extracting entities and relations has evolved from distributed extraction to joint extraction. The joint model overcomes the disadvantages of distributed extraction method and strengthens the information interaction between entities and relations. However, the existing methods of the joint model rarely pay attention to the semantic information between words, which have limitations in solving the problem of overlapping relations. In this paper, we propose an RMAN model for joint extraction of entities and relations, which includes multi-feature fusion encoder sentence representation and decoder sequence annotation. We first add a multi-head attention layer after Bi-LSTM to obtain sentence representations, and leverage the attention mechanism to capture relation-based sentence representations. Then, we perform sequence annotation on the sentence representation to obtain entity pairs. Experiments on NYT-single, NYT-multi and WebNLG datasets demonstrate that our model can efficiently extract overlapping triples, which outperforms other baselines. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Joint extraction of entities and relations; Multi-head attention; Relation feature; Sequence annotation","Extraction; Semantics; Attention mechanisms; Extraction method; Information interaction; Joint modeling; Multi-feature fusion; Relation-based; Semantic information; Long short-term memory",Article,Scopus,2-s2.0-85124699269
"Zhou C., Wu M., Lam S.-K.","56727065400;56162235200;55666712800;","A Unified Multi-Task Learning Architecture for Fast and Accurate Pedestrian Detection",2022,"IEEE Transactions on Intelligent Transportation Systems","23","2",,"982","996",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124671013&doi=10.1109%2fTITS.2020.3019390&partnerID=40&md5=d7c0dca5c2dbca643cffa9442cb2f3d1","We present a unified multi-Task learning architecture for fast and accurate pedestrian detection. Different from existing methods which often focus on either a new loss function or architecture, we propose an improved multi-Task convolutional neural network learning architecture to effectively and efficiently interfuse the task of pedestrian detection and semantic segmentation. To achieve this, we integrate a lightweight semantic segmentation branch to Faster R-CNN detection framework that enables end-To-end hard parameter sharing in order to boost the detection performance and maintain computational efficiency as follows. Firstly, a Semantic Segmentation to Feature Module (SS2FM) refines the convolutional features in RPN stage by integrating the features generated from the semantic segmentation branch. Secondly, a Semantic Segmentation to Confidence Module (SS2CM) refines the classification confidence in RPN stage by fusing it with the semantic segmentation confidence. We also introduce an effective anchor matching point transform to alleviate the problem of feature misalignment for heavily occluded pedestrians. The proposed unified multi-Task learning architecture lends itself well to more robust pedestrian detection in diverse scenarios with negligible computation overhead. In addition, the proposed architecture can achieve high detection performance with low resolution input images, which significantly reduces the computational complexity. Experiment results on CityPersons and Caltech datasets show that our method is the fastest among all state-of-The-Art pedestrian detection methods while exhibiting competitive detection performance. © 2000-2011 IEEE.","feature aggregation; Multi-Task learning; pedestrian detection; semantic segmentation","Computational efficiency; Convolution; Learning systems; Network architecture; Neural networks; Semantics; Convolutional neural network; Detection performance; Feature aggregation; Learning architectures; Loss functions; Multi tasks; Multitask learning; Neural network learning; Pedestrian detection; Semantic segmentation; Semantic Segmentation",Article,Scopus,2-s2.0-85124671013
"Kaiser D., Jacobs A.M., Cichy R.M.","55363021200;7402530721;23090237400;","Modelling brain representations of abstract concepts",2022,"PLoS Computational Biology","18","2","e1009837","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124620179&doi=10.1371%2fjournal.pcbi.1009837&partnerID=40&md5=a322eb9c15b1bf72ec43b4c37f80540b","Abstract conceptual representations are critical for human cognition. Despite their importance, key properties of these representations remain poorly understood. Here, we used computational models of distributional semantics to predict multivariate fMRI activity patterns during the activation and contextualization of abstract concepts. We devised a task in which participants had to embed abstract nouns into a story that they developed around a given background context. We found that representations in inferior parietal cortex were predicted by concept similarities emerging in models of distributional semantics. By constructing different model families, we reveal the models' learning trajectories and delineate how abstract and concrete training materials contribute to the formation of brain-like representations. These results inform theories about the format and emergence of abstract conceptual representations in the human brain. © 2022 Kaiser et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited",,"adult; article; computer model; female; functional magnetic resonance imaging; human; human experiment; inferior parietal cortex; learning; male; semantics",Article,Scopus,2-s2.0-85124620179
"Li Q., Tang X., Jian Y.","57311694700;55725150400;55635903900;","Learning to Reason on Tree Structures for Knowledge-Based Visual Question Answering",2022,"Sensors","22","4","1575","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124619159&doi=10.3390%2fs22041575&partnerID=40&md5=7a1bc97994c745624f718bbacb3adf58","Collaborative reasoning for knowledge-based visual question answering is challenging but vital and efficient in understanding the features of the images and questions. While previous methods jointly fuse all kinds of features by attention mechanism or use handcrafted rules to generate a layout for performing compositional reasoning, which lacks the process of visual reasoning and introduces a large number of parameters for predicting the correct answer. For conducting visual reasoning on all kinds of image–question pairs, in this paper, we propose a novel reasoning model of a question-guided tree structure with a knowledge base (QGTSKB) for addressing these problems. In addition, our model consists of four neural module networks: the attention model that locates attended regions based on the image features and question embeddings by attention mechanism, the gated reasoning model that forgets and updates the fused features, the fusion reasoning model that mines high-level semantics of the attended visual features and knowledge base and knowledge-based fact model that makes up for the lack of visual and textual information with external knowledge. Therefore, our model performs visual analysis and reasoning based on tree structures, knowledge base and four neural module networks. Experimental results show that our model achieves superior performance over existing methods on the VQA v2.0 and CLVER dataset, and visual reasoning experiments prove the interpretability of the model. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Attention mechanism; Compositional reasoning; Knowledge base; Neural module network; Tree structure","Forestry; Semantics; Trees (mathematics); Attention mechanisms; Collaborative reasonings; Compositional reasoning; Knowledge based; Module networks; Neural module network; Question Answering; Reasoning models; Tree structures; Visual reasoning; Knowledge based systems",Article,Scopus,2-s2.0-85124619159
"Kim J., Sohn M.","56763022700;8983941300;","Graph Representation Learning-Based Early Depression Detection Framework in Smart Home Environments",2022,"Sensors","22","4","1545","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124617821&doi=10.3390%2fs22041545&partnerID=40&md5=2ae8f6366aab4d9f8999ee98ba2e232e","Although the diagnosis and treatment of depression is a medical field, ICTs and AI technologies are used widely to detect depression earlier in the elderly. These technologies are used to identify behavioral changes in the physical world or sentiment changes in cyberspace, known as symptoms of depression. However, although sentiment and physical changes, which are signs of depression in the elderly, are usually revealed simultaneously, there is no research on them at the same time. To solve the problem, this paper proposes knowledge graph-based cyber–physical view (CPV)-based activity pattern recognition for the early detection of depression, also known as KARE. In the KARE framework, the knowledge graph (KG) plays key roles in providing cross-domain knowledge as well as resolving issues of grammatical and semantic heterogeneity required in order to integrate cyberspace and the physical world. In addition, it can flexibly express the patterns of different activities for each elderly. To achieve this, the KARE framework implements a set of new machine learning techniques. The first is 1D-CNN for attribute representation in relation to learning to connect the attributes of physical and cyber worlds and the KG. The second is the entity alignment with embedding vectors extracted by the CNN and GNN. The third is a graph extraction method to construct the CPV from KG with the graph representation learning and wrapper-based feature selection in the unsupervised manner. The last one is a method of activity-pattern graph representation based on a Gaussian Mixture Model and KL divergence for training the GAT model to detect depression early. To demonstrate the superiority of the KARE framework, we performed the experiments using real-world datasets with five state-of-the-art models in knowledge graph entity alignment. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Early detection of depression (EDD); Elderly; Graph neural networks; Graph representation learning; Knowledge graph; Smart home","Automation; Diagnosis; Domain Knowledge; Gaussian distribution; Graph neural networks; Graphic methods; Intelligent buildings; Learning systems; Pattern recognition; Semantics; Cyber physicals; Cyberspaces; Early detection of depression; Elderly; Graph neural networks; Graph representation; Graph representation learning; Knowledge graphs; Physical world; Smart homes; Knowledge graph",Article,Scopus,2-s2.0-85124617821
"Kim J.-W., Yoon H., Jung H.-Y.","57219550643;57273359500;57219551549;","Improved Spoken Language Representation for Intent Understanding in a Task-Oriented Dialogue System",2022,"Sensors","22","4","1509","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124453362&doi=10.3390%2fs22041509&partnerID=40&md5=90c1f1c087451841bbe7f738c2170831","Successful applications of deep learning technologies in the natural language processing domain have improved text-based intent classifications. However, in practical spoken dialogue applications, the users’ articulation styles and background noises cause automatic speech recognition (ASR) errors, and these may lead language models to misclassify users’ intents. To overcome the limited performance of the intent classification task in the spoken dialogue system, we propose a novel approach that jointly uses both recognized text obtained by the ASR model and a given labeled text. In the evaluation phase, only the fine-tuned recognized language model (RLM) is used. The experimental results show that the proposed scheme is effective at classifying intents in the spoken dialogue system containing ASR errors. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Intent understanding; Speech recognition; Spoken dialogue system; Spoken language modeling; Task-oriented dialogue system","Character recognition; Computational linguistics; Deep learning; Modeling languages; Natural language processing systems; Speech processing; Text processing; User profile; Automatic speech recognition; Dialogue systems; Intent understanding; Language model; Recognition error; Spoken dialogue system; Spoken language modeling; Spoken languages; Task-oriented; Task-oriented dialog system; Speech recognition",Article,Scopus,2-s2.0-85124453362
"Wong L.J., Michaels A.J.","57202197806;37006943500;","Transfer Learning for Radio Frequency Machine Learning: A Taxonomy and Survey",2022,"Sensors","22","4","1416","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124358041&doi=10.3390%2fs22041416&partnerID=40&md5=73206756d4c9b57283648c0d78923b76","Transfer learning is a pervasive technology in computer vision and natural language processing fields, yielding exponential performance improvements by leveraging prior knowledge gained from data with different distributions. However, while recent works seek to mature machine learning and deep learning techniques in applications related to wireless communications, a field loosely termed radio frequency machine learning, few have demonstrated the use of transfer learning techniques for yielding performance gains, improved generalization, or to address concerns of training data costs. With modifications to existing transfer learning taxonomies constructed to support transfer learning in other modalities, this paper presents a tailored taxonomy for radio frequency applications, yielding a consistent framework that can be used to compare and contrast existing and future works. This work offers such a taxonomy, discusses the small body of existing works in transfer learning for radio frequency machine learning, and outlines directions where future research is needed to mature the field. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning (DL); Machine learning (ML); Radio frequency machine learning (RFML); Transfer learning (TL)","Deep learning; Engineering education; Knowledge management; Learning algorithms; Natural language processing systems; Taxonomies; Deep learning; Exponentials; Learning techniques; Machine learning; Machine-learning; Pervasive technologies; Radio frequency machine learning; Radiofrequencies; Transfer learning; Radio waves",Article,Scopus,2-s2.0-85124358041
"Gao Y., Zhou H., Chen L., Shen Y., Guo C., Zhang X.","57223002914;23476248100;57208276028;57223016722;57444364800;57221516428;","Cross-Modal Object Detection Based on a Knowledge Update",2022,"Sensors","22","4","1338","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124230883&doi=10.3390%2fs22041338&partnerID=40&md5=5bef458a02f22aab467aeee4052d7fc6","As an important field of computer vision, object detection has been studied extensively in recent years. However, existing object detection methods merely utilize the visual information of the image and fail to mine the high-level semantic information of the object, which leads to great limitations. To take full advantage of multi-source information, a knowledge update-based multi-modal object recognition model is proposed in this paper. Specifically, our method initially uses Faster R-CNN to regionalize the image, then applies a transformer-based multimodal encoder to encode visual region features (region-based image features) and textual features (semantic relationships between words) corresponding to pictures. After that, a graph convolutional network (GCN) inference module is introduced to establish a relational network in which the points denote visual and textual region features, and the edges represent their relationships. In addition, based on an external knowledge base, our method further enhances the region-based relationship expression capability through a knowledge update module. In summary, the proposed algorithm not only learns the accurate relationship between objects in different regions of the image, but also benefits from the knowledge update through an external relational database. Experimental results verify the effectiveness of the proposed knowledge update module and the independent reasoning ability of our model. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Graph convolutional network; Knowledge update; Multimodal encoder; Multimodality","Convolution; Convolutional neural networks; Knowledge based systems; Network coding; Object detection; Semantics; Convolutional networks; Cross-modal; Graph convolutional network; Knowledge update; Multi-modal; Multi-modality; Multimodal encoder; Objects detection; Region feature; Region-based; Object recognition",Article,Scopus,2-s2.0-85124230883
"Kocab A., Davidson K., Snedeker J.","56586121700;57210731109;6603155929;","The Emergence of Natural Language Quantification",2022,"Cognitive Science","46","2","e13097","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124173787&doi=10.1111%2fcogs.13097&partnerID=40&md5=dc8ca3c8fe610b0933ae3171251d900a","Classical quantifiers (like “all,” “some,” and “none”) express relationships between two sets, allowing us to make generalizations (like “no elephants fly”). Devices like these appear to be universal in human languages. Is the ubiquity of quantification due to a universal property of the human mind or is it attributable to more gradual convergence through cultural evolution? We investigated whether classical quantifiers are present in a new language emerging in isolation from other languages, Nicaraguan Sign Language (NSL). An observational study of historical data collected in the 1990s found evidence of potential quantifier forms. To confirm the quantificational meaning of these signs, we designed a production study that elicited, from three age cohorts of NSL signers (N = 17), three types of quantifiers: universal (all), existential (some), and negative (none). We find evidence for these classical quantifiers in the very first generation of signers, suggesting they may reflect a universal property of human cognition or a very rapid construction process. © 2022 Cognitive Science Society LLC","Language emergence; Language evolution; Quantifiers; Semantics; Sign language","article; clinical article; cognition; human; human experiment; observational study; semantics; sign language",Article,Scopus,2-s2.0-85124173787
"Cai L., Zhu L., Zhang H., Zhu X.","57439338200;57202375521;56032535100;56033548300;","DA-GAN: Dual Attention Generative Adversarial Network for Cross-Modal Retrieval",2022,"Future Internet","14","2","43","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124087028&doi=10.3390%2ffi14020043&partnerID=40&md5=a35d7689444eb48be8a7a980c0b0e2aa","Cross-modal retrieval aims to search samples of one modality via queries of other modalities, which is a hot issue in the community of multimedia. However, two main challenges, i.e., heterogeneity gap and semantic interaction across different modalities, have not been solved efficaciously. Reducing the heterogeneous gap can improve the cross-modal similarity measurement. Meanwhile, modeling cross-modal semantic interaction can capture the semantic correlations more accurately. To this end, this paper presents a novel end-to-end framework, called Dual Attention Generative Adversarial Network (DA-GAN). This technique is an adversarial semantic representation model with a dual attention mechanism, i.e., intra-modal attention and inter-modal attention. Intra-modal attention is used to focus on the important semantic feature within a modality, while intermodal attention is to explore the semantic interaction between different modalities and then represent the high-level semantic correlation more precisely. A dual adversarial learning strategy is designed to generate modality-invariant representations, which can reduce the cross-modal heterogeneity efficiently. The experiments on three commonly used benchmarks show the better performance of DA-GAN than these competitors. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Cross-model retrieval; Deep representation learning; Generative adversarial network; Inter-modal attention; Intra-modal attention","Benchmarking; Deep learning; Semantics; Cross model; Cross-modal; Cross-model retrieval; Deep representation learning; End to end; Inter-modal attention; Intra-modal attention; Modal semantics; Semantic interactions; Similarity measurements; Generative adversarial networks",Article,Scopus,2-s2.0-85124087028
"Mahmoud S., Saif O., Nabil E., Abdeen M., Elnainay M., Torki M.","57437717600;57219986732;55351831400;57225312298;55884206800;57204968646;","AR-Sanad 280K: A Novel 280K Artificial Sanads Dataset for Hadith Narrator Disambiguation",2022,"Information (Switzerland)","13","2","55","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124069286&doi=10.3390%2finfo13020055&partnerID=40&md5=3afef33dd4250d76c399fa39ebfd4b03","Determining hadith authenticity is vitally important in the Islamic religion because hadiths record the sayings and actions of Prophet Muhammad (PBUH), and they are the second source of Islamic teachings following the Quran. When authenticating a hadith, the reliability of the hadith narrators is a big factor that hadith scholars consider. However, many narrators share similar names, and the narrators’ full names are not usually included in the narration chains of hadiths. Thus, first, ambiguous narrators need to be identified. Then, their reliability level can be determined. There are no available datasets that could help address this problem of identifying narrators. Here, we present a new dataset that contains narration chains (sanads) with identified narrators. The AR-Sanad 280K dataset has around 280K artificial sanads and could be used to identify 18,298 narrators. After creating the AR-Sanad 280K dataset, we address the narrator disambiguation in several experimental setups. The hadith narrator disambiguation is modeled as a multiclass classification problem with 18,298 class labels. We test different representations and models in our experiments. The best results were achieved by finetuning BERT-Based deep learning model (AraBERT). We obtained a 92.9 Micro F1 score and 30.2 sanad error rate (SER) on the validation set of our artificial sanads AR-Sanad 280K dataset. Furthermore, we extracted a real test set from the sanads of the famous six books in Islamic hadith. We evaluated the best model on the real test data, and we achieved 83.5 Micro F1 score and 60.6 sanad error rate. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","AraBERT; Arabic dataset; Deep learning; Islamic hadith; Narrator disambiguation; Natural language processing","Deep learning; AraBERT; Arabic dataset; Class labels; Deep learning; Error rate; F1 scores; Islamic hadith; Multiclass classification problems; Narrator disambiguation; Reliability level; Natural language processing systems",Article,Scopus,2-s2.0-85124069286
"Gong N., Yao N., Guo S.","57193794493;15623377800;57211643447;","Seeds: Sampling-Enhanced Embeddings",2022,"IEEE Transactions on Neural Networks and Learning Systems","33","2",,"577","586",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124054894&doi=10.1109%2fTNNLS.2020.3028099&partnerID=40&md5=a60800ace52fe660e7ee88a81163fb2e","Finding a desirable sampling estimator has a profound impact on the development of static word embedding models, such as continue-bag-of-words (CBOW) and skip gram (SG), which have been generally accepted as popular low-resource algorithms to generate task-agnostic word representations. Due to the prevalence of large-scale pretrained models, less attention has been paid to these static models in the recent years. However, compared with the dynamic embedding models (e.g., BERT), these static models are straightforward to interpret, cost effective to train, and out-of-box to deploy, thus are still widely used in various downstream models until now. Therefore, it is still of considerable significance to study and improve them, especially the crucial components shared by these static models. In this article, we focus on negative sampling (NS), a key component shared by the sampling-based static models, by investigating and mitigating some critical problems of the sampling core. Concretely, we propose Seeds, a sampling enhanced embedding framework, to learn static word embeddings by a new algorithmic innovation for replacing the NS estimator, in which multifactor global priors are considered dynamically for different training pairs. Then, we implement this framework by four concrete models. For the first two implementations, namely CBOW-GP and SG-GP, both negative words and positive auxiliaries are sampled. And for the other two implementations, CBOW-GN and SG-GN, estimations are simplified by sampling only the negative instances. Extensive experimental results across a variety of standard intrinsic and extrinsic tasks demonstrate that embeddings learned by the proposed models outperform their NS-based counterparts, such as CBOW-NS and SG-NS, as well as other strong baselines. © 2012 IEEE.","Natural language processing; negative sampling (NS); word embedding; word representation","Concretes; Cost effectiveness; Embeddings; Bag of words; Cost effective; Dynamic embedding; Embeddings; Large-scales; Negative sampling; Sampling-based; Static modelling; Word embedding; Word representations; Natural language processing systems; Agnostic; algorithm; article; attention; embedding; human; plant seed; prevalence",Article,Scopus,2-s2.0-85124054894
"Hilton K., Siami Namin A., Jones K.S.","57439452200;57225168960;57211133385;","Metaphor identification in cybersecurity texts: a lightweight linguistic approach",2022,"SN Applied Sciences","4","2","60","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124010033&doi=10.1007%2fs42452-022-04939-8&partnerID=40&md5=5e63ec4fc0f34bf60e8dc3e0fec2ba8a","The use of metaphor in cybersecurity discourse has become a topic of interest because of its ability to aid communication about abstract security concepts. In this paper, we borrow from existing metaphor identification algorithms and general theories to create a lightweight metaphor identification algorithm, which uses only one external source of knowledge. The algorithm also introduces a real time corpus builder for extracting collocates; this is, identifying words that appear together more frequently than chance. We implement several variations of the introduced algorithm and empirically evaluate the output using the TroFi dataset, a de facto evaluation dataset in metaphor research. We find first, contrary to our expectation, that adding word sense disambiguation to our metaphor identification algorithm decreases its performance. Second, we find, that our lightweight algorithms perform comparably to their existing, more complex, counterparts. Finally, we present the results of several case studies to observe the utility of the algorithm for future research in linguistic metaphor identification in text related to cybersecurity texts and threats. © 2022, The Author(s).","Cyber security; Linguistic analysis; Metaphor; Natural language processing","Abstracting; Cybersecurity; Linguistics; Cyber security; External sources; General theory; Identification algorithms; Linguistic analysis; Linguistic approach; Metaphor; Performance; Real- time; Word Sense Disambiguation; Natural language processing systems",Article,Scopus,2-s2.0-85124010033
"Hu N., Zhou H., Liu A.-A., Huang X., Zhang S., Jin G., Guo J., Li X.","57214150078;57207843883;23480048200;57435405700;57388316400;57435541700;57435269800;57216765628;","Collaborative Distribution Alignment for 2D image-based 3D shape retrieval",2022,"Journal of Visual Communication and Image Representation","83",,"103426","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123867348&doi=10.1016%2fj.jvcir.2021.103426&partnerID=40&md5=e3b00beb90d02e687351380e4be99e88","Retrieving 3D shapes with 2D images has become a popular research area nowadays, and a great deal of work has been devoted to reducing the discrepancy between 3D shapes and 2D images to improve retrieval performance. However, most approaches ignore the semantic information and decision boundaries of the two domains, and cannot achieve both domain alignment and category alignment in one module. In this paper, a novel Collaborative Distribution Alignment (CDA) model is developed to address the above existing challenges. Specifically, we first adopt a dual-stream CNN, following a similarity guided constraint module, to generate discriminative embeddings for input 2D images and 3D shapes (described as multiple views). Subsequently, we explicitly introduce a joint domain-class alignment module to dynamically learn a class-discriminative and domain-agnostic feature space, which can narrow the distance between 2D image and 3D shape instances of the same underlying category, while pushing apart the instances from different categories. Furthermore, we apply a decision boundary refinement module to avoid generating class-ambiguity embeddings by dynamically adjusting inconsistencies between two discriminators. Extensive experiments and evaluations on two challenging benchmarks, MI3DOR and MI3DOR-2, demonstrate the superiority of the proposed CDA method for 2D image-based 3D shape retrieval task. © 2022 Elsevier Inc.","3D shape retrieval; Cross-domain retrieval; Multi-view learning","Alignment; Image enhancement; Semantics; 2D images; 3-D shape; 3D shape retrieval; Cross-domain; Cross-domain retrieval; Decision boundary; Embeddings; Image-based; Multi-view learning; Research areas; Embeddings",Article,Scopus,2-s2.0-85123867348
"Poostchi H., Piccardi M.","26421297200;7003728868;","BiLSTM-SSVM: Training the BiLSTM with a Structured Hinge Loss for Named-Entity Recognition",2022,"IEEE Transactions on Big Data","8","1",,"203","212",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123705891&doi=10.1109%2fTBDATA.2019.2938163&partnerID=40&md5=ba15fb3a1853ea0eea29e45dfdbd5e34","Building on the achievements of the BiLSTM-CRF in named-entity recognition (NER), this paper introduces the BiLSTM-SSVM, an equivalent neural model where training is performed using a structured hinge loss. The typical loss functions used for evaluating NER are entity-level variants of the F_1F1 score such as the CoNLL and MUC losses. Unfortunately, the common loss function used for training NER - the cross entropy - is only loosely related to the evaluation losses. For this reason, in this paper we propose a training approach for the BiLSTM-CRF that leverages a hinge loss bounding the CoNLL loss from above. In addition, we present a mixed hinge loss that bounds either the CoNLL loss or the Hamming loss based on the density of entity tokens in each sentence. The experimental results over four benchmark languages (English, German, Spanish and Dutch) show that training with the mixed hinge loss has led to small but consistent improvements over the cross entropy across all languages and four different evaluation measures. © 2015 IEEE.","Loss-augmented inference; Named-entity recognition; Natural language processing; Non-decomposable evaluation loss; Sequential labeling","Artificial intelligence; Entropy; Function evaluation; Cross entropy; Entity-level; Evaluation measures; Labelings; Loss functions; Loss-augmented inference; Named entity recognition; Neural modelling; Non-decomposable evaluation loss; Sequential labeling; Natural language processing systems",Article,Scopus,2-s2.0-85123705891
"Song L., Wang Z., Yu M., Zhang Y., Florian R., Gildea D.","57198355269;53664832900;35174012000;56066648800;35487184700;6603603942;","Evidence Integration for Multi-Hop Reading Comprehension With Graph Neural Networks",2022,"IEEE Transactions on Knowledge and Data Engineering","34","2",,"631","639",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123594949&doi=10.1109%2fTKDE.2020.2982894&partnerID=40&md5=45c204e59acb1947596fcba70825a914","Multi-hop reading comprehension focuses on one type of factoid question, where a system needs to properly integrate multiple pieces of evidence to correctly answer a question. Previous work approximates global evidence with local coreference information, encoding coreference chains with DAG-styled GRU layers within a gated-attention reader. However, coreference is limited in providing information for rich inference. We introduce a new method for better connecting global evidence, which forms more complex graphs compared to DAGs. To perform evidence integration on our graphs, we investigate two recent graph neural networks, namely graph convolutional network (GCN) and graph recurrent network (GRN). Experiments on two standard datasets show that richer global information leads to better answers. Our approach shows highly competitive performances on these datasets without deep language models (such as ELMo). © 1989-2012 IEEE.","graph neural network; multi-hop reasoning; Natural language processing; question answering","Graph neural networks; Recurrent neural networks; Convolutional networks; Coreference; Coreference chains; Factoid questions; Graph neural networks; Information encoding; Multi-hop reasoning; Multi-hops; Question Answering; Reading comprehension; Natural language processing systems",Article,Scopus,2-s2.0-85123594949
"Li H., Wang Y., Lyu Z., Shi J.","57211503963;57222817085;56286393800;56271552000;","Multi-Task Learning for Recommendation over Heterogeneous Information Network",2022,"IEEE Transactions on Knowledge and Data Engineering","34","2",,"789","802",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123594149&doi=10.1109%2fTKDE.2020.2983409&partnerID=40&md5=32546aa4a4d8bc443b1d86612f2ba91e","Traditional recommender systems (RS) only consider homogeneous data and cannot fully model heterogeneous information of complex objects and relations. Recent advances in the study of Heterogeneous Information Network (HIN) have shed some light on how to leverage heterogeneous information in RS. However, existing HIN-based recommendation models assume HIN is invariable and merely use HIN as a data source for assisting recommendation, which limits their performance. In this paper, we propose a multi-task learning framework, called MTRec, for recommendation over HIN. MTRec relies on self-attention mechanism to learn the semantics of meta-paths in HIN and jointly optimizes the tasks of both recommendation and link prediction. Using a Bayesian task weight learner, MTRec is able to achieve the balance of two tasks during optimization automatically. Moreover, MTRec provides good interpretabilities of recommendation through a 'translation' mechanism which is used to model the three-way interactions among users, items and the meta-paths connecting them. Experimental results demonstrate the superiority of MTRec over state-of-the-art HIN-based recommendation models, and the case studies we provide illustrate that MTRec enhances the explainability of RS. © 1989-2012 IEEE.","heterogeneous information network; multi-task learning; Recommender systems","Information services; Recommender systems; Semantics; Attention mechanisms; Complex objects; Data-source; Heterogeneous information; Heterogeneous information network; Information networks; Learn+; Multitask learning; Network-based; Performance; Learning systems",Article,Scopus,2-s2.0-85123594149
"Liu W., Pang J., Du Q., Li N., Yang S.","57215301438;24344798200;57223038921;56562130300;57427639300;","A Method of Short Text Representation Fusion with Weighted Word Embeddings and Extended Topic Information",2022,"Sensors","22","3","1066","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123509306&doi=10.3390%2fs22031066&partnerID=40&md5=fd599a2cd6307dbe804c4e24c95ac326","Short text representation is one of the basic and key tasks of NLP. The traditional method is to simply merge the bag-of-words model and the topic model, which may lead to the problem of ambiguity in semantic information, and leave topic information sparse. We propose an unsuper-vised text representation method that involves fusing word embeddings and extended topic infor-mation. Following this, two fusion strategies of weighted word embeddings and extended topic information are designed: static linear fusion and dynamic fusion. This method can highlight im-portant semantic information, flexibly fuse topic information, and improve the capabilities of short text representation. We use classification and prediction tasks to verify the effectiveness of the method. The testing results show that the method is valid. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Information fusion; Short text representation; Topic information; Word embeddings","Information retrieval; Semantics; Bag-of-words models; Embeddings; Representation method; Semantics Information; Short text representation; Short texts; Text representation; Topic information; Topic Modeling; Word embedding; Embeddings; article; embedding; prediction",Article,Scopus,2-s2.0-85123509306
"Wang Q., Li Y., Wang Y., Ren J.","56237686800;57207919226;56888617900;56012213300;","An automatic algorithm for software vulnerability classification based on CNN and GRU",2022,"Multimedia Tools and Applications","81","5",,"7103","7124",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123467660&doi=10.1007%2fs11042-022-12049-1&partnerID=40&md5=f2cd66b20317d38baee22d20359e4080","In order to improve the management efficiency of software vulnerability classification, reduce the risk of system being attacked and destroyed, and save the cost for vulnerability repair, this paper proposes an automatic algorithm for Software Vulnerability Classification based on convolutional neural network (CNN) and gate recurrent unit neural network (GRU), called SVC-CG. It has conducted a fusion between the models of CNN and GRU according to their advantages (CNN is good at extracting local vector features of vulnerability text and GRU is good at extracting global features related to the context of vulnerability text). The merger of the features extracted by the complementary models can represent the semantic and grammatical information more accurately. Firstly, the Skip-gram language model based on Word2Vec is used to train and generate the word vector, and the words in each vulnerability text are mapped into the space with limited dimensions to represent the semantic information. Then the CNN is used to extract the local features of the text vector, and the GRU is used to extract the global features related to the text context. We combine two complementary models to construct a SVC-CG neural network algorithm, which can represent semantic and grammatical information more accurately to realize automatic classification of vulnerabilities. The experiment uses the vulnerability data from the national vulnerability database (NVD) to train and evaluate the SVC-CG algorithm. Through experimental comparison and analysis, the SVC-CG algorithm proposed in this paper has a good performance on Macro recall rate, Macro precision rate and Macro F1-score. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Neural network; Software security; Vulnerability classification","Classification (of information); Convolutional neural networks; Network security; Recurrent neural networks; Static Var compensators; Vector spaces; Automatic algorithms; Complementary model; Convolutional neural network; Global feature; Grammatical information; Neural-networks; Semantics Information; Software security; Software vulnerabilities; Vulnerability classifications; Semantics",Article,Scopus,2-s2.0-85123467660
"Aceta C., Fernández I., Soroa A.","57219988202;42761280800;7801335998;","KIDE4I: A Generic Semantics-Based Task-Oriented Dialogue System for Human-Machine Interaction in Industry 5.0",2022,"Applied Sciences (Switzerland)","12","3","1192","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123452384&doi=10.3390%2fapp12031192&partnerID=40&md5=3628a94d5716d747067223d0184e60e4","In Industry 5.0, human workers and their wellbeing are placed at the centre of the production process. In this context, task-oriented dialogue systems allow workers to delegate simple tasks to industrial assets while working on other, more complex ones. The possibility of naturally interacting with these systems reduces the cognitive demand to use them and triggers acceptation. Most modern solutions, however, do not allow a natural communication, and modern techniques to obtain such systems require large amounts of data to be trained, which is scarce in these scenarios. To overcome these challenges, this paper presents KIDE4I (Knowledge-drIven Dialogue framEwork for Industry), a semantic-based task-oriented dialogue system framework for industry that allows workers to naturally interact with industrial systems, is easy to adapt to new scenarios and does not require great amounts of data to be constructed. This work also reports the process to adapt KIDE4I to new scenarios. To validate and evaluate KIDE4I, it has been adapted to four use cases that are relevant to industrial scenarios following the described methodology, and two of them have been evaluated through two user studies. The system has been considered as accurate, useful, efficient, not demanding cognitively, flexible and fast. Furthermore, subjects view the system as a tool to improve their productivity and security while carrying out their tasks. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Collaborative robotics; Human-machine interaction; Industry 5.0; Natural language processing; Semantics; Task-oriented dialogue systems",,Article,Scopus,2-s2.0-85123452384
"Xu W., Wang W., Portanova J., Chander A., Campbell A., Pakhomov S., Ben-Zeev D., Cohen T.","57223250135;57425437000;57216537179;57223249299;57425437100;55655567400;23468345100;8436258000;","Fully automated detection of formal thought disorder with Time-series Augmented Representations for Detection of Incoherent Speech (TARDIS)",2022,"Journal of Biomedical Informatics","126",,"103998","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123418426&doi=10.1016%2fj.jbi.2022.103998&partnerID=40&md5=110ffe78d73e4dbb05d5c78a638259bb","Formal thought disorder (ThD) is a clinical sign of schizophrenia amongst other serious mental health conditions. ThD can be recognized by observing incoherent speech - speech in which it is difficult to perceive connections between successive utterances and lacks a clear global theme. Automated assessment of the coherence of speech in patients with schizophrenia has been an active area of research for over a decade, in an effort to develop an objective and reliable instrument through which to quantify ThD. However, this work has largely been conducted in controlled settings using structured interviews and depended upon manual transcription services to render audio recordings amenable to computational analysis. In this paper, we present an evaluation of such automated methods in the context of a fully automated system using Automated Speech Recognition (ASR) in place of a manual transcription service, with “audio diaries” collected in naturalistic settings from participants experiencing Auditory Verbal Hallucinations (AVH). We show that performance lost due to ASR errors can often be restored through the application of Time-Series Augmented Representations for Detection of Incoherent Speech (TARDIS), a novel approach that involves treating the sequence of coherence scores from a transcript as a time-series, providing features for machine learning. With ASR, TARDIS improves average AUC across coherence metrics for detection of severe ThD by 0.09; average correlation with human-labeled derailment scores by 0.10; and average correlation between coherence estimates from manual and ASR-derived transcripts by 0.29. In addition, TARDIS improves the agreement between coherence estimates from manual transcripts and human judgment and correlation with self-reported estimates of AVH symptom severity. As such, TARDIS eliminates a fundamental barrier to the deployment of automated methods to detect linguistic indicators of ThD to monitor and improve clinical care in serious mental illness. © 2022 Elsevier Inc.","Auditory Verbal Hallucination; Automatic Speech Recognition; Coherence in Speech; Formal Thought Disorder; Natural Language Processing; Neural Word Embeddings","Audio recordings; Automation; Diseases; Learning algorithms; Natural language processing systems; Speech; Time series; Auditory verbal hallucination; Automated methods; Automated speech recognition; Automatic speech recognition; Coherence in speech; Embeddings; Formal thought disorder; Fully automated; Neural word embedding; Times series; Speech recognition; analytic method; Article; automatic speech recognition; automation; controlled study; convolutional neural network; ecological momentary assessment; Hamilton Program for Schizophrenia Voices Questionnaire; human; machine learning; mental disease; natural language processing; questionnaire; rating scale; recurrent neural network; schizophrenia; self report; semantics; signal processing; structured interview; Thought and Language Disorder Scale; thought disorder; Time Series Augmented Representations for Detection of Incoherent Speech",Article,Scopus,2-s2.0-85123418426
"Yi X.-L., Hua R., Fu Y., Zheng D.-L., Wang Z.-Y.","57423586800;35770708800;57423881000;57216950379;57424183200;","RNIC-A retrospect network for image captioning",2022,"Soft Computing","26","4",,"1501","1507",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123374069&doi=10.1007%2fs00500-021-06622-3&partnerID=40&md5=5460cdb6a701fbada7ef5c5c3d922440","As cross-domain research combining computer vision and natural language processing, the current image captioning research mainly considers how to improve the visual features; less attention has been paid to utilizing the inherent properties of language to boost captioning performance. Facing this challenge, we proposed a textual attention mechanism, which can obtain semantic relevance between words by scanning all generated words. The retrospect network for image captioning (RNIC) proposed in this paper aims to improve input and prediction process by using textual attention. Concretely, the textual attention mechanism is applied to the model simultaneously with the visual attention mechanism to provide the input of the model with the maximum information required for generating captions. In this way, our model can learn to collaboratively attend on both visual and textual features. Moreover, the semantic relevance between words obtained by retrospect is used as the basis for prediction, so that the decoder can simulate the human language system and better make predictions based on the already generated contents. We evaluate the effectiveness of our model on the COCO image captioning datasets and achieve superior performance over the previous methods. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Image caption; LSTM; Retrospect; Textual attention; Visual attention","Behavioral research; Forecasting; Image enhancement; Long short-term memory; Semantics; Visual languages; Attention mechanisms; Image caption; Image captioning; LSTM; Performance; Retrospect; Semantic relevance; Textual attention; Visual Attention; Visual feature; Natural language processing systems",Article,Scopus,2-s2.0-85123374069
"İnce M.","56184054800;","Automatic and intelligent content visualization system based on deep learning and genetic algorithm",2022,"Neural Computing and Applications","34","3",,"2473","2493",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123067015&doi=10.1007%2fs00521-022-06887-1&partnerID=40&md5=a8c4df99a137c19749cfdda1c833ac03","Increasing demand in distance education, e-learning, web-based learning, and other digital sectors (e.g., entertainment) has led to excessive amounts of e-content. Learning objects (LOs) are among the most important components of electronic content (e-content) and are preserved in learning object repositories (LORs). LORs produce different types of electronic content. In producing e-content, several visualization techniques are employed to attract users and ensure a better understanding of the provided information. Many of these visualization systems match images with corresponding text using methods such as semantic web, ontologies, natural language processing, statistical techniques, neural networks, and deep neural networks. Unlike these methods, in this study, an automatic and intelligent content visualization system is developed using deep learning and popular artificial intelligence techniques. The proposed system includes subsystems that segment images to panoptic image instances and use these image instances to generate new images using a genetic algorithm, an evolution-based technique that is one of the best-known artificial intelligence methods. This large-scale proposed system was used to test different amounts of LOs for various science fields. The results show that the developed system can be efficiently used to create visually enhanced content for digital use. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Deep learning; e-content visualization; Genetic algorithm; Panoptic segmentation","Computer aided instruction; Deep neural networks; E-learning; Genetic algorithms; Image segmentation; Learning algorithms; Natural language processing systems; Deep learning; E - learning; Electronic content; Electronic content visualization; Learning object repositories; Learning objects; Panoptic segmentation; Visualization system; Visualization technique; Web-based-learning; Visualization",Article,Scopus,2-s2.0-85123067015
"Mezzi R., Yahyaoui A., Krir M.W., Boulila W., Koubaa A.","57417210400;57204815106;57214516063;37088273900;15923354900;","Mental Health Intent Recognition for Arabic-Speaking Patients Using the Mini International Neuropsychiatric Interview (MINI) and BERT Model",2022,"Sensors","22","3","846","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123055371&doi=10.3390%2fs22030846&partnerID=40&md5=af2b1891ee3fc4ef7fba9e24d7ddfa13","For many years, mental health has been hidden behind a veil of shame and prejudice. In 2017, studies claimed that 10.7% of the global population suffered from mental health disorders. Recently, people started seeking relaxing treatment through technology, which enhanced and ex-panded mental health care, especially during the COVID-19 pandemic, where the use of mental health forums, websites, and applications has increased by 95%. However, these solutions still have many limits, as existing mental health technologies are not meant for everyone. In this work, an up-to-date literature review on state-of-the-art of mental health and healthcare solutions is provided. Then, we focus on Arab-speaking patients and propose an intelligent tool for mental health intent recognition. The proposed system uses the concepts of intent recognition to make mental health diagnoses based on a bidirectional encoder representations from transformers (BERT) model and the International Neuropsychiatric Interview (MINI). Experiments are conducted using a dataset collected at the Military Hospital of Tunis in Tunisia. Results show excellent performance of the proposed system (the accuracy is over 92%, the precision, recall, and F1 scores are over 94%) in mental health patient diagnosis for five aspects (depression, suicidality, panic disorder, social phobia, and adjustment disorder). In addition, the tool was tested and evaluated by medical staff at the Military Hospital of Tunis, who found it very interesting to help decision-making and prioritizing patient appointment scheduling, especially with a high number of treated patients every day. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","BERT model; Intent recognition; Machine learning; Mental health; MINI; Natural language processing; Psychiatry","Behavioral research; Decision making; Diagnosis; Hospitals; Learning algorithms; Natural language processing systems; Patient treatment; Scheduling; Bidirectional encoder representation from transformer model; Global population; Health disorders; Health technology; Intent recognition; Mental health; Military hospitals; Model and the international neuropsychiatric interview; Psychiatry; Transformer modeling; Machine learning; human; mental health; pandemic; psychological rating scale; COVID-19; Humans; Mental Health; Pandemics; Psychiatric Status Rating Scales; SARS-CoV-2",Article,Scopus,2-s2.0-85123055371
"Murakami R., Chakraborty B.","57222172472;22333246700;","Investigating the Efficient Use of Word Embedding with Neural-Topic Models for Interpretable Topics from Short Texts",2022,"Sensors","22","3","852","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123052608&doi=10.3390%2fs22030852&partnerID=40&md5=8d5d7d7724fa16b31d596c96d283a961","With the rapid proliferation of social networking sites (SNS), automatic topic extraction from various text messages posted on SNS are becoming an important source of information for understanding current social trends or needs. Latent Dirichlet Allocation (LDA), a probabilistic generative model, is one of the popular topic models in the area of Natural Language Processing (NLP) and has been widely used in information retrieval, topic extraction, and document analysis. Unlike long texts from formal documents, messages on SNS are generally short. Traditional topic models such as LDA or pLSA (probabilistic latent semantic analysis) suffer performance degradation for short-text analysis due to a lack of word co-occurrence information in each short text. To cope with this problem, various techniques are evolving for interpretable topic modeling for short texts, pretrained word embedding with an external corpus combined with topic models is one of them. Due to recent developments of deep neural networks (DNN) and deep generative models, neuraltopic models (NTM) are emerging to achieve flexibility and high performance in topic modeling. However, there are very few research works on neural-topic models with pretrained word embedding for generating high-quality topics from short texts. In this work, in addition to pretrained word embedding, a fine-tuning stage with an original corpus is proposed for training neural-topic models in order to generate semantically coherent, corpus-specific topics. An extensive study with eight neural-topic models has been completed to check the effectiveness of additional fine-tuning and pretrained word embedding in generating interpretable topics by simulation experiments with several benchmark datasets. The extracted topics are evaluated by different metrics of topic coherence and topic diversity. We have also studied the performance of the models in classification and clustering tasks. Our study concludes that though auxiliary word embedding with a large external corpus improves the topic coherency of short texts, an additional fine-tuning stage is needed for generating more corpus-specific topics from short-text data. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Coherent topic; Fine-tuning short-text data; Neural-topic model; Pretrained word embedding","Data mining; Deep neural networks; Embeddings; Natural language processing systems; Semantics; Social networking (online); Coherent topic; Embeddings; Fine tuning; Fine-tuning short-text data; Latent Dirichlet allocation; Neural-topic model; Pretrained word embedding; Short texts; Text data; Topic Modeling; Statistics; cluster analysis; information retrieval; natural language processing; text messaging; Cluster Analysis; Information Storage and Retrieval; Natural Language Processing; Neural Networks, Computer; Text Messaging",Article,Scopus,2-s2.0-85123052608
"Hoffmann M., Bergmann R.","57200742564;57193551665;","Using Graph Embedding Techniques in Process-Oriented Case-Based Reasoning",2022,"Algorithms","15","2","27","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123051552&doi=10.3390%2fa15020027&partnerID=40&md5=3fc1bcc6125ba02b43326c7bb81ce764","Similarity-based retrieval of semantic graphs is a core task of Process-Oriented Case-Based Reasoning (POCBR) with applications in real-world scenarios, e.g., in smart manufacturing. The involved similarity computation is usually complex and time-consuming, as it requires some kind of inexact graph matching. To tackle these problems, we present an approach to modeling similarity measures based on embedding semantic graphs via Graph Neural Networks (GNNs). Therefore, we first examine how arbitrary semantic graphs, including node and edge types and their knowledgerich semantic annotations, can be encoded in a numeric format that is usable by GNNs. Given this, the architecture of two generic graph embedding models from the literature is adapted to enable their usage as a similarity measure for similarity-based retrieval. Thereby, one of the two models is more optimized towards fast similarity prediction, while the other model is optimized towards knowledgeintensive, more expressive predictions. The evaluation examines the quality and performance of these models in preselecting retrieval candidates and in approximating the ground-truth similarities of a graph-matching-based similarity measure for two semantic graph domains. The results show the great potential of the approach for use in a retrieval scenario, either as a preselection model or as an approximation of a graph similarity measure. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Case-Based Reasoning; Graph embedding; Graph encoding; MAC/FAC retrieval; Neural networks; Process-Oriented Case-Based Reasoning; Siamese Graph Neural Networks; Similarity-based retrieval","Embeddings; Graph neural networks; Graphic methods; Knowledge graph; Network coding; Semantic Web; Semantics; Casebased reasonings (CBR); Graph embeddings; Graph encoding; Graph neural networks; MAC/FAC retrieval; Neural-networks; Process-oriented; Process-oriented case-based reasoning; Siamese graph neural network; Similarity-based retrievals; Case based reasoning",Article,Scopus,2-s2.0-85123051552
"Wang L., Meng Z.","57412858600;7201894858;","Multichannel Two-Dimensional Convolutional Neural Network Based on Interactive Features and Group Strategy for Chinese Sentiment Analysis",2022,"Sensors","22","3","714","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122880976&doi=10.3390%2fs22030714&partnerID=40&md5=93d1ecb5bf06f4ecd2ba9ba31ea0b23e","In Chinese sentiment analysis tasks, many existing methods tend to use recurrent neural networks (e.g., long short-term memory networks and gated recurrent units) and standard one-dimensional convolutional neural networks (1D-CNN) to extract features. This is because a recurrent neural network can deal with the order dependence of the data to a certain extent and the one-dimensional convolution can extract local features. Although these methods have good performance in sentiment analysis tasks, recurrent neural networks (RNNs) cannot be parallelized, resulting in time-inefficiency, and the standard 1D-CNN can only extract a single sample feature, with the result that the feature information cannot be fully utilized. To this end, in this paper, we propose a multichannel two-dimensional convolutional neural network based on interactive features and group strategy (MCNN-IFGS) for Chinese sentiment analysis. Firstly, we no longer use word encoding technology but use character-based integer encoding to retain more fine-grained information. Besides, in character-level vectors, the interactive features of different elements are introduced to improve the dimensionality of feature vectors and supplement semantic information so that the input matches the model network. In order to ensure that more sentiment features are learned, group strategies are used to form several feature mapping groups, so the learning object is converted from the traditional single sample to the learning of the feature mapping group, so as to achieve the purpose of learning more features. Finally, multichannel two-dimensional convolutional neural networks with different sizes of convolution kernels are used to extract sentiment features of different scales. The experimental results on the Chinese dataset show that our proposed method outperforms other baseline and state-of-the-art methods. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Feature mapping group; Group strategy; Interactive features; Multichannel; Two-dimensional convolutional neural network","Convolution; Convolutional neural networks; Data mining; Encoding (symbols); Mapping; Recurrent neural networks; Semantics; Signal encoding; Convolutional neural network; Feature mapping; Feature mapping group; Group strategy; Interactive features; Multi channel; Network-based; Sentiment analysis; Two-dimensional; Two-dimensional convolutional neural network; Sentiment analysis; algorithm; China; semantics; Algorithms; China; Neural Networks, Computer; Semantics; Sentiment Analysis",Article,Scopus,2-s2.0-85122880976
"Zha Z.-J., Liu D., Zhang H., Zhang Y., Wu F.","36626639900;57204976622;55366935100;55902721000;7403465570;","Context-Aware Visual Policy Network for Fine-Grained Image Captioning",2022,"IEEE Transactions on Pattern Analysis and Machine Intelligence","44","2",,"710","722",,4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122829367&doi=10.1109%2fTPAMI.2019.2909864&partnerID=40&md5=35236f039c88f4d6daacacc7f5d3fe4e","With the maturity of visual detection techniques, we are more ambitious in describing visual content with open-vocabulary, fine-grained and free-form language, i.e., the task of image captioning. In particular, we are interested in generating longer, richer and more fine-grained sentences and paragraphs as image descriptions. Image captioning can be translated to the task of sequential language prediction given visual content, where the output sequence forms natural language description with plausible grammar. However, existing image captioning methods focus only on language policy while not visual policy, and thus fail to capture visual context that are crucial for compositional reasoning such as object relationships (e.g., 'man riding horse') and visual comparisons (e.g., 'small(er) cat'). This issue is especially severe when generating longer sequences such as a paragraph. To fill the gap, we propose a Context-Aware Visual Policy network (CAVP) for fine-grained image-to-language generation: image sentence captioning and image paragraph captioning. During captioning, CAVP explicitly considers the previous visual attentions as context, and decides whether the context is used for the current word/sentence generation given the current visual attention. Compared against traditional visual attention mechanism that only fixes a single visual region at each step, CAVP can attend to complex visual compositions over time. The whole image captioning model - CAVP and its subsequent language policy network - can be efficiently optimized end-to-end by using an actor-critic policy gradient method. We have demonstrated the effectiveness of CAVP by state-of-the-art performances on MS-COCO and Stanford captioning datasets, using various metrics and sensible visualizations of qualitative visual context. © 1979-2012 IEEE.","Image captioning; policy network; reinforcement learning; visual context","Gradient methods; Natural language processing systems; Optimization; Reinforcement learning; Visual languages; 'current; Context-Aware; Fine grained; Freeforms; Image captioning; Policy networks; Visual Attention; Visual content; Visual context; Visual detection; Behavioral research",Article,Scopus,2-s2.0-85122829367
"Wang Q., Wan J., Chan A.B.","57209217850;57190122124;14015159100;","On Diversity in Image Captioning: Metrics and Methods",2022,"IEEE Transactions on Pattern Analysis and Machine Intelligence","44","2",,"1035","1049",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122823894&doi=10.1109%2fTPAMI.2020.3013834&partnerID=40&md5=ef915972082f844000fa70e3e0f5e503","Diversity is one of the most important properties in image captioning, as it reflects various expressions of important concepts presented in an image. However, the most popular metrics cannot well evaluate the diversity of multiple captions. In this paper, we first propose a metric to measure the diversity of a set of captions, which is derived from latent semantic analysis (LSA), and then kernelize LSA using CIDEr (R. Vedantam et al., 2015) similarity. Compared with mBLEU (R. Shetty et al., 2017), our proposed diversity metrics show a relatively strong correlation to human evaluation. We conduct extensive experiments, finding there is a large gap between the performance of the current state-of-the-art models and human annotations considering both diversity and accuracy; the models that aim to generate captions with higher CIDEr scores normally obtain lower diversity scores, which generally learn to describe images using common words. To bridge this 'diversity' gap, we consider several methods for training caption models to generate diverse captions. First, we show that balancing the cross-entropy loss and CIDEr reward in reinforcement learning during training can effectively control the tradeoff between diversity and accuracy of the generated captions. Second, we develop approaches that directly optimize our diversity metric and CIDEr score using reinforcement learning. These proposed approaches using reinforcement learning (RL) can be unified into a self-critical (S. J. Rennie et al., 2017) framework with new RL baselines. Third, we combine accuracy and diversity into a single measure using an ensemble matrix, and then maximize the determinant of the ensemble matrix via reinforcement learning to boost diversity and accuracy, which outperforms its counterparts on the oracle test. Finally, inspired by determinantal point processes (DPP), we develop a DPP selection algorithm to select a subset of captions from a large number of candidate captions. The experimental results show that maximizing the determinant of the ensemble matrix outperforms other methods considerably improving diversity and accuracy. © 1979-2012 IEEE.","adversarial training; diverse captions; diversity metric; Image captioning; policy gradient; reinforcement learning","Reinforcement learning; Adversarial training; Diverse caption; Diversity metrics; Ensemble matrices; Image captioning; Latent Semantic Analysis; Point process; Policy gradient; Property; Strong correlation; Semantics; algorithm; article; cider; controlled study; entropy; human; reinforcement; reward",Article,Scopus,2-s2.0-85122823894
"Viji D., Revathy S.","56644902000;54914992500;","A hybrid approach of Weighted Fine-Tuned BERT extraction with deep Siamese Bi – LSTM model for semantic text similarity identification",2022,"Multimedia Tools and Applications","81","5",,"6131","6157",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122722230&doi=10.1007%2fs11042-021-11771-6&partnerID=40&md5=230e6710a7c56cc32e6916d07e9429dd","The conventional semantic text-similarity methods requires high amount of trained labeled data and also human interventions. Generally, it neglects the contextual-information and word-orders information resulted in data sparseness problem and latitudinal-explosion issue. Recently, deep-learning methods are used for determining text-similarity. Hence, this study investigates NLP application tasks usage in detecting text-similarity of question pairs or documents and explores the similarity score predictions. A new hybridized approach using Weighted Fine-Tuned BERT Feature extraction with Siamese Bi-LSTM model is implemented. The technique is employed for determining question pair sets using Semantic-text-similarity from Quora dataset. The text features are extracted using BERT process, followed by words embedding with weights. The features along with weight values, are represented as embedded vectors, are subjected to various layers of Siamese Networks. The embedded vectors of input text features were trained by using Deep Siamese Bi-LSTM model, in various layers. Finally, similarity scores are determined for each sentence, and the semantic text-similarity is learned. The performance evaluation of proposed-framework is established with respect to accuracy rate, precision value, F1 score data and Recall values parameters compared with other existing text-similarity detection methods. The proposed-framework exhibited higher efficiency rate with 91% in accuracy level in determining semantic-text-similarity compared with other existing algorithms. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","BERT; Bi-LSTM; CNN; Embedded vectors; NLP; Semantic text-similarity; Siamese networks","Extraction; Long short-term memory; Natural language processing systems; Network layers; Semantic Web; BERT; Bi-LSTM; CNN; Embedded vector; Hybrid approach; Semantic text-similarity; Siamese network; Similarity scores; Text feature; Text similarity; Semantics",Article,Scopus,2-s2.0-85122722230
"Farnsworth S., Gurdin G., Vargas J., Mulyar A., Lewinski N., McInnes B.T.","57408042200;57386752500;57407496300;57204507885;12767458600;25646668800;","Extracting experimental parameter entities from scientific articles",2022,"Journal of Biomedical Informatics","126",,"103970","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122641816&doi=10.1016%2fj.jbi.2021.103970&partnerID=40&md5=0a4179de9c3a30814242a739f15686cf","Systematic reviews are labor-intensive processes to combine all knowledge about a given topic into a coherent summary. Despite the high labor investment, they are necessary to create an exhaustive overview of current evidence relevant to a research question. In this work, we evaluate three state-of-the-art supervised multi-label sequence classification systems to automatically identify 24 different experimental design factors for the categories of Animal, Dose, Exposure, and Endpoint from journal articles describing the experiments related to toxicity and health effects of environmental agents. We then present an in depth analysis of the results evaluating the lexical diversity of the design parameters with respect to model performance, evaluating the impact of tokenization and non-contiguous mentions, and finally evaluating the dependencies between entities within the category entities. We demonstrate that in general, algorithms that use embedded representations of the sequences out-perform statistical algorithms, but that even these algorithms struggle with lexically diverse entities. © 2021 Elsevier Inc.","Named entity recognition; Natural language processing; Systematic review","Classification (of information); Design of experiments; 'current; Experimental parameters; Labor intensive process; Multi-labels; Named entity recognition; Research questions; Scientific articles; Sequence classification; State of the art; Systematic Review; Natural language processing systems; algorithm; article; experimental design; human; investment; natural language processing; systematic review",Article,Scopus,2-s2.0-85122641816
"Ahmed U., Srivastava G., Lin J.C.-W.","57204392052;57202588447;56449520400;","Reliable customer analysis using federated learning and exploring deep-attention edge intelligence",2022,"Future Generation Computer Systems","127",,,"70","79",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122538969&doi=10.1016%2fj.future.2021.08.028&partnerID=40&md5=7dd9d31abf3bd16c59db65409b8ebaa3","The Internet of Things (IoT) and smart cities are flourishing with distributed systems in mobile wireless networks. As a result, an enormous amount of data are being generated for devices at the network edge. This results in privacy concerns, sensor data management issues, and data utilization issues. In this research, we propose a collaborative clustering method where the exchange of raw data is not required. The attention-based model used with a federated learning framework. The edge devices compute the model updates using local data and send them to the server for aggregation. Repetition is performed in multiple rounds until a convergence point reached. The transaction data used to train the attention model that gives a low dimensional embedding. Afterwards, we share the convergence model among the client/stores. Then, efficient pattern mining methods known as a clustering-based dynamic method (CBDM) are applied. For experimentation, we used retail store data to cluster the customer based on purchase behaviour. The proposed clustering method used semantic embedding to extract and then cluster them by discovering relevant patterns. The method achieved the 0.75 ROC values for the random distribution and 0.70 for the fixed distribution. The clustering method can help to reduce communication costs while ensuring privacy. © 2021 The Authors","Attention network; Clustering; Edge intelligence; Federated learning; Market analysis; Mobile wireless networks","Cluster analysis; Deep learning; Information management; Internet of things; Retail stores; Semantics; Wireless networks; Attention network; Clustering methods; Clusterings; Customer analysis; Edge intelligence; Federated learning; Market analysis; Mobile wireless network; Network edges; Privacy concerns; Sales",Article,Scopus,2-s2.0-85122538969
"Sanyal J., Rubin D., Banerjee I.","57223245241;7202307112;36095937900;","A weakly supervised model for the automated detection of adverse events using clinical notes",2022,"Journal of Biomedical Informatics","126",,"103969","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122520279&doi=10.1016%2fj.jbi.2021.103969&partnerID=40&md5=b97503da4ce2fc174f69a3f4c5542ca1","With clinical trials unable to detect all potential adverse reactions to drugs and medical devices prior to their release into the market, accurate post-market surveillance is critical to ensure their safety and efficacy. Electronic health records (EHR) contain rich observational patient data, making them a valuable source to actively monitor the safety of drugs and devices. While structured EHR data and spontaneous reporting systems often underreport the complexities of patient encounters and outcomes, free-text clinical notes offer greater detail about a patient's status. Previous studies have proposed machine learning methods to detect adverse events from clinical notes, but suffer from manually extracted features, reliance on costly hand-labeled data, and lack of validation on external datasets. To address these challenges, we develop a weakly-supervised machine learning framework for adverse event detection from unstructured clinical notes and evaluate it on insulin pump failure as a test case. Our model accurately detected cases of pump failure with 0.842 PR AUC on the holdout test set and 0.815 PR AUC when validated on an external dataset. Our approach allowed us to leverage a large dataset with far less hand-labeled data and can be easily transferred to additional adverse events for scalable post-market surveillance. © 2021 Elsevier Inc.","Insulin pump failure; Natural language processing; Scalable post-market surveillance; Unstructured clinical notes","Commerce; Hospital data processing; Large dataset; Learning algorithms; Monitoring; Pumps; Supervised learning; Adverse events; Automated detection; Clinical notes; Insulin pump failure; Insulin pumps; Labeled data; Market surveillance; Pump failure; Scalable post-market surveillance; Unstructured clinical note; Natural language processing systems; adverse outcome; area under the curve; Article; automation; clinical article; cohort analysis; device failure; diabetes mellitus; human; medical documentation; medical record; postmarketing surveillance; supervised machine learning",Article,Scopus,2-s2.0-85122520279
"Kim S., Choi Y., Won J.-H., Mi Oh J., Lee H.","57405315000;57218206734;57221134512;55375582800;55811592700;","An annotated corpus from biomedical articles to construct a drug-food interaction database",2022,"Journal of Biomedical Informatics","126",,"103985","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122495606&doi=10.1016%2fj.jbi.2022.103985&partnerID=40&md5=a7de98376cbd4c21f64c0bc6e572dad2","Motivation: While drug-food interaction (DFI) may undermine the efficacy and safety of drugs, DFI detection has been difficult because a well-organized database for DFI did not exist. To construct a DFI database and build a natural language processing system extracting DFI from biomedical articles, we formulated the DFI extraction tasks and manually annotated texts that could have contained DFI information. In this article, we introduced a new annotated corpus for extracting DFI, the DFI corpus. Results: The DFI corpus contains 2270 abstracts of biomedical articles accessible through PubMed and 2498 sentences that contain DFI and/or drug-drug information (DDI), a substantial amount of information about drug/food entities, evidence-levels of abstracts and relations between named entities. BERT models pre-trained on the biomedical domain achieved a F1 score 55.0% in extracting DFI key-sentences. To the best of our knowledge, the DFI corpus is the largest public corpus for drug-food interaction. Availability and implementation: Our corpus is available at https://github.com/ccadd-snu/corpus-for-DFI-extraction. © 2022 Elsevier Inc.","Biomedical corpora; Drug interaction; Drug-food interaction; Information extraction; Natural language processing","Abstracting; Data mining; Database systems; Drug interactions; Amount of information; Biomedical corpus; Biomedical domain; F1 scores; Information extraction; Interaction database; Interaction detection; Interaction extraction; Interaction information; Named entities; Natural language processing systems; cannabidiol; carboplatin; cardiovascular agent; cinnamic acid; cisplatin; cyclophosphamide; doxorubicin; iron; levothyroxine; nadolol; simvastatin; temozolomide; warfarin; Article; data extraction; drug database; food drug interaction; human; information retrieval; natural language processing",Article,Scopus,2-s2.0-85122495606
"Yuan Z., Zhao Z., Sun H., Li J., Wang F., Yu S.","57218628572;57220891101;57200255663;57397332800;57397710700;56324605100;","CODER: Knowledge-infused cross-lingual medical term embedding for term normalization",2022,"Journal of Biomedical Informatics","126",,"103983","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122317264&doi=10.1016%2fj.jbi.2021.103983&partnerID=40&md5=15ecc74cf33637fb0b12876635d4da42","Objective: This paper aims to propose knowledge-aware embedding, a critical tool for medical term normalization. Methods: We develop CODER (Cross-lingual knowledge-infused medical term embedding) via contrastive learning based on a medical knowledge graph (KG) named the Unified Medical Language System, and similarities are calculated utilizing both terms and relation triplets from the KG. Training with relations injects medical knowledge into embeddings and can potentially improve their performance as machine learning features. Results: We evaluate CODER based on zero-shot term normalization, semantic similarity, and relation classification benchmarks, and the results show that CODER outperforms various state-of-the-art biomedical word embeddings, concept embeddings, and contextual embeddings. Conclusion: CODER embeddings excellently reflect semantic similarity and relatedness of medical concepts. One can use CODER for embedding-based medical term normalization or to provide features for machine learning. Similar to other pretrained language models, CODER can also be fine-tuned for specific tasks. Codes and models are available at https://github.com/GanjinZero/CODER. © 2022 Elsevier Inc.","Contrastive learning; Cross-lingual; Knowledge graph embedding; Medical term normalization; Medical term representation","Graph embeddings; Machine learning; Semantics; Contrastive learning; Cross-lingual; Graph embeddings; Knowledge graph embedding; Knowledge graphs; Medical term normalization; Medical term representation; Medical terms; Normalisation; Term representation; Knowledge graph; article; embedding; human; human experiment; language; machine learning; Unified Medical Language System",Article,Scopus,2-s2.0-85122317264
"Qiao X., Zheng Q., Cao Y., Lau R.W.H.","56985203700;57204471415;55504302300;7103010017;","Instance-Aware Scene Layout Forecasting",2022,"International Journal of Computer Vision","130","2",,"504","516",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122304811&doi=10.1007%2fs11263-021-01560-x&partnerID=40&md5=6af120a6f59950bb4a2388b22dbcccf8","Forecasting scene layout is of vital importance in many vision applications, e.g., enabling autonomous vehicles to plan actions early. It is a challenging problem as it involves understanding of the past scene layouts and the diverse object interactions in the scene, and then forecasting what the scene will look like at a future time. Prior works learn a direct mapping from past pixels to future pixel-wise labels and ignore the underlying object interactions in the scene, resulting in temporally incoherent and averaged predictions. In this paper, we propose a learning framework to forecast semantic scene layouts (represented by instance maps) from an instance-aware perspective. Specifically, our framework explicitly models the dynamics of individual instances and captures their interactions in a scene. Under this formulation, we are able to enforce instance-level constraints to forecast scene layouts by effectively reasoning about their spatial and semantic relations. Experimental results show that our model can predict sharper and more accurate future instance maps than the baselines and prior methods, yielding state-of-the-art performances on short-term, mid-term and long-term scene layout forecasting. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Instance interaction; Layout forecasting; Scene layout; Scene understanding","Pixels; Semantics; Autonomous Vehicles; Direct mapping; Diverse objects; Instance interaction; Layout forecasting; Learn+; Object interactions; Scene layout; Scene understanding; Vision applications; Forecasting",Article,Scopus,2-s2.0-85122304811
"Hu L., Xiang Y., Zhang J., Shi Z., Wang W.","57216963391;36656828000;36079632000;57396517400;7501765230;","Aerodynamic data predictions based on multi-task learning[Formula presented]",2022,"Applied Soft Computing","116",,"108369","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122250770&doi=10.1016%2fj.asoc.2021.108369&partnerID=40&md5=ac795256b4360168c347da28e1bd2115","The quality of datasets is one of the key factors that affect the accuracy of aerodynamic data models. For example, in the uniformly sampled Burgers’ dataset, insufficient high-speed data is overwhelmed by massive low-speed data. Predicting high-speed data is more difficult than predicting low-speed data, owing to the fact that the number of high-speed data is limited, i.e. the quality of the Burgers’ dataset is not satisfactory. To improve quality of datasets, traditional methods usually employ data resampling technology to produce enough data for the insufficient parts in the original datasets before modeling, which increases computational costs. Motivated by the mixtures of experts in natural language processing, we propose a multi-task learning (MTL) scheme in the field of aerodynamic data predictions to eliminate the need for data resampling. Our MTL is a datasets quality-adaptive learning scheme, which combines task allocation and aerodynamic characteristic learning together to disperse the pressure of an entire learning task. The task allocation divides a whole learning task into several independent subtasks, while the aerodynamic characteristic learning learns these subtasks simultaneously to achieve better precisions. Two experiments with poor quality datasets are conducted to verify the data quality-adaptivity of the MTL to datasets. The results show that the MTL whose subtasks are divided by the K-means is more accurate than fully connected networks (FCNs), generative adversarial networks (GANs) and radical basis function neural networks (RBFNNs) in poor quality datasets. © 2021 Elsevier B.V.","Aerodynamic data modeling; ClusterNet; Machine learning; Muti-task learning; The K-means","Aerodynamics; Forecasting; Generative adversarial networks; Linearization; Natural language processing systems; Speed; Aerodynamic data; Aerodynamic data modeling; Clusternet; Data prediction; High-speed data; K-means; Muti-task learning; Subtask; Task learning; The K-mean; Learning algorithms",Article,Scopus,2-s2.0-85122250770
"Bouarroudj W., Boufaida Z., Bellatreche L.","57201500859;22333535200;15073752300;","Named entity disambiguation in short texts over knowledge graphs",2022,"Knowledge and Information Systems","64","2",,"325","351",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122243482&doi=10.1007%2fs10115-021-01642-9&partnerID=40&md5=888abe3218d26f8b10fb454071407450","The ever-growing usage of knowledge graphs (KGs) positions named entity disambiguation (NED) at the heart of designing accurate KG-driven systems such as query answering systems (QAS). According to the current research, most studies dealing with NED on KGs involve long texts, which is not the case of short text fragments, identified by their limited contexts. The accuracy of QASs strongly depends on the management of such short text. This limitation motivates this paper, which studies the NED problem on KGs, involving only short texts. First, we propose a NED approach including the following steps: (i) context expansion using WordNet to measure its similarity to the resource context. (ii) Exploiting coherence between entities in queries that contain more than one entity, such as “Is Michelle Obama the wife of Barack Obama?”. (iii) Taking into account the relations between words to calculate their similarity with the properties of a resource. (iv) the use of syntactic features. The NED solution approach is compared to state-of-the-art approaches using five datasets. The experimental results show that our approach outperforms these systems by 27% in the F-measure. A system called Welink, implementing our proposal, is available on GitHub, and it is also accessible via a REST API. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Entity linking; Linked open data; Named entity disambiguation; Queries; Semantic and syntactic features; Short texts","Knowledge graph; Natural language processing systems; Open Data; Semantics; Syntactics; Driven system; Entity linking; Knowledge graphs; Linked open datum; Named entity disambiguations; Query; Query answering systems; Semantic features; Short texts; Syntactic features; Linked data",Article,Scopus,2-s2.0-85122243482
"Suissa O., Elmalech A., Zhitomirsky-Geffet M.","57213151397;55753219100;25825790600;","Text analysis using deep neural networks in digital humanities and information science",2022,"Journal of the Association for Information Science and Technology","73","2",,"268","287",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122047502&doi=10.1002%2fasi.24544&partnerID=40&md5=7e9404cc42d3cb22ded20a46c872a54e","Combining computational technologies and humanities is an ongoing effort aimed at making resources such as texts, images, audio, video, and other artifacts digitally available, searchable, and analyzable. In recent years, deep neural networks (DNN) dominate the field of automatic text analysis and natural language processing (NLP), in some cases presenting a super-human performance. DNNs are the state-of-the-art machine learning algorithms solving many NLP tasks that are relevant for Digital Humanities (DH) research, such as spell checking, language detection, entity extraction, author detection, question answering, and other tasks. These supervised algorithms learn patterns from a large number of “right” and “wrong” examples and apply them to new examples. However, using DNNs for analyzing the text resources in DH research presents two main challenges: (un)availability of training data and a need for domain adaptation. This paper explores these challenges by analyzing multiple use-cases of DH studies in recent literature and their possible solutions and lays out a practical decision model for DH experts for when and how to choose the appropriate deep learning approaches for their research. Moreover, in this paper, we aim to raise awareness of the benefits of utilizing deep learning models in the DH community. © 2021 Association for Information Science and Technology.",,"Deep neural networks; Learning algorithms; Learning systems; Natural language processing systems; Neural networks; Text mining; Computational technology; Digital humanities; Entity extractions; Language detection; Multiple use-cases; NAtural language processing; Question Answering; Supervised algorithm; Deep learning; article; awareness; deep learning; deep neural network; human; human experiment; humanities; information science",Article,Scopus,2-s2.0-85122047502
"Pally R.J., Samadi S.","57388049500;35270122400;","Application of image processing and convolutional neural networks for flood image classification and semantic segmentation",2022,"Environmental Modelling and Software","148",,"105285","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121855603&doi=10.1016%2fj.envsoft.2021.105285&partnerID=40&md5=86f0a7d8b5906d9eaea8ba23b92b3f88","Deep learning algorithms are exceptionally valuable tools for collecting and analyzing the catastrophic readiness and countless actionable flood data. Convolutional neural networks (CNNs) are one form of deep learning algorithms widely used in computer vision which can be used to study flood images and assign learnable weights to various objects in the image. Here, we leveraged and discussed how connected vision systems can be used to embed cameras, image processing, CNNs, and data connectivity capabilities for flood label detection. We built a training database service of >9000 images (image annotation service) including the image geolocation information by streaming relevant images from social media platforms, Department of Transportation (DOT) 511 traffic cameras, the US Geological Survey (USGS) live river cameras, and images downloaded from search engines. We then developed a new python package called “FloodImageClassifier” to classify and detect objects within the collected flood images. “FloodImageClassifier” includes various CNNs architectures such as YOLOv3 (You look only once version 3), Fast R–CNN (Region-based CNN), Mask R–CNN, SSD MobileNet (Single Shot MultiBox Detector MobileNet), and EfficientDet (Efficient Object Detection) to perform both object detection and segmentation simultaneously. Canny Edge Detection and aspect ratio concepts are also included in the package for flood water level estimation and classification. The pipeline is smartly designed to train a large number of images and calculate flood water levels and inundation areas which can be used to identify flood depth, severity, and risk. “FloodImageClassifier” can be embedded with the USGS live river cameras and 511 traffic cameras to monitor river and road flooding conditions and provide early intelligence to emergency response authorities in real-time. © 2021 Elsevier Ltd","Convolutional neural networks; Flood label detection; Floodwater level and extend estimation; Image processing; “FloodImageClassifier” package","Aspect ratio; Cameras; Convolution; Convolutional neural networks; Deep learning; Edge detection; Image classification; Image segmentation; Learning algorithms; Media streaming; Object detection; Object recognition; Rivers; Search engines; Semantics; Water levels; Convolutional neural network; Flood label detection; Flood waters; Floodwater level and extend estimation; Images processing; Region-based; Traffic camera; U.S geological surveys; US Geological Survey; “floodimageclassifier” package; Floods; computer vision; connectivity; database; flood frequency; image classification; image processing; segmentation; United States",Article,Scopus,2-s2.0-85121855603
"Zhang J., Li C., Liu G., Min M., Wang C., Li J., Wang Y., Yan H., Zuo Z., Huang W., Chen H.","8711547600;57245812500;57386755200;57387131500;57216531616;45861310200;57226325362;7403395986;57386755300;57203556460;8876008200;","A CNN-transformer hybrid approach for decoding visual neural activity into text",2022,"Computer Methods and Programs in Biomedicine","214",,"106586","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121795531&doi=10.1016%2fj.cmpb.2021.106586&partnerID=40&md5=5756709c67265601d0fc6f0ee2741c6b","Background and Objective: Most studies used neural activities evoked by linguistic stimuli such as phrases or sentences to decode the language structure. However, compared to linguistic stimuli, it is more common for the human brain to perceive the outside world through non-linguistic stimuli such as natural images, so only relying on linguistic stimuli cannot fully understand the information perceived by the human brain. To address this, an end-to-end mapping model between visual neural activities evoked by non-linguistic stimuli and visual contents is demanded. Methods: Inspired by the success of the Transformer network in neural machine translation and the convolutional neural network (CNN) in computer vision, here a CNN-Transformer hybrid language decoding model is constructed in an end-to-end fashion to decode functional magnetic resonance imaging (fMRI) signals evoked by natural images into descriptive texts about the visual stimuli. Specifically, this model first encodes a semantic sequence extracted by a two-layer 1D CNN from the multi-time visual neural activity into a multi-level abstract representation, then decodes this representation, step by step, into an English sentence. Results: Experimental results show that the decoded texts are semantically consistent with the corresponding ground truth annotations. Additionally, by varying the encoding and decoding layers and modifying the original positional encoding of the Transformer, we found that a specific architecture of the Transformer is required in this work. Conclusions: The study results indicate that the proposed model can decode the visual neural activities evoked by natural images into descriptive text about the visual stimuli in the form of sentences. Hence, it may be considered as a potential computer-aided tool for neuroscientists to understand the neural mechanism of visual information processing in the human brain in the future. © 2021","Brain decoding; CNN; Deep learning; fMRI; Transformer","Abstracting; Brain; Convolutional neural networks; Decoding; Deep learning; Encoding (symbols); Functional neuroimaging; Neurons; Semantics; Signal encoding; Brain decoding; Convolutional neural network; Deep learning; End to end; Functional magnetic resonance imaging; Human brain; Natural images; Neural activity; Transformer; Visual stimulus; Magnetic resonance imaging; article; brain; computer vision; controlled study; convolutional neural network; deep learning; functional magnetic resonance imaging; human; human experiment; language; neuroscientist; brain; brain mapping; diagnostic imaging; nuclear magnetic resonance imaging; vision; Brain; Brain Mapping; Humans; Magnetic Resonance Imaging; Neural Networks, Computer; Visual Perception",Article,Scopus,2-s2.0-85121795531
"Macêdo J.B., das Chagas Moura M., Aichele D., Lins I.D.","57204951658;57223261074;57224212461;27467582900;","Identification of risk features using text mining and BERT-based models: Application to an oil refinery",2022,"Process Safety and Environmental Protection","158",,,"382","399",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121322209&doi=10.1016%2fj.psep.2021.12.025&partnerID=40&md5=ebde6a03677cdcc8e9bb6cae7027a714","The uncontrollable release of hazardous substances may lead to catastrophic accidents. In this context, risk studies are aimed at recommending either preventive measures or designing safeguards to mitigate the consequences. To that end, risk experts postulate possible leakages, then identify their causes and consequences, and finally evaluate and classify the risks into categories. These analyses rely on examination different engineering textual documents and attendance numerous meetings, which is very time consuming. Moreover, this qualitative process of hazard identification and assessment are usually the first steps of quantitative risk analysis (QRA) and is paramount to ensure its quality. Therefore, we here propose to use text mining and fine-tuned trained bidirectional encoder representations from transformers (BERT) models to support and reduce the efforts required for completing the early stages of QRA. Our idea is to apply these techniques to identify the potential consequences of accidents related to the operation of an oil refinery and classify each scenario in terms of severity of the consequence and likelihood of occurrence. The proposed method was applied to an actual oil refinery and presented very promising results. The potential consequences, the severity and likelihood categories were predicted with a mean accuracy of 97.42%, 86.44%, and 94.34% respectively. The models resulting from this research were embedded into a web-based app that is called HALO (hazard analysis based on language processing for oil refineries). © 2021","Hazard assessment; Hazard identification; Natural language processing; Oil refineries; Text mining","Accidents; Data mining; Natural language processing systems; Petroleum refining; Quality control; Risk analysis; Risk assessment; Catastrophic accidents; Hazard Assessment; Hazard identification; Hazardous substances; Identification of risks; Model application; Oil refineries; Preventive measures; Quantitative risk analysis; Text-mining; Hazards",Article,Scopus,2-s2.0-85121322209
"Geneva N., Zabaras N.","57190393795;7006370831;","Transformers for modeling physical systems",2022,"Neural Networks","146",,,"272","289",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121150805&doi=10.1016%2fj.neunet.2021.11.022&partnerID=40&md5=84f133f2a757b18e315749d192648af3","Transformers are widely used in natural language processing due to their ability to model longer-term dependencies in text. Although these models achieve state-of-the-art performance for many language related tasks, their applicability outside of the natural language processing field has been minimal. In this work, we propose the use of transformer models for the prediction of dynamical systems representative of physical phenomena. The use of Koopman based embeddings provides a unique and powerful method for projecting any dynamical system into a vector representation which can then be predicted by a transformer. The proposed model is able to accurately predict various dynamical systems and outperform classical methods that are commonly used in the scientific machine learning literature.1 © 2021 Elsevier Ltd","Deep learning; Koopman; Physics; Self-attention; Surrogate modeling; Transformers","Deep learning; Dynamical systems; Learning algorithms; Modeling languages; Deep learning; Koopman; Long-term dependencies; Physical phenomena; Physical systems; Self-attention; State-of-the-art performance; Surrogate modeling; Transformer; Transformer modeling; Natural language processing systems; language; machine learning; natural language processing; Language; Machine Learning; Natural Language Processing",Article,Scopus,2-s2.0-85121150805
"Wang S., Ye X., Gu Y., Wang J., Meng Y., Tian J., Hou B., Jiao L.","55940463600;57201534925;57200602110;57367641600;57225803325;57262017900;57367852800;57226593766;","Multi-label semantic feature fusion for remote sensing image captioning",2022,"ISPRS Journal of Photogrammetry and Remote Sensing","184",,,"1","18",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120905588&doi=10.1016%2fj.isprsjprs.2021.11.020&partnerID=40&md5=31a0dd6e2ae284fc6e3d9ef07382abeb","For remote sensing image (RSI) captioning tasks, two-stage RSI captioning methods have achieved high performance because they introduce the results of other RSI tasks, such as image classification as prior information. However, most previous works treat image classification in the two-stage RSI captioning method as a single-label classification task, which cannot adequately describe the entire contents of complex RSIs and may cause semantic ambiguity. To settle this problem and further refine image feature representation, we introduce multi-label classification into two-stage RSI captioning to provide sufficient and accurate prior semantic information and propose a multi-label semantic feature fusion (MLSFF) framework. Specifically, we design a robust multi-label semantic attribute extractor to extract multi-label semantic attributes of RSIs. To obtain discriminative feature representations for RSI captioning, we propose two cross-modal semantic feature fusion operators that fuse the extracted semantic attributes and the image feature extracted by the convolutional neural network. The results of extensive numerical experiments show that the proposed method can achieve state-of-the-art performance on the UCM-Captions, Sydney-Captions, and RSICD datasets. Specifically, on the UCM-Captions dataset, our method achieves a gain of 8.2% in Sm score over the SAT (LAM) method (Zhang et al., 2019c). On the Sydney-Captions dataset, our method improves the Sm score by 17.4% compared with the TCE loss-based method (Li et al., 2020a). On the RSICD, our method outperforms the multi-level attention method (Li et al., 2020b) by 3.2% in terms of the Sm score. Code is available at https://github.com/xtye5025/MLSFF. © 2021","Cross-modal feature fusion; Feature representation; Multi-label classification; Remote sensing image captioning; Vision and language","Image classification; Image fusion; Numerical methods; Remote sensing; Semantics; Cross-modal; Cross-modal feature fusion; Feature representation; Features fusions; Image captioning; Label semantics; Multi-labels; Remote sensing image captioning; Remote sensing images; Vision and language; Classification (of information); data set; image classification; information processing; remote sensing; semantic standardization",Article,Scopus,2-s2.0-85120905588
"Badreddine S., d'Avila Garcez A., Serafini L., Spranger M.","57219583665;6603397393;7005585409;23092382400;","Logic Tensor Networks",2022,"Artificial Intelligence","303",,"103649","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120848813&doi=10.1016%2fj.artint.2021.103649&partnerID=40&md5=c36a981614105da4a92886d366ad3c23","Attempts at combining logic and neural networks into neurosymbolic approaches have been on the increase in recent years. In a neurosymbolic system, symbolic knowledge assists deep learning, which typically uses a sub-symbolic distributed representation, to learn and reason at a higher level of abstraction. We present Logic Tensor Networks (LTN), a neurosymbolic framework that supports querying, learning and reasoning with both rich data and abstract knowledge about the world. LTN introduces a fully differentiable logical language, called Real Logic, whereby the elements of a first-order logic signature are grounded onto data using neural computational graphs and first-order fuzzy logic semantics. We show that LTN provides a uniform language to represent and compute efficiently many of the most important AI tasks such as multi-label classification, relational learning, data clustering, semi-supervised learning, regression, embedding learning and query answering. We implement and illustrate each of the above tasks with several simple explanatory examples using TensorFlow 2. The results indicate that LTN can be a general and powerful framework for neurosymbolic AI. © 2021 Elsevier B.V.","Deep learning and reasoning; Many-valued logics; Neurosymbolic AI","Classification (of information); Clustering algorithms; Computer circuits; Deep learning; Fuzzy logic; Many valued logics; Semantics; Supervised learning; Deep learning and reasoning; Distributed representation; Learn+; Logic networks; Many-valued logic; Neural-networks; Neuro-symbolic system; Neurosymbolic AI; Sub-symbolic; Symbolic knowledge; Tensors",Article,Scopus,2-s2.0-85120848813
"Li X., Wu C., Xue F., Yang Z., Lou J., Lu W.","57193211001;57192179491;56720069500;57365844900;57226830425;24173836000;","Ontology-based mapping approach for automatic work packaging in modular construction",2022,"Automation in Construction","134",,"104083","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120815026&doi=10.1016%2fj.autcon.2021.104083&partnerID=40&md5=2b8c1fa8fddfd24301ec9c6d77893958","Many cross-knowledge domain tasks involving various professional backgrounds have been transferred from construction sites to factories in modular construction (MC). In MC, forming optimal work packages which can handle the complexity of product breakdown structures and dynamic project progress is critical for task planning and execution. However, forming MC work packages is time-consuming and ineffective because it is performed manually while not adequately considering domain knowledge. To address the problem, this study proposes a dynamic ontology-based mapping (DOM) approach to automatically generate semantic-enriched work packages. For this purpose, ontologies of MC products, topology, and tasks are established to incorporate domain knowledge. Then, a customized Latent Dirichlet Allocation (LDA) model for mapping products to tasks and a weighted hierarchical clustering model for grouping dynamic tasks into work packages are developed. The effectiveness of the DOM approach is tested in an MC case project and controlled experiments. The results demonstrate that the DOM approach can significantly increase the accuracy and efficiency of the dynamic work packaging process while reducing planning time compared to conventional methods, which thus improve the collaborative management and performance of MC projects. © 2021 Elsevier B.V.","Hierarchical clustering; Latent Dirichlet allocation; Modular construction; Ontology; Project planning; Work package","Domain Knowledge; Mapping; Ontology; Project management; Semantics; Statistics; Domain knowledge; Dynamic ontologies; Hier-archical clustering; Hierarchical Clustering; Knowledge domains; Latent Dirichlet allocation; Ontology's; Ontology-based; Project planning; Work packages; Modular construction",Article,Scopus,2-s2.0-85120815026
"Lytos A., Lagkas T., Sarigiannidis P., Argyriou V., Eleftherakis G.","57191837386;6507945237;12445587500;13806485100;6506837394;","Modelling argumentation in short text: A case of social media debate",2022,"Simulation Modelling Practice and Theory","115",,"102446","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120649661&doi=10.1016%2fj.simpat.2021.102446&partnerID=40&md5=0dda9806c14a4eba2cb0cbccf9e5aa4e","The technological leaps of artificial intelligence (AI) and the rise of machine learning have triggered significant progress in a plethora of natural language processing (NLP) and natural language understanding tasks. One of these tasks is argumentation mining which has received significant interest in recent years and is regarded as a key domain for future decision-making systems, behaviour modelling, and natural language understanding problems. Until recently, natural language modelling tasks, such as computational argumentation schemes, were often tested in controlled environments, such as persuasive essays, reducing unexpected behaviours that could occur in real-life settings, like a public debate on social media. Additionally, the growing demand for enhancing the trust and the explainability of the AI services has dictated the design and adoption of modelling schemes to increase the confidence in the outcomes of the AI solutions. This paper attempts to explore modelling argumentation in short text and proposes a novel framework for argumentation detection under the name Abstract Framework for Argumentation Detection (AFAD). Moreover, different proof-of-concept implementations are provided to examine the applicability of the proposed framework to very short text developing a rule-based mechanism and compare the results with data-driven solutions. Eventually, a combination of the deployed methods is applied increasing the correct predictions in the minority class on an imbalanced dataset. The findings suggest that the modelling process provides solid grounds for technical research while the hybrid solutions have the potential to be applied to a wide range of NLP-related tasks offering a deeper understanding of human language and reasoning. © 2021 Elsevier B.V.","Argumentation detection; Argumentation mining; Argumentation modelling; Computational linguistics; Natural language processing (NLP); Semantic similarity; Social media; Symbolic artificial intelligence","Abstracting; Artificial intelligence; Computational linguistics; Decision making; Learning algorithms; Modeling languages; Natural language processing systems; Semantics; Argumentation detection; Argumentation mining; Argumentation model; Decision-making systems; Natural language processing; Natural language understanding; Semantic similarity; Short texts; Social media; Symbolic artificial intelligence; Social networking (online)",Article,Scopus,2-s2.0-85120649661
"Jin C., Wang K., Han T., Lu Y., Liu A., Liu D.","14063414200;57361606200;57203719167;57221684100;57361606300;57138657900;","Segmentation of ore and waste rocks in borehole images using the multi-module densely connected U-net",2022,"Computers and Geosciences","159",,"105018","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120521992&doi=10.1016%2fj.cageo.2021.105018&partnerID=40&md5=40e6451b99a047c648d7f72f5d602a5a","During exploitation of metal mines, the delineation of orebody boundary is of great importance to control the dilution rate, ensure the mining efficiency and improve the economic benefits. At present, the delineation of orebody boundary is usually achieved by conventional methods, such as laboratory tests on rock powder, identification of ore samples under microscope and spectrum analysis etc. However, as the actual condition of orebody occurrence is extremely complex, the conventional delineation methods can hardly meet the requirements of full-length real-time efficient analysis for boreholes. As a result, the blasting design is out of step with the blasting mining process, leading to great economic loss. In this paper, a method for intelligent identification and automatic delineation of ore and waste rock boundary is proposed, based on the borehole imaging technology and the image segmentation method. The digital borehole imaging system is employed to obtain the borehole images in mine roadways. The Inception and DenseNet blocks are integrated to develop the multi-module densely connected U-net (MMDC-Unet), which can realize pixel-level semantic segmentation of borehole images by segmenting the borehole images of highly similar and highly randomly distributed ore according to the global and local image features. Based on the image segmentation results, the number of pixels for ore is counted to quantitatively analyze the distribution of ore and realize real-time rapid delineation of ore and waste rock boundary. By comparing the intelligent identification results with the chemical test results, it is found that the proposed method can greatly enhance the identification efficiency while satisfying the engineering requirements. © 2021 Elsevier Ltd","Borehole imaging; Intelligent identification; Ore and waste rock boundary; U-net","Blasting; Image segmentation; Losses; Ores; Pixels; Rocks; Semantics; Spectrum analysis; Borehole images; Borehole Imaging; Intelligent identification; MMDC-unet; Multimodule; Ore and waste rock boundary; Orebodies; Real- time; Rock boundaries; Waste rocks; Efficiency; borehole; image analysis; ore body; pixel",Article,Scopus,2-s2.0-85120521992
"Kim Y., Bang S., Sohn J., Kim H.","57225182378;57192543730;57221283823;23004894100;","Question answering method for infrastructure damage information retrieval from textual data using bidirectional encoder representations from transformers",2022,"Automation in Construction","134",,"104061","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120432405&doi=10.1016%2fj.autcon.2021.104061&partnerID=40&md5=5b9f2cf50eced9a8cd553383e78f38b2","Manual searching for infrastructure damage information from large amounts of textual data requires considerable time and effort. A fast and accurate collection of damage information from such data is necessary for effective infrastructure planning. In this study, a question answering method was proposed to provide users with infrastructure damage information from textual data automatically. The proposed method relies on a natural language model called bidirectional encoder representations from transformers for information retrieval. From the 143 reports collected from the National Hurricane Center, 533 question-answer pairs were formulated. The proposed model was trained with 435 pairs and tested with the remainder. The model was also tested with 43 question-answer pairs created using earthquake-related textual data and achieved F1-scores of 90.5% and 83.6% for the hurricane and earthquake datasets, respectively. © 2021 Elsevier B.V.","Bidirectional encoder representations from transformers; Disaster management; Information retrieval; Infrastructure management; Natural language processing; Question answering","Disaster prevention; Disasters; Earthquakes; Hurricanes; Information retrieval; Signal encoding; Bidirectional encoder representation from transformer; Damage information; Disaster management; Effective infrastructures; Infrastructure managements; Infrastructure planning; Large amounts; Question Answering; Question-answer pairs; Textual data; Natural language processing systems",Article,Scopus,2-s2.0-85120432405
"Karimiziarani M., Jafarzadegan K., Abbaszadeh P., Shao W., Moradkhani H.","57219378089;55675993000;55585799000;57075773900;6506246184;","Hazard risk awareness and disaster management: Extracting the information content of twitter data",2022,"Sustainable Cities and Society","77",,"103577","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120372221&doi=10.1016%2fj.scs.2021.103577&partnerID=40&md5=dc03cb155ebd91ce3e493811ad71888b","The use of social media platforms such as Twitter significantly increases during natural hazards. With the emergence of several social media platforms over the past decade, many studies have investigated the applications of these platforms during calamities. This study presents a comprehensive spatiotemporal analysis of textual content from millions of tweets shared on Twitter during Hurricane Harvey (2017) across several affected counties in southeast Texas. We propose a new Hazard Risk Awareness (HRA) Index, which considers multiple factors, including the number of tweets, population, internet use rate, and natural hazard characteristics per geographic location. We then map the HRA Index across southeast Texas. Utilizing a dataset of 18 million tweets, we employ Natural Language Processing (NLP) along with a set of statistical techniques to perform analysis on the textual data generated by Twitter users during Hurricane Harvey. This enables us to subdivide the tweet contents into several categories per county that would inform crisis management during the event. In all, our study provides valuable information at the county level before, during, and after Harvey that could significantly help disaster managers and responders to minimize the consequences of the event and improve the preparedness of the residents for it. Since HRA is derived based on the meteorological observations and some demographic information, depending on the availability of such dataset and the nature of the hazard (i.e., flood, wildfire, hurricane, and earthquake), this index can be calculated and employed for assessing the risk awareness of a community exposed to either of these natural hazards. © 2021","Disaster management; Hurricane Harvey; Natural language processing; Twitter","Disaster prevention; Disasters; Hazards; Hurricanes; Natural language processing systems; Risk assessment; Risk perception; Disaster management; Hazard risks; Hurricane harvey; Information contents; Multiple factors; Natural hazard; Risk awareness; Social media platforms; Spatiotemporal analysis; Textual content; Social networking (online)",Article,Scopus,2-s2.0-85120372221
"Alomari M., Li F., Hogg D.C., Cohn A.G.","57192179292;57357951600;57357951700;7005699215;","Online perceptual learning and natural language acquisition for autonomous robots",2022,"Artificial Intelligence","303",,"103637","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120333069&doi=10.1016%2fj.artint.2021.103637&partnerID=40&md5=489e3f7bfe28bfd2169b6ac19d094b6e","In this work, the problem of bootstrapping knowledge in language and vision for autonomous robots is addressed through novel techniques in grammar induction and word grounding to the perceptual world. In particular, we demonstrate a system, called OLAV, which is able, for the first time, to (1) learn to form discrete concepts from sensory data; (2) ground language (n-grams) to these concepts; (3) induce a grammar for the language being used to describe the perceptual world; and moreover to do all this incrementally, without storing all previous data. The learning is achieved in a loosely-supervised manner from raw linguistic and visual data. Moreover, the learnt model is transparent, rather than a black-box model and is thus open to human inspection. The visual data is collected using three different robotic platforms deployed in real-world and simulated environments and equipped with different sensing modalities, while the linguistic data is collected using online crowdsourcing tools and volunteers. The analysis performed on these robots demonstrates the effectiveness of the framework in learning visual concepts, language groundings and grammatical structure in these three online settings. © 2021","Grammar induction; Language acquisition; Language and vision; Language grounding","E-learning; Linguistics; Natural language processing systems; Visual languages; Grammar induction; Language acquisition; Language and vision; Language grounding; Learn+; Learning languages; Linguistic data; Natural language acquisition; Perceptual learning; Visual data; Robots",Article,Scopus,2-s2.0-85120333069
"Chang Z., Zhao D., Chen H., Li J., Liu P.","57202866752;55475662400;57353887300;57354671300;57354052200;","Event-centric multi-modal fusion method for dense video captioning",2022,"Neural Networks","146",,,"120","129",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119980169&doi=10.1016%2fj.neunet.2021.11.017&partnerID=40&md5=77473ba5ec74d35dcca5bcf496440792","Dense video captioning aims to automatically describe several events that occur in a given video, which most state-of-the-art models accomplish by locating and describing multiple events in an untrimmed video. Despite much progress in this area, most current approaches only encode visual features in the event location phase and they neglect the relationships between events, which may degrade the consistency of the description in the identical video. Thus, in the present study, we attempted to exploit visual–audio cues to generate event proposals and enhance event-level representations by capturing their temporal and semantic relationships. Furthermore, to compensate for the major limitation of not fully utilizing multimodal information in the description process, we developed an attention-gating mechanism that dynamically fuses and regulates the multi-modal information. In summary, we propose an event-centric multi-modal fusion approach for dense video captioning (EMVC) to capture the relationships between events and effectively fuse multi-modal information. We conducted comprehensive experiments to evaluate the performance of EMVC based on the benchmark ActivityNet Caption and YouCook2 data sets. The experimental results showed that our model achieved impressive performance compared with state-of-the-art methods. © 2021 Elsevier Ltd","Dense video captioning; Event-centric; Multi-modal fusion","Benchmarking; 'current; ART model; Dense video captioning; Event-centric; Fusion methods; Multi-modal fusion; Multi-modal information; Multiple events; Performance; State of the art; Semantics; article; attention; videorecording; attention; Attention",Article,Scopus,2-s2.0-85119980169
"Amer F., Hockenmaier J., Golparvar-Fard M.","57202510239;15725022300;34868104300;","Learning and critiquing pairwise activity relationships for schedule quality control via deep learning-based natural language processing",2022,"Automation in Construction","134",,"104036","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119901718&doi=10.1016%2fj.autcon.2021.104036&partnerID=40&md5=bfefeabf3b0d414af1add0dcedb33e3c","In construction, schedule mistakes causing delays beyond substantial completion dates cost contractors expensive liquidated damages. Hence, several industry guidelines, such as the DCMA's 14 point assessment, define schedule quality and offer systematic methods for ensuring it. These guidelines list “logic” as an essential control metric, and they require planners to ensure their schedules are free of missing or wrong logical dependencies. Checking the logic requires extensive construction domain knowledge, and planners perform it entirely manually as there are no available software solutions that support it. This paper offers a novel machine learning-based solution that learns construction scheduling domain knowledge from existing records completely automatically and applies it to validate the logic in input schedules achieving an F1 score of 88.3%. Furthermore, we tailor our method to use the learned knowledge to schedule a list of unordered activities. The details of the method, experimental results, benefits, and limitations are discussed. © 2021","Construction planning; Data mining; Machine learning; Natural language processing; Neural networks; Quality control","Computer circuits; Deep learning; Domain Knowledge; Learning algorithms; Natural language processing systems; Planning; Quality control; Construction planning; Construction schedules; Domain knowledge; Industry guidelines; Learn+; Liquidated damages; Logical dependencies; Neural-networks; Software solution; Systematic method; Data mining",Article,Scopus,2-s2.0-85119901718
"Datta S., Roberts K.","57211502829;36551508300;","Fine-grained spatial information extraction in radiology as two-turn question answering",2022,"International Journal of Medical Informatics","158",,"104628","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119901597&doi=10.1016%2fj.ijmedinf.2021.104628&partnerID=40&md5=5726a56567d92c4a3d9b20eda22dbfe2","Objectives: Radiology reports contain important clinical information that can be used to automatically construct fine-grained labels for applications requiring deep phenotyping. We propose a two-turn question answering (QA) method based on a transformer language model, BERT, for extracting detailed spatial information from radiology reports. We aim to demonstrate the advantage that a multi-turn QA framework provides over sequence-based methods for extracting fine-grained information. Methods: Our proposed method identifies spatial and descriptor information by answering queries given a radiology report text. We frame the extraction problem such that all the main radiology entities (e.g., finding, device, anatomy) and the spatial trigger terms (denoting the presence of a spatial relation between finding/device and anatomical location) are identified in the first turn. In the subsequent turn, various other contextual information that acts as important spatial roles with respect to a spatial trigger term are extracted along with identifying the spatial and other descriptor terms qualifying a radiological entity. The queries are constructed using separate templates for the two turns and we employ two query variations in the second turn. Results: When compared to the best-reported work on this task using a traditional sequence tagging method, the two-turn QA model exceeds its performance on every component. This includes promising improvements of 12, 13, and 12 points in the average F1 scores for identifying the spatial triggers, Figure, and Ground frame elements, respectively. Discussion: Our experiments suggest that incorporating domain knowledge in the query (a general description about a frame element) helps in obtaining better results for some of the spatial and descriptive frame elements, especially in the case of the clinical pre-trained BERT model. We further highlight that the two-turn QA approach fits well for extracting information for complex schema where the objective is to identify all the frame elements linked to each spatial trigger and finding/device/anatomy entity, thereby enabling the extraction of more comprehensive information in the radiology domain. Conclusion: Extracting fine-grained spatial information from text in the form of answering natural language queries holds potential in achieving better results when compared to more standard sequence labeling-based approaches. © 2021 Elsevier B.V.","Deep learning; Information extraction; Natural language processing; Question answering; Radiology report; Spatial information","Data mining; Deep learning; Domain Knowledge; Information retrieval; Query processing; Radiation; Radiology; Clinical information; Deep learning; Descriptors; Fine grained; Frame elements; Information extraction; Question Answering; Radiology reports; Spatial information extraction; Spatial informations; Natural language processing systems; anatomical location; article; deep learning; extraction; human; human experiment; natural language processing; radiology",Article,Scopus,2-s2.0-85119901597
"Gao G., Han C., Liu Z.","35408900700;57196464880;57221434324;","Perceiving informative key-points: A self-attention approach for person search",2022,"Signal Processing: Image Communication","101",,"116558","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119582882&doi=10.1016%2fj.image.2021.116558&partnerID=40&md5=1bad2b08d99a7282d495950ed3ef46e7","Though person re-identification has witnessed significant progress, person search, as a more practical task considering the unavailability of annotations of pedestrian bounding boxes, has progressed much slower because of less discriminative feature representation. To this end, we propose a novel self-attention based person search approach by perceiving informative implicit key-points with weak supervision. Firstly, we design the Self-attention Slice Part (SSP) module, to implicitly localize informative key-points by only taking a pre-defined number of points as supervision. Concretely, this module utilizes both channel-attention and spatial attention with weak supervision on partitioned pedestrian slices to get the most discriminative key-points. After that, we strengthen the self-attention weight for these cardinal key-points, and then, more robust feature representations for conducting person search can be obtained from these self-mined key-points. Meanwhile, the SSP also provides semantic alignment with the horizontally partitioned slices. Besides, to focus more on reducing the inner-class margin rather than enlarging inter-class distance, the Random Label Smooth (RLS) loss is defined for more robust classification. The RLS loss not only provides a larger margin hyperplane but also enhances the training efficiency. Therefore, all in all: (1) We propose an end-to-end person search framework by fully exploiting current object detection and person re-identification techniques jointly with weak supervision. (2) Our proposed weakly-supervised self-attention module is generic and can be plugged into any related tasks to improve the performance. (3) We conduct extensive experiments on popular benchmarks, including the dataset of CUHK-SYSU and PRW, and our approach outperforms most current state-of-the-art methods according to mAP and top-1 evaluation metrics. © 2021 Elsevier B.V.","Key-points; Person search; Self-attention; Weakly supervised","Semantics; 'current; Bounding-box; Discriminative features; Feature representation; Keypoints; Person re identifications; Person search; Self-attention; Spatial attention; Weakly supervised; Object detection",Article,Scopus,2-s2.0-85119582882
"Seifermann S., Heinrich R., Werle D., Reussner R.","57024010800;56352523400;56152484100;6603589366;","Detecting violations of access control and information flow policies in data flow diagrams",2022,"Journal of Systems and Software","184",,"111138","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119376943&doi=10.1016%2fj.jss.2021.111138&partnerID=40&md5=002435952487f2d1859afcc67dc40449","The security of software-intensive systems is frequently attacked. High fines or loss in reputation are potential consequences of not maintaining confidentiality, which is an important security objective. Detecting confidentiality issues in early software designs enables cost-efficient fixes. A Data Flow Diagram (DFD) is a modeling notation, which focuses on essential, functional aspects of such early software designs. Existing confidentiality analyses on DFDs support either information flow control or access control, which are the most common confidentiality mechanisms. Combining both mechanisms can be beneficial but existing DFD analyses do not support this. This lack of expressiveness requires designers to switch modeling languages to consider both mechanisms, which can lead to inconsistencies. In this article, we present an extended DFD syntax that supports modeling both, information flow and access control, in the same language. This improves expressiveness compared to related work and avoids inconsistencies. We define the semantics of extended DFDs by clauses in first-order logic. A logic program made of these clauses enables the automated detection of confidentiality violations by querying it. We evaluate the expressiveness of the syntax in a case study. We attempt to model nine information flow cases and six access control cases. We successfully modeled fourteen out of these fifteen cases, which indicates good expressiveness. We evaluate the reusability of models when switching confidentiality mechanisms by comparing the cases that share the same system design, which are three pairs of cases. We successfully show improved reusability compared to the state of the art. We evaluated the accuracy of confidentiality analyses by executing them for the fourteen cases that we could model. We experienced good accuracy. © 2021 The Author(s)","Access control; Data flow diagram; Information flow","Computer circuits; Data flow analysis; Data flow graphs; Data transfer; Flowcharting; Logic programming; Modeling languages; Reusability; Semantics; Software design; Syntactics; XML; Control-flow; Cost-efficient; Data flow diagrams; Functional aspects; Information flow control; Information flow policies; Information flows; Modeling notation; Security objectives; Software intensive systems; Access control",Article,Scopus,2-s2.0-85119376943
"Li P., Zhang P., Xu X.","57210250079;57202889449;57214875655;","Graph convolutional network meta-learning with multi-granularity POS guidance for video captioning",2022,"Neurocomputing","472",,,"294","305",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119186542&doi=10.1016%2fj.neucom.2020.12.137&partnerID=40&md5=1280beb05acf006e1f94ecc258438dd9","Video as information carrier has gained overwhelming popularity in city surveillance and social networks, such as WeChat, Weibo, and TikTok. To bridge the semantic gap between video content (e.g., user and landmark building) and textual information (e.g., user location), video captioning has emerged as an attracting technique in recent years. Existing works mostly focus on sentence-level Part-of-Speech (POS) information and use Long Short-Term Memory (LSTM) as encoder, which neglects word or phrase-level POS information and also fails to globally consider long-range temporal relations among video frames. To address the drawbacks, we leverage multi-granularity POS guidance to learn Graph Convolutional Network (GCN) via meta-learning, abbreviated as GMMP (GCN Meta-learning with Multi-granularity POS), for generating high-quality captions for videos. It models temporal dependency by treating frames as nodes in the graph, and captures POS information of words and phrases by multi-granularity POS attention mechanism. We adopt meta-learning to better learn GCN by maximizing the reward of generated caption in a reinforcement task and also the probability of ground-truth caption in a supervised task, simultaneously. Experiments have verified the advantages of our GMMP model on several benchmark data sets. © 2021 Elsevier B.V.","Graph convolutional networks; Meta-learning; Multi-granularity part-of-speech; Video captioning","Convolution; Convolutional neural networks; Security systems; Semantics; Convolutional networks; Graph convolutional network; Information carriers; Learn+; Metalearning; Multi-granularity; Multi-granularity part-of-speech; Part Of Speech; Speech information; Video captioning; Long short-term memory; article; attention; human; human experiment; probability; reinforcement (psychology); reward; speech; videorecording",Article,Scopus,2-s2.0-85119186542
"Yan M., Yang J., Chen C., Zhou J.T., Pan Y., Zeng Z.","55885243600;35194801900;57191481439;56335714100;57222403476;57194337704;","Enhanced gradient learning for deep neural networks",2022,"IET Image Processing","16","2",,"365","377",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118599561&doi=10.1049%2fipr2.12353&partnerID=40&md5=74115a91f5d47a93e3f4687d04fc632c","Deep neural networks have achieved great success in both computer vision and natural language processing tasks. How to improve the gradient flows is crucial in training very deep neural networks. To address this challenge, a gradient enhancement approach is proposed through constructing the short circuit neural connections. The proposed short circuit is a unidirectional neural connection that back propagates the sensitivities rather than gradients in neural networks from the deep layers to the shallow layers. Moreover, the short circuit is further formulated as a gradient truncation operation in its connecting layers, which can be plugged into the backbone models without introducing extra training parameters. Extensive experiments demonstrate that the deep neural networks, with the help of short circuit connection, gain a large margin of improvement over the baselines on both computer vision and natural language processing tasks. The work provides the promising solution to the low-resource scenarios, such as, intelligence transport systems of computer vision, question answering of natural language processing. © 2021 The Authors. IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology",,"Computer vision; Multilayer neural networks; Natural language processing systems; Circuit connections; Deep layer; Gradient flow; Gradient learning; Images processing; Large margins; Neural-networks; Shallowest layers; Training parameters; Transport systems; Deep neural networks",Article,Scopus,2-s2.0-85118599561
"Rogers A.W., Vega-Ramon F., Yan J., del Río-Chanona E.A., Jing K., Zhang D.","57324929400;57292465400;57222144061;56537019200;8256925800;56535576900;","A transfer learning approach for predictive modeling of bioprocesses using small data",2022,"Biotechnology and Bioengineering","119","2",,"411","422",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118532126&doi=10.1002%2fbit.27980&partnerID=40&md5=1772322d5ad3e0b59798863455604ae7","Predictive modeling of new biochemical systems with small data is a great challenge. To fill this gap, transfer learning, a subdomain of machine learning that serves to transfer knowledge from a generalized model to a more domain-specific model, provides a promising solution. While transfer learning has been used in natural language processing, image analysis, and chemical engineering fault detection, its application within biochemical engineering has not been systematically explored. In this study, we demonstrated the benefits of transfer learning when applied to predict dynamic behaviors of new biochemical processes. Two different case studies were presented to investigate the accuracy, reliability, and advantage of this innovative modeling approach. We thoroughly discussed the different transfer learning strategies and the effects of topology on transfer learning, comparing the performance of the transfer learning models against benchmark kinetic and data-driven models. Furthermore, strong connections between the underlying process mechanism and the transfer learning model's optimal structure were highlighted, suggesting the interpretability of transfer learning to enable more accurate prediction than a naive data-driven modeling approach. Therefore, this study shows a novel approach to effectively combining data from different resources for bioprocess simulation. © 2021 Wiley Periodicals LLC",,"Biochemistry; Chemical detection; Fault detection; Learning algorithms; Learning systems; Natural language processing systems; Structural optimization; Bioprocesses; Data problems; Data-driven model; Learning models; Microalgal photo-production; Modeling approach; Predictive models; Small data; Small data problem; Transfer learning; Benchmarking; article; biochemistry; bioprocess; prediction; reliability; simulation; transfer of learning",Article,Scopus,2-s2.0-85118532126
"Daszczuk W.B.","6507224767;","Graphic modeling in Distributed Autonomous and Asynchronous Automata (DA3)",2022,"Software and Systems Modeling","21","1",,"363","398",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118392298&doi=10.1007%2fs10270-021-00917-7&partnerID=40&md5=9d3ea8b1f2acc732f8e970bb2c6edbff","Automated verification of distributed systems becomes very important in distributed computing. The graphical insight into the system in the early and late stages of the project is essential. In the design phase, the visual input helps to articulate the collaborative distributed components clearly. The formal verification gives evidence of correctness or malfunction, but in the latter case, graphical simulation of counterexample helps for better understanding design errors. For these purposes, we invented Distributed Autonomous and Asynchronous Automata (DA3), which have the same semantics as the formal verification base—Integrated Model of Distributed Systems (IMDS). The IMDS model reflects the natural characteristics of distributed systems: unicasting, locality, autonomy, and asynchrony. Distributed automata have all of these features because they share the same semantics as IMDS. In formalism, the unified system definition has two views: the server view of the cooperating distributed nodes and the agent view of the migrating agents performing distributed computations. The automata have two formally equivalent forms that reflect two views: Server DA3 for observing servers exchanging messages, and Agent DA3 for tracking agents, which visit individual servers in their progress of distributed calculations. We present the DA3 formulation based on the IMDS formalism and their application to design and verify distributed systems in the Dedan environment. DA3 formalism is compared with other concepts of distributed automata known from the literature. © 2021, The Author(s).","Distributed automata; Distributed system modeling; Distributed systems; Formal methods; Graphic modeling","Automata theory; Distributed computer systems; Semantics; Asynchronous automata; Automated verification; Design phase; Distributed automaton; Distributed components; Distributed systems model; Graphics modelling; Integrated model of distributed systems; Late stage; Two views; Formal verification",Article,Scopus,2-s2.0-85118392298
"Islam M.S., Ali M.E., Kang Y.-B., Sellis T., Choudhury F.M., Roy S.","57226522221;57204152461;54393439600;7004880755;55744631400;57221142039;","Keyword aware influential community search in large attributed graphs",2022,"Information Systems","104",,"101914","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117765448&doi=10.1016%2fj.is.2021.101914&partnerID=40&md5=af6cf7033b10ba13e4e98d062f12517d","Influential community search (ICS) on a graph finds a closely connected group of vertices having a dominance over other groups of vertices. The ICS has many applications in recommendations, event organization, and so on. In this paper, we introduce a new variant of ICS, namely keyword-aware influential community query (KICQ), that finds the communities with the highest influential scores and whose keywords match with the query terms (a set of keywords) and predicates (AND or OR). It is challenging to find such communities from a large network as the traditional pre-computation approach is not applicable with the change of query terms at every instance of the search. To solve this problem, we design two efficient algorithms: (i) a branch-and-bound approach that exploits the bounds computed from already explored communities to prune the search space, and (ii) a novel index based approach that hierarchically organizes sub-communities and keywords with associated bounds to quickly identify the desired communities. We propose a new influence measure for a community that considers both the cohesiveness and influence of the community and eliminates the need for specifying values of internal parameters of a network. We present detailed experiments and a case study to demonstrate the effectiveness and efficiency of the proposed approaches. © 2021","Community search; Community search in attributed graph; Influential community search; Semantic keyword; Social network","Branch and bound method; Graph theory; Semantic Web; Social networking (online); Attributed graphs; Branch-and-bound approaches; Community search; Community search in attributed graph; Influential community search; Larger networks; Pre-computation; Query terms; Semantic keyword; Social network; Semantics",Article,Scopus,2-s2.0-85117765448
"Rohaan D., Topan E., Groothuis-Oudshoorn C.G.M.","57306258000;26642748200;57306100300;","Using supervised machine learning for B2B sales forecasting: A case study of spare parts sales forecasting at an after-sales service provider",2022,"Expert Systems with Applications","188",,"115925","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117723448&doi=10.1016%2fj.eswa.2021.115925&partnerID=40&md5=22000033cdf13db8d12c6474bc466a48","In this paper, we present a method to use advance demand information (ADI), taking the form of request for quotation (RFQ) data, in B2B sales forecasting. We apply supervised machine learning and Natural Language Processing techniques to analyze and learn from RFQs. We apply and test our approach in a case study at a large after-sales service and maintenance provider. After evaluation we found that our approach identifies ∼ 70% of actual sales (recall) with a precision rate of ∼ 50%, which represents a performance improvement of slightly more than a factor 2.5 over the current labor-intensive manual process at the service and maintenance provider. Our research contributes to literature by giving step-by-step guidance on incorporating artificial intelligence in B2B sales forecasting and revealing potential pitfalls along the way. Furthermore, our research gives an indication of the performance improvement that can be expected when adopting supervised machine learning into B2B sales forecasting. © 2021 The Author(s)","B2B sales forecasting; Imbalanced data; Information Extraction; Natural Language Processing (NLP); Prioritization on sales potential; Supervised machine learning","Information use; Learning algorithms; Natural language processing systems; Sales; Supervised learning; After-sales services; B2B sale forecasting; Case-studies; Imbalanced data; Information extraction; Natural language processing; Prioritization; Prioritization on sale potential; Sales forecasting; Supervised machine learning; Forecasting",Article,Scopus,2-s2.0-85117723448
"Martinez-Gil J., Chaves-Gonzalez J.M.","57188967922;24467192200;","Interpretable ontology meta-matching in the biomedical domain using Mamdani fuzzy inference",2022,"Expert Systems with Applications","188",,"116025","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116890532&doi=10.1016%2fj.eswa.2021.116025&partnerID=40&md5=5b5739df8f9c976b81cb40494825c96a","Ontology meta-matching techniques have been consolidated as one of the best approaches to face the problem of discovering semantic relationships between knowledge models that belong to the same domain but have been developed independently. After more than a decade of research, the community has reached a stage of maturity characterized by increasingly better results and aspects such as the robustness and scalability of solutions have been solved. However, the resulting models remain practically intelligible to a human operator. In this work, we present a novel approach based on Mamdani fuzzy inference exploiting a model very close to natural language. This fact has a double objective: to achieve results with high degrees of accuracy but at the same time to guarantee the interpretability of the resulting models. After validating our proposal with several ontological models popular in the biomedical field, we can conclude that the results obtained are promising. © 2021 Elsevier Ltd","Biomedical ontologies; Biomedical ontology matching; Knowledge engineering; Mamdani inference","Fuzzy inference; Natural language processing systems; Semantics; Biomedical domain; Biomedical ontologies; Biomedical ontology matching; Mamdani fuzzy inferences; Mamdani inference; Matching techniques; Matchings; Ontology matching; Ontology's; Ontology",Article,Scopus,2-s2.0-85116890532
"Wan B., Jiang W., Fang Y.-M., Zhu M., Li Q., Liu Y.","57195545076;56039225700;8435698900;57288648700;57289669700;57264548700;","Revisiting image captioning via maximum discrepancy competition",2022,"Pattern Recognition","122",,"108358","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116647674&doi=10.1016%2fj.patcog.2021.108358&partnerID=40&md5=44e728cd23198769caf674a735d7347e","Image captioning is a hot research topic bridging computer vision and natural language processing during the past several decades. It has achieved great progress with the help of large-scale datasets and deep learning techniques. Though the variety of image captioning models (ICMs), the performance of ICMs have got stuck in a bottleneck judging from the publicly published results. Considering the marginal performance gains brought by recent ICMs, we raise the following question: “what about the performances of the recent ICMs achieve on in-the-wild images? To clarify this question, we compare existing ICMs by evaluating their generalization ability. Specifically, we propose a novel method based on maximum discrepancy competition to diagnose existing ICMs. Firstly, we establish a new test set containing only informative images selected by adopting maximum discrepancy competition on the existing ICMs, from an arbitrary large-scale raw image set. Secondly, a small-scale and low-cost subjective annotation experiment is conducted on the new test set. Thirdly, we rank the generalization ability of the existing ICMs by comparing their performances on the new test set. Finally, the keys of different ICMs are demonstrated based on a detailed analysis of experimental results. Our analysis yields several interesting findings, including that 1) Using simultaneously low- and high-level object features may be an effective tool to boost the generalization ability for the Transformer based ICMs. 2) Self-attention mechanism may provide better modelling ability for inter- and intra-modal data than other attention-based mechanisms. 3) Constructing an ICM with a multistage language decoder may be a promising way to improve its performance. © 2021","Attention mechanism; Image captioning; Model comparison","Deep learning; Large dataset; Natural language processing systems; Attention mechanisms; Generalization ability; Hot research topics; Image captioning; Large-scale datasets; Learning techniques; Models comparisons; Performance; Performance Gain; Test sets; Modal analysis",Article,Scopus,2-s2.0-85116647674
"Huang N., Luo Y., Zhang Q., Han J.","57214954569;29767663600;55624487042;14522692900;","Discriminative unimodal feature selection and fusion for RGB-D salient object detection",2022,"Pattern Recognition","122",,"108359","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116630689&doi=10.1016%2fj.patcog.2021.108359&partnerID=40&md5=3edfacd69987b4f88a9090a2db5e645d","Most existing RGB-D salient object detectors make use of the complementary information of RGB-D images to overcome the challenging scenarios, e.g., low contrast, clutter backgrounds. However, these models generally neglect the fact that one of the input images may be poor in quality. This will adversely affect the discriminative ability of cross-modal features when the two channels are fused directly. To address this issue, a novel end-to-end RGB-D salient object detection model is proposed in this paper. At the core of our model is a Semantic-Guided Modality-Weight Map Generation (SG-MWMG) sub-network, producing modality-weight maps to indicate which regions on both modalities are high-quality regions, given input RGB-D images and the guidance of their semantic information. Based on it, a Bi-directional Multi-scale Cross-modal Feature Fusion (Bi-MCFF) module is presented, where the interactions of the features across different modalities and scales are exploited by using a novel bi-directional structure for better capturing cross-scale and cross-modal complementary information. The experimental results on several benchmark datasets verify the effectiveness and superiority of the proposed method over some state-of-the-art methods. © 2021","Discriminative unimodal feature selection; Multi-scale cross-modal feature fusion; RGB-D salient object detection; Semantic information","Object detection; Object recognition; Semantics; Cross-modal; Discriminative unimodal feature selection; Features fusions; Features selection; Multi-scale cross-modal feature fusion; Multi-scales; RGB-D salient object detection; Salient object detection; Semantics Information; Unimodal; Feature extraction",Article,Scopus,2-s2.0-85116630689
"Loscos D., Martí-Oliet N., Rodríguez I.","57279514700;6602757423;21835143600;","Generalization and completeness of stochastic local search algorithms",2022,"Swarm and Evolutionary Computation","68",,"100982","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116075154&doi=10.1016%2fj.swevo.2021.100982&partnerID=40&md5=256411608911467eb0a755b614427192","We generalize Stochastic Local Search (SLS) heuristics into a unique formal model. This model has two key components: a common structure designed to be as large as possible and a parametric structure intended to be as small as possible. Each heuristic is obtained by instantiating the parametric part in a different way. Particular instances for Genetic Algorithms (GA), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO) are presented. Then, we use our model to prove the Turing-completeness of SLS algorithms in general. The proof uses our framework to construct a GA able to simulate any Turing machine. This Turing-completeness implies that determining any non-trivial property concerning the relationship between the inputs and the computed outputs is undecidable for GA and, by extension, for the general set of SLS methods (although not necessarily for each particular method). Similar proofs are more informally presented for PSO and ACO. © 2021","Computability; Evolutionary computation; Formal languages; Generalization; Operational semantics; Stochastic local search; Swarm intelligence; Turing-completeness","Ant colony optimization; Calculations; Genetic algorithms; Local search (optimization); Machinery; Particle swarm optimization (PSO); Semantic Web; Semantics; Stochastic models; Stochastic systems; Swarm intelligence; Turing machines; Ant colonies; Colony optimization; Common structures; Formal modeling; Generalisation; Local search algorithm; Local search heuristics; Operational semantics; Stochastic local searches; Turing completeness; Formal languages",Article,Scopus,2-s2.0-85116075154
"Singh U.P., Singh K.P., Thakur M.","57216689092;56227275100;36017796100;","NucNormZSL: nuclear norm-based domain adaptation in zero-shot learning",2022,"Neural Computing and Applications","34","3",,"2353","2374",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116065871&doi=10.1007%2fs00521-021-06461-1&partnerID=40&md5=0abe2f6c569d85dc0aa45cf4d311ea2d","The ability of human beings to recognize novel concepts has attracted significant attention in the research community. Zero-shot learning, also known as zero-data learning, seeks to build models that can recognize novel class instances even without “seeing” them during their training; some description of novel classes is, however, required. In this work, we pose zero-shot learning as a dictionary learning problem to learn the projection functions from feature to semantic space as dictionaries in the source and target domains. To get a robust projection mapping in the source domain, we introduce nuclear norm to achieve low-rank solutions. Further, this low-ranked dictionary is used as a regularizer in the target domain so that the knowledge contained in the source dictionary is utilized in the target domain. In our experiments, source domain contains the seen class images, their ground truths and attribute representations while corresponding data for unseen class are contained in the target domain. We also use label propagation as an alternative to the nearest neighbor search in the semantic space for class-label assignment. Our proposed model, NucNormZSL, achieves state-of-the-art results for the Large Attribute (LAD) dataset and remains fairly competitive with existing approaches on Animals with Attributes-2 (AWA2), Caltech-UCSD Birds (CUB) and SUN datasets in the conventional setting and generalized setting. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Dictionary learning; Domain adaptation; Generalized zero-shot recognition; Indirect attribute prediction; Nuclear norm minimization; Projection domain shift","Large dataset; Learning systems; Semantics; Dictionary learning; Domain adaptation; Generalized zero-shot recognition; Human being; Indirect attribute prediction; Nuclear norm minimizations; Projection domain; Projection domain shift; Semantic Space; Target domain; Nearest neighbor search",Article,Scopus,2-s2.0-85116065871
"Zhang D., Meng Y., Zhang C., Zhou J.","57222554826;35205979400;57278916900;57208359925;","From multiple solitons to noise-like pulse in a passively mode-locked erbium-doped fiber laser",2022,"Optics Communications","504",,"127468","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115991387&doi=10.1016%2fj.optcom.2021.127468&partnerID=40&md5=7726ccb0b8579e743356c35a576dc644","Operation states of multiple solitons (MS), noise-like pulses (NLP), and a special pattern of MS coexist with NLP have been observed in a passively mode-locked erbium-doped fiber laser. By increasing the pump power or adjusting the polarization state of the cavity, the operation state from MS to NLP will go through three states: pure MS, MS & NLP, and pure NLP. The experiment results show that the state transition from pure MS to MS & NLP is mutational, while the state transition from MS & NLP to pure NLP is gradual. To our knowledge, this is the first time detailed exploration of the conversion process from MS to NLP operation in mode-locked fiber laser. © 2021 Elsevier B.V.","Multiple solitons; Noise-like pulse; Nonlinear optics; Passive mode locking","Erbium; Fiber lasers; Mode-locked fiber lasers; Natural language processing systems; Passive mode locking; Solitons; Conversion process; Erbium-doped fiber lasers; Multiple soliton; Noise-like pulse; Operation state; Passive mode-locking; Passively mode-locked; Polarization state; Pump power; State transitions; Nonlinear optics",Article,Scopus,2-s2.0-85115991387
"Huan J.L., Sekh A.A., Quek C., Prasad D.K.","57274258800;57220090017;8869387400;35746873900;","Emotionally charged text classification with deep learning and sentiment semantic",2022,"Neural Computing and Applications","34","3",,"2341","2351",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115839677&doi=10.1007%2fs00521-021-06542-1&partnerID=40&md5=846a7f5e1d1975ada3d9eeea2eeb7edf","Text classification is one of the widely used phenomena in different natural language processing tasks. State-of-the-art text classifiers use the vector space model for extracting features. Recent progress in deep models, recurrent neural networks those preserve the positional relationship among words achieve a higher accuracy. To push text classification accuracy even higher, multi-dimensional document representation, such as vector sequences or matrices combined with document sentiment, should be explored. In this paper, we show that documents can be represented as a sequence of vectors carrying semantic meaning and classified using a recurrent neural network that recognizes long-range relationships. We show that in this representation, additional sentiment vectors can be easily attached as a fully connected layer to the word vectors to further improve classification accuracy. On the UCI sentiment labelled dataset, using the sequence of vectors alone achieved an accuracy of 85.6%, which is better than 80.7% from ridge regression classifier—the best among the classical technique we tested. Additional sentiment information further increases accuracy to 86.3%. On our suicide notes dataset, the best classical technique—the Naíve Bayes Bernoulli classifier, achieves accuracy of 71.3%, while our classifier, incorporating semantic and sentiment information, exceeds that at 75% accuracy. © 2021, The Author(s).","LSTM; Sentiment analysis; Text classification","Classification (of information); Information retrieval systems; Long short-term memory; Regression analysis; Semantics; Vector spaces; Vectors; Classical techniques; Extracting features; LSTM; Positional relationship; Recent progress; Sentiment analysis; State of the art; Text classification; Text classifiers; Vector space models; Sentiment analysis",Article,Scopus,2-s2.0-85115839677
"Jha A.K., Mithun S., Sherkhane U.B., Jaiswar V., Shi Z., Kalendralis P., Kulkarni C., Dinesh M.S., Rajamenakshi R., Sunder G., Purandare N., Wee L., Rangarajan V., Van Soest J., Dekker A.","57197687905;57163516700;57221671059;57221677446;57204588291;57204593162;57273601800;57204668762;25961228200;55920010400;7003293572;57213352753;6603448872;56156330000;57225379184;","Implementation of Big Imaging Data Pipeline Adhering to FAIR Principles for Federated Machine Learning in Oncology",2022,"IEEE Transactions on Radiation and Plasma Medical Sciences","6","2",,"207","213",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115751945&doi=10.1109%2fTRPMS.2021.3113860&partnerID=40&md5=5f59c0de39dade6fe16a82050e02c554","Cancer is a fatal disease and one of the leading causes of death worldwide. The cure rate in cancer treatment remains low; hence, cancer treatment is gradually shifting toward personalized treatment. Artificial intelligence (AI) and radiomics have been recognized as one of the potential areas of research in personalized medicine in oncology. Several researchers have identified the capabilities of AI and radiomics to characterize phenotype and there by predict the outcome of treatment in oncology. Although AI and radiomics have shown promising initial results in diagnosis and treatment in oncology, these technologies are also facing challenges of standardization and scalability. In the last few years, researchers have been trying to develop a research infrastructure for federated machine learning that increases the usability of Big Data for clinical research. These research infrastructures are based on the findable, accessible, interoperable, and reusable (i.e., FAIR) data principles. The India-Dutch 'big imaging data approach for oncology in a Netherlands India collaboration' (BIONIC) is a jointly funded initiative by the Dutch Research Council (NWO) and the Indian Ministry of Electronics and Information Technology (MeitY), aiming to introduce radiomic-based research into clinical environments using federated machine learning on geographically dispersed collections of FAIR data. This article described a prototype end-to-end research infrastructure implemented through the BIONIC partnership into a leading cancer care public hospital in India. © 2017 IEEE.","accessible; and reusable (FAIR) data; Artificial intelligence (AI); findable; interoperable; machine learning; natural language processing (NLP); radiomics","Big data; Clinical research; Diagnosis; Diseases; Hospitals; Learning algorithms; Machine learning; Medical imaging; Natural language processing systems; Oncology; Biomedical imaging; Data pipelines; FAIR data.; Fatal disease; Features extraction; Imaging data; Machine-learning; Radiomic; Research infrastructure; Data mining",Article,Scopus,2-s2.0-85115751945
"Shi W., Liu H., Liu M.","57207096415;55769747023;56118690300;","Image-to-video person re-identification using three-dimensional semantic appearance alignment and cross-modal interactive learning",2022,"Pattern Recognition","122",,"108314","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115657178&doi=10.1016%2fj.patcog.2021.108314&partnerID=40&md5=c42cccece107c21950a1885dc2b5e21d","Image-to-video person re-identification (I2V ReID), which aims to retrieve human targets between image-based queries and video-based galleries, has recently become a new research focus. However, the appearance misalignment and modality misalignment in both images and videos caused by pose variations, camera views, misdetections, and different data types, make I2V ReID still challenging. To this end, we propose a deep I2V ReID pipeline based on three-dimensional semantic appearance alignment (3D-SAA) and cross-modal interactive learning (CMIL) to address the aforementioned two challenges. Specifically, in the 3D-SAA module, the aligned local appearance images extracted by dense 3D human appearance estimation are in conjunction with global image and video embedding streams to learn more fine-grained identity features. The aligned local appearance images are further semantically aggregated by the proposed multi-branch aggregation network to weaken the negligible body parts. Moreover, to overcome the influence of modality misalignment, a CMIL module enables the communication between global image and video streams by interactively propagating the temporal information in videos to the channels of image feature maps. Extensive experiments on challenging MARS, DukeMTMC-VideoReID and iLIDS-VID datasets, show the superiority of our approach. © 2021","Appearance alignment; Cross-modal learning; Person re-identification","Educational technology; Image processing; Learning systems; Semantics; Appearance alignment; Camera view; Cross-modal; Cross-modal learning; Datatypes; Image-based; Interactive learning; Person re identifications; Pose variation; Research focus; Alignment",Article,Scopus,2-s2.0-85115657178
"Qian K., Tian L.","43561729800;57221994603;","A topic-based multi-channel attention model under hybrid mode for image caption",2022,"Neural Computing and Applications","34","3",,"2207","2216",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115633500&doi=10.1007%2fs00521-021-06557-8&partnerID=40&md5=2bf5bedb0e9c69f3c31b3a088fc6bfb1","Automatically generating captions of an image is not closely related to every spatial area of the visual information, but always related to the topic of the image expression. Aiming at the decoupling problem of visual spatial feature attention and semantic decoder, a topic-based multi-channel attention model (TMA) under hybrid mode for image caption is proposed. First, natural language processing (NLP) technology is used to preprocess the caption references, including filtering stop words, analyzing word frequency and constructing a semantic network graph with node labels. Then, combined with the image features extracted by the convolutional neural network (CNN), a semantic perception network is designed to achieve cross-domain prediction from image to topic. Next, a topic-based multi-channel attention fusion mechanism is proposed to realize image-text attention fusion representation under the joint action of the global spatial features of the image, the local semantic features of the graph nodes and the hidden layer features of the long short-term memory (LSTM) decoder. Finally, multi-task loss function is used to train the TMA. Experimental results show that the proposed model has better evaluation performance with topic-focused attention than state-of-the-art (SOTA) methods. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Attention fusion mechanism; Image caption; Multi-channel attention; Topic-based","Convolutional neural networks; Decoding; Image fusion; Long short-term memory; Natural language processing systems; Network coding; Attention fusion mechanism; Attention model; Fusion mechanism; Hybrid mode; Image caption; Multi channel; Multi-channel attention; Spatial area; Spatial features; Topic-based; Semantics",Article,Scopus,2-s2.0-85115633500
"Gao J., Luo X., Wang H.","57220206691;23479777500;57207240009;","Chinese causal event extraction using causality-associated graph neural network",2022,"Concurrency and Computation: Practice and Experience","34","3","e6572","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115005644&doi=10.1002%2fcpe.6572&partnerID=40&md5=9655ffe676e4b76f438cfb83cded3e1a","Causal event extraction (CEE) aims to identify and extract cause-effect event pairs from texts, which is a fundamental task in natural language processing. Recent research treat CEE as a sequence labeling problem. However, the linguistic complexity and ambiguity of textual description results in the low accuracy of extractors. To address the above issues, considering the prior knowledge like the causal network constructed based on the causal indicators, which can represent information transition between cause and effect, may helpful for CEE. In this article, we propose causality-associated graph neural network to incorporate in-domain knowledge by taking important causal words into account. External causal knowledge is modeled as causal associated graph (CAG). Then we use graph neural networks (GNN) to capture the complex relationship of intraevent mentions and interevent causality in a sentence based on the relationship obtained from CAG. Finally, sentence sequence and prior causal knowledge of GNN embedding are fed into multiscaled convolution and bidirectional long short-term memory networks. Experimental results on two datasets show that our method outperforms the state-of-the-art baseline. © 2021 John Wiley & Sons Ltd.",,"Complex networks; Extraction; Information analysis; Natural language processing systems; Cause and effects; Complex relationships; Graph neural networks; Linguistic complexity; NAtural language processing; Recent researches; Sequence Labeling; Textual description; Neural networks",Article,Scopus,2-s2.0-85115005644
"Han J., Sarica S., Shi F., Luo J.","57190442322;57205251791;57201642754;36844141600;","Semantic Networks for Engineering Design: State of the Art and Future Directions",2022,"Journal of Mechanical Design, Transactions of the ASME","144","2","020802","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114859565&doi=10.1115%2f1.4052148&partnerID=40&md5=52a96a13e26c1404b5d7bb849131c01e","In the past two decades, there has been increasing use of semantic networks in engineering design for supporting various activities, such as knowledge extraction, prior art search, idea generation, and evaluation. Leveraging large-scale pre-trained graph knowledge databases to support engineering design-related natural language processing (NLP) tasks has attracted a growing interest in the engineering design research community. Therefore, this study aims to provide a survey of the state-of-the-art semantic networks for engineering design and propositions of future research to build and utilize large-scale semantic networks as knowledge bases to support engineering design research and practice. The survey shows that WordNet, ConceptNet, and other semantic networks, which contain common-sense knowledge or are trained on non-engineering data sources, are primarily used by engineering design researchers to develop methods and tools. Meanwhile, there are emerging efforts in constructing engineering and technical-contextualized semantic network databases, such as B-Link and TechNet, through retrieving data from technical data sources and employing unsupervised machine learning approaches. On this basis, we recommend six strategic future research directions to advance the development and uses of large-scale semantic networks for artificial intelligence applications in engineering design. © 2022 American Society of Mechanical Engineers (ASME). All rights reserved.",,"Arts computing; Engineering education; Engineering research; Knowledge management; Natural language processing systems; Semantic Web; Semantics; Surveys; Commonsense knowledge; Engineering design; Engineering design researches; Future research directions; Knowledge database; Knowledge extraction; NAtural language processing; Unsupervised machine learning; Knowledge representation",Article,Scopus,2-s2.0-85114859565
"Zheng Q., Guo Y., Wang Z., Andrasik F., Kuang Z., Li J., Xu S., Hu X.","57256354200;55712519900;57257060900;57204668145;57256354300;57201011746;57208179705;7404710283;","Exploring Weibo users’ attitudes toward lesbians and gays in Mainland China: A natural language processing and machine learning approach",2022,"Computers in Human Behavior","127",,"107021","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114845298&doi=10.1016%2fj.chb.2021.107021&partnerID=40&md5=f0dd10e57966bb0635e0e63f320a76ac","Informed by the ABC model of attitude, this study employed natural language processing and machine learning techniques to explore the status quo, trends, and features of Weibo users’ attitudes toward individuals who are lesbian or gay based on 1,813,218 Weibo retrieved posts that pertained to lesbians and gays from January 1, 2010, to December 31, 2019. Although the general attitudes, feeling/emotional reactions, and levels of support for the rights of lesbians and gays were identified as relatively positive for Weibo users and became even more so over time, the expressed knowledge of lesbians and gays was relatively inaccurate and the four aspects of attitude varied significantly as a function of individual-level variables (such as gender, age, education, sexual orientation, and verification type). The implications of these findings and suggestions that may improve the public acceptance of lesbians and gays are discussed. © 2021 Elsevier Ltd","Attitude; China; Gay; Lesbian; Weibo","Learning algorithms; Machine learning; Natural language processing systems; ABC model; Attitude; China; Gay; Lesbian; Machine learning approaches; Machine learning techniques; Mainland chinas; Status quo; User attitudes; Social networking (online); adult; article; China; education; female; gender; homosexual female; homosexual male; human; human experiment; machine learning; male; natural language processing; sexual orientation",Article,Scopus,2-s2.0-85114845298
"Gasparetti F.","6506843445;","Discovering prerequisite relations from educational documents through word embeddings",2022,"Future Generation Computer Systems","127",,,"31","41",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114786966&doi=10.1016%2fj.future.2021.08.021&partnerID=40&md5=7137b54542faed47b2e5cc567eeda0b0","Inferring prerequisite relations among educational documents, in terms of prior knowledge required to understand and complete assignments about certain topics, is a crucial task for instructional designers and teachers. Massive open online courses, electronic textbooks, public encyclopedias and repositories of learning objects and other forms of informative content create a huge availability of educational material, which can be exploited in online platforms for distance education, both for recommending specific resources and personalized learning paths. But public taxonomies of prerequisites, or learning object metadata useful to trace down prerequisites are not generally available. A description of a new approach for prerequisite discovering in educational documents is given. It is based on word embeddings, that is, statistical language models for the representation of text-based learning objects in low-dimensional latent spaces. It takes advantage of the latent representations to identify prerequisites in a binary classification setting. The accuracy of the approach is validated by means of an experimental benchmark covering multiple datasets of educational material. © 2021 Elsevier B.V.","Curriculum sequencing; E-learning; Learning object; Prerequisite relations; Word embeddings","Curricula; E-learning; Natural language processing systems; Curriculum sequencing; E - learning; Educational materials; Embeddings; Instructional designer; Learning objects; Prerequisite relation; Prior-knowledge; Teachers'; Word embedding; Embeddings",Article,Scopus,2-s2.0-85114786966
"Lin B., Zhu Y., Liang X.","57219625419;57198948667;55926362100;","Atom correlation based graph propagation for scene graph generation",2022,"Pattern Recognition","122",,"108300","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114701007&doi=10.1016%2fj.patcog.2021.108300&partnerID=40&md5=8049cf0e09ea737eeb90bd9f4a1aaf9f","Long-tailed distribution in the dataset is one of the major problems of the scene graph generation task. Previous methods attempt to alleviate this by introducing human commonsense knowledge in the form of statistical correlations between object pairs. However, the reasoning path they used is usually composable and the prior knowledge they employed is generally image-specific, making the knowledge learning less flexible, stable and holistic. In this paper, we propose Atom Correlation Based Graph Propagation (AC-GP) for the scene graph generation task. Specifically, diverse atom correlations between objects and their relationships are explored by separating relationships to form new semantic nodes and decomposing the compound reasoning paths. Based on these atom correlations, the knowledge graphs are introduced for the feature enhancement by information propagating in the global category space. By exploiting atom correlations, the introduced prior knowledge can be more common and easy to learn. Moreover, propagating the knowledge in the global category space enables the model aware of more comprehensive and holistic knowledge. As a result, the model capacity and stability can be effectively improved to mine infrequent and missed relationships. Experimental results on two benchmark datasets: Visual Relation Detection (VRD) and Visual Genome (VG) show the superiority of the proposed AC-GP over strong baseline methods. © 2021 Elsevier Ltd","Atom correlation; Category space; Knowledge graph; Long-tailed distribution; Scene graph generation","Atoms; Knowledge management; Semantics; Atom correlation; Category space; Commonsense knowledge; Graph generation; Knowledge graphs; Long-tailed distributions; Prior-knowledge; Scene graph generation; Scene-graphs; Statistical correlation; Knowledge graph",Article,Scopus,2-s2.0-85114701007
"Zhang L., Wu X.","57218524208;55714996700;","Multi-task framework based on feature separation and reconstruction for cross-modal retrieval",2022,"Pattern Recognition","122",,"108217","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114600720&doi=10.1016%2fj.patcog.2021.108217&partnerID=40&md5=2a8c19e532e766f4072b94726bda942a","Cross-modal retrieval has become a hot research topic in both computer vision and natural language processing areas. Learning intermediate common space for features of different modalities has become one of mainstream methods. In this paper, we propose a novel multi-task framework based on feature separation and reconstruction (mFSR) for cross-modal retrieval based on common space learning methods, which introduces feature separation module to deal with information asymmetry between different modalities, and introduces image and text reconstruction module to improve the quality of feature separation module. Extensive experiments on MS-COCO and Flickr30K datasets demonstrate that feature separation and specific information reconstruction can significantly improve the baseline performance of cross-modal image-caption retrieval. © 2021","Cross-modal retrieval; Feature separation; Image reconstruction; Text reconstruction","Image enhancement; Learning algorithms; Learning systems; Natural language processing systems; Separation; Common spaces; Cross-modal; Cross-modal retrieval; Feature reconstruction; Feature separation; Hot research topics; Images reconstruction; Multi tasks; Separation module; Text reconstruction; Image reconstruction",Article,Scopus,2-s2.0-85114600720
"E Elahi G.M.M., Yang Y.-H.","57211979983;57196350724;","Online learnable keyframe extraction in videos and its application with semantic word vector in action recognition",2022,"Pattern Recognition","122",,"108273","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114308867&doi=10.1016%2fj.patcog.2021.108273&partnerID=40&md5=d444478e20850d9e8ef84e584bf57d3d","Video processing has become a popular research direction in computer vision due to its various applications such as video summarization, action recognition, etc. Recently, deep learning-based methods have achieved impressive results in action recognition. However, these methods need to process a full video sequence to recognize the action, even though many of the frames in the video sequence are similar and non-essential to recognizing a particular action. Additionally, these non-essential frames increase the computational cost and can confuse a method in action recognition. Instead, the important frames called keyframes not only are helpful in recognizing an action but also can reduce the processing time of each video sequence in classification or in other applications, e.g. summarization. As well, current methods in video processing have not yet been demonstrated in an online fashion. Motivated by the above, we propose an online learnable module for keyframe extraction. This module can be used to select key shots in video and thus, can be applied to video summarization. The extracted keyframes can be used as input to any deep learning-based classification model to recognize action. We also propose a plugin module to use the semantic word vector as input along with keyframes and a novel train/test strategy for the classification models. To our best knowledge, this is the first time such an online module and train/test strategy have been proposed. The experimental results on many commonly used datasets in video summarization and in action recognition have demonstrated the effectiveness of the proposed module. © 2021 Elsevier Ltd","Action recognition; Learnable threshold; Online keyframes; Video summarization","Deep learning; Extraction; Semantics; Video signal processing; Action recognition; Key-frame extraction; Key-frames; Learnable threshold; Non essential; Online keyframe; Video processing; Video sequences; Video summarization; Word vectors; Video recording",Article,Scopus,2-s2.0-85114308867
"Žagar A., Robnik-Šikonja M.","57221714857;55900495300;","Cross-lingual transfer of abstractive summarizer to less-resource language",2022,"Journal of Intelligent Information Systems","58","1",,"153","173",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114300014&doi=10.1007%2fs10844-021-00663-8&partnerID=40&md5=f5452c88fecfc7fc4dca594e2b15b32a","Automatic text summarization extracts important information from texts and presents the information in the form of a summary. Abstractive summarization approaches progressed significantly by switching to deep neural networks, but results are not yet satisfactory, especially for languages where large training sets do not exist. In several natural language processing tasks, a cross-lingual model transfer is successfully applied in less-resource languages. For summarization, the cross-lingual model transfer was not attempted due to a non-reusable decoder side of neural models that cannot correct target language generation. In our work, we use a pre-trained English summarization model based on deep neural networks and sequence-to-sequence architecture to summarize Slovene news articles. We address the problem of inadequate decoder by using an additional language model for the evaluation of the generated text in target language. We test several cross-lingual summarization models with different amounts of target data for fine-tuning. We assess the models with automatic evaluation measures and conduct a small-scale human evaluation. Automatic evaluation shows that the summaries of our best cross-lingual model are useful and of quality similar to the model trained only in the target language. Human evaluation shows that our best model generates summaries with high accuracy and acceptable readability. However, similar to other abstractive models, our models are not perfect and may occasionally produce misleading or absurd content. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Abstractive summarization; Automatic summarization; Cross-lingual embeddings; Deep neural networks; Language models; Text generation","Decoding; Deep neural networks; Natural language processing systems; Neural networks; Automatic evaluation; Automatic text summarization; Human evaluation; Language model; NAtural language processing; Sequence architectures; Summarization models; Target language; Quality control",Article,Scopus,2-s2.0-85114300014
"Zeng B., Yang H., Liu S., Xu M.","57196723371;57210557953;57204814028;57223128725;","Learning for target-dependent sentiment based on local context-aware embedding",2022,"Journal of Supercomputing","78","3",,"4358","4376",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114040516&doi=10.1007%2fs11227-021-04047-1&partnerID=40&md5=488735614240e40ec2b1ad9c815905a5","Target-dependent sentiment classification is a fine-grained task of natural language processing to analyze the sentiment polarity of the targets. In order to address the difficulty of locating important sentiment information of targeted sentiment classification, recent research mostly applies attention mechanisms to capture the information of important context words, while the attention mechanism is subject to many drawbacks, e.g., dependent on network architecture and expensive. Recent studies show the significant effect of the local context focus (LCF) mechanism in capturing the relatedness between a target’s sentiment and its local context. However, the LCF simply applies the fusion of global and local context features to classify sentiment, neglecting to empower the network to be aware of deep information of local context. In this paper, we propose a novel local context-aware network (LCA-Net) based on the local context embedding (LCE). Moreover, accompanied by the sentiment classification loss, the local context prediction (LCP) loss is proposed to enhance the LCE. The experimental results on three commonly used datasets, i.e., the Laptop and Restaurant datasets from SemEval-2014 and a Twitter social dataset, show that all the LCA-Net variants achieve promising performance improvement compared to existing approaches in extracting local context features. Besides, we implement the LCA-Net with different neural networks, validating the transferability of LCA architecture. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","BERT; Local context embedding; Local context prediction; Target-dependent sentiment classification","Embeddings; Life cycle; Natural language processing systems; Network architecture; Theorem proving; Attention mechanisms; Context-word; Fine grained; Local contexts; NAtural language processing; Recent researches; Sentiment classification; Classification (of information)",Article,Scopus,2-s2.0-85114040516
"Lin Y., Liu Z., Gao X.","55265036200;57192586797;7403873062;","Sensitivity of N400 effect during speech comprehension under the uni- And bi-modality conditions",2022,"Tsinghua Science and Technology","27","1","9515707","141","149",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113709423&doi=10.26599%2fTST.2021.9010008&partnerID=40&md5=34d2f2eb6998fdfb9c030f783ea05831","N400 is an objective electrophysiological index in semantic processing for brain. This study focuses on the sensitivity of N400 effect during speech comprehension under the uni- and bi-modality conditions. Varying the Signal-to-Noise Ratio (SNR) of speech signal under the conditions of Audio-only (A), Visual-only (V, i.e., lip-reading), and Audio-Visual (AV), the semantic priming paradigm is used to evoke N400 effect and measure the speech recognition rate. For the conditions A and high SNR AV, the N400 amplitudes in the central region are larger; for the conditions of V and low SNR AV, the N400 amplitudes in the left-frontal region are larger. The N400 amplitudes of frontal and central regions under the conditions of A, AV, and V are consistent with speech recognition rate of behavioral results. These results indicate that audio-cognition is better than visual-cognition at high SNR, and visual-cognition is better than audio-cognition at low SNR. © 1996-2012 Tsinghua University Press.","audio-visual integration; audio-visual speech; auditory noise; Signal-to-Noise Ratio (SNR)","Audio acoustics; Behavioral research; Electrophysiology; Semantics; Speech; Speech recognition; Frontal regions; High SNR; Lip reading; Low SNR; Semantic primings; Semantic processing; Speech signals; Visual Cognition; Signal to noise ratio",Article,Scopus,2-s2.0-85113709423
"Xie W., Yu H., Li Y., Dai M., Long X., Li N., Wang Y.","57211517258;57200608128;57246768300;57208304847;57216256259;56723160100;35207058700;","Estimation of entity-level land use and its application in urban sectoral land use footprint: A bottom-up model with emerging geospatial data",2022,"Journal of Industrial Ecology","26","1",,"309","322",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113606739&doi=10.1111%2fjiec.13191&partnerID=40&md5=627787e7aed2f65834c52a9cc8888b95","Land is an essential resource tomaintain the functioning of the socio-economic system. Due to sectoral land data limitations, previous studies were primarily restricted to a coarse sectoral level or focused mainly on the global and national scales. However,fine-scale land use data are required to provide tailored implications for municipal sustainable development. With emerging geographic data and novel methods, including point of interest data, road network data, and natural language processing, a bottom-up model is developed to estimate the entity-level artificial impervious land use. Then, we conducted a case study in Shanghai to investigate the spatial features, footprints, and intensities of sectoral land use. Our results indicated that 42 sectors in Shanghai had diverse spatial patterns. The transportation sector had the highest level of agglomeration among all sectors, and the manufacturing industry's adjacent land patches had higher sectoral heterogeneities than the service sector. The transportation sector had the largest direct and embodied land use footprint. The residential-related sectors had higher land use intensities, while the high value-added service sectors showed lower land use intensities. Our study indicates that this model offers a novel way of extracting entity-level spatial land use information and is applicable for socio-economic metabolism research. Future studies could incorporate remote sensing images and multiple databases to achieve higher resolution. © 2021 by Yale University.","industrial ecology; input–output analysis; land use; natural language processing; socio-economic metabolism; spatial analysis","Economics; Metropolitan area networks; Motor transportation; Natural language processing systems; Remote sensing; Service industry; Higher resolution; Manufacturing industries; NAtural language processing; Remote sensing images; Road network data; Socio-economic systems; Transportation sector; Value added service; Land use; data set; estimation method; land use; remote sensing; satellite imagery; service sector; spatiotemporal analysis; China; Shanghai",Article,Scopus,2-s2.0-85113606739
"Wu J., Du J., Wang F., Yang C., Jiang X., Hu J., Yin B., Zhang J., Dai L.","57226131591;18041858900;57230519300;57226877559;57230910600;56461667900;57230712000;57191866675;57230910700;","A multimodal attention fusion network with a dynamic vocabulary for TextVQA",2022,"Pattern Recognition","122",,"108214","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113399982&doi=10.1016%2fj.patcog.2021.108214&partnerID=40&md5=fc6ac6cc7f8644fb5b32c3a4b14ca59f","Visual question answering (VQA) is a well-known problem in computer vision. Recently, Text-based VQA tasks are getting more and more attention because text information is very important for image understanding. The key to this task is to make good use of text information in the image. In this work, we propose an attention-based encoder-decoder network that combines the multimodal information of visual, linguistic, and location features together. By using the attention mechanism to focus on key features to the question, our multimodal feature fusion can provide more accurate information to improve the performance. Furthermore, we present a decoder with attention map loss, which can not only predict complex answers but also deal with a dynamic vocabulary to reduce the decoding space. Compared with softmax-based cross entropy loss which can only handle a fixed-length vocabulary, the attention map loss significantly improves the accuracy and efficiency. Our method achieved the first place of all three tasks in the ICDAR2019 robust reading challenge on scene text visual question answering (ST-VQA). © 2021 Elsevier Ltd","Attention map; Dynamic vocabulary; Multimodal fusion; ST-VQA","Natural language processing systems; Attention map; Dynamic vocabulary; Encoder-decoder; Multi-modal; Multi-modal fusion; Multi-modal information; Question Answering; Question Answering Task; Scene text visual question answering; Text information; Decoding",Article,Scopus,2-s2.0-85113399982
"Meesala S.R., Subramanian S.","57230338700;57213723241;","Feature based opinion analysis on social media tweets with association rule mining and multi-objective evolutionary algorithms",2022,"Concurrency and Computation: Practice and Experience","34","3","e6586","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113353050&doi=10.1002%2fcpe.6586&partnerID=40&md5=2699d639c3d0b96640e6dcaa6019f665","Social media platform has achieved wide popularity in presenting the user-generated information online. The proliferation of user-generated content through social networking sites can enhance the existing transportation system. This work adds to the existing research by proposing a novel system to assess transit rider's reviews on the quality of transport services using Twitter information. A novel framework of a multi-objective evolutionary approach with association rule mining is proposed for feature-based opinion analysis on social media transit reviews. Feature-based opinion analysis is performed in two steps such as feature extraction and opinion analysis. First, the corpus of association rules is generated with opinion analysis, morphological and syntactic analysis, and semantic representation. Then, multi-objective flower pollination, multi-objective gray wolf, multi-objective moth flame, and multi-objective cat swarm optimization are applied to discover high-quality association rules on the transit user's opinion. The experimental results indicate that multi-objective cat swarm optimization using association rule mining performs better in terms of confidence, coverage, the interestingness of the rules, computational time, mean average support, and mean average confidence than the other proposed as existing approaches. It is observed that MCSO-ARM achieved an improvement in the range of 0.21%–0.27% in terms of confidence, 0.10%–0.13% in coverage, 0.10%–0.25% in interestingness, the computational time of 27.5 s, mean average support value of 0.10%–0.30% and mean of 0.20%–0.30% compared to the existing MPSO-ARM and other proposed MGWO-ARM, MMFO-ARM, and MFPO-ARM approaches. © 2021 John Wiley & Sons Ltd.",,"Association rules; Data mining; Multiobjective optimization; Semantics; Social networking (online); Syntactics; Computational time; Multi objective evolutionary algorithms; Multi-objective evolutionary; Semantic representation; Social media platforms; Social networking sites; Transportation system; User-generated content; Evolutionary algorithms",Article,Scopus,2-s2.0-85113353050
"Xu X., Gao T., Wang Y., Xuan X.","56194017200;57226888525;57218520212;57226889384;","Event temporal relation extraction with attention mechanism and graph neural network",2022,"Tsinghua Science and Technology","27","1","9515702","79","90",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113206803&doi=10.26599%2fTST.2020.9010063&partnerID=40&md5=24e9619bcc3a1ada0a83c213b2e3498e","Event temporal relation extraction is an important part of natural language processing. Many models are being used in this task with the development of deep learning. However, most of the existing methods cannot accurately obtain the degree of association between different tokens and events, and event-related information cannot be effectively integrated. In this paper, we propose an event information integration model that integrates event information through multilayer bidirectional long short-term memory (Bi-LSTM) and attention mechanism. Although the above scheme can improve the extraction performance, it can still be further optimized. To further improve the performance of the previous scheme, we propose a novel relational graph attention network that incorporates edge attributes. In this approach, we first build a semantic dependency graph through dependency parsing, model a semantic graph that considers the edges' attributes by using top-k attention mechanisms to learn hidden semantic contextual representations, and finally predict event temporal relations. We evaluate proposed models on the TimeBank-Dense dataset. Compared to previous baselines, the Micro-F1 scores obtained by our models improve by 3.9% and 14.5%, respectively. © 1996-2012 Tsinghua University Press.","attention mechanism; graph attention network; neural network; temporal relation extraction","Deep learning; Extraction; Natural language processing systems; Semantics; Attention mechanisms; Degree of association; Dependency parsing; Graph neural networks; Information integration models; NAtural language processing; Semantic dependency; Temporal relation; Long short-term memory",Article,Scopus,2-s2.0-85113206803
"Flores A.M., Pavan M.C., Paraboni I.","57226504309;57219593586;14027249800;","User profiling and satisfaction inference in public information access services",2022,"Journal of Intelligent Information Systems","58","1",,"67","89",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111769912&doi=10.1007%2fs10844-021-00661-w&partnerID=40&md5=4618749522eb1c5322130c104bed3467","Public information access services are provided by dozens of countries around the world as a means to promote transparency and democracy, and present a number of research opportunities for the development of computational models that help understand both users and their needs. Based on these observations, the present work discusses how the use of Natural Language Processing (NLP) methods may harvest valuable knowledge about citizen-government communication in user profiling and satisfaction inference tasks. More specifically, from a large text dataset of this kind, we build a number of models using a range of supervised machine learning methods - including bidirectional long short-term memory networks (LSTMs), pre-trained context-sensitive embeddings (BERT) and others - and show that these outperform textual and non-textual baseline alternatives alike. This outcome makes a case in favour of NLP methods for these tasks, and paves the way for further applications in the public information access domain. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","E-government; Information access; Natural language processing; Text classification","Information retrieval; Large dataset; Natural language processing systems; Supervised learning; Computational model; Context sensitive; Government communication; NAtural language processing; Public information; Research opportunities; Short term memory; Supervised machine learning; Learning systems",Article,Scopus,2-s2.0-85111769912
"Chang J.-W.","55570534700;","Enabling progressive system integration for AIoT and speech-based HCI through semantic-aware computing",2022,"Journal of Supercomputing","78","3",,"3288","3324",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111085595&doi=10.1007%2fs11227-021-03996-x&partnerID=40&md5=03fcd0284c0b0e63af3397f7d63687f9","A novel integration architecture for speech-based human–computer interaction was developed using a progressive growth framework and semantic-aware computing. The architecture can integrate different services and can address the diversity of Internet of Things platforms. A natural language understanding (NLU) agent is proposed as a controller of IoT hubs and hybrid cloud services. The NLU agent with semantic-aware computing can effectively achieve a context-sensitive topic correlation and user intent analysis. Through a modularized design, the proposed progressive growth framework allows the NLU agent to chat about many different issues, such as current affairs and music. Local and cloud services can be loaded based on user demands, such as IoT platforms and hybrid cloud services. We developed and introduced three applications in daily life as case studies to demonstrate their potential and values. With the proposed integration architecture, users can develop many valuable applications according to their demands in various industries. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Hybrid cloud services; IoT hub; Natural language processing; Natural language understanding; Speech-based HCI","Human computer interaction; Semantics; Web services; Computer interaction; Context sensitive; Different services; Integration architecture; Intent analysis; Modularized design; Natural language understanding; Progressive systems; Internet of things",Article,Scopus,2-s2.0-85111085595
"Ren L., Xu B., Lin H., Zhang J., Yang L.","57220752204;57198894858;24468572400;57219484893;55733088000;","An attention network via pronunciation, lexicon and syntax for humor recognition",2022,"Applied Intelligence","52","3",,"2690","2702",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108375223&doi=10.1007%2fs10489-021-02580-3&partnerID=40&md5=f769f31040581e12adc178c0035f624c","Humor is one of the most common and attractive expressions in our daily life. It is usually witty and funny. Humor recognition is an interesting but difficult task in natural language processing. Some recent works have used deep neural networks to recognize humorous text. In a different approach, we start from a new perspective based on humor linguistics, including pronunciation, lexicon, and syntax, for recognizing humor based on neural networks, in order to capture humorous incongruity and ambiguity. Specifically, we propose an attention network via pronunciation, lexicon, and syntax (ANPLS) for humor recognition. The ANPLS model contains four units, namely, the pronunciation understanding unit, the lexicon understanding unit, the syntax analysis unit, and the context understanding unit. The pronunciation understanding unit is used to extract the pronunciation-based humor features. The lexicon understanding unit is used to solve the polysemy in humor. The syntax analysis unit aims to capture the syntax information of humor. The context understanding unit is used to obtain the contextual humor features. These four units may have different levels of importance for humor recognition so that we further apply an attention mechanism to assign different weights to these four units. We conduct experiments on three popular datasets, namely, the SemEval2017 Task7 dataset, the 16000 One-Liners dataset, and the Pun of the Day dataset. The experimental results demonstrate that our model can achieve comparable or state-of-the-art performance compared with the existing models. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Attention mechanism; Humor linguistics; Humor recognition; Natural language processing; Polysemy","Character recognition; Deep neural networks; Natural language processing systems; Syntactics; Attention mechanisms; Daily lives; NAtural language processing; State-of-the-art performance; Syntax analysis; Neural networks",Article,Scopus,2-s2.0-85108375223
"Singh A., Singh T.D., Bandyopadhyay S.","57211680217;55032148000;23092721400;","Attention based video captioning framework for Hindi",2022,"Multimedia Systems","28","1",,"195","207",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108083408&doi=10.1007%2fs00530-021-00816-3&partnerID=40&md5=53fa83ed65fe93786409ea1a07984c68","In recent times, active research is going on for bridging the gap between computer vision and natural language. In this paper, we attempt to address the problem of Hindi video captioning. In a linguistically diverse country like India, it is important to provide a means which can help in understanding the visual entities in native languages. In this work, we employ a hybrid attention mechanism by extending the soft temporal attention mechanism with a semantic attention to make the system able to decide when to focus on visual context vector and semantic input. The visual context vector of the input video is extracted using 3D convolutional neural network (3D CNN) and a Long Short-Term Memory (LSTM) recurrent network with attention module is used for decoding the encoded context vector. We experimented on a dataset built in-house for Hindi video captioning by translating MSR- VTT dataset followed by post-editing. Our system achieves 0.369 CIDEr score and 0.393 METEOR score and outperformed other baseline models including RMN (Reasoning Module Networks)-based model. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Deep learning; Hindi captions; Hindi video captioning; LSTM","Convolutional neural networks; Semantics; Visual languages; Attention mechanisms; Baseline models; Module networks; Native language; Natural languages; Recurrent networks; Visual context; Visual entities; Long short-term memory",Article,Scopus,2-s2.0-85108083408
"Pérez Pozo Á., de la Rosa J., Ros S., González-Blanco E., Hernández L., de Sisto M.","57222025738;55203236600;7005158730;57190153324;57222031867;57224541472;","A bridge too far for artificial intelligence?: Automatic classification of stanzas in Spanish poetry",2022,"Journal of the Association for Information Science and Technology","73","2",,"258","267",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107898736&doi=10.1002%2fasi.24532&partnerID=40&md5=509dc4c503dee81fb20145573315039e","The rise in artificial intelligence and natural language processing techniques has increased considerably in the last few decades. Historically, the focus has been primarily on texts expressed in prose form, leaving mostly aside figurative or poetic expressions of language due to their rich semantics and syntactic complexity. The creation and analysis of poetry have been commonly carried out by hand, with a few computer-assisted approaches. In the Spanish context, the promise of machine learning is starting to pan out in specific tasks such as metrical annotation and syllabification. However, there is a task that remains unexplored and underdeveloped: stanza classification. This classification of the inner structures of verses in which a poem is built upon is an especially relevant task for poetry studies since it complements the structural information of a poem. In this work, we analyzed different computational approaches to stanza classification in the Spanish poetic tradition. These approaches show that this task continues to be hard for computers systems, both based on classical machine learning approaches as well as statistical language models and cannot compete with traditional computational paradigms based on the knowledge of experts. © 2021 The Authors. Journal of the Association for Information Science and Technology published by Wiley Periodicals LLC on behalf of Association for Information Science and Technology.",,"Computer aided analysis; Machine learning; Natural language processing systems; Semantics; Automatic classification; Computational approach; Computational paradigm; Machine learning approaches; NAtural language processing; Statistical language models; Structural information; Syntactic complexity; Classification (of information)",Article,Scopus,2-s2.0-85107898736
"Cao X., Liu Y.","57222053076;55960857900;","Coarse-grained decomposition and fine-grained interaction for multi-hop question answering",2022,"Journal of Intelligent Information Systems","58","1",,"21","41",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107565437&doi=10.1007%2fs10844-021-00645-w&partnerID=40&md5=827725ce2a5c813ff8ab4990322d7f90","In recent years, question answering (QA) and reading comprehension (RC) has attracted much attention, and most research on QA has focused on multi-hop QA task which requires connecting multiple pieces of evidence scattered in a long context to answer the question. The key to the multi-hop QA task is semantic feature interaction between documents and questions, which is widely processed by Bi-directional Attention Flow (Bi-DAF), but Bi-DAF generally captures only the surface semantics of words in complex questions, and fails to capture implied semantic feature of intermediate answers, as well as ignoring parts of contexts related to the question and failing to extract the most important parts of multiple documents. In this paper, we propose a new model architecture for multi-hop question answering by applying two completion strategies:(1) Coarse-Grained complex question Decomposition (CGDe) strategy is introduced to decompose complex questions into simple ones without any additional annotations; (2) Fine-Grained Interaction (FGIn) strategy is introduced to explicitly represent each word in documents and extract more comprehensive and accurate sentences related to the inference path. The above two strategies are combined and tested on the SQuAD and HotpotQA datasets, and the experimental results show that our method outperforms state-of-the-art baselines. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Coarse-grained complex question decomposition; Complex questions; Fine-grained interaction; Question answering","Semantics; Coarse-grained; Complex questions; Model architecture; Multiple documents; Question Answering; Reading comprehension; Semantic features; State of the art; Natural language processing systems",Article,Scopus,2-s2.0-85107565437
"Andrushchenko M., Sandberg K., Turunen R., Marjanen J., Hatavara M., Kurunmäki J., Nummenmaa T., Hyvärinen M., Teräs K., Peltonen J., Nummenmaa J.","57217022899;57224195110;55535820800;56755247600;25027613500;56013948900;35488325500;22979755600;57189369553;7101970478;6507393852;","Using parsed and annotated corpora to analyze parliamentarians' talk in Finland",2022,"Journal of the Association for Information Science and Technology","73","2",,"288","302",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107229944&doi=10.1002%2fasi.24500&partnerID=40&md5=2417bbd6514f44159aba642b07efac0c","We present a search system for grammatically analyzed corpora of Finnish parliamentary records and interviews with former parliamentarians, annotated with metadata of talk structure and involved parliamentarians, and discuss their use through carefully chosen digital humanities case studies. We first introduce the construction, contents, and principles of use of the corpora. Then we discuss the application of the search system and the corpora to study how politicians talk about power, how ideological terms are used in political speech, and how to identify narratives in the data. All case studies stem from questions in the humanities and the social sciences, but rely on the grammatically parsed corpora in both identifying and quantifying passages of interest. Finally, the paper discusses the role of natural language processing methods for questions in the (digital) humanities. It makes the claim that a digital humanities inquiry of parliamentary speech and interviews with politicians cannot only rely on computational humanities modeling, but needs to accommodate a range of perspectives starting with simple searches, quantitative exploration, and ending with modeling. Furthermore, the digital humanities need a more thorough discussion about how the utilization of tools from information science and technologies alter the research questions posed in the humanities. © 2021 The Authors. Journal of the Association for Information Science and Technology published by Wiley Periodicals LLC on behalf of Association for Information Science and Technology.",,"Linguistics; Natural language processing systems; Safety devices; Case-studies; Digital humanities; Finland; NAtural language processing; Parsed corpora; Research questions; Science and Technology; Search system; Search engines; article; Finland; human; human experiment; humanities; information science; interview; metadata; narrative; natural language processing; public figure; sociology; speech",Article,Scopus,2-s2.0-85107229944
"Kraas A.","42261972300;","On the automation-supported derivation of domain-specific UML profiles considering static semantics",2022,"Software and Systems Modeling","21","1",,"51","79",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106532494&doi=10.1007%2fs10270-021-00890-1&partnerID=40&md5=7585b90c7eaecc4b9c110096409b0113","In the light of standardization, the model-driven engineering (MDE) is becoming increasingly important for the development of DSLs, in addition to traditional approaches based on grammar formalisms. Metamodels define the abstract syntax and static semantics of a DSL and can be created by using the language concepts of the Meta Object Facility (MOF) or by defining a UML profile. Both metamodels and UML profiles are often provided for standardized DSLs, and the mappings of metamodels to UML profiles are usually specified informally in natural language, which also applies for the static semantics of metamodels and/or UML profiles, which has the disadvantage that ambiguities can occur, and that the static semantics must be manually translated into a machine-processable language. To address these weaknesses, we propose a new automated approach for deriving a UML profile from the metamodel of a DSL. One novelty is that subsetting or redefining metaclass attributes are mapped to stereotype attributes whose values are computed at runtime via automatically created OCL expressions. The automatic transfer of the static semantics of a DSL to a UML profile is a further contribution of our approach. Our DSL Metamodeling and Derivation Toolchain (DSL-MeDeTo) implements all aspects of our proposed approach in Eclipse. This enabled us to successfully apply our approach to the two DSLs Test Description Language (TDL) and Specification and Description Language (SDL). © 2021, The Author(s).",,"Natural language processing systems; Semantics; Turing machines; Automated approach; Description languages; Grammar formalisms; Meta object facilities (MOF); Model-driven Engineering; Natural languages; Static semantics; Traditional approaches; Digital subscriber lines",Article,Scopus,2-s2.0-85106532494
"Geng B.","57223366175;","Text segmentation for patent claim simplification via Bidirectional Long-Short Term Memory and Conditional Random Field",2022,"Computational Intelligence","38","1",,"205","215",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105729727&doi=10.1111%2fcoin.12455&partnerID=40&md5=c99434576477fd6f05bfd4b3dadaade0","Text simplification is a vital work for comprehending patent claims due to its complex syntactic structures and lengthy sentences. Therefore, almost all patent analysis practitioners cannot be able to directly and intuitively understand patent essence even through some common natural language processing (NLP) tools are applied to parse these patent claim paragraph or sentences. Universal text analysis tools above is almost useless, or even crashed when applied to some complex paragraphs of patent claims. Therefore, it is necessary to propose a patent text oriented simplification approach to help patent researchers grasp the essence of patent quickly and intuitively. Motivated by the above reason, we in this article propose a simplification method based on deep learning to segment patent claim into shorter and comprehensible sentences for downstream tasks of patent analysis. The proposed approach contains two stages: on one stage, we use a machine learning approach of conditional random field (CRF) to decompose syntactically complex paragraphs into coarse-grained level sentences with simplified structures and complete semantics; on another stage, a deep Learning architecture of bidirectional long-short term memory (Bi-LSTM)-CRF is applied to segment coarse-grained and lengthy sentences of former stage into fined-grained and shorter sentences. Compared with a series of baselines, our patent segmentation architecture based on deep learning of Bi-LSTM-CRF achieves higher performance than any other methods on the evaluation measures of precision, recall, and F1. © 2021 Wiley Periodicals LLC.","Bi-LSTM; CRF; patent claim; sentence segmentation","Brain; Deep learning; Image segmentation; Learning systems; Memory architecture; Natural language processing systems; Patents and inventions; Random processes; Semantics; Syntactics; Turing machines; Architecture-based; Conditional random field; Evaluation measures; Learning architectures; Machine learning approaches; NAtural language processing; Simplification method; Syntactic structure; Long short-term memory",Article,Scopus,2-s2.0-85105729727
"Pfob A., Barr R.G., Duda V., Büsch C., Bruckner T., Spratte J., Nees J., Togawa R., Ho C., Fastner S., Riedel F., Schaefgen B., Hennigs A., Sohn C., Heil J., Golatta M.","57203567060;57222567183;7005592571;57220522044;34567844800;36097078200;56181701800;57193810766;57223132798;57220635359;57038354200;56910285700;41361266600;7102538437;36082454300;24536992100;","A New Practical Decision Rule to Better Differentiate BI-RADS 3 or 4 Breast Masses on Breast Ultrasound",2022,"Journal of Ultrasound in Medicine","41","2",,"427","436",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104971064&doi=10.1002%2fjum.15722&partnerID=40&md5=6c9ecaa30e7a003879c99d2f84888b57","Objectives: The BI-RADS classification provides a standardized way to describe ultrasound findings in breast cancer diagnostics. However, there is little information regarding which BI-RADS descriptors are most strongly associated with malignancy, to better distinguish BI-RADS 3 (follow-up imaging) and 4 (diagnostic biopsy) breast masses. Methods: Patients were recruited as part of an international, multicenter trial (NCT02638935). The trial enrolled 1294 women (6 excluded) categorized as BI-RADS 3 or 4 upon routine B-mode ultrasound examination. Ultrasound images were evaluated by three expert physicians according to BI-RADS. All patients underwent histopathological confirmation (reference standard). We performed univariate and multivariate analyses (chi-square test, logistic regression, and Krippendorff's alpha). Results: Histopathologic evaluation showed malignancy in 368 of 1288 masses (28.6%). Upon performing multivariate analysis, the following descriptors were significantly associated with malignancy (P <.05): age ≥50 years (OR 8.99), non-circumscribed indistinct (OR 4.05) and microlobulated margin (OR 2.95), nonparallel orientation (OR 2.69), and calcification (OR 2.64). A clinical decision rule informed by these results demonstrated a 97% sensitivity and missed fewer cancers compared to three physician experts (range of sensitivity 79–95%) and a previous decision rule (sensitivity 59%). Specificity was 44% versus 22–83%, respectively. The inter-reader reliability of the BI-RADS descriptors and of the final BI-RADS score was fair-moderate. Conclusions: A patient should undergo a diagnostic biopsy (BI-RADS 4) instead of follow-up imaging (BI-RADS 3) if the patient is 50 years or older or exhibits at least one of the following features: calcification, nonparallel orientation of mass, non-circumscribed margin, or posterior shadowing. © 2021 The Authors. Journal of Ultrasound in Medicine published by Wiley Periodicals LLC on behalf of American Institute of Ultrasound in Medicine.",,"Biomineralization; Biopsy; Bone; Diseases; Fault tolerant computer systems; Logistic regression; Medical imaging; Natural language processing systems; Statistical tests; Ultrasonics; Breast cancer diagnostics; Clinical decision rules; Krippendorff's alphas; Multi variate analysis; Nonparallel orientation; Reference standard; Ultrasound examination; Ultrasound images; Multivariant analysis; breast; breast tumor; clinical trial; diagnostic imaging; echography; echomammography; female; human; middle aged; multicenter study; reproducibility; retrospective study; Breast; Breast Neoplasms; Female; Humans; Middle Aged; Reproducibility of Results; Retrospective Studies; Ultrasonography; Ultrasonography, Mammary",Article,Scopus,2-s2.0-85104971064
"Cao Q., Huang H.","57221376451;7405614195;","VSRN: Visual-Semantic Relation Network for Video Visual Relation Inference",2022,"IEEE Transactions on Circuits and Systems for Video Technology","32","2",,"768","777",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103283157&doi=10.1109%2fTCSVT.2021.3068214&partnerID=40&md5=ac684a90e109c037b6cbb3ead229af58","Video visual relation inference refers to the task of automatically detecting the relation triplets between the observed objects in videos with the form of ${ < subject, predicate, object>}$ , which requires correctly labeling each detected object and their interaction predicates. Despite the recent advances in image visual relation detection using deep learning techniques, relation inference in videos remains a challenging topic. On one hand, since the introduction of temporal information, it needs to model the rich spatio-temporal visual information for objects and videos. On the other hand, wild videos are often annotated with incomplete relation triplet tags and a few of them are semantically overlapped. However, previous methods adopt hand-crafted visual features extracted from the trajectories, describing local appearance characteristics of isolated objects. And they treat the problem as a multi-class classification task, which makes the relation tags mutually exclusive. To address the above issues, we propose a novel model, termed Visual-Semantic Relation Network (VSRN). In this network, we leverage three-dimensional convolution kernel to capture spatio-temporal features, and encode global visual features in videos through pooling operation on each time slice. Moreover, the semantic collocations between objects are also incorporated so as to obtain comprehensive representations of the relationships. For relation classification, we treat the problem as a multi-label classification task and regard each tag to be independent to predict various relationships. Additionally, we modify commonly used evaluation metric, video-wise recall, to a pair-wise metric (Roop) for testing the performance of models in predicting multiple relationships for the object pairs, Extensive experimental results on two large-scale datasets demonstrate the effectiveness of our proposed model which significantly outperforms the previous works. © 2021 IEEE.","Feature representation; Neural network; Video analysis; Visual relation inference","Classification (of information); Deep learning; Large dataset; Learning systems; Semantic Web; Semantics; Global visual features; Large-scale datasets; Learning techniques; Multi label classification; Multi-class classification; Relation classifications; Spatio temporal features; Temporal information; Object detection",Article,Scopus,2-s2.0-85103283157
"Bai T., Guan H., Wang S., Wang Y., Huang L.","54681594100;57222523272;57201042137;57190290352;55770288600;","Traditional Chinese medicine entity relation extraction based on CNN with segment attention",2022,"Neural Computing and Applications","34","4",,"2739","2748",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103076208&doi=10.1007%2fs00521-021-05897-9&partnerID=40&md5=952f8c221befedb994408a58ada82100","Extracting medical entity relations from Traditional Chinese Medicine (TCM) related article is crucial to connect domain knowledge between TCM with modern medicine. Herb accounts for the majority of Traditional Chinese Medicine, so our work mainly focuses on herb. The problem would be effectively solved by extracting herb-related entity relations from PubMed literature. In order to realize the entity relation mining, we propose a novel deep-learning model with improved layers without manual feature engineering. We design a new segment attention mechanism based on Convolutional Neural Network, which enables extracting local semantic features through word embedding. Then we classify the relations by connecting different embedding features. We first test this method on the Chemical-Induced Disease task and the experiment show better result comparing to other state-of-the-art deep learning methods. Further, we apply this method to a herbal-related data set (Herbal-Disease and Herbal Chemistry, HD-HC) constructed from PubMed to explore entity relation classification. The experiment shows superior results than other baseline methods. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Convolutional neural network; Relation extraction; Segment attention mechanism; Traditional Chinese medicine","Classification (of information); Convolutional neural networks; Deep learning; Embeddings; Learning systems; Semantics; Attention mechanisms; Baseline methods; Entity relation extractions; Feature engineerings; Learning methods; Local semantic features; Relation classifications; Traditional Chinese Medicine; Medicine",Article,Scopus,2-s2.0-85103076208
"Deng J., Li L., Zhang B., Wang S., Zha Z., Huang Q.","57222328381;56182887500;57219687889;55904652300;36626639900;8435766200;","Syntax-Guided Hierarchical Attention Network for Video Captioning",2022,"IEEE Transactions on Circuits and Systems for Video Technology","32","2",,"880","892",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102308734&doi=10.1109%2fTCSVT.2021.3063423&partnerID=40&md5=1e1508bc25e5ae601da60004c82f17af","Video captioning is a challenging task that aims to generate linguistic description based on video content. Most methods only incorporate visual features (2D/3D) as input for generating visual and non-visual words in the caption. However, generating non-visual words usually depends more on sentence-context than visual features. The wrong non-visual words can reduce the sentence fluency and even change the meaning of sentence. In this paper, we propose a syntax-guided hierarchical attention network (SHAN), which leverages semantic and syntax cues to integrate visual and sentence-context features for captioning. First, a globally-dependent context encoder is designed to extract the global sentence-context feature that facilitates generating non-visual words. Then, we introduce hierarchical content attention and syntax attention to adaptively integrate features in terms of temporality and feature characteristics respectively. Content attention helps focus on time intervals related to the semantic of current word, while cross-modal syntax attention uses syntax information to model importance of different features for target word's generation. Moreover, such hierarchical attention can enhance the model interpretability for captioning. Experiments on MSVD and MSR-VTT datasets show the comparable performance of our method compared with current methods. © 2021 IEEE.","Content attention; Global sentence-context; Syntax attention; Video captioning","Semantics; Context features; Cross-modal; Interpretability; Linguistic descriptions; Target words; Time interval; Video contents; Visual feature; Syntactics",Article,Scopus,2-s2.0-85102308734
"Lyu S., Cheng J., Wu X., Cui L., Chen H., Miao C.","57218799024;57222725481;57218925248;55632676200;35261486600;8850060600;","Auxiliary Learning for Relation Extraction",2022,"IEEE Transactions on Emerging Topics in Computational Intelligence","6","1",,"182","191",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098770550&doi=10.1109%2fTETCI.2020.3040444&partnerID=40&md5=fb96ee4fa8898a0c2d11656bf1e89e61","Relation extraction aims to predict a semantic relation between entities in a sentence, which is usually regarded as a classification problem. However, due to the limited relation set, many semantic relations are labeled as a special artificial relation type, termed no_relation, if they are beyond the predefined relation set. Existing methods treat this artificial relation type no_relation as a common semantic relation without taking its rich semantics into account. In this paper, a novel auxiliary learning method is proposed to excavate the semantic information of no_relation, resulting in the improvement of generalization performance. The auxiliary learning method focuses on the model learning phase and introduces a binary classification task that treats the artificial relation type no_relation as negative class and the rest semantic types as positive class. The binary classification task, named auxiliary learning task, pays more attention to no_relation with a cost-sensitive loss by assigning higher cost on the misclassification of negative samples than positive ones. An additional reward is provided to the main prediction task by the auxiliary learning task, which leads to a better representation for relation extraction. Significant improvements are consistently achieved when state-of-the-art models are equipped with the auxiliary learning task on SemEval-2010 Task 8 and the large-scale TACRED. Especially, new state-of-the-art performance is achieved on SemEval-2010 Task 8 by the proposed method. Meanwhile, the motivation for introducing the auxiliary learning method is further reinforced by extensive experiments. © 2017 IEEE.","Auxiliary learning; dependency-based; neural networks; pretrained language models; relation extraction; sequence-based","Extraction; Semantics; Binary classification; Generalization performance; Misclassifications; Negative samples; Relation extraction; Semantic information; Semantic relations; State-of-the-art performance; Learning systems",Article,Scopus,2-s2.0-85098770550
"Amin M., Shah B., Sharif A., Ali T., Kim K.-I., Anwar S.","57208594495;55354253000;57210203620;25824711900;57215675010;57212662725;","Android malware detection through generative adversarial networks",2022,"Transactions on Emerging Telecommunications Technologies","33","2","e3675","","",,10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069892612&doi=10.1002%2fett.3675&partnerID=40&md5=9864c889326f4582315c9677290be317","Mobile and cell devices have empowered end users to tweak their cell phones more than ever and introduce applications just as we used to with personal computers. Android likewise portrays an uprise in mobile devices and personal digital assistants. It is an open-source versatile platform fueling incalculable hardware units, tablets, televisions, auto amusement frameworks, digital boxes, and so forth. In a generally shorter life cycle, Android also has additionally experienced a mammoth development in application malware. In this context, a toweringly large measure of strategies has been proposed in theory for the examination and detection of these harmful applications for the Android platform. These strategies attempt to both statically reverse engineer the application and elicit meaningful information as features manually or dynamically endeavor to quantify the runtime behavior of the application to identify malevolence. The overgrowing nature of Android malware has enormously debilitated the support of protective measures, which leaves the platforms such as Android feeble for novel and mysterious malware. Machine learning is being utilized for malware diagnosis in mobile phones as a common practice and in Android distinctively. It is important to specify here that these systems, however, utilize and adapt the learning-based techniques, yet the overhead of hand-created features limits ease of use of such methods in reality by an end user. As a solution to this issue, we mean to make utilization of deep learning–based algorithms as the fundamental arrangement for malware examination on Android. Deep learning turns up as another way of research that has bid the scientific community in the fields of vision, speech, and natural language processing. Of late, models set up on deep convolution networks outmatched techniques utilizing handmade descriptive features at various undertakings. Likewise, our proposed technique to cater malware detection is by design a deep learning model making use of generative adversarial networks, which is responsible to detect the Android malware via famous two-player game theory for a rock-paper-scissor problem. We have used three state-of-the-art datasets and augmented a large-scale dataset of opcodes extracted from the Android Package Kit bytecode and used in our experiments. Our technique achieves F1 score of 99% with a receiver operating characteristic of 99% on the bytecode dataset. This proves the usefulness of our technique and that it can generally be adopted in real life. © 2019 John Wiley & Sons, Ltd.",,"Android (operating system); Cellular telephones; Deep learning; Digital devices; Game theory; Large dataset; Learning systems; Life cycle; Malware; Natural language processing systems; Open systems; Personal digital assistants; Adversarial networks; Android platforms; Large-scale dataset; NAtural language processing; Protective measures; Receiver operating characteristics; Scientific community; Shorter life cycles; Mobile security",Article,Scopus,2-s2.0-85069892612
"Woo J., Park S.H., Kim H.K.","55976409400;57208083891;7410133266;","Profane or Not: Improving Korean Profane Detection using Deep Learning",2022,"KSII Transactions on Internet and Information Systems","16","1",,"305","318",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124622335&doi=10.3837%2ftiis.2022.01.017&partnerID=40&md5=06afee3fd2c7ff58ea5a759bec793179","Abusive behaviors have become a common issue in many online social media platforms. Profanity is common form of abusive behavior in online. Social media platforms operate the filtering system using popular profanity words lists, but this method has drawbacks that it can be bypassed using an altered form and it can detect normal sentences as profanity. Especially in Korean language, the syllable is composed of graphemes and words are composed of multiple syllables, it can be decomposed into graphemes without impairing the transmission of meaning, and the form of a profane word can be seen as a different meaning in a sentence. This work focuses on the problem of filtering system mis-detecting normal phrases with profane phrases. For that, we proposed the deep learning-based framework including grapheme and syllable separation-based word embedding and appropriate CNN structure. The proposed model was evaluated on the chatting contents from the one of the famous online games in South Korea and generated 90.4% accuracy. Copyright © 2022 KSII.","Convolutional neural network; Deep learning; Natural language processing; Profanity; Text mining","Convolutional neural networks; Deep learning; Social networking (online); Convolutional neural network; Deep learning; Embeddings; Filtering systems; Korean language; On-line games; Online social medias; Profanity; Social media platforms; Word lists; Natural language processing systems",Article,Scopus,2-s2.0-85124622335
"Zhao D., Zhang B., Zhu X., Liu S., Yang L., Hou J.","57251451900;57199727175;57251545200;57215336030;57102081600;15131997800;","2.1 µm, 1.52 µJ square-wave noise-like pulse from an all-fiber figure-of-9 Ho-doped oscillator and single-stage amplifier",2022,"Optics Express","30","3",,"3601","3610",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123429565&doi=10.1364%2fOE.448990&partnerID=40&md5=5c06c773f63c060d441b855126d78dd0","A 2.1 µm, high energy square-wave noise-like pulse (NLP) in an all-fiber Ho-doped fiber laser is proposed, which consists of an oscillator and a single-stage amplifier. In the figure-of-9 oscillator, mode-locking is achieved based on the nonlinear amplifying loop mirror, employing a long gain fiber to provide sufficient gain in 2.1 µm band and optimizing the cavity length to obtain maximum pulse energy output. With appropriate pump power and polarization state, the oscillator emits a 175.1 nJ square-wave NLP with center wavelength of 2102.2 nm and spike width of 540 fs. The 3-dB spectral width and pulse envelope width are 11.2 nm and 6.95 ns, respectively. The single-stage amplifier employs a bi-directional pump scheme. After amplification, 5.8 W NLP with a slope efficiency of 56.8% is obtained. The pulse energy of NLP is scaled to 1.52 µJ, which is the highest pulse energy of NLP at 2.1 µm to the best of our knowledge. The obtained high-energy square-wave NLP-fiber laser has great potential in mid-infrared laser generation. © 2022 Optica Publishing Group",,"Acoustooptical devices; Fiber amplifiers; Fibers; Natural language processing systems; All fiber; Doped fiber; Energy; Gain fibers; Modelocking; Nonlinear amplifying loop mirrors; Oscillator modes; Pulse energies; Single stage amplifiers; Square-wave; Fiber lasers",Article,Scopus,2-s2.0-85123429565
"Jia Z., Dong M., Ru J., Xue L., Yang S., Li C.","24831602900;57355461900;45761295600;57355462000;57355168100;57355462100;","STCM-Net: A symmetrical one-stage network for temporal language localization in videos",2022,"Neurocomputing","471",,,"194","207",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120046236&doi=10.1016%2fj.neucom.2021.11.019&partnerID=40&md5=d371431904ed293765bf8f799a23ce86","The task of temporal language localization in the video is to locate a video segment through natural language description for an untrimmed video. Compared with the general video localization task, it is more flexible and complex, which can accurately locate various scenes described by any natural language without making video labels in advance. It can be widely used for the field such as video retrieval and robot intelligent cognition. The main challenges of this task are the extraction of sentence semantics and the integration of contextual information in videos. Among them, contextual video integration can be optimized through the two-dimensional temporal adjacent network. Therefore, complete extraction of the potential information in the query sentence is necessary to solve the task more granularly. At the same time, we found a large amount of time-related information in the query sentence, which helps improve the localization accuracy. Thus, in this paper, we first define the time concept in a sentence and then propose a Sentence Time Concept Mining Network (STCM-Net), an symmetrical one-stage network. Can effectively extract the time concept contained in the query sentence, it can optimize the process of target localization and improve the localization performance. We also evaluate the proposed STCM-Net on three challenging public benchmarks: Charades-STA, ActivityNet Captions, and TACoS. Our STCM-Net gets encouraging improvements compared with the state-of-the-art approaches. © 2021 Elsevier B.V.","Computer vision; Sentence semantic mining; Video segment localization; Video understanding","Extraction; Intelligent robots; Semantic Segmentation; Semantics; Concept mining; Localisation; Mining network; Natural languages; Semantics mining; Sentence semantic mining; Temporal language; Video segment localization; Video segments; Video understanding; Computer vision; article; computer vision; human; human experiment; language; mining; semantics; videorecording",Article,Scopus,2-s2.0-85120046236
"Sun C., Shen D.","55484791300;57221404147;","Towards deep entity resolution via soft schema matching",2022,"Neurocomputing","471",,,"107","117",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119671927&doi=10.1016%2fj.neucom.2021.10.106&partnerID=40&md5=14084bb380b22e540864d7bb0d747f8a","Entity resolution (ER) leads a key role in data preprocessing. ER identifies records corresponding to the same real-world entity. Recent years have witnessed a growing trend of deep learning based ER (deep ER). However, previous deep ER works do not fully utilize schema semantics, since they either use hard schema matching or disregard schema matching. In this work, we flexibly exploit schema matching to enhance deep ER. We define and implement soft schema matching, where attributes are flexibly associated in probabilities. Attribute associations are generated by aggregating token connections in coarse deep ER. Then we incorporate soft schema matching into hierarchical attention networks for ER, which tremendously improves resolution quality, especially for complex data and corrupted data. Different attentions are utilized for particular sub-tasks in ER networks, such as self-attention for contextualization, inter-attention for alignment and intra-attention for weighting. Finally comprehensive experiments are run over common data, complex data and corrupted data. Evaluation results show that our approach surpasses previous works. © 2021 Elsevier B.V.","Attention network; Data preprocessing; Deep learning; Entity resolution; Soft schema matching","Complex networks; Deep learning; Attention network; Complex data; Corrupted data; Data preprocessing; Deep learning; Entity resolutions; Real-world entities; Resolution quality; Schema matching; Soft schema matching; Semantics; article; attention network; deep learning; probability; semantics",Article,Scopus,2-s2.0-85119671927
"Liu M., Liu L., Cao J., Du Q.","57224082991;57207584612;57208002646;36664475600;","Co-attention network with label embedding for text classification",2022,"Neurocomputing","471",,,"61","69",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119624070&doi=10.1016%2fj.neucom.2021.10.099&partnerID=40&md5=c5b8f56fc75fd905adfd47c6d5666cd6","Most existing methods for text classification focus on extracting a highly discriminative text representation, which, however, is typically computationally inefficient. To alleviate this issue, label embedding frameworks are proposed to adopt the label-to-text attention that directly uses label information to construct the text representation for more efficient text classification. Although these label embedding methods have achieved promising results, there is still much space for exploring how to use the label information more effectively. In this paper, we seek to exploit the label information by further constructing the text-attended label representation with text-to-label attention. To this end, we propose a Co-attention Network with Label Embedding (CNLE) that jointly encodes the text and labels into their mutually attended representations. In this way, the model is able to attend to the relevant parts of both. Experiments show that our approach achieves competitive results compared with previous state-of-the-art methods on 7 multi-class classification benchmarks and 2 multi-label classification benchmarks. © 2021 Elsevier B.V.","Deep learning; Label embedding; Natural language processing; Text classification; Text-label co-attention","Classification (of information); Deep learning; Natural language processing systems; Text processing; Deep learning; Embedding method; Embeddings; Label embedding; Label information; State-of-the-art methods; Text classification; Text labels; Text representation; Text-label co-attention; Embeddings; article; attention network; deep learning; embedding; multilabel classification; natural language processing",Article,Scopus,2-s2.0-85119624070
"Xiao L., Xue Y., Wang H., Hu X., Gu D., Zhu Y.","57205603681;8232495100;57215111932;55496282300;57215592181;57216509958;","Exploring fine-grained syntactic information for aspect-based sentiment classification with dual graph neural networks",2022,"Neurocomputing","471",,,"48","59",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119457673&doi=10.1016%2fj.neucom.2021.10.091&partnerID=40&md5=6ae53ec5df5502edb2f22f3fcb4d1613","The goal of aspect-based sentiment classification (ASC) is to predict the corresponding emotion of a specific target of a sentence. In neural network-based methods for ASC, various sophisticated models such as Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN) are widespread. Recently, ongoing research has integrated syntactic structures into graph neural networks (GNN) to deal with ASC tasks. However, these methods are limited due to the noise and inefficient use of information of syntactic dependency trees. This paper proposes a novel GNN based deep learning model to overcome the deficiencies of prior studies. In the proposed model, to exploit the information in the syntactic dependency trees, a novel part-of-speech (POS) guided syntactic dependency graph is constructed for a relational graph attention network (RGAT) to eliminate the noises. Further, a syntactic distance attention-guided layer is designed for a densely connected graph convolutional network (DCGCN), which can fully extract semantic dependency between contextual words. Experiments on three public datasets are carried out to evaluate the effectiveness of the proposed model. Comparing to the baselines, our model, as a best alternative, achieves state-of-arts performance. © 2021 Elsevier B.V.","Aspect-based sentiment classification; Graph neural networks; Part-of-speech (POS) guided syntactic dependency graph; Syntactic distance attention guided layer","Classification (of information); Convolution; Convolutional neural networks; Deep neural networks; Forestry; Graph neural networks; Information use; Multilayer neural networks; Recurrent neural networks; Semantics; Trees (mathematics); Aspect-based sentiment classification; Dependency graphs; Graph neural networks; Network-based; Part Of Speech; Part-of-speech guided syntactic dependency graph; Sentiment classification; Syntactic dependencies; Syntactic dependency trees; Syntactic distance attention guided layer; Syntactics; article; attention network; comparative effectiveness; deep learning; human; human experiment; noise; speech",Article,Scopus,2-s2.0-85119457673
"Margulis E.H., Wong P.C.M., Turnbull C., Kubit B.M., McAuley J.D.","57220277120;14018817600;57423768000;55768648200;57222293356;","Narratives imagined in response to instrumental music reveal culture-bounded intersubjectivity",2022,"Proceedings of the National Academy of Sciences of the United States of America","119","4","e2110406119","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123351255&doi=10.1073%2fpnas.2110406119&partnerID=40&md5=eeb56b6054f6f2ed0f6baacef3909c46","The scientific literature sometimes considers music an abstract stimulus, devoid of explicit meaning, and at other times considers it a universal language. Here, individuals in three geographically distinct locations spanning two cultures performed a highly unconstrained task: they provided free-response descriptions of stories they imagined while listening to instrumental music. Tools from natural language processing revealed that listeners provide highly similar stories to the same musical excerpts when they share an underlying culture, but when they do not, the generated stories show limited overlap. These results paint a more complex picture of music's power: music can generate remarkably similar stories in listeners' minds, but the degree to which these imagined narratives are shared depends on the degree to which culture is shared across listeners. Thus, music is neither an abstract stimulus nor a universal language but has semantic affordances shaped by culture, requiring more sustained attention from psychology. © 2022 National Academy of Sciences. All rights reserved.","Culture; Imagination; Music; Narrative; Semantics",,Article,Scopus,2-s2.0-85123351255
"Huang H., Shang Y.-M., Sun X., Wei W., Mao X.","7405614195;57197783149;57460439100;56126506200;56949703000;","Three birds, one stone: A novel translation based framework for joint entity and relation extraction",2022,"Knowledge-Based Systems","236",,"107677","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121150141&doi=10.1016%2fj.knosys.2021.107677&partnerID=40&md5=10c238f444a44ce24b10e21ad4b0b034","Joint entity and relation extraction is an important task in natural language processing and knowledge graph construction. Existing studies mainly focus on three issues: redundant predictions, overlapping triples and relation connections. However, as far as we know, none of them is able to solve the three problems simultaneously in a unified architecture. To address this issue, in this paper, we propose a novel translation based unified framework. Specifically, the proposed framework contains two components: an entity tagger and a relation extractor. The former is used to recognize all candidate head entities and tail entities respectively. The latter predicts relations for every entity pair dynamically through ranking with translation mechanism. To show the superiority of the proposed framework, we instantiate it through the simplest binary entity tagger and TransE algorithm. Extensive experiments over two widely used datasets demonstrate that, even with the simplest components, the proposed framework can still achieve competitive performance with most previous baselines. Moreover, the framework is flexible. It enjoys further performance boost when employing more powerful entity tagger and knowledge graph embedding algorithm. © 2021 Elsevier B.V.","Entity and relation extraction; Knowledge graph embedding; Overlapping triples; Redundant predictions; Relation connections","Computational linguistics; Embeddings; Extraction; Natural language processing systems; Entity extractions; Graph embeddings; Knowledge graph embedding; Knowledge graphs; Overlapping triple; Processing graphs; Redundant prediction; Relation connection; Relation extraction; Simple++; Knowledge graph",Article,Scopus,2-s2.0-85121150141
"Dai Y., Shou L., Gong M., Xia X., Kang Z., Xu Z., Jiang D.","57214442355;57211168243;57214933038;57353892200;56720084800;22836748000;57226177359;","Graph Fusion Network for Text Classification",2022,"Knowledge-Based Systems","236",,"107659","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119951076&doi=10.1016%2fj.knosys.2021.107659&partnerID=40&md5=a2e0d1e383545c83572a80cf3140d15e","Text classification is an important and classical problem in natural language processing. Recently, Graph Neural Networks (GNNs) have been widely applied in text classification and achieved outstanding performance. Despite the success of GNNs on text classification, existing methods are still limited in two main aspects. On the one hand, transductive methods cannot easily adapt to new documents. Since transductive methods incorporate all documents into their text graph, they need to reconstruct the whole graph and retrain their system from scratch when new documents come. However, this is not applicable to real-world situations. On the other hand, many state-of-the-art algorithms ignore the quality of text graphs, which may lead to sub-optimal performance. To address these problems, we propose a Graph Fusion Network (GFN), which can overcome these limitations and boost text classification performance. In detail, in the graph construction stage, we build homogeneous text graphs with word nodes, which makes the learning system capable of making inference on new documents without rebuilding the whole text graph. Then, we propose to transform external knowledge into structural information and integrate different views of text graphs to capture more structural information. In the graph reasoning stage, we divide the process into three steps: graph learning, graph convolution, and graph fusion. In the graph learning step, we adopt a graph learning layer to further adapt text graphs. In the graph fusion step, we design a multi-head fusion module to integrate different opinions. Experimental results on five benchmarks demonstrate the superiority of our proposed method. © 2021 Elsevier B.V.","External knowledge; Graph fusion; Graph Neural Networks; Text classification","Classification (of information); Graph neural networks; Knowledge graph; Learning systems; Natural language processing systems; Text processing; Classical problems; External knowledge; Graph fusion; Graph neural networks; Performance; Real world situations; State-of-the-art algorithms; Structural information; Sub-optimal performance; Text classification; Graphic methods",Article,Scopus,2-s2.0-85119951076
"Yaghtin M., Sotudeh H., Nikseresht A., Mirzabeigi M.","55813776700;21735498000;57208684224;55146086200;","Modeling the co-citation dependence on semantic layers of co-cited documents",2022,"Online Information Review","46","1",,"59","78",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106291494&doi=10.1108%2fOIR-04-2020-0126&partnerID=40&md5=97fec4507c98f733a4a2dc0ccfa42d91","Purpose: Co-citation frequency, defined as the number of documents co-citing two articles, is considered as a quantitative, and thus, an efficient proxy of subject relatedness or prestige of the co-cited articles. Despite its quantitative nature, it is found effective in retrieving and evaluating documents, signifying its linkage with the related documents' contents. To better understand the dynamism of the citation network, the present study aims to investigate various content features giving rise to the measure. Design/methodology/approach: The present study examined the interaction of different co-citation features in explaining the co-citation frequency. The features include the co-cited works' similarities in their full-texts, Medical Subject Headings (MeSH) terms, co-citation proximity, opinions and co-citances. A test collection is built using the CITREC dataset. The data were analyzed using natural language processing (NLP) and opinion mining techniques. A linear model was developed to regress the objective and subjective content-based co-citation measures against the natural log of the co-citation frequency. Findings: The dimensions of co-citation similarity, either subjective or objective, play significant roles in predicting co-citation frequency. The model can predict about half of the co-citation variance. The interaction of co-opinionatedness and non-co-opinionatedness is the strongest factor in the model. Originality/value: It is the first study in revealing that both the objective and subjective similarities could significantly predict the co-citation frequency. The findings re-confirm the citation analysis assumption claiming the connection between the cognitive layers of cited documents and citation measures in general and the co-citation frequency in particular. Peer review: The peer review history for this article is available at https://publons.com/publon/10.1108/OIR-04-2020-0126. © 2021, Emerald Publishing Limited.","Citation proximity index; Co-citation; Co-opinionatedness; Content-based citation analysis; MeSH; Natural language processing; Semantic similarity; Syntactic similarity","Semantics; Sentiment analysis; Statistical tests; Citation analysis; Citation networks; Design/methodology/approach; Linear modeling; Medical subject headings; NAtural language processing; Opinion mining; Test Collection; Forecasting",Article,Scopus,2-s2.0-85106291494
"Xiao Y., Jin Y., Cheng R., Hao K.","57212312295;57201346599;57329705000;57329705100;","Hybrid attention-based transformer block model for distant supervision relation extraction",2022,"Neurocomputing","470",,,"29","39",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118890283&doi=10.1016%2fj.neucom.2021.10.037&partnerID=40&md5=fe934b05a808c02b6f6bf5766a6b2b8d","With an exponential explosive growth of various digital text information, it is challenging to efficiently obtain specific knowledge from massive unstructured text information. As one basic task for natural language processing (NLP), relation extraction (RE) aims to extract semantic relations between entity pairs based on the given text. To avoid manual labeling of datasets, distant supervision relation extraction (DSRE) has been widely used, aiming to utilize knowledge base to automatically annotate datasets. Unfortunately, this method heavily suffers from wrong labelling due to its underlying strong assumptions. To address this issue, we propose a new framework using hybrid attention-based Transformer block with multi-instance learning for DSRE. More specifically, the Transformer block is, for the first time, used as a sentence encoder, which mainly utilizes multi-head self-attention to capture syntactic information at the word level. Then, a novel sentence-level attention mechanism is proposed to calculate the bag representation, aiming to exploit all useful information in each sentence. Experimental results on the public dataset New York Times (NYT) demonstrate that the proposed approach can outperform the state-of-the-art algorithms on the adopted dataset, which verifies the effectiveness of our model on the DSRE task. © 2021 Elsevier B.V.","Distant supervision relation extraction (DSRE); Sentence-level attention; Transformer block","Extraction; Knowledge based systems; Semantics; Block modeling; Digital text; Distant supervision relation extraction; Explosive growth; Exponentials; Relation extraction; Sentence level; Sentence-level attention; Text information; Transformer block; Natural language processing systems; algorithm; article; attention; extraction; learning; New York",Article,Scopus,2-s2.0-85118890283
"Lauriola I., Lavelli A., Aiolli F.","57196413683;55886713300;8441217500;","An introduction to Deep Learning in Natural Language Processing: Models, techniques, and tools",2022,"Neurocomputing","470",,,"443","456",,6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112567461&doi=10.1016%2fj.neucom.2021.05.103&partnerID=40&md5=3fb8d718445a6b4b81331f75cc483edb","Natural Language Processing (NLP) is a branch of artificial intelligence that involves the design and implementation of systems and algorithms able to interact through human language. Thanks to the recent advances of deep learning, NLP applications have received an unprecedented boost in performance. In this paper, we present a survey of the application of deep learning techniques in NLP, with a focus on the various tasks where deep learning is demonstrating stronger impact. Additionally, we explore, describe, and revise the main resources in NLP research, including software, hardware, and popular corpora. Finally, we emphasize the main limits of deep learning in NLP and current research directions. © 2021 Elsevier B.V.","Deep Learning; Language Models; Natural Language Processing; Software; Transformer","Deep learning; Deep learning; Language modeling; Language processing; Modelling tools; Natural languages; Processing model; Processing technique; Processing tools; Software; Transformer; Natural language processing systems; article; deep learning; human; human experiment; natural language processing; software",Article,Scopus,2-s2.0-85112567461
"Çelebi A., Özgür A.","55413444600;56230487200;","Cluster-based mention typing for named entity disambiguation",2022,"Natural Language Engineering","28","1",,"1","37",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094896710&doi=10.1017%2fS1351324920000443&partnerID=40&md5=5d9a85afe57ddaab0b9517ddacf0c1eb","An entity mention in text such as “Washington” may correspond to many different named entities such as the city “Washington D.C.” or the newspaper “Washington Post.” The goal of named entity disambiguation (NED) is to identify the mentioned named entity correctly among all possible candidates. If the type (e.g., location or person) of a mentioned entity can be correctly predicted from the context, it may increase the chance of selecting the right candidate by assigning low probability to the unlikely ones. This paper proposes cluster-based mention typing for NED. The aim of mention typing is to predict the type of a given mention based on its context. Generally, manually curated type taxonomies such as Wikipedia categories are used. We introduce cluster-based mention typing, where named entities are clustered based on their contextual similarities and the cluster ids are assigned as types. The hyperlinked mentions and their context in Wikipedia are used in order to obtain these cluster-based types. Then, mention typing models are trained on these mentions, which have been labeled with their cluster-based types through distant supervision. At the NED phase, first the cluster-based types of a given mention are predicted and then, these types are used as features in a ranking model to select the best entity among the candidates. We represent entities at multiple contextual levels and obtain different clusterings (and thus typing models) based on each level. As each clustering breaks the entity space differently, mention typing based on each clustering discriminates the mention differently. When predictions from all typing models are used together, our system achieves better or comparable results based on randomization tests with respect to the state-of-the-art levels on four defacto test sets. © The Author(s), 2020. Published by Cambridge University Press","Clustering; Information extraction; Mention typing; Named entity disambiguation","Natural language processing systems; Cluster-based; Low probability; Named entities; Named entity disambiguations; Randomization tests; Ranking model; State of the art; Washington Post; Learning to rank",Article,Scopus,2-s2.0-85094896710
"Dale R.","7101657130;","$NLP: How to spend a billion dollars",2022,"Natural Language Engineering","28","1",,"125","136",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123944596&doi=10.1017%2fS1351324921000450&partnerID=40&md5=7d2a8e745370d329b249acf5b80595f4","Funding for AI start-ups in general is booming, and natural language processing as a subfield has not missed out. We take a closer look at early-stage funding over the last year—just over US$1B in total—for companies that offer solutions that are based on or make significant use of NLP, providing a picture of what funders think is innovative and bankable in this space, and we make some observations on notable trends and developments. © The Author(s), 2022. Published by Cambridge University Press",,"Artificial intelligence; Fundings; Subfields; Natural language processing systems",Article,Scopus,2-s2.0-85123944596
"Huang J., Han Z., Xu H., Liu H.","57329307000;57329512000;57202790396;57200280821;","Adapted transformer network for news recommendation",2022,"Neurocomputing","469",,,"119","129",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118833805&doi=10.1016%2fj.neucom.2021.10.049&partnerID=40&md5=9d74fad5e54e1ee682bf99625ca83dbd","Online news recommendation aims to provide personalized news for users according to their interests. Existing methods usually learn user preference from their historical reading records in a static and independent way, which ignore the dynamic interaction with the target candidate news. In fact, it is important to fully capture the semantic interaction between user?s historical news and candidate news since the user?s interests would be different in terms of different candidate news. In this paper, we propose a novel news recommendation model with an adapted transformer network. There are three parts in our approach, i.e., a news encoder to learn the semantic features of news, a user encoder to learn the initial representations of users, an adapted transformer module to learn the deep interaction between users and candidate news. The core is that we effectively integrate the historical clicked news and the candidate news into the transformer framework to capture their inherent relatedness. Besides, an additive attention layer is proposed to learn different informativeness of words since different words are differently useful for news or users? representation. We conduct extensive experiments on a real-world dataset from MSN news and the results indicates the effectiveness of our proposed approach. © 2021 Elsevier B.V.","Attention; News Recommendation; Transformer","Semantics; Attention; Dynamic interaction; Learn+; News recommendation; Novel news; Online news; Personalized news; Semantic interactions; Transformer; User's preferences; Signal encoding; article; attention",Article,Scopus,2-s2.0-85118833805
"Wu Y., Zeng M., Fei Z., Yu Y., Wu F.-X., Li M.","57221579501;57188999256;57201319565;57201737587;55843631800;56908226400;","KAICD: A knowledge attention-based deep learning framework for automatic ICD coding",2022,"Neurocomputing","469",,,"376","383",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096181776&doi=10.1016%2fj.neucom.2020.05.115&partnerID=40&md5=53e21278ca9ae13f1a1fddf6517dbd62","Automatic International Classification of Diseases (ICD) coding is an important task in the future of artificial intelligence healthcare. In recent years, a lot of traditional machine learning-based methods have been proposed, and they achieved good results on this task. However, these traditional machine learning-based methods for automatic ICD coding only focus on the semantic features of clinical notes and ignore the feature extraction of ICD titles that are the descriptions of ICD codes. In this paper, we propose a knowledge attention-based deep learning framework called KAICD for automatic ICD coding. KAICD makes full use of the clinic notes and the ICD titles. The semantic features of clinic notes are extracted by a multi-scale convolutional neural network. For ICD titles, we use attention-based Bidirectional Gated Recurrent Unit (Bi-GRU) to build a knowledge database, which can offer additional information. Depending on input clinic notes, we can use the attention mechanism to obtain different knowledge vectors from the knowledge database where some ICD titles are more relevant to the input clinic notes. Last, we concatenate the knowledge vectors and the semantic features of clinic notes, and use them for the final prediction. KAICD is tested on a public dataset Medical Information Mart for Intensive Care III (MIMIC III); it achieves micro-precision of 0.502, micro-recall of 0.428, and micro-f1 of 0.462, which outperforms other competing methods. Furthermore, the results of the ablation study show that the knowledge database of ICD titles learned by the attention-based Bi-GRU enhances the feature expression and improves the prediction performance. © 2020","Attention; Automatic ICD coding; Bidirectional Gated Recurrent Unit; Clinic notes; ICD titles; Knowledge database","Convolutional neural networks; Database systems; Recurrent neural networks; Semantics; Attention mechanisms; Feature expression; International classification of disease; Knowledge database; Learning frameworks; Medical information; Prediction performance; Semantic features; Learning systems; Article; automation; convolutional neural network; deep learning; feature extraction; gated recurrent unit network; human; information processing; intensive care; International Classification of Diseases; knowledge base; medical information; multi scale convolutional neural network; prediction; recall",Article,Scopus,2-s2.0-85096181776
"Walker V.R., Schmitt C.P., Wolfe M.S., Nowak A.J., Kulesza K., Williams A.R., Shin R., Cohen J., Burch D., Stout M.D., Shipkowski K.A., Rooney A.A.","7102764346;7202056836;7202794886;57195615254;57224140437;57220567603;57372197400;57371436700;57193190092;57372197500;56781526100;7003764422;","Evaluation of a semi-automated data extraction tool for public health literature-based reviews: Dextr",2022,"Environment International","159",,"107025","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121141085&doi=10.1016%2fj.envint.2021.107025&partnerID=40&md5=14b95c904570b0a3d75a4826d08b6db2","Introduction: There has been limited development and uptake of machine-learning methods to automate data extraction for literature-based assessments. Although advanced extraction approaches have been applied to some clinical research reviews, existing methods are not well suited for addressing toxicology or environmental health questions due to unique data needs to support reviews in these fields. Objectives: To develop and evaluate a flexible, web-based tool for semi-automated data extraction that: 1) makes data extraction predictions with user verification, 2) integrates token-level annotations, and 3) connects extracted entities to support hierarchical data extraction. Methods: Dextr was developed with Agile software methodology using a two-team approach. The development team outlined proposed features and coded the software. The advisory team guided developers and evaluated Dextr's performance on precision, recall, and extraction time by comparing a manual extraction workflow to a semi-automated extraction workflow using a dataset of 51 environmental health animal studies. Results: The semi-automated workflow did not appear to affect precision rate (96.0% vs. 95.4% manual, p = 0.38), resulted in a small reduction in recall rate (91.8% vs. 97.0% manual, p < 0.01), and substantially reduced the median extraction time (436 s vs. 933 s per study manual, p < 0.01) compared to a manual workflow. Discussion: Dextr provides similar performance to manual extraction in terms of recall and precision and greatly reduces data extraction time. Unlike other tools, Dextr provides the ability to extract complex concepts (e.g., multiple experiments with various exposures and doses within a single study), properly connect the extracted elements within a study, and effectively limit the work required by researchers to generate machine-readable, annotated exports. The Dextr tool addresses data-extraction challenges associated with environmental health sciences literature with a simple user interface, incorporates the key capabilities of user verification and entity connecting, provides a platform for further automation developments, and has the potential to improve data extraction for literature reviews in this and other fields. © 2021","Automation; Literature review; Machine learning; Natural language processing; Scoping review; Systematic evidence map; Systematic review; Text mining","Automation; Clinical research; Data mining; Extraction; Learning algorithms; Machine learning; User interfaces; Automated data; Data extraction; Environmental health; Extraction time; Literature reviews; Scoping review; Systematic evidence map; Systematic Review; User verification; Work-flows; Natural language processing systems; automation; data mining; literature review; machine learning; map; public health; accuracy; Article; automation; data extraction; data processing; environmental health; extraction time; feedback system; gold standard; information processing; literature; machine learning; natural language processing; nonhuman; public health; recall; software; usability; workflow",Article,Scopus,2-s2.0-85121141085
"Guarino A., Malandrino D., Zaccagnino R.","57201983991;7801383595;27868231900;","An automatic mechanism to provide privacy awareness and control over unwittingly dissemination of online private information",2022,"Computer Networks","202",,"108614","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119660458&doi=10.1016%2fj.comnet.2021.108614&partnerID=40&md5=65a8859c8f6d61277f5f7c38bcee14b5","Given the increasing popularity of social media and other Internet-related technologies, individuals spend a lot of time across different online activities, such as doing Google searches or credit card purchases, enjoying social networks interactions, performing job finding or travel plannings activities. Unfortunately, very often, individuals unwittingly disseminate a huge amount of personal and sensitive information that fundamentally represents an essential part of their private life. A large part of this information is embedded into text messages typed during online activities. Therefore, there is an increasing need for mechanisms to assist individuals during such activities, raising their awareness about potential violation of privacy at the time of disclosure; however, it is also essential to give them full control on whether and how to manage their data, thereby empowering them to make heedful decisions. The awareness can be realized through simple alert/highlight mechanisms, while the full control can be ensured by allowing users to make the final choice, that is, ignore warnings, or conversely accept them and thus (a) think twice before disseminating data (to avoid future regrets), or (b) anyway send data, but only after their anonymization. In this paper, we propose a novel approach based on machine learning and sentence embedding techniques with the primary goal of providing privacy awareness to users and, as a consequence, full control over their data during online activities. Our approach relies on the definition of four modules: (i) the Keyword module, which identifies personal and sensitive data in a text (from the syntactic point of view); (ii) the Topic module, which is devoted to understand the topic treated in text messages; (iii) the Sensitiveness module, which identifies sensitive information (from the semantic point of view) into text messages; lastly, (iv) the Personalization module, which goal is to learn the personal attitude of a user towards his/her own privacy (through opportune feedback) and therefore report the correct alert messages. We provided an implementation of such an approach, named Knoxly, as a prototype of a Google Chrome extension. The tool has undergone a preliminary experimental study to assess its effectiveness in terms of sensitive information identification accuracy, and its efficiency in terms of impact on user experience. © 2021 Elsevier B.V.","Experimental study; Machine learning; Privacy awareness and information leakage","Data privacy; E-learning; Search engines; Semantics; Social networking (online); Automatic mechanisms; Experimental study; Full control; Google+; Information leakage; Online activities; Privacy awareness; Privacy awareness and information leakage; Privacy information; Sensitive informations; Machine learning",Article,Scopus,2-s2.0-85119660458
"Lai Y., Papadopoulos S., Fuerst F., Pivo G., Sagi J., Kontokosta C.E.","57219033256;56597652800;35172743000;23061107800;8226225500;55070914600;","Building retrofit hurdle rates and risk aversion in energy efficiency investments",2022,"Applied Energy","306",,"118048","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118560500&doi=10.1016%2fj.apenergy.2021.118048&partnerID=40&md5=d279e971ca44aac4b082ebb86e1bbc07","Despite extensive empirical evidence of the environmental benefits of green buildings and the increasing urgency to reduce carbon emissions in cities, there has been limited widespread adoption of energy retrofit investments in existing buildings. In this paper, we empirically model financial returns to energy retrofit investments for more than 3600 multifamily and commercial buildings in New York City, using a comprehensive database of energy audits and renovation work extracted from city records using a natural language processing algorithm. Based on auditor cost and savings estimates, the median internal rate of return for adopted energy conservation measures is 21% for multifamily buildings and 25% for office properties. Logistic regression modeling demonstrates adoption rates are higher for office buildings than multifamily, and in both cases adopter buildings tend to be larger, higher value, and less energy efficient prior to retrofit implementation. The economically significant magnitudes of returns to adopted energy conservation measures raise important questions about why many property owners choose not to adopt. As such, we discuss incentive and regulatory mechanisms that can overcome financial and informational barriers to the adoption of energy efficiency measures. © 2021","Building energy; Energy efficiency; Financial analysis; Machine learning; Retrofit investment","Earnings; Energy efficiency; Historic preservation; Learning algorithms; Machine learning; Natural language processing systems; Office buildings; Retrofitting; Risk assessment; Building energy; Building retrofits; Energy conservation measures; Energy efficiency investments; Energy retrofit; Environmental benefits; Financial analysis; Green buildings; Retrofit investment; Risk aversion; Investments; conservation; database; energy conservation; energy efficiency; incentive; modeling; New York; New York [New York (STT)]; York",Article,Scopus,2-s2.0-85118560500
"Wang H., Qu Z., Zhou Q., Zhang H., Luo B., Xu W., Guo S., Li R.","57203550518;57189048446;57203187157;57255480500;57222322238;57190585649;7403649953;7404724385;","A Comprehensive Survey on Training Acceleration for Large Machine Learning Models in IoT",2022,"IEEE Internet of Things Journal","9","2",,"939","963",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114719754&doi=10.1109%2fJIOT.2021.3111624&partnerID=40&md5=88290d5fb30656ec9926bb4c5e3de69f","The ever-growing artificial intelligence (AI) applications have greatly reshaped our world in many areas, e.g., smart home, computer vision, natural language processing, etc. Behind these applications are usually machine learning (ML) models with extremely large size, which require huge data sets for accurate training to mine the value contained in the big data. Large ML models, however, can consume tremendous computing resources to achieve decent performance and thus, it is difficult to train them in resource-constrained Internet of Things (IoT) environments, which would prevent further development and application of AI techniques in the future. To deal with such challenges, there are many efforts on accelerating the training process for large ML models in IoT. In this article, we provide a comprehensive review on the recent advances toward reducing the computing cost during the training stage while maintaining comparable model accuracy. Specifically, the optimization algorithms that aim to improve the convergence rate are emphasized over various distributed learning architectures that exploit ubiquitous computing resources. Then, the article elaborates the computation hardware acceleration and communication optimization for collaborative training among multiple learning entities. Finally, the remaining challenges, future opportunities, and possible directions are discussed. © 2014 IEEE.","Distributed machine learning (ML); hardware-aided acceleration; large model training; training acceleration","Automation; Internet of things; Machine learning; Natural language processing systems; Ubiquitous computing; Collaborative training; Communication optimization; Development and applications; Hardware acceleration; Internet of Things (IOT); NAtural language processing; Optimization algorithms; Training acceleration; Large dataset",Article,Scopus,2-s2.0-85114719754
"Gong H., Shi L., Zhai X., Du Y., Zhang Z.","57221980831;57188955623;57193166528;57363986300;14053189500;","Assembly process case matching based on a multilevel assembly ontology method",2022,"Assembly Automation","42","1",,"80","98",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120716174&doi=10.1108%2fAA-05-2021-0065&partnerID=40&md5=9f17cae36790744bb72684f109bf3cee","Purpose: The purpose of this study is to achieve accurate matching of new process cases to historical process cases and then complete the reuse of process knowledge and assembly experience. Design/methodology/approach: By integrating case-based reasoning (CBR) and ontology technology, a multilevel assembly ontology is proposed. Under the general framework, the knowledge of the assembly domain is described hierarchically and associatively. On this basis, an assembly process case matching method is developed. Findings: By fully considering the influence of ontology individual, case structure, assembly scenario and introducing the correction factor, the similarity between non-correlated parts is significantly reduced. Compared with the Triple Matching-Distance Model, the degree of distinction and accuracy of parts matching are effectively improved. Finally, the usefulness of the proposed method is also proved by the matching of four practical assembly cases of precision components. Originality/value: The process knowledge in historical assembly cases is expressed in a specific ontology framework, which makes up for the defects of the traditional CBR model. The proposed matching method takes into account all aspects of ontology construction and can be used well in cross-ontology similarity calculations. © 2021, Emerald Publishing Limited.","Assembly case matching; Case-based reasoning; Complex product; Ontology; Semantic similarity","Assembly; Case based reasoning; Semantics; Assembly case matching; Assembly process; Case matching; Casebased reasonings (CBR); Complex products; Matchings; Multilevels; Ontology's; Process knowledge; Semantic similarity; Ontology",Article,Scopus,2-s2.0-85120716174
"Huang T., Li M., Qin X., Zhu W.","57205853564;57226655945;14023330000;8985800200;","A CNN-based policy for optimizing continuous action control by learning state sequences",2022,"Neurocomputing","468",,,"286","295",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118138176&doi=10.1016%2fj.neucom.2021.10.004&partnerID=40&md5=e832b87c5185cfcd83b817f1ee1700e9","Continuous action control is widespread in real-world applications. It controls an agent to take action in continuous space for transiting from one state to another until achieving the desired goal. The optimization of continuous action control is an important issue, which aims to find the optimal policy for the agent to achieve the desired goal with the lowest consumption in continuous action space. A useful tool for this issue is reinforcement learning where an optimal policy is learned for the agent by maximizing the cumulative reward of the state transitions. When updating the policy at each state, most existing reinforcement learning methods consider only the one-step transition of this state. However, for each state in continuous action control, the recognizable information is usually hidden in the sequence of its previous states, thus these methods cannot learn the policy effectively enough for continuous action control. In this paper, we propose a new policy, called convolutional deterministic policy, to solve this problem. Enlightened from the convolutional neural networks used in natural language processing, our convolutional deterministic policy uses convolutional neural networks to learn the recognizable information in the state sequences. Then for each collected state, we update the convolutional deterministic policy by not only the recognizable information in the one-step transition of this state but also the recognizable information in the sequence of its previous states. As a result, our convolutional deterministic policy can make the agent take better action. Based on an effective reinforcement learning method, TD3, the implementation of our convolutional deterministic policy is in CTD3. The theoretical analysis and the experiment illustrate that our CTD3 can learn the policy not only better than but also faster than the existing RL methods for continuous action control. The source code can be downloaded from https://github.com/grcai. © 2021 Elsevier B.V.","Continuous action control; Convolutional neural networks; Deterministic policy; Reinforcement learning; State sequences","Convolution; Convolutional neural networks; Learning algorithms; Natural language processing systems; Continuous action control; Continuous actions; Convolutional neural network; Deterministic policy; Deterministics; Learn+; Optimal policies; Reinforcement learning method; State sequences; Step transitions; Reinforcement learning; article; conformational transition; convolutional neural network; natural language processing; reinforcement (psychology); reward; theoretical study",Article,Scopus,2-s2.0-85118138176
"Bakhshi M., Nematbakhsh M., Mohsenzadeh M., Rahmani A.M.","56305099800;35616387700;26435355100;57204588830;","SParseQA: Sequential word reordering and parsing for answering complex natural language questions over knowledge graphs",2022,"Knowledge-Based Systems","235",,"107626","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120992141&doi=10.1016%2fj.knosys.2021.107626&partnerID=40&md5=19ba7d26392c7b587eab04e942c1410f","One of the effective approaches for answering natural language questions (NLQs) over knowledge graphs consists of two main stages. It first creates a query graph based on the NLQ and then matches this graph over the knowledge graph to construct a structured query. An obstacle in the first stage is the need to build question interpretations with candidate resources, even if some implicit phrases exist in the sentence. In the second stage, a serious problem is to map diverse NLQ relations to their corresponding predicates. To overcome these problems, in this paper, we propose a novel sequential word parsing-based method to construct and refine an uncertain question graph that is disambiguated directly over the knowledge graph. Instead of relying on the syntactic dependency relations and some predefined rules that recognize the relations and their arguments, we consider the identified entities and variables in the NLQ as well as their corresponding place in the structure of a query graph pattern to build question triples. First, by leveraging the ordered dependency tree of an NLQ, sentence words are reordered. Then the question graph structure is constructed by parsing the new sequence backward, starting from the identified items. Subsequently, the question graph is refined by eliminating the useless elements. Additionally, to improve the relation similarity measure in the graph similarity process, we exploit the knowledge hidden in a relation pattern taxonomy. Experimental studies over several benchmarks demonstrate that our proposed approach is effective as it achieves promising results in answering the complex NLQs. © 2021","RDF complex question answering; Relation pattern-based similarity; Sequential word reordering and parsing; Uncertain question graph construction","Graphic methods; Natural language processing systems; Query processing; Resource Description Framework (RDF); Syntactics; Complex questions; Graph construction; Knowledge graphs; Natural language questions; Question Answering; RDF complex question answering; Relation pattern-based similarity; Sequential word reordering and parsing; Uncertain question graph construction; Word reordering; Knowledge graph",Article,Scopus,2-s2.0-85120992141
"Wang J., Zhang Y., Yu L.-C., Zhang X.","56802804300;57223748571;35235928000;24802813900;","Contextual sentiment embeddings via bi-directional GRU language model",2022,"Knowledge-Based Systems","235",,"107663","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118853545&doi=10.1016%2fj.knosys.2021.107663&partnerID=40&md5=3293c9d7b04b9e854a5bf587d1d28dcf","Compared with conventional word embeddings, sentiment embeddings can distinguish words with similar contexts but opposite sentiment. They can be used to incorporate sentiment information from labeled corpora or lexicons by either end-to-end training or sentiment refinement. However, these methods present two major limitations. First, traditional approaches provide a fixed representation to each word but ignore the alternation of word meaning in different contexts. As a result, the polarity of a certain emotional word may vary with context, but will be assigned with a same representation. Another problem is the handling of out-of-vocabulary (OOV) or informal-writing sentiment words that would be assigned generic vectors (e.g., <UNK>). In addition, if affective words are not included in affective corpora or lexicons, they would be treated as neutral. Using such low-quality embeddings for building a neural model will reduce performance. This study proposes a training model of contextual sentiment embeddings. A stacked two-layer GRU model was used as the language model, simultaneously trained to incorporate semantic and sentiment information from labeled corpora and lexicons. To deal with OOV or informal-writing sentiment words, the WordPiece tokenizer was used to divide the text into subwords. The resulting model can be transferred to downstream applications by either feature extractor or fine-tuning. The results show that the proposed model can handle unseen or informal writing sentiment words and thus outperforms previously proposed methods. © 2021 The Author(s)","Contextual sentiment embeddings; Gated recurrent unit; Pre-trained language model; Sentiment analysis","Computational linguistics; Semantics; Sentiment analysis; Bi-directional; Contextual sentiment embedding; Embeddings; End to end; Gated recurrent unit; Language model; Pre-trained language model; Sentiment analysis; Traditional approaches; Word meaning; Embeddings",Article,Scopus,2-s2.0-85118853545
"Škvorc T., Gantar P., Robnik-Šikonja M.","57190075400;55699375300;55900495300;","MICE: Mining Idioms with Contextual Embeddings",2022,"Knowledge-Based Systems","235",,"107606","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118185442&doi=10.1016%2fj.knosys.2021.107606&partnerID=40&md5=5ebf36025a785052b607084ef1d0173e","Idiomatic expressions can be problematic for natural language processing applications as their meaning cannot be inferred from their constituting words. A lack of successful methodological approaches and sufficiently large datasets prevents the development of machine learning approaches for detecting idioms, especially for expressions that do not occur in the training set. We present an approach called MICE that uses contextual embeddings for that purpose. We present a new dataset of multi-word expressions with literal and idiomatic meanings and use it to train a classifier based on two state-of-the-art contextual word embeddings: ELMo and BERT. We show that deep neural networks using both embeddings perform much better than existing approaches and are capable of detecting idiomatic word use, even for expressions that were not present in the training set. We demonstrate the cross-lingual transfer of developed models and analyze the size of the required dataset. © 2021 Elsevier B.V.","Contextual embeddings; Cross-lingual transfer; Idiomatic expressions; Machine learning; Natural language processing; Word embeddings","Classification (of information); Deep neural networks; Large dataset; Mammals; Natural language processing systems; Contextual embedding; Cross-lingual; Cross-lingual transfer; Embeddings; Idiomatic expression; Idiomatics; Natural language processing applications; Training sets; Word embedding; Embeddings",Article,Scopus,2-s2.0-85118185442
"An N., Chen M., Lian L., Li P., Zhang K., Yu X., Yin Y.","57312684300;55510568900;35194128500;57223258003;57223257564;35114418500;8981026100;","Enabling the interpretability of pretrained venue representations using semantic categories",2022,"Knowledge-Based Systems","235",,"107623","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117919595&doi=10.1016%2fj.knosys.2021.107623&partnerID=40&md5=57d4b3ce29bb55a308fb27cf3b87b584","The growing popularity of location-based social networks gives rise to a tremendous amount of social check-ins data, which are broadly used in previous studies to produce dense venue representations for various trajectory mining tasks. In this work, we focus on the interpretability of venue representations, an essential property that existing methods fail to provide. We propose two novel models to generate interpretable and easy-to-understand venue representations. The first model, CEM, is a category-aware (a category may be a restaurant, a mall, etc.) check-in embedding model and generates venue and category representations by capturing the sequential patterns of check-in records. With the second model, XEM, each dimension of the venue representation corresponds to a semantic anchor (i.e., a category) and can be interpreted as a coherent topic. We conduct extensive experiments using real-world check-in datasets for venue similarity computation and venue semantic annotation, and empirically show that introducing interpretability to the venue representations improves the performance of various downstream tasks. © 2021 Elsevier B.V.","Check-ins; Embedding learning; Interpretable; Semantic mapping; Venue semantic representation","Semantics; Check-in; Check-ins; Embedding learning; Embeddings; Interpretability; Interpretable; Semantic category; Semantic representation; Semantics mappings; Venue semantic representation; Embeddings",Article,Scopus,2-s2.0-85117919595
"Zhong H., Ning Z., Li G., Li Z.","56014477200;35272886900;57226405120;57226400360;","A method of core concept extraction based on semantic-weight ranking",2022,"Concurrency and Computation: Practice and Experience","34","1","e6504","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111471816&doi=10.1002%2fcpe.6504&partnerID=40&md5=b7240963f3034403cf38df74450c12f4","In order to improve the efficiency of domain ontology concept extraction, this paper proposes a method of semantic-weight ranking to extract core concepts. Firstly, we make segmentation processing for corpus texts and obtain the candidate concept set, and then analyze the correlation between the candidate concepts through the method of semantic similarity to form a mesh graph structure. We secondly calculate the semantic-weight of the concept according to the generated mesh graph. Ultimately, we realize the core concept extraction via ranking different semantic-weight. At the same time, we give the fuzzy relationship representation of the core concepts through the statistical characteristics of the core concepts' membership. On the foundation of the extraction method, we carried out the experiments with the financial text and had achieved an accuracy rate of 81.8%, which could effectively extract the core concepts and play a positive role in the content construction, knowledge sharing, and knowledge reuse. © 2021 John Wiley & Sons, Ltd.","concept extraction; concept membership; ranking; semantic-weight","Extraction; Graph structures; Mesh generation; Concept extraction; Domain ontologies; Extraction method; Fuzzy relationship; Knowledge-sharing; Segmentation processing; Semantic similarity; Statistical characteristics; Semantics",Article,Scopus,2-s2.0-85111471816
"Xu H., Chai L., Luo Z., Li S.","57218657395;57218658381;57192367715;57204367236;","Stock movement prediction via gated recurrent unit network based on reinforcement learning with incorporated attention mechanisms",2022,"Neurocomputing","467",,,"214","228",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121256075&doi=10.1016%2fj.neucom.2021.09.072&partnerID=40&md5=c370edf71122dd8a3790d6e65e7d13d8","The recent advances usually mine market information from the chaotic data to conduct a stock movement prediction task. However, the current stock price movement prediction approaches mainly compute attention weighted sum of the global contextual semantic embeddings, which fails to combine local word-level or char-level ones to jointly learn news-level representation. Moreover, for Chinese stock price movement prediction task, some collected news texts are chaotic even irrelevant to the target stock. It suggests that the models need filter some news-level representations (viewed as noises) to enhance the performance. To that aim, we develop a novel stock price movement prediction network via bidirectional gated recurrent unit (GRU) network based on reinforcement learning (RL) with incorporated attention mechanism. In specific, to reduce the noise of news texts and learn news-level representation with more abundant semantics, two novel attention mechanisms respectively based on add and dot operation were first proposed in this work. We then design a novel GRU structure based on RL to filter some irrelated news-level representations (i.e., news-level noises) and capture abundant long-term dependencies. Finally, the experimental results show that the proposed model far outperforms the recent advances and achieves state-of-the-art performances. © 2021 Elsevier B.V.","Attention; Gated recurrent unit; Reinforcement learning; Stock movement prediction","Financial markets; Forecasting; Motion estimation; Semantics; Attention; Attention mechanisms; Chaotics; Gated recurrent unit; Movement prediction; Network-based; Prediction tasks; Stock movement; Stock movement prediction; Stock price movement predictions; Reinforcement learning; Article; China; digital filtering; forecasting; gated recurrent unit network; information processing; information retrieval; prediction; price; reinforcement learning (machine learning); stock market",Article,Scopus,2-s2.0-85121256075
"Chen Y., Wu C., Huang Y.","55954392900;57200913734;57298677400;","Enhancing structure modeling for relation extraction with fine-grained gating and co-attention",2022,"Neurocomputing","467",,,"282","291",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117254886&doi=10.1016%2fj.neucom.2021.10.002&partnerID=40&md5=4573017c1d65b60dba3ab2b387f82ba9","Relation extraction is a critical natural language processing task. Existing dependency-based models captured long-range syntactic relations, but they usually cannot fully exploit information from sentences. They often used hand-crafted rules to prune redundant edges from dependency trees, but suffer from the imbalance of including and removing contents. When incorporating sequence models, they usually ignored the semantic and syntactic interactions between words. In this paper, we propose to automatically learn relational dependency structures with a fine-grained gating strategy. We decompose the dependency tree into differently informative parts and apply different gating methods to each part. To further capture the word-level interactions, we propose to apply the co-attention mechanism to combine structure and sequence models. We apply a neural network to learn the affinity matrix and derive mutual attention weights between semantic and syntactic representations. We conduct experiments on two benchmark datasets and the results indicate the effectiveness of our method. © 2021 Elsevier B.V.","Co-attention; Fine-grained gating; Relation extraction","Extraction; Forestry; Natural language processing systems; Semantics; Co-attention; Dependency structures; Dependency trees; Fine grained; Fine-grained gating; Handcrafted rules; Learn+; Relation extraction; Sequence models; Structure models; Syntactics; article; attention; decomposition; extraction",Article,Scopus,2-s2.0-85117254886
"Zhan H., Xiong P., WANG X., Yang L.","57188580579;57212197890;57221484491;57202640568;","Visual question answering by pattern matching and reasoning",2022,"Neurocomputing","467",,,"323","336",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117201881&doi=10.1016%2fj.neucom.2021.10.016&partnerID=40&md5=05cdac184420788db487019a32e512a7","Traditional techniques for visual question answering (VQA) are mostly end-to-end neural network based, which often perform poorly (e.g., low accuracy) due to lack of understanding and reasoning. To overcome the weaknesses, we propose a comprehensive approach with following key features. (1) It represents inputs, i.e., an image I and a natural language question Qnl as an entity-attribute graph and a query graph, respectively, and employs pattern matching to find answers; (2) it leverages reinforcement learning based model to identify a set of policies that are used to guide visual tasks and construct an entity-attribute graph, based on Qnl; (3) it employs a novel method to parse a question Qnl and generate corresponding query graph Q(uo) for pattern matching; and (4) it integrates inference scheme to further improve result accuracy, in particular, it learns a graph-structured classifier for missing value inference and a co-occurrence matrix for candidate selection. With these features, our approach can not only process visual tasks efficiently, but also answer questions with high accuracy. To evaluate the performance of our approach, we conduct empirical studies on Soccer, Visual-Genome and GQA, and show that our approach outperforms the state-of-the-art methods in result accuracy and system efficiency. © 2021 Elsevier B.V.","Inference; Pattern matching; Reinforcement learning; Visual question answering","Image enhancement; Natural language processing systems; Pattern matching; Query processing; Vision; Visual languages; Attribute graphs; End to end; Inference; Neural-networks; Pattern-matching; Query graph; Question Answering; Traditional techniques; Visual question answering; Visual tasks; Reinforcement learning; article; classifier; diagnostic test accuracy study; empiricism; human; human experiment; language; reasoning; reinforcement (psychology); soccer",Article,Scopus,2-s2.0-85117201881
"Li Q., Zhang Y., Sun S., Wu J., Zhao X., Tan M.","57216491951;57191205490;57291841200;57205208967;57292530200;57291164600;","Cross-modality synergy network for referring expression comprehension and segmentation",2022,"Neurocomputing","467",,,"99","114",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116891692&doi=10.1016%2fj.neucom.2021.09.066&partnerID=40&md5=0fea7f58336b2d71cde6c77b8e286524","Referring expression comprehension and segmentation aim to locate and segment a referred instance in an image according to a natural language expression. However, existing methods tend to ignore the interaction between visual and language modalities for visual feature learning, and establishing a synergy between the visual and language modalities remains a considerable challenge. To tackle the above problems, we propose a novel end-to-end framework, Cross-Modality Synergy Network (CMS-Net), to address the two tasks jointly. In this work, we propose an attention-aware representation learning module to learn modal representations for both images and expressions. A language self-attention submodule is proposed in this module to learn expression representations by leveraging the intra-modality relations, and a language-guided channel-spatial attention submodule is introduced to obtain the language-aware visual representations under language guidance, which helps the model pay more attention to the referent-relevant regions in the images and relieve background interference. Then, we design a cross-modality synergy module to establish the inter-modality relations for modality fusion. Specifically, a language-visual similarity is obtained at each position of the visual feature map, and the synergy is achieved between the two modalities in both semantic and spatial dimensions. Furthermore, we propose a multi-scale feature fusion module with a selective strategy to aggregate the important information from multi-scale features, yielding target results. We conduct extensive experiments on four challenging benchmarks, and our framework achieves significant performance gains over state-of-the-art methods. © 2021 Elsevier B.V.","Attention mechanism; Cross-modality synergy; Referring expression comprehension; Referring expression segmentation","Benchmarking; Image segmentation; Natural language processing systems; Visual languages; Attention mechanisms; Cross modality; Cross-modality synergy; Learn+; Referring expression comprehension; Referring expression segmentation; Referring expressions; Submodules; Visual feature; Semantics; article; attention; comprehension; feature learning (machine learning); human; human experiment; language; protein expression",Article,Scopus,2-s2.0-85116891692
"Montanari A., Sala P.","7101889543;23092060700;","Reactive synthesis from interval temporal logic specifications",2022,"Theoretical Computer Science","899",,,"48","79",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120912267&doi=10.1016%2fj.tcs.2021.11.023&partnerID=40&md5=0aa6d70503862ffccd8c02c186d70ea8","In this paper, we deal with the synthesis problem for Halpern and Shoham's interval temporal logic HS extended with an equivalence relation ∼ over time points (HS[Formula presented] for short). The definition of the problem is analogous to that for MSO(ω,&lt;). Given an HS[Formula presented] formula φ and a finite set Σφ⋄ of proposition letters and temporal requests, it consists of determining, whether or not, for all possible valuations of elements in Σφ⋄ in every interval structure, there is a valuation of the remaining proposition letters and temporal requests such that the resulting structure is a model for φ. We focus on the decidability and complexity of the problem for some meaningful fragments of HS[Formula presented], whose modalities are drawn from the set {A(meets),A¯(metby),B(begunby),B¯(begins)} interpreted over finite linear orders and N. We prove that, over finite linear orders, the problem is decidable (ACKERMANN-hard) for [Formula presented] and undecidable for AA¯BB¯. Moreover, we show that if we replace finite linear orders by N, then it becomes undecidable even for ABB¯. Finally, we study the generalization of [Formula presented] to [Formula presented], where k is the number of distinct equivalence relations. Despite the fact that the satisfiability problem for [Formula presented], with k&gt;1, over finite linear orders, is already undecidable, we prove that, under a natural semantic restriction (refinement condition), the synthesis problem turns out to be decidable. © 2021 Elsevier B.V.","Complexity; Decidability; Interval temporal logic; Synthesis","Computer circuits; Semantics; Set theory; Temporal logic; Complexity; Equivalence relations; Finite set; Interval structures; Interval temporal logic; Linear order; Reactive synthesis; Synthesis problems; Temporal logic specifications; Time points; Computability and decidability",Article,Scopus,2-s2.0-85120912267
"Shao X., Li M., Yang Y., Li X., Han Z.","57202154088;57194229985;57196442995;55718164500;7402859192;","The Neural Basis of Semantic Prediction in Sentence Comprehension",2022,"Journal of Cognitive Neuroscience","34","2",,"236","257",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122680275&doi=10.1162%2fjocn_a_01793&partnerID=40&md5=ac0b494765245d162968df5e1ee129c6","Although prediction plays an important role in language comprehension, its precise neural basis remains unclear. This fMRI study investigated whether and how semantic-category-specific and common cerebral areas are recruited in predictive semantic processing during sentence comprehension. We manipulated the semantic constraint of sentence contexts, upon which a tool-related, a building-related, or no specific category of noun is highly predictable. This noun-predictability effect was measured not only over the target nouns but also over their preceding transitive verbs. Both before and after the appearance of target nouns, left anterior supramarginal gyrus was specifically activated for tool-related nouns and left parahippocampal place area was activated specifically for building-related nouns. The semantic-category common areas included a subset of left inferior frontal gyrus during the anticipation of incoming target nouns (activity enhancement for high predictability) and included a wide spread of areas (bilateral inferior frontal gyrus, left superior/middle temporal gyrus, left medial pFC, and left TPJ) during the integration of actually perceived nouns (activity reduction for high pre-dictability). These results indicated that the human brain recruits fine divisions of cortical areas to distinguish different semantic categories of predicted words, and anticipatory semantic processing relies, at least partially, on top–down prediction conducted in higher-level cortical areas. © 2021 Massachusetts Institute of Technology.",,"Brain; Semantics; Activity enhancement; Category specifics; Cerebral areas; Common areas; Cortical areas; Inferior frontal gyrus; Language comprehensions; Semantic category; Semantic constraints; Semantic processing; Forecasting; anticipation; article; comprehension; functional magnetic resonance imaging; human; human experiment; inferior frontal gyrus; language; middle temporal gyrus; parahippocampal place area; prediction; supramarginal gyrus; brain; brain mapping; diagnostic imaging; nuclear magnetic resonance imaging; semantics; Brain; Brain Mapping; Comprehension; Humans; Language; Magnetic Resonance Imaging; Semantics",Article,Scopus,2-s2.0-85122680275
"Fernández-González D., Gómez-Rodríguez C.","55358318900;23389267800;","Multitask Pointer Network for multi-representational parsing",2022,"Knowledge-Based Systems","236",,"107760","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120946036&doi=10.1016%2fj.knosys.2021.107760&partnerID=40&md5=499d3ea5eb818a88e19ef8439283bfea","Dependency and constituent trees are widely used by many artificial intelligence applications for representing the syntactic structure of human languages. Typically, these structures are separately produced by either dependency or constituent parsers. In this article, we propose a transition-based approach that, by training a single model, can efficiently parse any input sentence with both constituent and dependency trees, supporting both continuous/projective and discontinuous/non-projective syntactic structures. To that end, we develop a Pointer Network architecture with two separate task-specific decoders and a common encoder, and follow a multitask learning strategy to jointly train them. The resulting quadratic system, not only becomes the first parser that can jointly produce both unrestricted constituent and dependency trees from a single model, but also proves that both syntactic formalisms can benefit from each other during training, achieving state-of-the-art accuracies in several widely-used benchmarks such as the continuous English and Chinese Penn Treebanks, as well as the discontinuous German NEGRA and TIGER datasets. © 2021 The Authors","Computational linguistics; Constituent parsing; Deep learning; Dependency parsing; Natural language processing; Neural network; Parsing","Computational linguistics; Deep learning; Forestry; Learning algorithms; Natural language processing systems; Network architecture; Neural networks; Constituent parsing; Deep learning; Dependency parsing; Dependency trees; Human language; Neural-networks; Parsing; Quadratic systems; Single models; Syntactic structure; Syntactics",Article,Scopus,2-s2.0-85120946036
"He L., Zhang X., Li Z., Xiao P., Wei Z., Cheng X., Qu S.","57465676600;57466119200;57465384100;57465526900;57465384200;57466119300;7202361933;","A Chinese Named Entity Recognition Model of Maintenance Records for Power Primary Equipment Based on Progressive Multitype Feature Fusion",2022,"Complexity","2022",,"8114217","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125192728&doi=10.1155%2f2022%2f8114217&partnerID=40&md5=5024c552d2e46e0d0500fb022b443f74","Presently, the State Grid Corporation of China has accumulated a large amount of maintenance records for power primary equipment. Unfortunately, most of these records are unstructured data which lead to difficultly analyze and utilize them. The emergence of natural language processing technology and deep learning methods provide a solution for unstructured text data. This paper proposes a progressive multitype feature fusion model to recognize Chinese named entity of unstructured maintenance records for power primary equipment. Firstly, the textual characteristics and word separation difficulties of maintenance records are analyzed, then 7 main entity categories of power technical terms from unstructured maintenance records are chosen, and 3452 maintenance records are labeled by these categories, which is so called EPE-MR training dataset. Secondly, the standard test reports, standard maintenance, and fault analysis reports for three types of power primary equipment (namely, main transformer, circuit breaker, and isolating switch) are employed as corpus to train character embedding in order to obtain certain words representation ability of maintenance records. After that, progressive multilevel radicals feature extraction module is designed to get detailed and fine semantic information in a hierarchical manner. Further, radicals feature representation and character embedding are concatenated and sent to BiLSTM module to extract contextual information in order to improve Chinese entity recognition ability. Moreover, CRF is introduced to handle the dependencies among prediction labels and to output the optimal prediction sequence, which can easily obtain structured data of maintenance records. Finally, comparative experiments on public MSRA dataset, China People's Daily corpus, and EPE-MR dataset are implemented, respectively, which show the effectiveness of the proposed method. © 2022 Lanfei He et al.",,"Deep learning; Electric circuit breakers; Electric transformer testing; Maintenance; Natural language processing systems; Semantics; Chinese named entity recognition; Embeddings; Features fusions; Large amounts; Maintenance records; Multitype; Power; Primary equipments; Recognition models; Unstructured data; Embeddings",Article,Scopus,2-s2.0-85125192728
"Yang X., Ma H., Wang M.","57189215355;57463897800;55732025600;","Rumor Detection with Bidirectional Graph Attention Networks",2022,"Security and Communication Networks","2022",,"4840997","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125124024&doi=10.1155%2f2022%2f4840997&partnerID=40&md5=c045d2f170bcf72cb31ab07db792c5a6","In order to extract the relevant features of rumors effectively, this paper proposes a novel rumor detection model with bidirectional graph attention network on the basis of constructing a directed graph, named P-BiGAT. Firstly, this model builds the propagation tree and diffusion tree through the tweet comment and reposting relationship. Secondly, the improved graph attention network (GAT) is used to extract the propagation feature and the diffusion feature through two different directions, and the multihead attention mechanism is used to extract the semantic information of the source tweet. Finally, the propagation feature, diffusion feature, and semantic information representation of the source tweet are connected together through a fully connected layer, and the mapping function is used to determine the authenticity of the information. In addition, this paper also proposes a new node update method and applies it to the model in order to select neighbor node information effectively. Specifically, it can select the neighbor information node with larger weight to update the node according to the weight of the neighbor node. The results of the experiment show that the model is better than the baseline method of comparison in accuracy, precision, recall, and F1 measure on the public datasets. © 2022 Xiaohui Yang et al.",,"Directed graphs; Forestry; Semantics; Attention mechanisms; Detection models; Feature information; Information nodes; Information representation; Mapping functions; Multihead; Neighbour nodes; Relevant features; Semantics Information; Diffusion",Article,Scopus,2-s2.0-85125124024
"Liang W., Wang L., She J., Liu Y.","57463836500;57188659577;57463836600;57463723800;","Detecting Resource Release Bugs with Analogical Reasoning",2022,"Scientific Programming","2022",,"3518673","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125114856&doi=10.1155%2f2022%2f3518673&partnerID=40&md5=4d74ef03a6e682e52594e683215e07bf","The resource release bugs are a common type of serious programming bug. However, it is hard to catch them by using static detection for the lacking of comprehensive prior knowledge about the release functions. In this paper, a resource release bug detection method is proposed by introducing analogical reasoning on word vectors. First, the functions of the target source code are encoded into word vectors by the word embedding technique in natural language processing. Second, a two-stage reasoning method is developed for automatically identifying unknown resource release functions according to a few well-known seed functions. 3CosAvg algorithm is employed for the first stage, and a new algorithm is designed for the latter, called 3CosAddExchange. Finally, the identified release functions are translated into static analysis rules to detect potential bugs. The experiment shows that the proposed method is effective and efficient for the large-scale software project. Five unknown resource release bugs are successfully detected in the Linux kernel and confirmed by kernel developers. © 2022 Wentao Liang et al.",,"Computer operating systems; Natural language processing systems; Serious games; Analogical reasoning; Bug detection; Detection methods; Embedding technique; Prior-knowledge; Release function; Source codes; Static detections; Target source; Word vectors; Static analysis",Article,Scopus,2-s2.0-85125114856
"Zheng P.","57462374000;","Multisensor Feature Fusion-Based Model for Business English Translation",2022,"Scientific Programming","2022",,"3102337","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125091876&doi=10.1155%2f2022%2f3102337&partnerID=40&md5=fc1bd0c4c15fb8e0dd9b882ec0de82a8","With the wide use of computers, machine translation has been gradually applied in many fields from natural language processing, such as industry, education, and so on. Due to the increasing demand for multilanguage translation, it is an urgent problem to effectively improve the quality of text translation. Driven by the upsurge of artificial intelligence, neural network technology is increasingly integrated into the field of machine translation, which gradually expands the traditional machine translation method into neural machine translation method. With the continuous improvement of deep learning technology, machine translation has gradually integrated these methods and strategies and achieved good results in multiple tasks, but there are still some shortcomings. The most prominent problem is that, since word vector is the basis for the model to obtain semantic and grammatical information, the existing methods cannot obtain semantic and grammatical feature information, which greatly reduces the accuracy of English translation. Based on this, this paper proposed a method of splicing word vector with character- level and word-level encoding vector. The characterization of fusion of more word vector can effectively solve the word does not appear in the table, the word with some low frequency, can express meaning more complete information, performance directly affects the whole translation model, the results can be seen through the experiment, we put forward the characteristics of the fusion method and strategy, can effectively enhance the overall translation performance of the model. Copyright © 2022 Pingfei Zheng.",,"Computational linguistics; Computer aided language translation; Deep learning; Neural machine translation; Semantics; Vectors; Features fusions; Machine translation methods; Machine translations; Multi sensor; Multilanguages; Network technologies; Neural-networks; Performance; Urgent problems; Word vectors; Natural language processing systems",Article,Scopus,2-s2.0-85125091876
"Gong C., Bhargava R., Bajaj C.","57461958300;36719767900;7004410340;","Exploring the Study of miR-1301 Inhibiting the Proliferation and Migration of Squamous Cell Carcinoma YD-38 Cells through PI3K/AKT Pathway under Deep Learning Medical Images",2022,"Computational Intelligence and Neuroscience","2022",,"5865640","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125020118&doi=10.1155%2f2022%2f5865640&partnerID=40&md5=0e5eb1243dd7405b3989bb11bea6da2a","With the rapid development and application of deep learning medical image recognition, natural language processing, and other fields, at the same time, deep learning has become the most popular research direction in the field of image processing and recognition. Through deep learning medical image recognition technology, it is of great significance to explore the research of miR-1301. The purpose of this article is to use an improved CNN neural network model algorithm combined to contrast the experimental groups and use deep learning medical imaging technology to study the mechanism by which miR-1301 inhibits the proliferation of carcinoma YD-38 cells through the PI3K/AKT pathway. This paper studies the method of image recognition of squamous cell carcinoma YD-38 cells using a convolutional neural network (CNN). First, a CNN classification model for the characteristics of YD-38 cell images is constructed. Then, pretraining and dropout technology are used to improve and optimize the proposed CNN model to improve the robustness of the model. In this paper, the miR mimic group and the miR blank group and the PI3K/AKT pathway inhibitor Wortmannin were selected to jointly treat YD-38 cells. The expression of mRNA in miR-1301 in HGF-1 was determined using RT-PCR (real and real-time fluorescence and YD-38 cells). The blank plasmids and the miR-1301 mimic (miR-1301 mimic) were transfected into YD-38 cells. The experiments were divided into two groups in the miR-1301 blank group and the miR-1301 simulation groups, respectively. The proliferation capacity of YD-38 cells was prepared in 1.5 ml sterile EP tubes and then diluted with medium for the proliferation of the cells. The scratch test and Transwell test were used to detect the effect of miR-1-3p on the migration and invasion of liver cancer cells. In this paper, CCK-8 experiment, clone formation experiment, flow cytometry, scratch experiment, and Transwell chamber experiment are used to analyze the effects of target gene CAAP1 on the proliferation, apoptosis, migration, and invasion of liver cancer cells. This paper uses CCK-8 to detect five kinds of the effect of miRNA on the proliferation ability of liver cancer cells and the effect of miR-1-3p on the proliferation ability of liver cancer cells. Experimental studies have shown that, compared with the miR blank group, the expression of PI3K and p-AKT was significantly downregulated in the miR mimic group after 24, 48, and 72 hours and the phosphorylation level of AKT was also significantly reduced P<0.05. © 2022 Hindawi Limited. All rights reserved.",,"Cell death; Convolution; Deep learning; Diseases; Fluorescence microscopy; Image recognition; Medical imaging; Natural language processing systems; RNA; Convolutional neural network; Development and applications; Experimental groups; Image recognition technology; Images processing; Liver cancer cells; Medical image recognition; Model algorithms; Neural network model; Squamous cell carcinoma; Convolutional neural networks; microRNA; MIRN1301 microRNA, human; phosphatidylinositol 3 kinase; protein kinase B; cell motion; cell proliferation; genetics; human; metabolism; signal transduction; squamous cell carcinoma; Carcinoma, Squamous Cell; Cell Movement; Cell Proliferation; Deep Learning; Humans; MicroRNAs; Phosphatidylinositol 3-Kinases; Proto-Oncogene Proteins c-akt; Signal Transduction",Article,Scopus,2-s2.0-85125020118
"Wang J., Li H., Meng F., Wang P., Zheng Y., Yang X., Han J.","57212394211;57458779500;55426663000;57458324100;57454727800;57211214880;7406443759;","Chinese Text Implication Recognition Method based on ERNIE-Gram and CNN",2022,"Journal of Network Intelligence","7","1",,"129","146",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124879277&partnerID=40&md5=8aa599785070a5edfd84cc63cc180f24","Aiming at the problems of low accuracy in recognition of Chinese text implication and inability to better support machine reading comprehension, a Chinese text implication recognition method based on ERNIE-Gram and CNN is proposed. First, we use BERT word vector coding to encode the sentences to be input into the ERNIEGram model in three stages to improve the generalisation ability of the model. Then, the ERNIE-Gram model is used to obtain and integrate the semantic information of the text word level and sentence level, and different Warmup is introduced to optimize the learning rate in the ERNIE-Gram model training stage, and the ERNIE-Gram model parameters are continuously optimized. Finally, the fused semantic text is further extracted from the CNN for deeper semantic information, and the dimensionality reduction of the fully connected layer is used to identify the implied relationship of the text. The innovations are: (1) By introducing different Warmup in the ERNIE-Gram model to optimize the learning rate, it overcomes the shortcomings of insufficient semantic extraction due to model overfitting in traditional methods. (2) By adding a convolutional neural network to overcome the shortcomings of traditional methods of extracting semantic noise. Experimental results show that the accuracy of the method in identifying implication relations reaches 86 %, which lays a foundation for further efficient recognition of text implication. © The Authors.","CNN; ERNIE-Gram; Semantic information; Textual implication","Character recognition; Neural networks; Semantics; Chinese text; CNN; ERNIE-gram; Gram models; Learning rates; Reading comprehension; Recognition methods; Semantics Information; Textual implication; Word vectors; Learning algorithms",Article,Scopus,2-s2.0-85124879277
"Tian X., Wang J., Wen Y., Ma H.","7202379737;57221475907;57420448500;55723504800;","Multi-attribute scientific documents retrieval and ranking model based on GBDT and LR",2022,"Mathematical Biosciences and Engineering","19","4",,"3748","3766",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124807541&doi=10.3934%2fmbe.2022172&partnerID=40&md5=39b283e2c759a00cf9ebf8b7a2354f10","Scientific documents contain a large number of mathematical expressions and texts containing mathematical semantics. Simply using mathematical expressions or text to retrieve scientific documents can hardly meet retrieval needs. The real difficulty in retrieving scientific documents is to effectively integrate mathematical expressions and related textual features. Therefore, this study proposes a multi-attribute scientific documents retrieval and ranking model based on GBDT (gradient boosting decision tree) and LR (logistic regression) by integrating the expressions and text contained in scientific documents. First, the similarities of the five attributes are calculated, including mathematical expression symbols, mathematical expression sub-forms, mathematical expression context, scientific document keywords and the frequency of mathematical expressions. Next, the GBDT model is used to discretize and reorganize the five attributes. Finally, the reorganized features are input into the LR model, and the final retrieval and ranking results of scientific documents are obtained. The experiment in this study was carried out on the NTCIR dataset. The average value of the final MAP@20 of the scientific document recall was 81.92%. The average value of the scientific document ranking nDCG@20 was 86.05%. © 2022 the Author(s), licensee AIMS Press.","GBDT LR; Hesitation fuzzy sets; Mathematical expression; Multi-attribute feature; Scientific document retrieval and ranking","Information retrieval; Search engines; Semantics; Document ranking; Document Retrieval; Gradient boosting; Gradient boosting decision tree logistic regression; Hesitation fuzzy set; Logistics regressions; Mathematical expressions; Multi-attribute feature; Multi-attributes; Scientific document retrieval and ranking; Scientific documents; Decision trees",Article,Scopus,2-s2.0-85124807541
"Wan F., Yang Y., Zhu D., Yu H., Zhu A., Che G., Ma N., Rak E.","56039424600;57456966100;57267149700;57267789900;57219799182;57222402220;57204712002;57456773900;","Semantic Role Labeling Integrated with Multilevel Linguistic Cues and Bi-LSTM-CRF",2022,"Mathematical Problems in Engineering","2022",,"6300530","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124798742&doi=10.1155%2f2022%2f6300530&partnerID=40&md5=a0efa8599863849df1614ac489fec319","Chinese Semantic Role Labeling (SRL) is the core technology of semantic understanding. In the field of Chinese information processing, where statistical machine learning is still the mainstream, the traditional labeling methods rely heavily on the parsing degree of syntax and semantics of sentences. Therefore, the labeling precision is limited and cannot meet the current needs. This paper adopts the model based on a bidirectional long short-term memory network combined with the Conditional Random Field (Bi-LSTM-CRF). In the feature processing stage, pooling technology is combined with sampling and selecting multifeature vector groups to improve the performance of the sequence labeling model. Lexical, syntactic, and other multilevel linguistic features are integrated into the training to realize in-depth improvement of the original labeling model. Through several groups of experiments, the precision of model annotation in this paper has been significantly improved combined with linguistic-assisted analysis, which proves that it can optimize the annotation performance of the model by integrating relevant linguistic features into the model based on Bi-LSTM-CRF and sampling and extracting multifeature groups; the evaluation of F increases to 82.18 percent. Copyright © 2022 Fucheng Wan et al.",,"Random processes; Semantics; Syntactics; Chinese information processing; Core technology; Labelings; Memory network; Model-based OPC; Multifeatures; Multilevels; Semantic role labeling; Semantics understanding; Statistical machine learning; Long short-term memory",Article,Scopus,2-s2.0-85124798742
"Yang Q., Yang J.H.","57217875214;15623918300;","Virtual Reconstruction of Visually Conveyed Images under Multimedia Intelligent Sensor Network Node Layout",2022,"Journal of Sensors","2022",,"8367387","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124792161&doi=10.1155%2f2022%2f8367387&partnerID=40&md5=95d77a166aaf38fb555713fb357e64b8","In this paper, multimedia intelligent sensing technology is applied to the virtual reconstruction of images to construct or restore images to the communication media for visual communication. This paper proposes image virtual reconstruction theory based on visual communication research, treats image virtual reconstruction content as open data links and customized domain ontology, establishes an interdisciplinary interactive research framework through the technical means of visual communication, solves the problem of data heterogeneity brought by image virtual reconstruction, and finally establishes a three-dimensional visualization research method and principle of visual communication. The research firstly visual communication cuts into the existing conservation principles and proposes the necessity of image virtual reconstruction from the perspective of visual communication; secondly, the thinking mode of digital technology is different from human thinking mode, and the process of calculation ignores the emotional and spiritual values, but the realization of value rationality must be premised on instrumental rationality. This requires a content judgment and self-examination of the technical dimensional model of image virtual reconstruction on top of comprehensive literature and empirical evidence. In response to the research difficulties such as the constructivity of visual communication, the solution of image virtual reconstruction of visual communication is proposed based on the data collection method and literature characteristics. The process of introducing the tools of computer science into humanity research needs to be placed in a continuous critical theory system due to the uncontrollable and subjective nature of visual content, and finally, based on the construction of information models for image virtual reconstruction, the ontology and semantics of information modeling are thoroughly investigated, and the problems related to them, such as interpretation, wholeness, and interactivity, are analyzed and solved one by one. The transparency of image virtual reconstruction is enhanced through the introduction of interactive metadata, and this theoretical system of virtual restoration is put into practice in the Dunhuang digital display design project. © 2022 Qi Yang and Jong Hoon Yang.",,"3D modeling; Augmented reality; Data visualization; Image enhancement; Image reconstruction; Information management; Metadata; Ontology; Open Data; Restoration; Semantics; Sensor nodes; Three dimensional computer graphics; Virtual reality; Visualization; Communication media; Information Modeling; Intelligent sensing; Intelligent sensor networks; Node layouts; Restore image; Sensing technology; Sensor network nodes; Thinking modes; Virtual reconstruction; Visual communication",Article,Scopus,2-s2.0-85124792161
"Hamal O., Faddouli N.-E.E.","57207732368;57221388866;","Intelligent System Using Deep Learning for Answering Learner Questions in a MOOC",2022,"International Journal of Emerging Technologies in Learning","17","2",,"32","42",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124678097&doi=10.3991%2fIJET.V17I02.26605&partnerID=40&md5=41ed88f5afa5873cac82419486fdc2c5","Despite the great success of Massive Open Online Courses (MOOCs), the success rate of learners remains very low. Therefore, a lot of research has been done to understand and solve this abandonment problem. This work is part of the same effort which aims to improve learning in MOOCs. We offer an intelligent system capable of assisting the learner by providing answers to all his questions on the subjects covered in the MOOC. The architecture of our system is based on new advances in artificial intelligence, in particular the applications of deep learning in the field of natural language processing (NLP). The results obtained are quite interesting and demonstrate the relevance of our solution. © 2022. All Rights Reserved.","deep learning; improve learning; intelligent system; learner; MOOCs; NLP; question answering","Deep learning; Natural language processing systems; Deep learning; Improve learning; Learner; Massive open online course; Question Answering; Intelligent systems",Article,Scopus,2-s2.0-85124678097
"Zhao S., You F., Chang W., Zhang T., Hu M.","57214168521;57454190500;57454262100;57222349214;57215542768;","Augment BERT with average pooling layer for Chinese summary generation",2022,"Journal of Intelligent and Fuzzy Systems","42","3",,"1859","1868",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124674992&doi=10.3233%2fJIFS-211229&partnerID=40&md5=c2b68a8195853e1ccdcdccfd28ad0fb2","The BERT pre-Trained language model has achieved good results in various subtasks of natural language processing, but its performance in generating Chinese summaries is not ideal. The most intuitive reason is that the BERT model is based on character-level composition, while the Chinese language is mostly in the form of phrases. Directly fine-Tuning the BERT model cannot achieve the expected effect. This paper proposes a novel summary generation model with BERT augmented by the pooling layer. In our model, we perform an average pooling operation on token embedding to improve the model's ability to capture phrase-level semantic information. We use LCSTS and NLPCC2017 to verify our proposed method. Experimental data shows that the average pooling model's introduction can effectively improve the generated summary quality. Furthermore, different data needs to be set with varying pooling kernel sizes to achieve the best results through comparative analysis. In addition, our proposed method has strong generalizability. It can be applied not only to the task of generating summaries, but also to other natural language processing tasks. © 2022-IOS Press. All rights reserved.","average pooling; fine-Tuning bert; Summary generation; transformer","Semantics; Average pooling; Character level; Chinese language; Fine tuning; Fine-tuning bert; Language model; Performance; Subtask; Summary generation; Transformer; Natural language processing systems",Article,Scopus,2-s2.0-85124674992
"Duan J., Li L., Zhang M., Wang H.","8977960200;57454082100;55584796026;55485410900;","Hierarchical Preference Hash Network for News Recommendation",2022,"IEICE Transactions on Information and Systems","105","2",,"355","363",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124673013&doi=10.1587%2ftransinf.2021EDP7034&partnerID=40&md5=4a3b776b437cf4dd05238bf09dc2f5c7","Personalized news recommendation is becoming increasingly important for online news platforms to help users alleviate information overload and improve news reading experience. A key problem in news recommendation is learning accurate user representations to capture their interest. However, most existing news recommendation methods usually learn user representation only from their interacted historical news, while ignoring the clustering features among users. Here we proposed a hierarchical user preference hash network to enhance the representation of users' interest. In the hash part, a series of buckets are generated based on users' historical interactions. Users with similar preferences are assigned into the same buckets automatically. We also learn representations of users from their browsed news in history part. And then, a Route Attention is adopted to combine these two parts (history vector and hash vector) and get the more informative user preference vector. As for news representation, a modified transformer with category embedding is exploited to build news semantic representation. By comparing the hierarchical hash network with multiple news recommendation methods and conducting various experiments on the Microsoft News Dataset (MIND) validate the effectiveness of our approach on news recommendation. © 2022 The Institute of Electronics, Information and Communication Engineers.","Hierarchical hash network; Modified transformer; News recommendation","Hash functions; Clustering feature; Hierarchical hash network; Information overloads; Learn+; Modified transformer; News recommendation; Online news; Personalized news; Recommendation methods; User's preferences; Semantics",Article,Scopus,2-s2.0-85124673013
"Hiromoto M., Akima H., Ishihara T., Yamamoto T.","57454082200;57454296500;7402144529;57219174005;","SimpleZSL: Extremely Simple and Fast Zero-Shot Learning with Nearest Neighbor Classifiers",2022,"IEICE Transactions on Information and Systems","E105D","2",,"396","405",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124664645&doi=10.1587%2ftransinf.2021EDP7089&partnerID=40&md5=5e95e63f1373d56668e6de25f6b3b301","Zero-shot learning (ZSL) aims to classify images of unseen classes by learning relationship between visual and semantic features. Existing works have been improving recognition accuracy from various approaches, but they employ computationally intensive algorithms that require iterative optimization. In this work, we revisit the primary approach of the pattern recognition, i.e., nearest neighbor classifiers, to solve the ZSL task by an extremely simple and fast way, called SimpleZSL. Our algorithm consists of the following three simple techniques: (1) just averaging feature vectors to obtain visual prototypes of seen classes, (2) calculating a pseudo-inverse matrix via singular value decomposition to generate visual features of unseen classes, and (3) inferring unseen classes by a nearest neighbor classifier in which cosine similarity is used to measure distance between feature vectors. Through the experiments on common datasets, the proposed method achieves good recognition accuracy with drastically small computational costs. The execution time of the proposed method on a single CPU is more than 100 times faster than those of the GPU implementations of the existing methods with comparable accuracies. © 2022 The Institute of Electronics, Information and Communication Engineers.","Image recognition; Nearest neighbor classifier; Singular value decomposition; Zero-shot learning","Classification (of information); Image classification; Inverse problems; Iterative methods; Semantics; Singular value decomposition; Computationally intensive algorithms; Features vector; Iterative Optimization; Learning tasks; Nearest Neighbor classifier; Recognition accuracy; Semantic features; Simple++; Visual feature; Zero-shot learning; Image recognition",Article,Scopus,2-s2.0-85124664645
"Wei P., Zeng B., Liao W.","57192397558;7102493126;57219915897;","Joint intent detection and slot filling with wheel-graph attention networks",2022,"Journal of Intelligent and Fuzzy Systems","42","3",,"2409","2420",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124659164&doi=10.3233%2fJIFS-211674&partnerID=40&md5=c281f5c4e501e1a99b3117055e6157d9","Intent detection and slot filling are recognized as two very important tasks in a spoken language understanding (SLU) system. In order to model these two tasks at the same time, many joint models based on deep neural networks have been proposed recently and archived excellent results. In addition, graph neural network has made good achievements in the field of vision. Therefore, we combine these two advantages and propose a new joint model with a wheel-graph attention network (Wheel-GAT), which is able to model interrelated connections directly for single intent detection and slot filling. To construct a graph structure for utterances, we create intent nodes, slot nodes, and directed edges. Intent nodes can provide utterance-level semantic information for slot filling, while slot nodes can also provide local keyword information for intent detection. The two tasks promote each other and carry out end-To-end training at the same time. Experiments show that our proposed approach is superior to multiple baselines on ATIS and SNIPS datasets. Besides, we also demonstrate that using bi-directional encoder representation from transformer (BERT) model further boosts the performance of the SLU task. © 2022-IOS Press. All rights reserved.","attention mechanism; graph neural network; joint learning; Spoken language understanding","Deep neural networks; Directed graphs; Filling; Semantics; Wheels; Attention mechanisms; Directed edges; Graph neural networks; Graph structures; Intent detection; Joint learning; Joint models; Model-based OPC; Semantics Information; Spoken language understanding; Graph neural networks",Article,Scopus,2-s2.0-85124659164
"Adebisi E., Ojokoh B.A., Isinkaye F.O.","57450862600;36651575700;55135963100;","An Open Domain Factoid QA Framework with Improved Validation Techniques",2022,"International Journal of Information Science and Management","20","1",,"75","90",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124508056&partnerID=40&md5=4a3605b5dd2ac1be18f46ac592cc7324","The generic Question Answering (QA) framework processes questions by querying a knowledge base and extracting answers from retrieved passages using various Natural Language Processing techniques. The problem is validating whether the retrieved passages from the passage retrieval module contain expected answers to asked questions. Besides, extraction based on lexical and syntactic similarities alone is not enough coverage for scoring the correct answers in a QA framework. Therefore, this work aims to infuse validation techniques into the QA framework. Four similarity scores (Word Form (WF), Word Order (WO), Distance (DIST), and Semantic Similarity (SemSim)) were implemented for Answer Extraction. Instant snippets returned by the Google search engine were used as a corpus to generate candidate answer sets. On a dataset of 1370 factoid questions, the proposed method achieved an accuracy of 77.71%, precision of 77.91%, recall of 91.37%, and F1-measure of 91.37%. The results show that the inclusion of the validation techniques helps reduce the time spent by the system in analyzing passages without possible answers. The proposed system could be adapted for automatic QA Systems and grading factoid computer-based tests © 2022. International Journal of Information Science and Management. All Rights Reserved.","Factoid questions; Open domain; Question answering; Semantics; Textual entailment",,Article,Scopus,2-s2.0-85124508056
"Xu Y., Zhao T.","57218851822;57223082515;","Small-Scale Linguistic Steganalysis for Multi-Concealed Scenarios",2022,"IEEE Signal Processing Letters","29",,,"130","134",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124421693&doi=10.1109%2fLSP.2021.3128372&partnerID=40&md5=b8384b16dbc21dde232f1acd63be1773","Recently, due to the considerable feature expression ability of neural networks, deep linguistic steganalysis methods have been greatly developed. However, there are still two issues that need to be ameliorated. First, the prevailing linguistic steganalysis methods rely heavily on massive training data, which is labor-intensive and time-consuming. Second, these methods implement steganalysis only in different weak-concealed scenarios, the stego texts in each of which have only a single language style and payload. But in practice, the intercepted network samples are probably the mixture of the stego texts that possess different language styles and payloads, in which the semantic spatial distribution may be more chaotic than that in weak-concealed scenarios, thus making steganalysis more difficult. To address the above issues, a novel linguistic steganalysis method is proposed in this letter. First, the pre-trained BERT language model is constructed as an embedder to compensate for the shortage of data. Then, in addition to learning local and global semantic features, a feature interaction module is designed for exploring mutual effects between them. Furthermore, besides the typical cross-entropy loss, triplet loss is also introduced for the model training. In this way, the proposed method can refine more comprehensive and discriminative deep features in the intricate semantic space. The performance of the proposed method is compared with the representative linguistic steganalysis methods on datasets of different scales, and the experimental results reveal the superiority of the proposed method. © 2021 IEEE.","Feature extraction; Linguistics; Neural networks; Payloads; Semantics; Steganography; Training","Semantic Web; Steganography; Chaotics; Feature expression; Features extraction; Labor time; Labour-intensive; Neural-networks; Payload; Small scale; Steganalysis; Training data; Semantics",Article,Scopus,2-s2.0-85124421693
"Rao L.","57224499967;","Sentiment Analysis of English Text with Multilevel Features",2022,"Scientific Programming","2022",,"7605125","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124393850&doi=10.1155%2f2022%2f7605125&partnerID=40&md5=1804d257065d000bcf993d782d5ba313","In natural language processing, text sentiment analysis is one of the important branches. It refers to the use of text mining and other technologies to extract attitudes, opinions, and other information from texts containing emotional information for analysis. Traditional sentiment analysis methods can be roughly divided into two categories: one is dictionary-based methods, and the other is machine learning-based methods. The former relies on the quality of the sentiment dictionary, while the latter relies on a large amount of high-quality data, so both have certain limitations. In text sentiment analysis research, word-level and sentence-level sentiment information extraction is a basic research task and has important research value. Through research, it is found that domain knowledge and context are two important factors influencing the extraction of emotional information. To this end, this paper proposes a text sentiment analysis method that integrates multiple features and constructs three features, which are based on the sentiment value feature of the dictionary, the expression feature, and the improved semantic feature, which are combined to build a text sentiment classification model. Aiming at the colloquial, irregular, and diverse features of English social media texts, this paper proposes a multilevel feature representation method. The sentiment classification experiments on English text show that the multilevel features proposed in this paper can effectively improve the F1_macro and accuracy of multiple model classifications. Compared with the existing research, the model in this paper improves the effect the most obvious. © 2022 Li Rao.",,"Classification (of information); Domain Knowledge; Learning algorithms; Semantics; Analysis method; Emotional information; High quality data; Large amounts; Multilevels; Sentiment analysis; Sentiment classification; Sentiment dictionaries; Text-mining; Word level; Sentiment analysis",Article,Scopus,2-s2.0-85124393850
"Eddine M.M.C.","57060383100;","A New Concept of Electronic Text Based on Semantic Coding System for Machine Translation",2022,"ACM Transactions on Asian and Low-Resource Language Information Processing","21","1","3469655","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124304734&doi=10.1145%2f3469655&partnerID=40&md5=6a05d8d014a6c057c885587cff09e891","In the field of machine translation of texts, the ambiguity in both lexical (dictionary) and structural aspects is still one of the difficult problems. Researchers in this field use different approaches, the most important of which is machine learning in its various types. The goal of the approach that we propose in this article is to define a new concept of electronic text, which makes the electronic text free from any lexical or structural ambiguity. We used a semantic coding system that relies on attaching the original electronic text (via the text editor interface) with the meanings intended by the author. The author defines the meaning desired for each word that can be a source of ambiguity. The proposed approach in this article can be used with any type of electronic text (text processing applications, web pages, email text, etc.). Thanks to the approach that we propose and through the experiments that we have conducted using it, we can obtain a very high accuracy rate. We can say that the problem of lexical and structural ambiguity can be completely solved. With this new concept of electronic text, the text file contains not only the text but also with it the true sense of the exact meaning intended by the writer in the form of symbols. These semantic symbols are used during machine translation to obtain a translated text completely free of any lexical and structural ambiguity. © 2021 Association for Computing Machinery.","code; dictionary ambiguity; Semantic; structural ambiguity; text; word","Computational linguistics; Computer aided language translation; Machine translation; Signal encoding; Text processing; Websites; Code; Coding system; Dictionary ambiguity; Lexical ambiguity; Machine translations; Semantic coding; Structural ambiguity; Structural aspects; Text; Word; Semantics",Article,Scopus,2-s2.0-85124304734
"Phukon B., Anil A., Singh S.R., Sarmah P.","57444614900;56285847400;15045879400;33068408500;","Synonymy Expansion Using Link Prediction Methods: A Case Study of Assamese WordNet",2022,"ACM Transactions on Asian and Low-Resource Language Information Processing","21","1","3467966","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124304497&doi=10.1145%2f3467966&partnerID=40&md5=0d9614229ef3d1ef2541688fdccab4ad","WordNets built for low-resource languages, such as Assamese, often use the expansion methodology. This may result in missing lexical entries and missing synonymy relations. As the Assamese WordNet is also built using the expansion method, using the Hindi WordNet, it also has missing synonymy relations. As WordNets can be visualized as a network of unique words connected by synonymy relations, link prediction in complex network analysis is an effective way of predicting missing relations in a network. Hence, to predict the missing synonyms in the Assamese WordNet, link prediction methods were used in the current work that proved effective. It is also observed that for discovering missing relations in the Assamese WordNet, simple local proximity-based methods might be more effective as compared to global and complex supervised models using network embedding. Further, it is noticed that though a set of retrieved words are not synonyms per se, they are semantically related to the target word and may be categorized as semantic cohorts. © 2021 Association for Computing Machinery.","assamese; Automatic extraction; Indian languages; low resource languages; neural networks; semantic cohort; social network analysis; synonymy; synonymy network","Complex networks; Forecasting; Ontology; Semantic Web; Social networking (online); Assamese; Automatic extraction; Indian languages; Low resource languages; Neural-networks; Semantic cohort; Social Network Analysis; Synonymy; Synonymy network; Wordnet; Semantics",Article,Scopus,2-s2.0-85124304497
"Bhowmick R.S., Ganguli I., Paul J., Sil J.","57204880104;57192164837;57219482542;15066095900;","A Multimodal Deep Framework for Derogatory Social Media Post Identification of a Recognized Person",2022,"ACM Transactions on Asian and Low-Resource Language Information Processing","21","1","3447651","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124300510&doi=10.1145%2f3447651&partnerID=40&md5=efc5f8e7590c033288832b6d09059eb2","In today's era of digitization, social media platforms play a significant role in networking and influencing the perception of the general population. Social network sites have recently been used to carry out harmful attacks against individuals, including political and theological figures, intellectuals, sports and movie stars, and other prominent dignitaries, which may or may not be intentional. However, the exchange of such information across the general population inevitably contributes to social-economic, socio-political turmoil, and even physical violence in society. By classifying the derogatory content of a social media post, this research work helps to eradicate and discourage the upsetting propagation of such hate campaigns. Social networking posts today often include the picture of Memes along with textual remarks and comments, which throw new challenges and opportunities to the research community while identifying the attacks. This article proposes a multimodal deep learning framework by utilizing ensembles of computer vision and natural language processing techniques to train an encapsulated transformer network for handling the classification problem. The proposed framework utilizes the fine-Tuned state-of-The-Art deep learning-based models (e.g., BERT, Electra) for multilingual text analysis along with face recognition and the optical character recognition model for Meme picture comprehension. For the study, a new Facebook meme-post dataset is created with recorded baseline results. The subject of the created dataset and context of the work is more geared toward multilingual Indian society. The findings demonstrate the efficacy of the proposed method in the identification of social media meme posts featuring derogatory content about a famous/recognized individual. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.","deep learning; derogatory content; Indic languages; NLP; Social media analysis and security; transformer network","Deep learning; Face recognition; Natural language processing systems; Optical character recognition; Deep learning; Derogatory content; General population; Indic language; Media security; Multi-modal; Social media; Social media analysis; Social medium analyse and security; Transformer network; Social networking (online)",Article,Scopus,2-s2.0-85124300510
"Samadi M., Mousavian M., Momtazi S.","57226647804;57250991000;24479191100;","Persian Fake News Detection: Neural Representation and Classification at Word and Text Levels",2022,"ACM Transactions on Asian and Low-Resource Language Information Processing","21","1","3472620","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124299339&doi=10.1145%2f3472620&partnerID=40&md5=4348be1e0288caa9597510f372c4204e","Nowadays, broadcasting news on social media and websites has grown at a swifter pace, which has had negative impacts on both the general public and governments; hence, this has urged us to build a fake news detection system. Contextualized word embeddings have achieved great success in recent years due to their power to embed both syntactic and semantic features of textual contents. In this article, we aim to address the problem of the lack of fake news datasets in Persian by introducing a new dataset crawled from different news agencies, and propose two deep models based on the Bidirectional Encoder Representations from Transformers model (BERT), which is a deep contextualized pre-Trained model for extracting valuable features. In our proposed models, we benefit from two different settings of BERT, namely pool-based representation, which provides a representation for the whole document, and sequence representation, which provides a representation for each token of the document. In the former one, we connect a Single Layer Perceptron (SLP) to the BERT to use the embedding directly for detecting fake news. The latter one uses Convolutional Neural Network (CNN) after the BERT's embedding layer to extract extra features based on the collocation of words in a corpus. Furthermore, we present the TAJ dataset, which is a new Persian fake news dataset crawled from news agencies' websites. We evaluate our proposed models on the newly provided TAJ dataset as well as the two different Persian rumor datasets as baselines. The results indicate the effectiveness of using deep contextualized embedding approaches for the fake news detection task. We also show that both BERT-SLP and BERT-CNN models achieve superior performance to the previous baselines and traditional machine learning models, with 15.58% and 17.1% improvement compared to the reported results by Zamani et al. [30], and 11.29% and 11.18% improvement compared to the reported results by Jahanbakhsh-Nagadeh et al. [9]. © 2021 Association for Computing Machinery.","contextualized text representation; deep neural networks; fake news detection; Misinformation; Persian","Convolutional neural networks; Embeddings; Fake detection; Multilayer neural networks; Semantics; Text processing; Websites; Contextualized text representation; Convolutional neural network; Embeddings; Fake news detection; Misinformation; News agencies; Persians; Single-layer perceptrons; Text representation; Transformer modeling; Deep neural networks",Article,Scopus,2-s2.0-85124299339
"Alam M., Hussain S.U.","52763189800;55430960600;","Roman-Urdu-Parl: Roman-Urdu and Urdu Parallel Corpus for Urdu Language Understanding",2022,"ACM Transactions on Asian and Low-Resource Language Information Processing","21","1","3464424","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124288779&doi=10.1145%2f3464424&partnerID=40&md5=eb57845e50812d47742db5a87bd4be6e","Availability of corpora is a basic requirement for conducting research in a particular language. Unfortunately, for a morphologically rich language like Urdu, despite being used by over a 100 million people around the globe, the dearth of corpora is a major reason for the lack of attention and advancement in research. To this end, we present the first-ever large-scale publicly available Roman-Urdu parallel corpus, Roman-Urdu-Parl, with 6.37 million sentence-pairs. It is a huge corpus collected from diverse sources, annotated using crowd-sourcing techniques, and also assured for quality. It has a total of 92.76 million Roman-Urdu words, 92.85 million Urdu words, Roman-Urdu vocabulary of 42.9 K words, and Urdu vocabulary of 43.8 K words. Roman-Urdu-Parl has been built to ensure that it not only captures the morphological and linguistic features of the language but also the heterogeneity and variations arising due to demographic conditions. We validate the authenticity and quality of our corpus by using it to address two natural language processing research problems, that is, on learning word embeddings and building a machine transliteration system. Our contribution of the corpus leads to exceptional results in both settings, for example, our machine transliteration system sets a new state-of-The-Art with a Bilingual Evaluation Understudy (BLEU) score of 84.67. We believe that Roman-Urdu-Parl can serve as fuel for igniting and advancing works in many research areas related to the Urdu language. © 2022 Association for Computing Machinery.","deep learning; machine transliteration; neural machine translation; Roman-Urdu to Urdu transliteration","Deep learning; Linguistics; Crowd sourcing; Deep learning; Language understanding; Large-scales; Linguistic features; Machine transliteration; Morphological features; Parallel corpora; Roman-urdu to urdu transliteration; Transliteration system; Natural language processing systems",Article,Scopus,2-s2.0-85124288779
"Jahanbakhsh-Nagadeh Z., Feizi-Derakhshi M.-R., Sharifi A.","54791043400;35748449200;24725517500;","A Deep Content-Based Model for Persian Rumor Verification",2022,"ACM Transactions on Asian and Low-Resource Language Information Processing","21","1","3487289","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124280117&doi=10.1145%2f3487289&partnerID=40&md5=1764476f08ce519e085d878e56dde2d0","During the development of social media, there has been a transformation in social communication. Despite their positive applications in social interactions and news spread, it also provides an ideal platform for spreading rumors. Rumors can endanger the security of society in normal or critical situations. Therefore, it is important to detect and verify the rumors in the early stage of their spreading. Many research works have focused on social attributes in the social network to solve the problem of rumor detection and verification, while less attention has been paid to content features. The social and structural features of rumors develop over time and are not available in the early stage of rumor. Therefore, this study presented a content-based model to verify the Persian rumors on Twitter and Telegram early. The proposed model demonstrates the important role of content in spreading rumors and generates a better-integrated representation for each source rumor document by fusing its semantic, pragmatic, and syntactic information. First, contextual word embeddings of the source rumor are generated by a hybrid model based on ParsBERT and parallel CapsNets. Then, pragmatic and syntactic features of the rumor are extracted and concatenated with embeddings to capture the rich information for rumor verification. Experimental results on real-world datasets demonstrated that the proposed model significantly outperforms the state-of-The-Art models in the early rumor verification task. Also, it can enhance the performance of the classifier from 2% to 11% on Twitter and from 5% to 23% on Telegram. These results validate the model's effectiveness when limited content information is available. © 2021 Association for Computing Machinery.","contextual features; neural language model; ParsBERT; Persian rumor classification; Rumor verification; speech act","Embeddings; Semantics; Syntactics; Content-based; Contextual feature; Embeddings; Language model; Neural language model; ParsBERT; Persian rumor classification; Persians; Rumor verification; Speech acts; Social networking (online)",Article,Scopus,2-s2.0-85124280117
"De A., Bandyopadhyay D., Gain B., Ekbal A.","57212508017;57212508632;57212513458;23093674100;","A Transformer-Based Approach to Multilingual Fake News Detection in Low-Resource Languages",2022,"ACM Transactions on Asian and Low-Resource Language Information Processing","21","1","3472619","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124270042&doi=10.1145%2f3472619&partnerID=40&md5=462df9e070c013757dc55cfa6cd68c46","Fake news classification is one of the most interesting problems that has attracted huge attention to the researchers of artificial intelligence, natural language processing, and machine learning (ML). Most of the current works on fake news detection are in the English language, and hence this has limited its widespread usability, especially outside the English literate population. Although there has been a growth in multilingual web content, fake news classification in low-resource languages is still a challenge due to the non-Availability of an annotated corpus and tools. This article proposes an effective neural model based on the multilingual Bidirectional Encoder Representations from Transformer (BERT) for domain-Agnostic multilingual fake news classification. Large varieties of experiments, including language-specific and domain-specific settings, are conducted. The proposed model achieves high accuracy in domain-specific and domain-Agnostic experiments, and it also outperforms the current state-of-The-Art models. We perform experiments on zero-shot settings to assess the effectiveness of language-Agnostic feature transfer across different languages, showing encouraging results. Cross-domain transfer experiments are also performed to assess language-independent feature transfer of the model. We also offer a multilingual multidomain fake news detection dataset of five languages and seven different domains that could be useful for the research and development in resource-scarce scenarios. © 2021 Association for Computing Machinery.","Fake news detection; Hindi; Indonesian; low-resource languages; multilingual; Swahili; Vietnamese","Artificial intelligence; Fake detection; Learning algorithms; 'current; Domain agnostics; Domain specific; Fake news detection; Feature transfers; Hindi; Indonesian; Low resource languages; Swahilus; Vietnamese; Natural language processing systems",Article,Scopus,2-s2.0-85124270042
"Turan E., Orhan U.","18042753200;25121685900;","Confidence Indexing of Automated Detected Synsets: A Case Study on Contemporary Turkish Dictionary",2022,"ACM Transactions on Asian and Low-Resource Language Information Processing","21","1","3469724","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124264817&doi=10.1145%2f3469724&partnerID=40&md5=021759ec195df2eb06ae6bfc90fb98dd","In this study, a novel confidence indexing algorithm is proposed to minimize human labor in controlling the reliability of automatically extracted synsets from a non-machine-readable monolingual dictionary. Contemporary Turkish Dictionary of Turkish Language Association is used as the monolingual dictionary data. First, the synonym relations are extracted by traditional text processing methods from dictionary definitions and a graph is prepared in Lemma-Sense network architecture. After each synonym relation is labeled by a proper confidence index, synonym pairs with desired confidence indexes are analyzed to detect synsets with a spanning tree-based method. This approach can label synsets with one of three cumulative confidence levels (CL-1, CL-2, and CL-3). According to the confidence levels, synsets are compared with KeNet which is the only open access Turkish Wordnet. Consequently, while most matches with the synsets of KeNet is determined in CL-1 and CL-2 confidence levels, the synsets determined at CL-3 level reveal errors in the dictionary definitions. This novel approach does not find only the reliability of automatically detected synsets, but it can also point out errors of detected synsets from the dictionary. © 2021 Association for Computing Machinery.","confidence indexing; Machine-readable dictionary; spanning tree-based synset detection; synset confidence levels; WordNet","Indexing (of information); Network architecture; Semantics; Text processing; Confidence indexing; Confidence levels; Machine-readable dictionaries; Spanning tree; Spanning tree-based synset detection; Synset confidence level; Synsets; Tree-based; Wordnet; Ontology",Article,Scopus,2-s2.0-85124264817
"Ranasinghe T., Zampieri M.","57212349916;8948587300;","Multilingual Offensive Language Identification for Low-resource Languages",2022,"ACM Transactions on Asian and Low-Resource Language Information Processing","21","1","3457610","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124261725&doi=10.1145%2f3457610&partnerID=40&md5=7cf2c1d828c3b14c5458b9563f0bf82f","Offensive content is pervasive in social media and a reason for concern to companies and government organizations. Several studies have been recently published investigating methods to detect the various forms of such content (e.g., hate speech, cyberbullying, and cyberaggression). The clear majority of these studies deal with English partially because most annotated datasets available contain English data. In this article, we take advantage of available English datasets by applying cross-lingual contextual word embeddings and transfer learning to make predictions in low-resource languages. We project predictions on comparable data in Arabic, Bengali, Danish, Greek, Hindi, Spanish, and Turkish. We report results of 0.8415 F1 macro for Bengali in TRAC-2 shared task [23], 0.8532 F1 macro for Danish and 0.8701 F1 macro for Greek in OffensEval 2020 [58], 0.8568 F1 macro for Hindi in HASOC 2019 shared task [27], and 0.7513 F1 macro for Spanish in in SemEval-2019 Task 5 (HatEval) [7], showing that our approach compares favorably to the best systems submitted to recent shared tasks on these three languages. Additionally, we report competitive performance on Arabic and Turkish using the training and development sets of OffensEval 2020 shared task. The results for all languages confirm the robustness of cross-lingual contextual embeddings and transfer learning for this task. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.","cross-lingual embeddings; low-resource languages; Offensive language identification","Computer crime; Natural language processing systems; Bengalis; Cross-lingual; Cross-lingual embedding; Embeddings; Language identification; Low resource languages; Offensive language identification; Offensive languages; Transfer learning; Turkishs; Embeddings",Article,Scopus,2-s2.0-85124261725
"Saeed R., Afzal H., Abbas H., Fatima M.","57223111219;34771183700;57221959316;57226535646;","Enriching Conventional Ensemble Learner with Deep Contextual Semantics to Detect Fake News in Urdu",2022,"ACM Transactions on Asian and Low-Resource Language Information Processing","21","1","3461614","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124261357&doi=10.1145%2f3461614&partnerID=40&md5=53595bb9d6d8fd7ac78322dfa0e2dffb","Increased connectivity has contributed greatly in facilitating rapid access to information and reliable communication. However, the uncontrolled information dissemination has also resulted in the spread of fake news. Fake news might be spread by a group of people or organizations to serve ulterior motives such as political or financial gains or to damage a country's public image. Given the importance of timely detection of fake news, the research area has intrigued researchers from all over the world. Most of the work for detecting fake news focuses on the English language. However, automated detection of fake news is important irrespective of the language used for spreading false information. Recognizing the importance of boosting research on fake news detection for low resource languages, this work proposes a novel semantically enriched technique to effectively detect fake news in Urdu-a low resource language. A model based on deep contextual semantics learned from the convolutional neural network is proposed. The features learned from the convolutional neural network are combined with other n-gram-based features and are fed to a conventional majority voting ensemble classifier fitted with three base learners: Adaptive Boosting, Gradient Boosting, and Multi-Layer Perceptron. Experiments are performed with different models, and results show that enriching the traditional ensemble learner with deep contextual semantics along with other standard features shows the best results and outperforms the state-of-The-Art Urdu fake news detection model. © 2021 Association for Computing Machinery.","convolutional neural network; Deep contextual semantics; ensemble learning; majority voting; word embeddings","Convolutional neural networks; Deep neural networks; Information dissemination; Multilayer neural networks; Semantic Web; Semantics; Contextual semantics; Convolutional neural network; Deep contextual semantic; Embeddings; Ensemble learning; Information communication; Low resource languages; Majority voting; Reliable communication; Word embedding; Convolution",Article,Scopus,2-s2.0-85124261357
"Das S., Rai P., Chatterji S.","57446131000;57444615000;57216690738;","Deep Level Analysis of Legitimacy in Bengali News Sentences",2022,"ACM Transactions on Asian and Low-Resource Language Information Processing","21","1","3459928","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124251115&doi=10.1145%2f3459928&partnerID=40&md5=3e79ee0431d152efd7f526acfc38337a","The tremendous increase in the growth of misinformation in news articles has the potential threat for the adverse effects on society. Hence, the detection of misinformation in news data has become an appealing research area. The task of annotating and detecting distorted news article sentences is the immediate need in this research direction. Therefore, an attempt has been made to formulate the legitimacy annotation guideline followed by annotation and detection of the legitimacy in Bengali e-papers. The sentence-level manual annotation of Bengali news has been carried out in two levels, namely ""Level-1 Shallow Level Classification""and ""Level-2 Deep Level Classification""based on semantic properties of Bengali sentences. The tagging of 1,300 anonymous Bengali e-paper sentences has been done using the formulated guideline-based tags for both levels. The validation of the annotation guideline has been done by applying benchmark supervised machine learning algorithms using the lexical feature, syntactic feature, domain-specific feature, and Level-2 specific feature in both levels. Performance evaluation of these classifiers is done in terms of Accuracy, Precision, Recall, and F-Measure. In both levels, Support Vector Machine outperforms other benchmark classifiers with an accuracy of 72% and 65% in Level-1 and Level-2, respectively. © 2021 Association for Computing Machinery.","annotation guideline; Bengali news; legitimacy; multi-level classification; semantics","Learning algorithms; Support vector machines; Annotation guideline; Bengali news; Bengalis; Deep-levels; E-papers; Legitimacy; Level 2; Level-1; Multi-level classifications; News articles; Semantics",Article,Scopus,2-s2.0-85124251115
"Tran V.T., Quach H.D., Van P.V.D., Tran V.H.","57443536200;57221558774;57221557779;55645347700;","A Novel Metagenomic Binning Framework Using NLP Techniques in Feature Extraction",2022,"IPSJ Transactions on Bioinformatics","15",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124186840&doi=10.2197%2fipsjtbio.15.1&partnerID=40&md5=75635bced7081d30d554f161376ee599","Without traditional cultures, metagenomics studies the microorganisms sampled from the environment. In those studies, the binning step results serve as an input for the next step of metagenomic projects such as assembly and annotation. The main challenging issue of this process is due to the lack of explicit features of metagenomic reads, especially in the case of short-read datasets. There are two approaches, namely, supervised and unsupervised learning. Unfortunately, only about 1% of microorganisms in nature is annotated. That can cause problems for supervised approaches when an under-study dataset contains unknown species. It is well-known that the main challenging issue of this process is due to the lack of explicit features of metagenomic reads, especially in the case of short-read datasets. Previous studies usually assumed that reads in a taxonomic label have similar k-mer distributions. Our new method is to use Natural Language Processing (NLP) techniques in generating feature vectors. Additionally, the paper presents a comprehensive unsupervised framework in order to apply different embeddings categorized as notable NLP techniques in topic modeling and sentence embedding. The experimental results present our proposed approach’s comparative performance with other previous studies on simulated datasets, showing the feasibility of applying NLP for metagenomic binning. The program can be found at https://github.com/vandinhvyphuong/NLPBimeta. © 2022 Information Processing Society of Japan","Metagenomic binning; NLP; Short reads; Topic modelling","Embeddings; Microorganisms; Embeddings; Features extraction; K-mer distributions; Language processing techniques; Metagenomic binning; Metagenomics; Short reads; Supervised and unsupervised learning; Topic Modeling; Traditional cultures; Natural language processing systems",Article,Scopus,2-s2.0-85124186840
"Fairbank M., Samothrakis S., Citi L.","55220668000;36677887400;15623297800;","Deep Learning in Target Space",2022,"Journal of Machine Learning Research","23",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124183618&partnerID=40&md5=15a9161f6b4be993ba40bc229b7ca7e2","Deep learning uses neural networks which are parameterised by their weights. The neural networks are usually trained by tuning the weights to directly minimise a given loss function. In this paper we propose to re-parameterise the weights into targets for the firing strengths of the individual nodes in the network. Given a set of targets, it is possible to calculate the weights which make the firing strengths best meet those targets. It is argued that using targets for training addresses the problem of exploding gradients, by a process which we call cascade untangling, and makes the loss-function surface smoother to traverse, and so leads to easier, faster training, and also potentially better generalisation, of the neural network. It also allows for easier learning of deeper and recurrent network structures. The necessary conversion of targets to weights comes at an extra computational expense, which is in many cases manageable. Learning in target space can be combined with existing neural-network optimisers, for extra gain. Experimental results show the speed of using target space, and examples of improved generalisation, for fully-connected networks and convolutional networks, and the ability to recall and process long time sequences and perform natural-language processing with recurrent networks. © 2022 Michael Fairbank, Spyros Samothrakis and Luca Citi.","Cascade Untangling; Deep Learning; Exploding Gradients; Neural Networks; Targets","Natural language processing systems; Cascade untangling; Deep learning; Exploding gradient; Firing strength; Generalisation; Loss functions; Neural-networks; Parameterized; Recurrent networks; Target space; Recurrent neural networks",Article,Scopus,2-s2.0-85124183618
"Patil S., Chavan L., Mukane J., Vora D., Chitre V.","57441163600;57441163700;57440763100;39862393600;55247048800;","State-of-the-Art Approach to e-Learning with Cutting Edge NLP Transformers: Implementing Text Summarization, Question and Distractor Generation, Question Answering",2022,"International Journal of Advanced Computer Science and Applications","13","1",,"445","453",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124108547&doi=10.14569%2fIJACSA.2022.0130155&partnerID=40&md5=40a26a4854ac9db10a9e7661361d32ae","Amid the worldwide wave of pandemic lockdowns, there has been a remarkable growth in E-learning. Online learning has become a challenge for students. It has become difficult for students to find the content they need. The mounting accessibility of textual content has necessitated comprehensive study in the areas of automatic text summarization and question generation. Multiple Choice Questions is very smooth for evaluations, and its assessment is implemented through computerized applications in order that results may be declared within some hours, and the evaluation system is 100% pure. The system proposes an interactive reading platform where the user can upload an E-Book and get textual summary and generates questions like MCQs, fill in the blanks and one word. The user can also evaluate the questions answered. The proposed system is an all-in-one interactive reading platform. © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.","Machine intelligence; Natural language processing; Neural networks; Predictive models; Text processing","Cutting tools; E-learning; Learning algorithms; Learning systems; Natural language processing systems; Neural networks; Cutting edges; E - learning; Machine intelligence; Neural-networks; Online learning; Predictive models; Question Answering; State-of-the-art approach; Text Summarisation; Text-processing; Text processing",Article,Scopus,2-s2.0-85124108547
"Abbas S., Khan M.U., Lee S.U.-J., Abbas A., Bashir A.K.","57220815879;57220811228;56499567800;57189391152;57193954653;","A Review of NLIDB with Deep Learning: Findings, Challenges and Open Issues",2022,"IEEE Access","10",,,"14927","14945",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124098880&doi=10.1109%2fACCESS.2022.3147586&partnerID=40&md5=10cb50ba98254041d51e357a19f4a367","Relational databases are storage for a massive amount of data. Knowledge of structured query language is a prior requirement to access that data. That is not possible for all non-technical personals, leading to the need for a system that translates text to SQL query itself rather than the user. Text to SQL task is also crucial because of its economic and industrial value. Natural Language Interface to Database (NLIDB) is the system that supports the text-to-SQL task. Developing the NLIDB system is a long-standing problem. Previously they were built based on domain-specific ontologies via pipelining methods. Recently a rising variety of Deep learning ideas and techniques brought this area to the attention again. Now end to end Deep learning models is being proposed for the task. Some publicly available datasets are being used for experimentation of the contributions, making the comparison process convenient. In this paper, we review the current work, summarize the research trends, and highlight challenging issues of NLIDB with Deep learning models. We discussed the importance of datasets, prediction model approaches and open challenges. In addition, methods and techniques are also summarized, along with their influence on the overall structure and performance of NLIDB systems. This paper can help future researchers start having prior knowledge of findings and challenges in NLIDB with Deep learning approaches. © 2013 IEEE.","database; deep learning; natural language; natural language processing; NLIDB; structured language; Text to SQL","Deep learning; Digital storage; Query languages; Query processing; Deep learning; Learning models; Natural language interface to database; Natural languages; Relational Database; SQL query; Standing problems; Structured language; Structured Query Language; Text to SQL; Natural language processing systems",Article,Scopus,2-s2.0-85124098880
"Yuan J., Zhu S., Huang S., Zhang H., Xiao Y., Li Z., Wang M.","57200789539;57440681900;57441185200;55366935100;57441185300;57020064900;55533321600;","Discriminative Style Learning for Cross-Domain Image Captioning",2022,"IEEE Transactions on Image Processing","31",,,"1723","1736",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124085579&doi=10.1109%2fTIP.2022.3145158&partnerID=40&md5=725bb41eb2d94c942989840a05cd3363","The cross-domain image captioning, which is trained on a source domain and generalized to other domains, usually faces the large domain shift problem. Although prior work has attempted to leverage both paired source and unpaired target data to minimize this shift, the performance is still unsatisfactory. One main reason lies in the large discrepancy in language expression between two domains, where diverse language styles are adopted to describe an image from different views, resulting in different semantic descriptions for an image. To tackle this problem, this paper proposes a Style-based Cross-domain Image Captioner (SCIC) which incorporates the discriminative style information into the encoder-decoder framework, and interprets an image as a special sentence according to external style instructions. Technically, we design a novel 'Instruction-based LSTM', which adds the instruct gate to collect a style instruction, and then outputs a specified format according to that instruction. Two objectives are designed to train I-LSTM: 1) generating correct image descriptions and 2) generating correct styles, thus the model is expected to accurately capture the semantic meanings of an image by the special caption as well as understand the syntactic structure of the caption. We use MS-COCO as the source domain, and Oxford-102, CUB-200, Flickr30k as the target domains. Experimental results demonstrate that our model consistently outperforms the previous methods, and the style information incorporating with I-LSTM significantly improves the performance, with 5% CIDEr improvements at least on all datasets. © 1992-2012 IEEE.","Cross-domain; image captioning; instruction-based LSTM; style","Decoding; Long short-term memory; Syntactics; Cross-domain; Encoder-decoder; Image captioning; Image descriptions; Instruction-based LSTM; Large domain; Performance; Semantic descriptions; Style; Two domains; Semantics; article; cider; human; human experiment; learning; social media",Article,Scopus,2-s2.0-85124085579
"Clauwaert J., Waegeman W.","57215310687;22952272900;","Novel Transformer Networks for Improved Sequence Labeling in genomics",2022,"IEEE/ACM Transactions on Computational Biology and Bioinformatics","19","1",,"97","106",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124053682&doi=10.1109%2fTCBB.2020.3035021&partnerID=40&md5=cbd3deae6405641afc21cec482c3639a","In genomics, a wide range of machine learning methodologies have been investigated to annotate biological sequences for positions of interest such as transcription start sites, translation initiation sites, methylation sites, splice sites and promoter start sites. In recent years, this area has been dominated by convolutional neural networks, which typically outperform previously-designed methods as a result of automated scanning for influential sequence motifs. However, those architectures do not allow for the efficient processing of the full genomic sequence. As an improvement, we introduce transformer architectures for whole genome sequence labeling tasks. We show that these architectures, recently introduced for natural language processing, are better suited for processing and annotating long DNA sequences. We apply existing networks and introduce an optimized method for the calculation of attention from input nucleotides. To demonstrate this, we evaluate our architecture on several sequence labeling tasks, and find it to achieve state-of-the-art performances when comparing it to specialized models for the annotation of transcription start sites, translation initiation sites and 4mC methylation in E. coli. © 2004-2012 IEEE.","deep learning; Genomics; sequence labeling; transformer networks","Alkylation; Computer architecture; Deep learning; DNA sequences; Escherichia coli; Learning algorithms; Natural language processing systems; Network architecture; Neural networks; Automated scanning; Biological sequences; Convolutional neural network; Deep learning; Sequence Labeling; Sequence motifs; Splice site; Transcription start site; Transformer network; Translation initiation site; Genome; Escherichia coli; genomics; machine learning; nucleotide sequence; Base Sequence; Escherichia coli; Genomics; Machine Learning; Neural Networks, Computer",Article,Scopus,2-s2.0-85124053682
"Alhaidari L., Alyoubi K., Alotaibi F.","57439627000;57144527700;57055832400;","Detecting Irony in Arabic Microblogs using Deep Convolutional Neural Networks",2022,"International Journal of Advanced Computer Science and Applications","13","1",,"750","756",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124047201&doi=10.14569%2fIJACSA.2022.0130187&partnerID=40&md5=ceccb3aadc9e58fa2751b8fb3a553744","A considerable amount of research has been developed lately to analyze social media with the intention of understanding and exploiting the available information. Recently, irony has took a significant role in human communication as it has been increasingly used in many social media platforms. In Natural Language Processing (NLP), irony recognition is an important yet difficult problem to solve. It is considered to be a complex linguistic phenomenon in which people means the opposite of what they literally say. Due to its significance, it becomes essential to analyze and detect irony in subjective texts to improve the analysis tools to classify people opinion automatically. This paper explores how deep learning methods can be employed to the detection of irony in Arabic language with the help of Word2vec term representations that converts words to vectors. We applied two different deep learning models; Convolutional Neural Network (CNN) and Bidirectional Long Short-Term Memory (BiLSTM). We tested our frameworks with a manually annotated datasets that was collected using Tweet Scraper. The best result was achieved by the CNN model with an F1 score of 0.87 © 2022,International Journal of Advanced Computer Science and Applications.All Rights Reserved","Automatic irony detection; Machine learning; Natural language processing; Verbal irony","Convolution; Convolutional neural networks; Deep neural networks; Learning algorithms; Social networking (online); Analysis tools; Automatic irony detection; Convolutional neural network; Human communications; Learning methods; Linguistic phenomena; Micro-blog; Social media; Social media platforms; Verbal irony; Natural language processing systems",Article,Scopus,2-s2.0-85124047201
"Luthfi E.T., Yusoh Z.I.M., Aboobaider B.M.","57203486433;36706921900;57222044861;","BERT based Named Entity Recognition for Automated Hadith Narrator Identification",2022,"International Journal of Advanced Computer Science and Applications","13","1",,"604","611",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124042225&doi=10.14569%2fIJACSA.2022.0130173&partnerID=40&md5=d64b9e49c664bda99b7a250b8ca05d67","Hadith serves as a second source of Islamic law for Muslims worldwide, especially in Indonesia, which has the world's most significant Muslim population of 228.68 million people. However, not all Hadith texts have been certified and approved for use, and several falsified Hadiths make it challenging to distinguish between authentic and fabricated Hadiths. In terms of Hadith science, determining the authenticity of a Hadith can be accomplished by examining its Sanad and Matn. Sanad is an essential aspect of the Hadith because it indicates the chain of the Narrator who transmits the Hadith. The research reported in this paper provides an advanced Natural Language Processing (NLP) technique for identifying and authenticating the Narrator of Hadith as a part of Sanad, utilizing Named Entity Recognition (NER) to address the necessity of authenticating the Hadith. The NER technique described in the research adds an extra feed-forward classifier to the last layer of the pre-trained BERT model. In the testing process using Cahya/bert-base-indonesian-1.5G, the proposed solution received an overall F1-score of 99.63 percent. On the Hadith Narrator Identification using other Hadith passages, the final examination yielded a 98.27 percent F1-score © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.","Bert; Bert fine-tune; Hadith authentication; Hadith narrator; Named entity recognition; Natural language processing; Ner; Nlp","Natural language processing systems; Bert; Bert fine-tune; F1 scores; Hadith authentication; Hadith narrator; Indonesia; Language processing techniques; Named entity recognition; Ner; Nlp; Authentication",Article,Scopus,2-s2.0-85124042225
"Jia X., Wang L.","57216866955;57192099417;","Attention enhanced capsule network for text classification by encoding syntactic dependency trees with graph convolutional neural network",2022,"PeerJ Computer Science","7",,"e831","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124040238&doi=10.7717%2fPEERJ-CS.831&partnerID=40&md5=a06dcdd6dd53e10bb5435d6127963f2d","Text classification is a fundamental task in many applications such as topic labeling, sentiment analysis, and spam detection. The text syntactic relationship and word sequence are important and useful for text classification. How to model and incorporate them to improve performance is one key challenge. Inspired by human behavior in understanding text. In this paper, we combine the syntactic relationship, sequence structure, and semantics for text representation, and propose an attention-enhanced capsule network-based text classification model. Specifically, we use graph convolutional neural networks to encode syntactic dependency trees, build multihead attention to encode dependencies relationship in text sequence, merge with semantic information by capsule network at last. Extensive experiments on five datasets demonstrate that our approach can effectively improve the performance of text classification compared with state-of-the-art methods. The result also shows capsule network, graph convolutional neural network, and multi-headed attention has integration effects on text classification tasks. © 2022 Jia and Wang. All Rights Reserved.","Artificial Intelligence; Capsule network; Computational Linguistics; Data Mining and Machine Learning; Graph convolutional neural network; Multi-headed attention; Natural Language and Speech; Syntactic dependency tree; Text classification","Behavioral research; Classification (of information); Computational linguistics; Data mining; Encoding (symbols); Forestry; Graph neural networks; Machine learning; Network coding; Semantics; Sentiment analysis; Trees (mathematics); Capsule network; Data mining and machine learning; Labelings; Machine-learning; Multi-headed attention; Natural languages; Natural speech; Sentiment analysis; Syntactic dependency trees; Text classification; Convolution",Article,Scopus,2-s2.0-85124040238
"Alhaj F., Al-Haj A., Sharieh A., Jabri R.","57209274506;57439966100;57201986526;36087558600;","Improving Arabic Cognitive Distortion Classification in Twitter using BERTopic",2022,"International Journal of Advanced Computer Science and Applications","13","1",,"854","860",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124018308&doi=10.14569%2fIJACSA.2022.0130199&partnerID=40&md5=250f7463c75a9aa294a10862670c8843","Social media platforms allow users to share thoughts, experiences, and beliefs. These platforms represent a rich resource for natural language processing techniques to make inferences in the context of cognitive psychology. Some inaccurate and biased thinking patterns are defined as cognitive distortions. Detecting these distortions helps users restructure how to perceive thoughts in a healthier way. This paper proposed a machine learning-based approach to improve cognitive distortions’ classification of the Arabic content over Twitter. One of the challenges that face this task is the text shortness, which results in a sparsity of co-occurrence patterns and a lack of context information (semantic features). The proposed approach enriches text representation by defining the latent topics within tweets. Although classification is a supervised learning concept, the enrichment step uses unsupervised learning. The proposed algorithm utilizes a transformer-based topic modeling (BERTopic). It employs two types of document representations and performs averaging and concatenation to produce contextual topic embeddings. A comparative analysis of F1-score, precision, recall, and accuracy is presented. The experimental results demonstrate that our enriched representation outperformed the baseline models by different rates. These encouraging results suggest that using latent topic distribution, obtained from the BERTopic technique, can improve the classifier’s ability to distinguish between different CD categories. © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.","Arabic tweets; Bertopic; Cognitive distortions’ classification; Machine learning; Social media; Supervised learning; Topic modeling; Transformers; Unsupervised learning","Learning algorithms; Natural language processing systems; Semantics; Social networking (online); Supervised learning; Arabic tweet; Bertopic; Co-occurrence pattern; Cognitive distortion’ classification; Cognitive psychology; Language processing techniques; Social media; Social media platforms; Topic Modeling; Transformer; Unsupervised learning",Article,Scopus,2-s2.0-85124018308
"Ansari A.A.R.H., Vora D.R.","57438768000;39862393600;","NLI-GSC: A Natural Language Interface for Generating SourceCode",2022,"International Journal of Advanced Computer Science and Applications","13","1",,"842","853",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123992793&doi=10.14569%2fIJACSA.2022.0130198&partnerID=40&md5=27d4c2a72e7c87b415c5ae007e2c8d3a","There are many different programming languages and each programming language has its own structure or way of writing the code, it becomes difficult to learn and frequently switch between different programming languages. Due to this reason, a person working with multiple programming languages needs to look at documentations frequently which costs time and effort. In the past few years, there have been significant increase in the amount of papers published on this topic, each providing a unique solution to this problem. Many of these papers are based on applying NLP concepts in unique configuration to get the desired results. Some have used AI along with NLP to train the system to generate source-code in specific language, and some have trained the AI directly without pre-processing the dataset with NLP. All of these papers face two problems: a lack of proper dataset for this particular application and each paper can convent natural language into only one specified programming language source-code. This proposed system shows that a language independent solution is a feasible alternate for writing source-code without having full knowledge about a programming language. The proposed system uses Natural Language Processing to convert Natural Language into programming language-independent pseudo code using custom Named Entity Recognition and save it in XML (eXtensible Markup Language) format which is an intermediate step. Then, using traditional programming, this system converts the generated pseudo code into programming language-dependent source-code. In this paper, another novel method has been proposed to create dataset from scratch using predefined structure that is filled with predefined keywords creating unique combination of training dataset. © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.","Artificial intelligence (ai); Entity recognition (er); Natural language interface (nli); Natural language processing (nlp); Pseudocode generation; Source code generation","Ada (programming language); Codes (symbols); Hypertext systems; XML; Artificial intelligence; Entity recognition; Natural language interface; Natural language interfaces; Natural language processing; Pseudo codes; Pseudocode generation; Source code generation; Source codes; Natural language processing systems",Article,Scopus,2-s2.0-85123992793
"Shen D., Qin C., Zhu H., Xu T., Chen E., Xiong H.","57204471714;57222015135;54582012700;56330270200;35228685900;7201935465;","Joint Representation Learning with Relation-Enhanced Topic Models for Intelligent Job Interview Assessment",2022,"ACM Transactions on Information Systems","40","1","15","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123959539&doi=10.1145%2f3469654&partnerID=40&md5=6be607f6ebbbbfe36a27e62dbabbc6ea","The job interview is considered as one of the most essential tasks in talent recruitment, which forms a bridge between candidates and employers in fitting the right person for the right job. While substantial efforts have been made on improving the job interview process, it is inevitable to have biased or inconsistent interview assessment due to the subjective nature of the traditional interview process. To this end, in this article, we propose three novel approaches to intelligent job interview by learning the large-scale real-world interview data. Specifically, we first develop a preliminary model, named Joint Learning Model on Interview Assessment (JLMIA), to mine the relationship among job description, candidate resume, and interview assessment. Then, we further design an enhanced model, named Neural-JLMIA, to improve the representative capability by applying neural variance inference. Last, we propose to refine JLMIA with Refined-JLMIA (R-JLMIA) by modeling individual characteristics for each collection, i.e., disentangling the core competences from resume and capturing the evolution of the semantic topics over different interview rounds. As a result, our approaches can effectively learn the representative perspectives of different job interview processes from the successful job interview records in history. In addition, we exploit our approaches for two real-world applications, i.e., person-job fit and skill recommendation for interview assessment. Extensive experiments conducted on real-world data clearly validate the effectiveness of our models, which can lead to substantially less bias in job interviews and provide an interpretable understanding of job interview assessment. © 2021 Association for Computing Machinery.","Interview assessment; latent variable model; neural topic model; representation disentanglement; sequential data","Employment; Learning systems; Semantics; Interview assessment; Job interviews; Joint learning; Latent variable modeling; Learning models; Neural topic model; Real-world; Representation disentanglement; Sequential data; Topic Modeling; Job analysis",Article,Scopus,2-s2.0-85123959539
"Li S., Li Y., Zhao W.X., Ding B., Wen J.-R.","57216690581;56273199400;57221138506;21741739200;7402697777;","Interpretable Aspect-Aware Capsule Network for Peer Review Based Citation Count Prediction",2022,"ACM Transactions on Information Systems","40","1","11","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123933833&doi=10.1145%2f3466640&partnerID=40&md5=2a8f5c935d1da14ca4b050a60c515b3b","Citation count prediction is an important task for estimating the future impact of research papers. Most of the existing works utilize the information extracted from the paper itself. In this article, we focus on how to utilize another kind of useful data signal (i.e., peer review text) to improve both the performance and interpretability of the prediction models.Specially, we propose a novel aspect-Aware capsule network for citation count prediction based on review text. It contains two major capsule layers, namely the feature capsule layer and the aspect capsule layer, with two different routing approaches, respectively. Feature capsules encode the local semantics from review sentences as the input of aspect capsule layer, whereas aspect capsules aim to capture high-level semantic features that will be served as final representations for prediction. Besides the predictive capacity, we also enhance the model interpretability with two strategies. First, we use the topic distribution of the review text to guide the learning of aspect capsules so that each aspect capsule can represent a specific aspect in the review. Then, we use the learned aspect capsules to generate readable text for explaining the predicted citation count. Extensive experiments on two real-world datasets have demonstrated the effectiveness of the proposed model in both performance and interpretability. © 2021 Association for Computing Machinery.","capsule network; Citation count prediction; peer review","Semantics; Capsule network; Citation count prediction; Data signals; Interpretability; Peer review; Performance; Prediction modelling; Prediction-based; Research papers; Routing approach; Forecasting",Article,Scopus,2-s2.0-85123933833
"Lekkas D., Gyorda J.A., Price G.D., Wortzman Z., Jacobson N.C.","57219924140;57436619800;57220907543;57436591900;37761530000;","Using the COVID-19 Pandemic to Assess the Influence of News Affect on Online Mental Health-Related Search Behavior across the United States: Integrated Sentiment Analysis and the Circumplex Model of Affect",2022,"Journal of Medical Internet Research","24","1","e32731","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123901309&doi=10.2196%2f32731&partnerID=40&md5=66f134bbca391c0d712ff6cb686e93e1","Background: The digital era has ushered in an unprecedented volume of readily accessible information, including news coverage of current events. Research has shown that the sentiment of news articles can evoke emotional responses from readers on a daily basis with specific evidence for increased anxiety and depression in response to coverage of the recent COVID-19 pandemic. Given the primacy and relevance of such information exposure, its daily impact on the mental health of the general population within this modality warrants further nuanced investigation. Objective: Using the COVID-19 pandemic as a subject-specific example, this work aimed to profile and examine associations between the dynamics of semantic affect in online local news headlines and same-day online mental health term search behavior over time across the United States. Methods: Using COVID-19–related news headlines from a database of online news stories in conjunction with mental health–related online search data from Google Trends, this paper first explored the statistical and qualitative affective properties of state-specific COVID-19 news coverage across the United States from January 23, 2020, to October 22, 2020. The resultant operationalizations and findings from the joint application of dictionary-based sentiment analysis and the circumplex theory of affect informed the construction of subsequent hypothesis-driven mixed effects models. Daily state-specific counts of mental health search queries were regressed on circumplex-derived features of semantic affect, time, and state (as a random effect) to model the associations between the dynamics of news affect and search behavior throughout the pandemic. Search terms were also grouped into depression symptoms, anxiety symptoms, and nonspecific depression and anxiety symptoms to model the broad impact of news coverage on mental health. Results: Exploratory efforts revealed patterns in day-to-day news headline affect variation across the first 9 months of the pandemic. In addition, circumplex mapping of the most frequently used words in state-specific headlines uncovered time-agnostic similarities and differences across the United States, including the ubiquitous use of negatively valenced and strongly arousing language. Subsequent mixed effects modeling implicated increased consistency in affective tone (SpinVA β=–.207; P&lt;.001) as predictive of increased depression-related search term activity, with emotional language patterns indicative of affective uncontrollability (FluxA β=.221; P&lt;.001) contributing generally to an increase in online mental health search term frequency. Conclusions: This study demonstrated promise in applying the circumplex model of affect to written content and provided a practical example for how circumplex theory can be integrated with sentiment analysis techniques to interrogate mental health–related associations. The findings from pandemic-specific news headlines highlighted arousal, flux, and spin as potentially significant affect-based foci for further study. Future efforts may also benefit from more expansive sentiment analysis approaches to more broadly test the practical application and theoretical capabilities of the circumplex model of affect on text-based data. ©Damien Lekkas, Joseph A Gyorda, George D Price, Zoe Wortzman, Nicholas C Jacobson.","Affect; Anxiety; Behavior; Circumplex; Coronavirus; COVID-19; Depression; Generalized mixed models; Information seeking; Internet; Mental health; Natural language processing; News; Online health information; Online search behavior; Sentiment","anxiety; Article; coronavirus disease 2019; data base; depression; human; integrated health care system; language; mental health; online system; pandemic; search engine; semantics; sentiment analysis; United States; epidemiology; mental health; COVID-19; Humans; Mental Health; Pandemics; SARS-CoV-2; Sentiment Analysis; United States",Article,Scopus,2-s2.0-85123901309
"Zogan H., Razzak I., Wang X., Jameel S., Xu G.","57219765683;57204101897;36189130500;54581046500;8987733300;","Explainable depression detection with multi-aspect features using a hybrid deep learning model on social media",2022,"World Wide Web","25","1",,"281","304",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123862011&doi=10.1007%2fs11280-021-00992-2&partnerID=40&md5=d8d67d7439b582bc4fb8c15246b203d0","The ability to explain why the model produced results in such a way is an important problem, especially in the medical domain. Model explainability is important for building trust by providing insight into the model prediction. However, most existing machine learning methods provide no explainability, which is worrying. For instance, in the task of automatic depression prediction, most machine learning models lead to predictions that are obscure to humans. In this work, we propose explainable Multi-Aspect Depression Detection with Hierarchical Attention Network MDHAN, for automatic detection of depressed users on social media and explain the model prediction. We have considered user posts augmented with additional features from Twitter. Specifically, we encode user posts using two levels of attention mechanisms applied at the tweet-level and word-level, calculate each tweet and words’ importance, and capture semantic sequence features from the user timelines (posts). Our hierarchical attention model is developed in such a way that it can capture patterns that leads to explainable results. Our experiments show that MDHAN outperforms several popular and robust baseline methods, demonstrating the effectiveness of combining deep learning with multi-aspect features. We also show that our model helps improve predictive performance when detecting depression in users who are posting messages publicly on social media. MDHAN achieves excellent performance and ensures adequate evidence to explain the prediction. © 2022, The Author(s).","Deep learning; Depression detection; Explainability; Machine learning; Social network","Deep learning; Forecasting; Semantics; Deep learning; Depression detection; Explainability; Learning models; Machine learning methods; Medical domains; Model prediction; Multi aspects; Social media; Social network; Social networking (online)",Article,Scopus,2-s2.0-85123862011
"Liu K., El-Gohary N.","56927149000;14519146500;","Improved similarity assessment and spectral clustering for unsupervised linking of data extracted from bridge inspection reports",2022,"Advanced Engineering Informatics","51",,"101496","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123825365&doi=10.1016%2fj.aei.2021.101496&partnerID=40&md5=b28a08e01e12ec2b64e6628cb9b92e4c","Textual bridge inspection reports are important data sources for supporting data-driven bridge deterioration prediction and maintenance decision making. Information extraction methods are available to extract data/information from these reports to support data-driven analytics. However, directly using the extracted data/information in data analytics is still challenging because, even within the same report, there exist multiple data records that describe the same entity, which increases the dimensionality of the data and adversely affects the performance of the analytics. The first step to address this problem is to link the multiple records that describe the same entity and same type of instances (e.g., all cracks on a specific bridge deck), so that they can be subsequently fused into a single unified representation for dimensionality reduction without information loss. To address this need, this paper proposes a spectral clustering-based method for unsupervised data linking. The method includes: (1) a concept similarity assessment method, which allows for assessing concept similarity even when corpus or semantic information is not available for the application at hand; (2) a record similarity assessment method, which captures and uses similarity assessment dependencies to reduce the number of falsely-linked records; and (3) an improved spectral clustering method, which uses iterative bi-partitioning to better link records in an unsupervised way and to address the transitive closure problem. The proposed data linking method was evaluated in linking records extracted from ten bridge inspection reports. It achieved an average precision, recall, and F-1 measure of 96.2%, 88.3%, and 92.1%, respectively. © 2021","Bridges; Data linking/linkage; Deterioration prediction; Maintenance decision making; Similarity assessment; Spectral clustering; Unsupervised machine learning","Bridges; Data mining; Decision making; Deterioration; Inspection; Iterative methods; K-means clustering; Machine learning; Semantics; Bridge inspection; Data driven; Data informations; Data linking/linkage; Datalinking; Deterioration prediction; Maintenance decision making; Similarity assessment; Spectral clustering; Unsupervised machine learning; Data Analytics",Article,Scopus,2-s2.0-85123825365
"Vrublevskyi V., Marchenko O.","57216151170;44661392700;","Development and Analysis of a Sentence Semantics Representation Model",2022,"Cybernetics and Systems Analysis","58","1",,"16","23",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123822612&doi=10.1007%2fs10559-022-00430-9&partnerID=40&md5=fb4018ac2d587c135a1579854cfee2eb","The authors overview an efficient and simple representation model of sentence semantics in the context of a paraphrase identification problem. A dependency tree is chosen as the main structure to represent connections between words in a sentence. To represent word semantics, pre-trained word representation models are used. Based on these two key components, several features helping to precisely identify paraphrases are designed. The conducted experiments proved this model to be efficient. The model application results are quite close to those of state-of-the-art models. © 2022, Springer Science+Business Media, LLC, part of Springer Nature.","dependency tree; natural language processing; paraphrase identification; semantic similarity; word embeddings","Forestry; Semantics; Dependency trees; Embeddings; Identification problem; Main structure; Paraphrase identifications; Representation model; Semantic representation; Semantic similarity; Simple++; Word embedding; Natural language processing systems",Article,Scopus,2-s2.0-85123822612
"Chen Z., Qian T.","57211393145;57424177500;","Retrieve-and-Edit Domain Adaptation for End2End Aspect Based Sentiment Analysis",2022,"IEEE/ACM Transactions on Audio Speech and Language Processing","30",,,"659","672",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123789361&doi=10.1109%2fTASLP.2022.3146052&partnerID=40&md5=70478edf764a13a975e75ab39ddd6efb","End-to-end aspect based sentiment analysis (E2E-ABSA) aims to jointly extract aspect terms and predict aspect-level sentiment for opinion reviews. Though supervised methods show effectiveness for E2E-ABSA tasks, the annotation cost is extremely high due to the necessity of fine-grained labels. Recent attempts alleviate this problem using the domain adaptation technique to transfer the word-level common knowledge across domains. However, the biggest issue in domain adaptation, i.e., how to transfer the domain-specific words like pizza and delicious in the source 'Restaurant' to the target 'Laptop' domain, has not been resolved. In this paper, we propose a novel domain adaptation method to address this issue by enhancing the transferability of domain-specific source words in a retrieve-and-edit way. Specifically, for all source words, we first retrieve the transferable prototypes from unlabeled target data via their syntactic and semantic roles. We then edit the source words to enhance their transferability by absorbing the knowledge carried in prototypes. Finally, we design an end-to-end framework to jointly accomplish cross-domain aspect term extraction and aspect-level sentiment classification. We conduct extensive experiments on four real-world datasets. The results prove that, by introducing transferable prototypes, our method significantly outperforms the state-of-the-art methods, achieving an absolute 3.95% F1 increase over the best baseline. © 2014 IEEE.","domain adaptation; End-to-end aspect based sentiment analysis; retrieve-and-edit; transferable prototypes","Computational linguistics; Data mining; Job analysis; Semantics; Syntactics; Annotation; Domain adaptation; End to end; End-to-end aspect based sentiment analyse; Prototype; Retrieve-and-edit; Sentiment analysis; Task analysis; Transferable prototype; Sentiment analysis",Article,Scopus,2-s2.0-85123789361
"Wazery Y.M., Saleh M.E., Alharbi A., Ali A.A.","57196093973;57433445100;57209217043;57203909028;","Abstractive Arabic Text Summarization Based on Deep Learning",2022,"Computational Intelligence and Neuroscience","2022",,"1566890","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123770628&doi=10.1155%2f2022%2f1566890&partnerID=40&md5=1f1d4e46247e1e2d7ac76d682e542c87","Text summarization (TS) is considered one of the most difficult tasks in natural language processing (NLP). It is one of the most important challenges that stand against the modern computer system's capabilities with all its new improvement. Many papers and research studies address this task in literature but are being carried out in extractive summarization, and few of them are being carried out in abstractive summarization, especially in the Arabic language due to its complexity. In this paper, an abstractive Arabic text summarization system is proposed, based on a sequence-to-sequence model. This model works through two components, encoder and decoder. Our aim is to develop the sequence-to-sequence model using several deep artificial neural networks to investigate which of them achieves the best performance. Different layers of Gated Recurrent Units (GRU), Long Short-Term Memory (LSTM), and Bidirectional Long Short-Term Memory (BiLSTM) have been used to develop the encoder and the decoder. In addition, the global attention mechanism has been used because it provides better results than the local attention mechanism. Furthermore, AraBERT preprocess has been applied in the data preprocessing stage that helps the model to understand the Arabic words and achieves state-of-the-art results. Moreover, a comparison between the skip-gram and the continuous bag of words (CBOW) word2Vec word embedding models has been made. We have built these models using the Keras library and run-on Google Colab Jupiter notebook to run seamlessly. Finally, the proposed system is evaluated through ROUGE-1, ROUGE-2, ROUGE-L, and BLEU evaluation metrics. The experimental results show that three layers of BiLSTM hidden states at the encoder achieve the best performance. In addition, our proposed system outperforms the other latest research studies. Also, the results show that abstractive summarization models that use the skip-gram word2Vec model outperform the models that use the CBOW word2Vec model. © 2022 Y.M. Wazery et al.",,"Brain; Decoding; Information retrieval; Natural language processing systems; Signal encoding; Text processing; Arabic texts; Attention mechanisms; Bag of words; Extractive summarizations; Modern computer systems; Performance; Research studies; Sequence models; System capabilities; Text Summarisation; Long short-term memory; language; natural language processing; publication; Deep Learning; Language; Natural Language Processing; Neural Networks, Computer; Publications",Article,Scopus,2-s2.0-85123770628
"Yu L., Lu Y., Shen Y., Zhao J., Zhao J.","57198422442;57190569944;57198551361;56286001500;57217106430;","PBDiff: Neural network based program-wide diffing method for binaries",2022,"Mathematical Biosciences and Engineering","19","3",,"2774","2799",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123766698&doi=10.3934%2fmbe.2022127&partnerID=40&md5=61506cd6a472dfbe58ee112638bfc42e","Program-wide binary code diffing is widely used in the binary analysis field, such as vulnerability detection. Mature tools, including BinDiff and TurboDiff, make program-wide diffing using rigorous comparison basis that varies across versions, optimization levels and architectures, leading to a relatively inaccurate comparison result. In this paper, we propose a program-wide binary diffing method based on neural network model that can make diffing across versions, optimization levels and architectures. We analyze the target comparison files in four different granularities, and implement the diffing by both top down process and bottom up process according to the granularities. The top down process aims to narrow the comparison scope, selecting the candidate functions that are likely to be similar according to the call relationship. Neural network model is applied in the bottom up process to vectorize the semantic features of candidate functions into matrices, and calculate the similarity score to obtain the corresponding relationship between functions to be compared. The bottom up process improves the comparison accuracy, while the top down process guarantees efficiency. We have implemented a prototype PBDiff and verified its better performance compared with state-of-the-art BinDiff, Asm2vec and TurboDiff. The effectiveness of PBDiff is further illustrated through the case study of diffing and vulnerability detection in real-world firmware files. © 2022 the Author(s), licensee AIMS Press.","Binary diffing; Feature extraction; IoT security; Neural network; Vulnerability detection","Feature extraction; Internet of things; Network architecture; Network security; Semantics; Binary diffing; Bottom-up process; Features extraction; IoT security; Neural network model; Neural-networks; Optimization architecture; Optimization levels; Top-down process; Vulnerability detection; Firmware",Article,Scopus,2-s2.0-85123766698
"Ren R., Zapata M., Castro J.W., Dieste O., Acuna S.T.","57210745367;56818703100;25721592400;6506099479;6602208682;","Experimentation for Chatbot Usability Evaluation: A Secondary Study",2022,"IEEE Access","10",,,"12430","12464",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123734741&doi=10.1109%2fACCESS.2022.3145323&partnerID=40&md5=aa9b237fb18645afcd9036f07f9766d0","Interest in chatbot development is on the rise. As a usability evaluation is an essential step in chatbot development, the number of experimental studies on chatbot usability has grown as well. As a result, we think a systematic mapping study is opportune. We analyzed more than 700 sources and retrieved 28 primary studies. By aggregating the research questions and examining the characteristics and metrics used to evaluate the usability of chatbots in experiments, it is possible to identify the state of the art in chatbot usability experimentation. We conducted a systematic mapping study to identify the research questions, characteristics, and metrics used to evaluate the usability of chatbots in experiments. Most experiments adopted a within-subjects design. On the other hand, few experiments provided raw data, and only one of the identified papers was part of a family of experiments. Effectiveness, efficiency, and satisfaction are usability characteristics used to identify how well users can learn and use chatbots to achieve their goals and how satisfied users are during the interaction. Generally, the experimental results revealed that chatbots have several advantages (e.g., they provide a real-time response and they improve ease of use) and some shortcomings (e.g., natural language processing, which is rated as the weakness most in need of improvement). This research offers an overview of chatbot usability experimentation. The increasing interest in this area is very recent, as works did not start to be published until 2018. Chatbot usability experiments should be more replicable to improve the reliability and transparency of the experimental results. © 2013 IEEE.","chatbots; experiments; family of experiments; systematic mapping study; Usability","Mapping; Natural language processing systems; Chatbots; Family of experiment; Research questions; Size measurements; Social networking (online); State of the art; Systematic; Systematic mapping studies; Usability evaluation; Usability engineering",Article,Scopus,2-s2.0-85123734741
"Skvorc T., Lavrac N., Robnik-Sikonja M.","57190075400;7004388979;55900495300;","NeSyChair: Automatic Conference Scheduling Combining Neuro-Symbolic Representations and Constrained Clustering",2022,"IEEE Access","10",,,"10880","10897",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123711457&doi=10.1109%2fACCESS.2022.3144932&partnerID=40&md5=b9709fc2049b52ab0b37079915ca58ff","Creating the schedule for an academic conference is a time-consuming task. A typical conference schedule consists of sessions containing papers addressing the same research topic. To construct a schedule, conference papers must be grouped according to their research topic, and the obtained groups should fit the assigned time slots. This paper proposes an approach to automating the schedule-creation process. We use multilingual, neuro-symbolic paper representations and novel constrained clustering to group papers into clusters of predetermined size with the same topic fitting the schedule structure. In the process, we combine machine-learning, natural language processing, network analysis, and combinatorial optimization. We tested the components of the proposed approach on a newly created database of papers from six machine learning conferences, which were manually labeled by their research topics. The entire system was tested on two real-world conferences in a multilingual setting. The developed methodology is incorporated into an interactive automatic conference-scheduling system NeSyChair (Neuro-Symbolic Conference Chair), which can be used to create and improve conference schedules. © 2013 IEEE.","Machine learning; natural language processing; optimization; supervised learning; unsupervised learning","Combinatorial optimization; Job analysis; Learning algorithms; Machine components; Machine learning; Natural language processing systems; Academic conferences; Conference papers; Constrained clustering; Features extraction; Optimisations; Research topics; Schedule; Symbolic representation; Task analysis; Time-consuming tasks; Scheduling",Article,Scopus,2-s2.0-85123711457
"Zhao H., Xie J., Wang H.","57431614100;57430816600;57431814000;","Graph Convolutional Network Based on Multi-Head Pooling for Short Text Classification",2022,"IEEE Access","10",,,"11947","11956",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123698114&doi=10.1109%2fACCESS.2022.3146303&partnerID=40&md5=c086450a8f4ad55d6186f321d6b4496c","The short text, sparse features, and the lack of training data, etc. are still the key bottlenecks that restrict the successful application of traditional text classification methods. To address these problems, we propose a Multi-head-Pooling-based Graph Convolutional Network (MP-GCN) for semi-supervised short text classification, and introduce its three architectures, which focus on the node representation learning of 1-order, 12-order of isomorphic graphs, and 1-order of heterogeneous graphs, respectively. It only focuses on the structural information of the text graph and does not need pre-training word embedding as the initial node feature. A graph pooling based on self-attention is introduced to evaluate and select important nodes, and the multi-head method is used to provide multiple representation subspaces for pooling without adding trainable parameters. Experimental results demonstrated that, without using pre-training embedding, MP-GCN outperforms state-of-the-art models across five benchmark datasets. © 2013 IEEE.","artificial intelligence; Graph convolutional network; natural language processing; text classification","Classification (of information); Convolution; Deep learning; Embeddings; Graph theory; Job analysis; Natural language processing systems; Convolutional networks; Deep learning; Embeddings; Features extraction; Graph convolutional network; Matrix decomposition; Pre-training; Short text classifications; Task analysis; Text categorization; Text processing",Article,Scopus,2-s2.0-85123698114
"Hammad M., Babur O., Basit H.A., Van Den Brand M.","56490230800;55949449000;12241632000;57192113699;","Clone-Seeker: Effective Code Clone Search Using Annotations",2022,"IEEE Access","10",,,"11696","11713",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123678269&doi=10.1109%2fACCESS.2022.3145686&partnerID=40&md5=730b003ff941a6acfc6cf5a66f334ba1","Source code search plays an important role in software development, e.g. for exploratory development or opportunistic reuse of existing code from a code base. Often, exploration of different implementations with the same functionality is needed for tasks like automated software transplantation, software diversification, and software repair. Code clones, which are syntactically or semantically similar code fragments, are perfect candidates for such tasks. Searching for code clones involves a given search query to retrieve the relevant code fragments. We propose a novel approach called Clone-Seeker that focuses on utilizing clone class features in retrieving code clones. For this purpose, we generate metadata for each code clone in the form of a natural language document. The metadata includes a pre-processed list of identifiers from the code clones augmented with a list of keywords indicating the semantics of the code clone. This keyword list can be extracted from a manually annotated general description of the clone class, or automatically generated from the source code of the entire clone class. This approach helps developers to perform code clone search based on a search query written either as source code terms, or as natural language. With various experiments, we show that (1) Clone-Seeker is effective in finding clones from BigCloneBench dataset by applying code queries and natural language queries; 2) Clone-Seeker has a higher recall when searching for semantic code clones (i.e., Type-4) in BigCloneBench than the state-of-the-art; 3) Clone-Seeker is a generalized technique as it is effective in finding clones in Project CodeNet dataset by applying code queries and natural language queries. 4) Clone-Seeker with manual annotation outperforms other variants in finding clones on the basis of natural language queries © 2013 IEEE.","Annotation; code clone; code clone search; information retrieval; keyword extraction","Cloning; Codes (symbols); Computer programming languages; Computer software reusability; Natural language processing systems; Semantics; Software design; Annotation; Code clone; Code clone search; Code fragments; Keywords extraction; Natural language queries; Natural languages; Search queries; Source codes; Metadata",Article,Scopus,2-s2.0-85123678269
"Takhanov R., Kolmogorov V.","16231683300;7006596141;","Combining pattern-based CRFs and weighted context-free grammars",2022,"Intelligent Data Analysis","26","1",,"257","272",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123619151&doi=10.3233%2fIDA-205623&partnerID=40&md5=df11c5b4f676417593dfbc9b3d0b5eec","We consider two models for the sequence labeling (tagging) problem. The first one is a Pattern-Based Conditional Random Field (PB), in which the energy of a string (chain labeling) x=x1⁢... ⁢xn∈Dn is a sum of terms over intervals [i,j] where each term is non-zero only if the substring xi⁢... ⁢xj equals a prespecified word w∈Λ. The second model is a Weighted Context-Free Grammar (WCFG) frequently used for natural language processing. PB and WCFG encode local and non-local interactions respectively, and thus can be viewed as complementary. We propose a Grammatical Pattern-Based CRF model (GPB) that combines the two in a natural way. We argue that it has certain advantages over existing approaches such as the Hybrid model of Benedí and Sanchez that combines N-grams and WCFGs. The focus of this paper is to analyze the complexity of inference tasks in a GPB such as computing MAP. We present a polynomial-time algorithm for general GPBs and a faster version for a special case that we call Interaction Grammars. © 2022 - IOS Press. All rights reserved.","conditional random fields; interaction grammars; Pattern-based; weighted context-free grammar","Context free languages; Natural language processing systems; Polynomial approximation; Random processes; Context-free grammars; Energy; Interaction grammar; Labelings; Non-local interactions; Pattern-based; Sequence Labeling; Sub-strings; Tagging problem; Weighted context-free grammar; Context free grammars",Article,Scopus,2-s2.0-85123619151
"Angel Deborah S., Rajendram S.M., Tt M., Rajalakshmi S.","37013147900;13006124400;57429530700;57200149409;","Contextual emotion detection on text using gaussian process and tree based classifiers",2022,"Intelligent Data Analysis","26","1",,"119","132",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123602484&doi=10.3233%2fIDA-205587&partnerID=40&md5=da62eff10df005b8f4f72b847d03df0c","It is challenging for machine as well as humans to detect the presence of emotions such as sadness or disgust in a sentence without adequate knowledge about the context. Contextual emotion detection is a challenging problem in natural language processing. As the use of digital agents have increased in text messaging applications, it is essential for these agents to provide sensible responses to its users. The present work demonstrates the effectiveness of Gaussian process detecting contextual emotions present in a sentence. The results obtained are compared with Decision Tree and ensemble models such as Random Forest, AdaBoost and Gradient Boost. Out of the five models built on a small dataset with class imbalance, it has been found that Gaussian Process classifier predicts emotions better than the other classifiers. Gaussian Process classifier performs better by taking predictive variance into account. © 2022 - IOS Press. All rights reserved.","decision trees; Emotion recognition; gaussian processes; gradient methods; machine learning","Adaptive boosting; Character recognition; Classification (of information); Decision trees; Gaussian distribution; Gaussian noise (electronic); Machine learning; Natural language processing systems; Speech recognition; Decision-tree model; Emotion detection; Emotion recognition; Ensemble models; Gaussian Processes; Gradient's methods; Process-based; Random forests; Text-messaging; Tree-based; Gradient methods",Article,Scopus,2-s2.0-85123602484
"Allahgholi M., Rahmani H., Javdani D., Sadeghi-Adl Z., Bender A., Módos D., Weiss G.","57210211723;57216792350;57210202630;57428648600;8779307500;55240175600;57429530400;","DDREL: From drug-drug relationships to drug repurposing",2022,"Intelligent Data Analysis","26","1",,"221","237",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123586772&doi=10.3233%2fIDA-215745&partnerID=40&md5=d728ca9195426992546a79c1211ddd78","Analyzing the relationships among various drugs is an essential issue in the field of computational biology. Different kinds of informative knowledge, such as drug repurposing, can be extracted from drug-drug relationships. Scientific literature represents a rich source for the retrieval of knowledge about the relationships between biological concepts, mainly drug-drug, disease-disease, and drug-disease relationships. In this paper, we propose DDREL as a general-purpose method that applies deep learning on scientific literature to automatically extract the graph of syntactic and semantic relationships among drugs. DDREL remarkably outperforms the existing human drug network method and a random network respected to average similarities of drugs' anatomical therapeutic chemical (ATC) codes. DDREL is able to shed light on the existing deficiency of the ATC codes in various drug groups. From the DDREL graph, the history of drug discovery became visible. In addition, drugs that had repurposing score 1 (diflunisal, pargyline, fenofibrate, guanfacine, chlorzoxazone, doxazosin, oxymetholone, azathioprine, drotaverine, demecarium, omifensine, yohimbine) were already used in additional indication. The proposed DDREL method justifies the predictive power of textual data in PubMed abstracts. DDREL shows that such data can be used to 1- Predict repurposing drugs with high accuracy, and 2- Reveal existing deficiencies of the ATC codes in various drug groups. © 2022 - IOS Press. All rights reserved.","deep learning; Drug-drug relationships; repurposing drugs; text mining; word embedding","Data mining; Deep learning; Drug products; Computational biology; Deep learning; Drug-drug relationship; Embeddings; Repurposing; Repurposing drug; Scientific literature; Semantic relationships; Text-mining; Word embedding; Semantics",Article,Scopus,2-s2.0-85123586772
"Linder J., La Fleur A., Chen Z., Ljubetič A., Baker D., Kannan S., Seelig G.","57209408700;57194410092;57426946800;36131772900;55463548900;57202672539;7004244336;","Interpreting neural networks for biological sequences by learning stochastic masks",2022,"Nature Machine Intelligence","4","1",,"41","54",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123463856&doi=10.1038%2fs42256-021-00428-6&partnerID=40&md5=bcd705d70aa88cb220f257093278d45a","Sequence-based neural networks can learn to make accurate predictions from large biological datasets, but model interpretation remains challenging. Many existing feature attribution methods are optimized for continuous rather than discrete input patterns and assess individual feature importance in isolation, making them ill-suited for interpreting nonlinear interactions in molecular sequences. Here, building on work in computer vision and natural language processing, we developed an approach based on deep learning—scrambler networks—wherein the most important sequence positions are identified with learned input masks. Scramblers learn to predict position-specific scoring matrices where unimportant nucleotides or residues are scrambled by raising their entropy. We apply scramblers to interpret the effects of genetic variants, uncover nonlinear interactions between cis-regulatory elements, explain binding specificity for protein–protein interactions, and identify structural determinants of de novo-designed proteins. We show that scramblers enable efficient attribution across large datasets and result in high-quality explanations, often outperforming state-of-the-art methods. © 2022, The Author(s), under exclusive licence to Springer Nature Limited.",,"Deep learning; Natural language processing systems; Proteins; Stochastic systems; Accurate prediction; Biological sequences; Individual features; Input patterns; Learn+; Model interpretations; Neural-networks; Nonlinear interactions; Position specific scoring matrix; Stochastics; Large dataset",Article,Scopus,2-s2.0-85123463856
"Li C., Ma K.","57426252000;56662518300;","Entity recognition of Chinese medical text based on multi-head self-attention combined with BILSTM-CRF",2022,"Mathematical Biosciences and Engineering","19","3",,"2206","2218",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123455338&doi=10.3934%2fMBE.2022103&partnerID=40&md5=1e7367a0bf690282fcbee8da6455d7fd","Named entities are the main carriers of relevant medical knowledge in Electronic Medical Records (EMR). Clinical electronic medical records lead to problems such as word segmentation ambiguity and polysemy due to the specificity of Chinese language structure, so a Clinical Named Entity Recognition (CNER) model based on multi-head self-attention combined with BILSTM neural network and Conditional Random Fields is proposed. Firstly, the pre-trained language model organically combines char vectors and word vectors for the text sequences of the original dataset. The sequences are then fed into the parallel structure of the multi-head self-attention module and the BILSTM neural network module, respectively. By splicing the output of the neural network module to obtain multi-level information such as contextual information and feature association weights. Finally, entity annotation is performed by CRF. The results of the multiple comparison experiments show that the structure of the proposed model is very reasonable and robust, and it can effectively improve the Chinese CNER model. The model can extract multi-level and more comprehensive text features, compensate for the defect of long-distance dependency loss, with better applicability and recognition performance. © 2022 the Author(s), licensee AIMS Press.","Bi-directional long-short term memory; Clinical named entity recognition; Conditional random fields; Electronic medical records; Multi-head self-attention","Character recognition; Natural language processing systems; Random processes; Bi-directional; Bi-directional long-short term memory; Clinical named entity recognition; Entity recognition; Multi-head self-attention; Multilevels; Named entities; Named entity recognition; Neural-networks; Recognition models; Medical computing",Article,Scopus,2-s2.0-85123455338
"Chang S., Francis Siu M.-F., Li H., Luo X.","57391520500;57397201300;8692514900;35099137200;","Evolution pathways of robotic technologies and applications in construction",2022,"Advanced Engineering Informatics","51",,"101529","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123353718&doi=10.1016%2fj.aei.2022.101529&partnerID=40&md5=c78e450ba8c071aa1323e4806c4c69eb","Recent research has focused on developing new automated construction robots. However, the evolution pathways of robotics technologies and their specific applications are yet to be explored analytically. This study aimed to trace these evolution pathways using critical juncture timings to describe them. First, data were collected from the literature using a two-step literature selection method. Robotic technologies and application terms were then extracted and labelled using natural language processing techniques. Next, the probability of the selected terms was calculated as importance weighting. Finally, the critical junctures were identified by clustering the weightings using the k-means clustering algorithm. The proposed method revealed three critical junctures, thereby identifying four development stages since 1983. It was found that the needs for construction robots emerged after 2015. The robot configuration evolved from large-scale machinery to smaller and movable vehicles. Image-related sensor systems and processing algorithms, such as 3D cameras and deep learning algorithms, have become popular after 2009. These improvements allow for more accurate operation in unknown and complex environments. Inspection works may be the golden chance to further advance robot implementation. © 2022 Elsevier Ltd","Construction robotics; Critical juncture; Development trend; Evolution pathway; Natural language processing","Cameras; Deep learning; K-means clustering; Learning algorithms; Machinery; Robotics; Automated construction; Construction robotics; Construction robots; Critical juncture; Development trends; Evolution pathways; Recent researches; Robotic technologies; Robotics applications; Technologies and applications; Natural language processing systems",Article,Scopus,2-s2.0-85123353718
"Ge C., Sun H., Song Y.-Z., Ma Z., Liao J.","57200371269;57190948696;57307467600;55479111500;57226839870;","Exploring Local Detail Perception for Scene Sketch Semantic Segmentation",2022,"IEEE Transactions on Image Processing","31",,,"1447","1461",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123351113&doi=10.1109%2fTIP.2022.3142511&partnerID=40&md5=f284965939b4fcb6390e0f0cdf20cfe9","In this paper, we aim to explore the fine-grained perception ability of deep models for the newly proposed scene sketch semantic segmentation task. Scene sketches are abstract drawings containing multiple related objects. It plays a vital role in daily communication and human-computer interaction. The study has only recently started due to a main obstacle of the absence of large-scale datasets. The currently available dataset SketchyScene is composed of clip art-style edge maps, which lacks abstractness and diversity. To drive further research, we contribute two new large-scale datasets based on real hand-drawn object sketches. A general automatic scene sketch synthesis process is developed to assist with new dataset composition. Furthermore, we propose to enhancing local detail perception in deep models to realize accurate stroke-oriented scene sketch segmentation. Due to the inherent differences between hand-drawn sketches and natural images, extreme low-level local features of strokes are incorporated to improve detail discrimination. Stroke masks are also integrated into model training to guide the learning attention. Extensive experiments are conducted on three large-scale scene sketch datasets. Our method achieves state-of-the-art performance under four evaluation metrics and yields meaningful interpretability via visual analytics. 1941-0042 © 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.","Airplanes; Automobiles; Feature extraction; Image segmentation; Layout; Semantics; Task analysis","Computer vision; Deep learning; Drawing (graphics); Human computer interaction; Image enhancement; Semantic Segmentation; Deep learning; Features extraction; Images segmentations; Large-scale datasets; Layout; Local detail perception; Scene sketch; Semantic segmentation; Sketch dataset; Task analysis; Semantics; attention; human; perception; semantics; Attention; Humans; Perception; Semantics",Article,Scopus,2-s2.0-85123351113
"Liu Y., Zhang X., Huang F., Zhang B., Li Z.","57206821378;56025133500;57196054434;57218111334;57279510300;","Cross-Attentional Spatiooral Semantic Graph Networks for Video Question Answering",2022,"IEEE Transactions on Image Processing","31",,,"1684","1696",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123350484&doi=10.1109%2fTIP.2022.3142526&partnerID=40&md5=28ef8c3139510951b77bf05806a2e9c3","Due to the rich spatiooral visual content and complex multimodal relations, Video Question Answering (VideoQA) has become a challenging task and attracted increasing attention. Current methods usually leverage visual attention, linguistic attention, or self-attention to uncover latent correlations between video content and question semantics. Although these methods exploit interactive information between different modalities to improve comprehension ability, inter- and intra-modality correlations cannot be effectively integrated in a uniform model. To address this problem, we propose a novel VideoQA model called Cross-Attentional Spatiooral Semantic Graph Networks (CASSG). Specifically, a multi-head multi-hop attention module with diversity and progressivity is first proposed to explore fine-grained interactions between different modalities in a crossing manner. Then, heterogeneous graphs are constructed from the cross-attended video frames, clips, and question words, in which the multi-stream spatiooral semantic graphs are designed to synchronously reasoning inter- and intra-modality correlations. Last, the global and local information fusion method is proposed to coalesce the local reasoning vector learned from multi-stream spatiooral semantic graphs and the global vector learned from another branch to infer the answer. Experimental results on three public VideoQA datasets confirm the effectiveness and superiority of our model compared with state-of-the-art methods. © 1992-2012 IEEE.","heterogeneous graph; inter- and intra-modality correlations; Video question answering","Behavioral research; Graphic methods; Job analysis; Semantic Web; Cognition; Correlation; Head; Heterogeneous graph; Inter-modality and intra-modality correlation; Intermodality; Question Answering; Spatio-temporal; Task analysis; Video question answering; Semantics",Article,Scopus,2-s2.0-85123350484
"Tumpa S.N., Gavrilova M.L.","56607412500;26643382700;","Template Aging in Multi-Modal Social Behavioral Biometrics",2022,"IEEE Access","10",,,"8487","8501",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123350013&doi=10.1109%2fACCESS.2022.3144145&partnerID=40&md5=2a205a293352f04712450177d086dbf0","The uniqueness of social interactions on online social networks draws attention to cybersecurity research. Social Behavioral Biometric (SBB) systems extract unique patterns from online communication traits trails and generate digital fingerprints for user identification. However, with time those behavioral patterns change. These affect the authentication ability of a SBB system. In this paper, we have combined for the first time textual, contextual and interpersonal communicative information of users in online social networks to develop a biometric system. The SBB traits are combined using the weighted sum rule score level fusion algorithm with the genetic algorithm employed to choose the feature weights. The effects of template aging on the individual SBB traits and overall system have been analyzed for the first time. The proposed system achieves the recognition accuracy of 99.25% and outperforms all prior research on SBB. The experimental results on permanence evaluation demonstrate that the developed system can perform remarkably well despite the template aging effect. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/","Aging; Biometrics (access control); Blogs; Feature extraction; Social networking (online); Vocabulary; Writing","Behavioral research; Biometrics; Blogs; Genetic algorithms; Natural language processing systems; Online systems; Social networking (online); Behavioural Biometric; Biometric (access control); Feature permanence; Features extraction; Features fusions; Social behavioral biometric; Social networking (online); Template aging; User identification; Vocabulary; Writing; Access control",Article,Scopus,2-s2.0-85123350013
"Guo J.","56927101200;","Deep learning approach to text analysis for human emotion detection from big data",2022,"Journal of Intelligent Systems","31","1",,"113","126",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123314663&doi=10.1515%2fjisys-2022-0001&partnerID=40&md5=564dc721f3f0d87b0e492dc2012c2650","Emotional recognition has arisen as an essential field of study that can expose a variety of valuable inputs. Emotion can be articulated in several means that can be seen, like speech and facial expressions, written text, and gestures. Emotion recognition in a text document is fundamentally a content-based classification issue, including notions from natural language processing (NLP) and deep learning fields. Hence, in this study, deep learning assisted semantic text analysis (DLSTA) has been proposed for human emotion detection using big data. Emotion detection from textual sources can be done utilizing notions of Natural Language Processing. Word embeddings are extensively utilized for several NLP tasks, like machine translation, sentiment analysis, and question answering. NLP techniques improve the performance of learning-based methods by incorporating the semantic and syntactic features of the text. The numerical outcomes demonstrate that the suggested method achieves an expressively superior quality of human emotion detection rate of 97.22% and the classification accuracy rate of 98.02% with different state-of-the-art methods and can be enhanced by other emotional word embeddings. © 2022 Jia Guo, published by De Gruyter.","deep learning; human emotion detection; NLP; text analysis","Big data; Character recognition; Deep learning; Information retrieval systems; Numerical methods; Semantics; Sentiment analysis; Deep learning; Embeddings; Emotion detection; Emotion recognition; Emotional recognition; Facial Expressions; Human emotion; Human emotion detection; Learning approach; Written texts; Embeddings",Article,Scopus,2-s2.0-85123314663
"Comito C., Falcone D., Forestiero A.","6602651162;54912110400;6507072398;","AI-Driven Clinical Decision Support: Enhancing Disease Diagnosis Exploiting Patients Similarity",2022,"IEEE Access","10",,,"6878","6888",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123311776&doi=10.1109%2fACCESS.2022.3142100&partnerID=40&md5=a182b9b7d66a2ad0d6be84dc0d4461fb","Detecting diseases at early stage can help to overcome and treat them accurately. A Clinical Decision Support System (CDS) facilitates the identification of diseases together with the most suitable treatments. In this paper, we propose a CDS framework able to integrate heterogeneous health data from different sources, such as laboratory test results, basic information of patients, health records and social media data. Using the data so collected, innovative machine learning and deep learning approaches can be employed. A neural network model for predicting patients' future health conditions is proposed. The approach employs word embedding to model the semantic relations of hospital admissions, symptoms and diagnosis, and it introduces a mechanism to measure the relationships of different diagnosis in terms of symptoms similarity to exploit for the prediction task. Several CDSs, including diagnostic decision support systems for inferring patient diagnosis, have been proposed in the literature. However, these methods typically focus on a single patient and apply manually or automatically constructed decision rules to produce a diagnosis. Even worst, they consider only a single medical condition, whereas it is not uncommon that a patient has more than one medical condition at the same time. The novelty of the proposed approach is the combination of supervised and unsupervised artificial intelligence methods allowing to combine several and heterogeneous data sources related to a multitude of patients and concerning different medical conditions. Furthermore, with respect to previous approaches, the diagnosis prediction problem is formulated to predict the exact diagnosis in terms of semantic meaning by exploiting Natural Language Processing concepts. Experimental results, performed on a real-world EHR dataset, show that the proposed approach is effective and accurate and provides clinically meaningful interpretations. The obtained outcomes are promising for future extensions of the framework that could be a valuable means for automatic inferring disease diagnosis. © 2013 IEEE.","Clinical decision support system; Digital patient; Disease diagnosis prediction; Patient similarity; Word embedding","Computer aided diagnosis; Decision support systems; Deep learning; Diseases; Embeddings; Forecasting; Medical imaging; Natural language processing systems; Patient treatment; Clinical decision support systems; Digital patient; Disease diagnose prediction; Disease diagnosis; Embeddings; Medical diagnostic imaging; Medical services; Patient similarity; Predictive models; Word embedding; Semantics",Article,Scopus,2-s2.0-85123311776
"Kim M., Kang P.","57422785700;14034324500;","Text Embedding Augmentation Based on Retraining With Pseudo-Labeled Adversarial Embedding",2022,"IEEE Access","10",,,"8363","8376",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123310058&doi=10.1109%2fACCESS.2022.3142843&partnerID=40&md5=3a133f425ff83e7822c231c45b9050ea","Pre-trained language models (LMs) have been shown to achieve outstanding performance in various natural language processing tasks; however, these models have a significantly large number of parameters to handle large-scale text corpora during the pre-training process, and thus, they entail the risk of overfitting when fine-tuning for small task-oriented datasets is conducted. In this paper, we propose a text embedding augmentation method to prevent such overfitting. The proposed method applies augmentation to a text embedding by generating an adversarial embedding, which is not identical to original input embedding but maintaining the characteristics of the original input embedding, using PGD-based adversarial training for input text data. A pseudo-label that is identical to the label of the input text is then assigned to adversarial embedding to conduct retraining by using adversarial embedding and pseudo-label as input embedding and label pair for a separate LM. Experimental results on several text classification benchmark datasets demonstrated that the proposed method effectively prevented overfitting, which commonly occurs when adjusting a large-scale pre-trained LM to a specific task. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/","Data models; Extrapolation; Interpolation; Semantics; Task analysis; Training; Transformers","Job analysis; Natural language processing systems; Semantics; Text processing; Adversarial training; Embeddings; Generating; Language model; Overfitting; Pseudo-label; Retraining; Task analysis; Text embedding augmentation; Transformer; Classification (of information)",Article,Scopus,2-s2.0-85123310058
"Feng A., Zhang X., Song X.","57192194319;57219034270;57213456784;","Unrestricted Attention May Not Be All You Need–Masked Attention Mechanism Focuses Better on Relevant Parts in Aspect-Based Sentiment Analysis",2022,"IEEE Access","10",,,"8518","8528",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123307040&doi=10.1109%2fACCESS.2022.3142178&partnerID=40&md5=0615ade78d4ab89fb0ddcacecc782b10","Aspect-Based Sentiment Analysis (ABSA) is one of the highly challenging tasks in natural language processing. It extracts fine-grained sentiment information in user-generated reviews, as it aims at predicting the polarities towards predefined aspect categories or relevant entities in free text. Previous deep learning approaches usually rely on large-scale pre-trained language models and the attention mechanism, which applies the complete computed attention weights and does not place any restriction on the attention assignment. We argue that the original attention mechanism is not the ideal configuration for ABSA, as for most of the time only a small portion of terms are strongly related to the sentiment polarity of an aspect or entity. In this paper, we propose a masked attention mechanism customized for ABSA, with two different approaches to generate the mask. The first method sets an attention weight threshold that is determined by the maximum of all weights, and keeps only attention scores above the threshold. The second selects the top words with the highest weights. Both remove the lower score parts that are assumed to be less relevant to the aspect of focus. By ignoring part of input that is claimed irrelevant, a large proportion of input noise is removed, keeping the downstream model more focused and reducing calculation cost. Experiments on the Multi-Aspect Multi-Sentiment (MAMS) and SemEval-2014 datasets show significant improvements over state-of-the-art pre-trained language models with full attention, which displays the value of the masked attention mechanism. Recent work shows that simple self-attention in Transformer quickly degenerates to a rank-1 matrix, and masked attention may be another cure for that trend. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/","Bit error rate; Data mining; Deep learning; Semantics; Sentiment analysis; Task analysis; Transformers","Bit error rate; Computational linguistics; Deep learning; Job analysis; Semantics; Sentiment analysis; Attention mechanisms; Bit-error rate; Deep learning; Language model; Masked attention; Pre-trained language model; Sentiment analysis; Task analysis; Transformer; Data mining",Article,Scopus,2-s2.0-85123307040
"Zhou Y., Tang Y., Yi M., Xi C., Lu H.","57235137400;57421673800;57421673900;57235392500;56131319800;","CTI View: APT Threat Intelligence Analysis System",2022,"Security and Communication Networks","2022",,"9875199","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123251568&doi=10.1155%2f2022%2f9875199&partnerID=40&md5=f6b5da7f36c489d40ff197d6b11e331b","With the development of advanced persistent threat (APT) and the increasingly severe situation of network security, the strategic defense idea with the concept of ""active defense, traceability, and countermeasures""arises at the historic moment, thus cyberspace threat intelligence (CTI) has become increasingly valuable in enhancing the ability to resist cyber threats. Based on the actual demand of defending against the APT threat, we apply natural language processing to process the cyberspace threat intelligence (CTI) and design a new automation system CTI View, which is oriented to text extraction and analysis for the massive unstructured cyberspace threat intelligence (CTI) released by various security vendors. The main work of CTI View is as follows: (1) to deal with heterogeneous CTI, a text extraction framework for threat intelligence is designed based on automated test framework, text recognition technology, and text denoising technology. It effectively solves the problem of poor adaptability when crawlers are used to crawl heterogeneous CTI; (2) using regular expressions combined with blacklist and whitelist mechanism to extract the IOC and TTP information described in CTI effectively; (3) according to the actual requirements, a model based on bidirectional encoder representations from transformers (BERT) is designed to complete the entity extraction algorithm for heterogeneous threat intelligence. In this paper, the GRU layer is added to the existing BERT-BiLSTM-CRF model, and we evaluate the proposed model on the marked dataset and get better performance than the current mainstream entity extraction mode. © 2022 Yinghai Zhou et al.",,"Automation; Character recognition; Cybersecurity; Extraction; Network security; Active countermeasures; Active defense; Analysis system; Cyber threats; Cyberspaces; Entity extractions; Intelligence analysis; Networks security; Strategic defence; Text extraction; Natural language processing systems",Article,Scopus,2-s2.0-85123251568
"Yang X., Zhang W.","57271328700;57276366500;","Graph Convolutional Networks for Cross-Modal Information Retrieval",2022,"Wireless Communications and Mobile Computing","2022",,"6133142","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123203082&doi=10.1155%2f2022%2f6133142&partnerID=40&md5=0013a2bf7fcdd9b4cdc113487758cf9c","In recent years, due to the wide application of deep learning and more modal research, the corresponding image retrieval system has gradually extended from traditional text retrieval to visual retrieval combined with images and has become the field of computer vision and natural language understanding and one of the important cross-research hotspots. This paper focuses on the research of graph convolutional networks for cross-modal information retrieval and has a general understanding of cross-modal information retrieval and the related theories of convolutional networks on the basis of literature data. Modal information retrieval is designed to combine high-level semantics with low-level visual capabilities in cross-modal information retrieval to improve the accuracy of information retrieval and then use experiments to verify the designed network model, and the result is that the model designed in this paper is more accurate than the traditional retrieval model, which is up to 90%. © 2022 Xianben Yang and Wei Zhang.",,"Convolutional neural networks; Deep learning; Information retrieval; Search engines; Semantics; Visual languages; Accuracy of information; Convolutional networks; Cross-modal; High level semantics; Hotspots; Image retrieval systems; Literature data; Natural language understanding; Text retrieval; Visual retrieval; Convolution",Article,Scopus,2-s2.0-85123203082
"Pan W., Li J., Gao L., Yue L., Yang Y., Deng L., Deng C.","57420448900;57419927300;57420274600;57419580100;57419927400;57419580200;57419927500;","Semantic Graph Neural Network: A Conversion from Spam Email Classification to Graph Classification",2022,"Scientific Programming","2022",,"6737080","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123201421&doi=10.1155%2f2022%2f6737080&partnerID=40&md5=6a95858c6c313c22b4319eafd7f6d243","In this study, we propose a method named Semantic Graph Neural Network (SGNN) to address the challenging task of email classification. This method converts the email classification problem into a graph classification problem by projecting email into a graph and applying the SGNN model for classification. The email features are generated from the semantic graph; hence, there is no need of embedding the words into a numerical vector representation. The method performance is tested on the different public datasets. Experiments in the public dataset show that the presented method achieves high accuracy in the email classification test against a few public datasets. The performance is better than the state-of-the-art deep learning-based method in terms of spam classification. © 2022 Weisen Pan et al.",,"Classification (of information); Deep learning; Graph neural networks; Semantic Web; Semantics; Statistical tests; Email classification; Embeddings; Graph classification; Graph neural networks; Neural network model; Performance; Public dataset; Semantic graphs; Spam e-mails; Vector representations; Electronic mail",Article,Scopus,2-s2.0-85123201421
"Zhou Y.","57420274300;","Natural Language Processing with Improved Deep Learning Neural Networks",2022,"Scientific Programming","2022",,"6028693","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123187188&doi=10.1155%2f2022%2f6028693&partnerID=40&md5=e27c07469a920b52e8a9fc5fee126789","As one of the core tasks in the field of natural language processing, syntactic analysis has always been a hot topic for researchers, including tasks such as Questions and Answer (Q&A), Search String Comprehension, Semantic Analysis, and Knowledge Base Construction. This paper aims to study the application of deep learning and neural network in natural language syntax analysis, which has significant research and application value. This paper first studies a transfer-based dependent syntax analyzer using a feed-forward neural network as a classifier. By analyzing the model, we have made meticulous parameters of the model to improve its performance. This paper proposes a dependent syntactic analysis model based on a long-term memory neural network. This model is based on the feed-forward neural network model described above and will be used as a feature extractor. After the feature extractor is pretrained, we use a long short-term memory neural network as a classifier of the transfer action, and the characteristics extracted by the syntactic analyzer as its input to train a recursive neural network classifier optimized by sentences. The classifier can not only classify the current pattern feature but also multirich information such as analysis of state history. Therefore, the model is modeled in the analysis process of the entire sentence in syntactic analysis, replacing the method of modeling independent analysis. The experimental results show that the model has achieved greater performance improvement than baseline methods. © 2022 YiTao Zhou.",,"Classification (of information); Deep neural networks; Feedforward neural networks; Knowledge based systems; Natural language processing systems; Semantics; Feature extractor; Feed forward neural net works; Hot topics; Knowledge-base construction; Learning neural networks; Neural-networks; Performance; Semantic analysis; Semantics knowledge; Syntactic analysis; Syntactics",Article,Scopus,2-s2.0-85123187188
"Wang Z., Albarghouthi A., Prakriya G., Jha S.","57219742939;36336887800;37122582600;7202728236;","Interval universal approximation for neural networks",2022,"Proceedings of the ACM on Programming Languages","6","POPL","3498675","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123169845&doi=10.1145%2f3498675&partnerID=40&md5=2985fada9dd36aaef1960d1d70895a66","To verify safety and robustness of neural networks, researchers have successfully applied abstract interpretation, primarily using the interval abstract domain. In this paper, we study the theoretical power and limits of the interval domain for neural-network verification. First, we introduce the interval universal approximation (IUA) theorem. IUA shows that neural networks not only can approximate any continuous function f (universal approximation) as we have known for decades, but we can find a neural network, using any well-behaved activation function, whose interval bounds are an arbitrarily close approximation of the set semantics of f (the result of applying f to a set of inputs). We call this notion of approximation interval approximation. Our theorem generalizes the recent result of Baader et al. from ReLUs to a rich class of activation functions that we call squashable functions. Additionally, the IUA theorem implies that we can always construct provably robust neural networks under g.,""∞-norm using almost any practical activation function. Second, we study the computational complexity of constructing neural networks that are amenable to precise interval analysis. This is a crucial question, as our constructive proof of IUA is exponential in the size of the approximation domain. We boil this question down to the problem of approximating the range of a neural network with squashable activation functions. We show that the range approximation problem (RA) is a ""2-intermediate problem, which is strictly harder than NP-complete problems, assuming coNPg.,NP. As a result, IUA is an inherently hard problem: No matter what abstract domain or computational tools we consider to achieve interval approximation, there is no efficient construction of such a universal approximator. This implies that it is hard to construct a provably robust network, even if we have a robust network to start with. © 2022 Owner/Author.","Abstract Interpretation; Universal Approximation","Chemical activation; Computational complexity; Model checking; Semantics; Abstract domains; Abstract interpretations; Activation functions; Approximation theorem; Interval approximation; Interval domain; Neural-networks; Power; Robust network; Universal approximation; Abstracting",Article,Scopus,2-s2.0-85123169845
"Li W., Xu J., Chen Q.","57417114100;57417253800;57417253900;","Knowledge Distillation-Based Multilingual Code Retrieval",2022,"Algorithms","15","1","25","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123051385&doi=10.3390%2fa15010025&partnerID=40&md5=02fa2f5ce488590f6ab4c724fa582d84","Semantic code retrieval is the task of retrieving relevant codes based on natural language queries. Although it is related to other information retrieval tasks, it needs to bridge the gaps between the language used in the code (which is usually syntax-specific and logic-specific) and the natural language which is more suitable for describing ambiguous concepts and ideas. Existing approaches study code retrieval in a natural language for a specific programming language, however it is unwieldy and often requires a large amount of corpus for each language when dealing with multilingual scenarios.Using knowledge distillation of six existing monolingual Teacher Models to train one Student Model – MPLCS (Multi-Programming Language Code Search), this paper proposed a method to support multi-programing language code search tasks. MPLCS has the ability to incorporate multiple languages into one model with low corpus requirements. MPLCS can study the commonality between different programming languages and improve the recall accuracy for small dataset code languages. As for Ruby used in this paper, MPLCS improved its MRR score by 20 to 25%. In addition, MPLCS can compensate the low recall accuracy of monolingual models when perform language retrieval work on other programming languages. And in some cases, MPLCS’ recall accuracy can even outperform the recall accuracy of monolingual models when they perform language retrieval work on themselves. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Code search; Knowledge distillation; Multilingualities","Codes (symbols); Computer programming languages; Natural language processing systems; Semantics; Code retrievals; Code search; Knowledge distillation; Large amounts; Multi-programming; Multilinguality; Natural language queries; Natural languages; Semantic codes; Teacher models; Distillation",Article,Scopus,2-s2.0-85123051385
"Caldarini G., Jaf S., McGarry K.","57417231700;55216782200;7006097830;","A Literature Survey of Recent Advances in Chatbots",2022,"Information (Switzerland)","13","1","41","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123046681&doi=10.3390%2finfo13010041&partnerID=40&md5=6af6a0dba506c4ebe9798d531afd42ee","Chatbots are intelligent conversational computer systems designed to mimic human conversation to enable automated online guidance and support. The increased benefits of chatbots led to their wide adoption by many industries in order to provide virtual assistance to customers. Chatbots utilise methods and algorithms from two Artificial Intelligence domains: Natural Language Processing and Machine Learning. However, there are many challenges and limitations in their application. In this survey we review recent advances on chatbots, where Artificial Intelligence and Natural Language processing are used. We highlight the main challenges and limitations of current work and make recommendations for future research investigation. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Chatbot; ChatScript; Conversation systems; Conversational agents; Conversational entities; Conversational modelling; Conversational system; Embodied conversational agents; Human-computer dialogue system; Social chatbots","Human computer interaction; Learning algorithms; Machine learning; Natural language processing systems; Online systems; Surveys; User interfaces; Chatbots; Chatscript; Conversation systems; Conversational agents; Conversational entity; Conversational model; Conversational systems; Dialogue systems; Embodied conversational agent; Human-computer dialog system; Human-computer dialogues; Social chatbot; Speech processing",Article,Scopus,2-s2.0-85123046681
"Qasim R., Bangyal W.H., Alqarni M.A., Ali Almazroi A.","57370937800;36449048100;56728454300;50860912200;","A Fine-Tuned BERT-Based Transfer Learning Approach for Text Classification",2022,"Journal of Healthcare Engineering","2022",,"3498123","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123044999&doi=10.1155%2f2022%2f3498123&partnerID=40&md5=0e2c27fdbd014f440e93cbe51de01cdc","Text Classification problem has been thoroughly studied in information retrieval problems and data mining tasks. It is beneficial in multiple tasks including medical diagnose health and care department, targeted marketing, entertainment industry, and group filtering processes. A recent innovation in both data mining and natural language processing gained the attention of researchers from all over the world to develop automated systems for text classification. NLP allows categorizing documents containing different texts. A huge amount of data is generated on social media sites through social media users. Three datasets have been used for experimental purposes including the COVID-19 fake news dataset, COVID-19 English tweet dataset, and extremist-non-extremist dataset which contain news blogs, posts, and tweets related to coronavirus and hate speech. Transfer learning approaches do not experiment on COVID-19 fake news and extremist-non-extremist datasets. Therefore, the proposed work applied transfer learning classification models on both these datasets to check the performance of transfer learning models. Models are trained and evaluated on the accuracy, precision, recall, and F1-score. Heat maps are also generated for every model. In the end, future directions are proposed. © 2022 Rukhma Qasim et al.",,"Automation; Classification (of information); Coronavirus; Data mining; Learning systems; Social networking (online); Text processing; Data mining tasks; Entertainment industry; Filtering process; Information retrieval problems; Learning approach; Multiple tasks; Social media; Targeted marketing; Text classification; Transfer learning; Natural language processing systems; albert base-v2 classification algorithm; Article; bart large classification algorithm; bert base classification algorithm; bert large classification algorithm; classification algorithm; comparative study; coronavirus disease 2019; data accuracy; deep learning; disinformation; distilbert classification algorithm; electra small classification algorithm; fine tuned bidirectional encoder representation from transformers; human; machine learning; roberta base classification algorithm; roberta large classification algorithm; social media; text classification; transfer of learning; xgboost neural network; xlm roberta base classification algorithm; natural language processing; COVID-19; Disinformation; Humans; Machine Learning; Natural Language Processing; SARS-CoV-2",Article,Scopus,2-s2.0-85123044999
"Jamali A., Mahdianpari M.","56909712300;57190371939;","Swin Transformer and Deep Convolutional Neural Networks for Coastal Wetland Classification Using Sentinel-1, Sentinel-2, and LiDAR Data",2022,"Remote Sensing","14","2","359","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122965038&doi=10.3390%2frs14020359&partnerID=40&md5=fabe1957d421efc12ae2fa8a220e0326","The use of machine learning algorithms to classify complex landscapes has been revolution-ized by the introduction of deep learning techniques, particularly in remote sensing. Convolutional neural networks (CNNs) have shown great success in the classification of complex high-dimensional remote sensing imagery, specifically in wetland classification. On the other hand, the state-of-the-art natural language processing (NLP) algorithms are transformers. Although the transformers have been studied for a few remote sensing applications, the integration of deep CNNs and transformers has not been studied, particularly in wetland mapping. As such, in this study, we explore the potential and possible limitations to be overcome regarding the use of a multi-model deep learning network with the integration of a modified version of the well-known deep CNN network of VGG-16, a 3D CNN network, and Swin transformer for complex coastal wetland classification. Moreover, we discuss the potential and limitation of the proposed multi-model technique over several solo models, including a random forest (RF), support vector machine (SVM), VGG-16, 3D CNN, and Swin transformer in the pilot site of Saint John city located in New Brunswick, Canada. In terms of F-1 score, the multi-model network obtained values of 0.87, 0.88, 0.89, 0.91, 0.93, 0.93, and 0.93 for the recognition of shrub wetland, fen, bog, aquatic bed, coastal marsh, forested wetland, and freshwater marsh, respectively. The results suggest that the multi-model network is superior to other solo classifiers from 3.36% to 33.35% in terms of average accuracy. Results achieved in this study suggest the high potential for integrating and using CNN networks with the cutting-edge transformers for the classification of complex landscapes in remote sensing. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","3D convolutional neural network; Coastal wetlands; Deep learning; New Brunswick; Random forest; Support vector machine; Swin transformer","Complex networks; Convolution; Convolutional neural networks; Decision trees; Deep neural networks; Natural language processing systems; Random forests; Remote sensing; Wetlands; 3d convolutional neural network; Coastal wetlands; Convolutional neural network; Deep learning; Multi-modelling; New brunswick; Random forests; Support vectors machine; Swin transformer; Wetland classification; Support vector machines",Article,Scopus,2-s2.0-85122965038
"Lin M., Jing W., Di D., Chen G., Song H.","57222960364;26641216600;57209536493;55733416300;57199094588;","Multi-Scale U-Shape MLP for Hyperspectral Image Classification",2022,"IEEE Geoscience and Remote Sensing Letters","19",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122896166&doi=10.1109%2fLGRS.2022.3141547&partnerID=40&md5=1a66e8d68801bc2e5d9df7bedf32183a","Hyperspectral images (HSIs) have significant applications in various domains, since they register numerous semantic and spatial information in the spectral band with spatial variability of spectral signatures. Two critical challenges in identifying pixels of the HSI are, respectively, representing the correlated information among the local and global, as well as the abundant parameters of the model. To tackle this challenge, we propose a multi-scale U-shape multi-layer perceptron (MUMLP) a model consisting of the designed multi-scale channel (MSC) block and the U-shape multi-layer perceptron (UMLP) structure. MSC transforms the channel dimension and mixes spectral band feature to embed the deep-level representation adequately. UMLP is designed by the encoder–decoder structure with multi-layer perceptron layers, which is capable of compressing the large-scale parameters. Extensive experiments are conducted to demonstrate that our model can outperform state-of-the-art methods across the board on three wide-adopted public datasets, namely Pavia University (PaviaU), Houston 2013, and Houston 2018. © 2022 IEEE.","Geoscience and remote sensing; Image coding; Registers; Semantics; Shape; Transforms; Unified modeling language","Classification (of information); Image classification; Semantics; Compression model; Houston; Hyperspectral image; Hyperspectral image classification; Multi-scale shape; Multi-scales; Multilayers perceptrons; Semantics Information; Spectral band; U shape; Spectroscopy; algorithm; detection method; image analysis; image classification; parameter estimation; pixel; Houston; Texas; United States",Article,Scopus,2-s2.0-85122896166
"Sun X., Ding B.","57226804915;57412962300;","Neural Network with Hierarchical Attention Mechanism for Contextual Topic Dialogue Generation",2022,"IEEE Access","10",,,"4628","4639",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122893071&doi=10.1109%2fACCESS.2022.3140820&partnerID=40&md5=aa3815750a8dde812973f65c8a4a43c2","The encoder-decoder model has achieved remarkable results in natural language generation. However, in the dialogue generation work, we often ignore the influence of the dialogue context information and topic information in the generation, resulting in the generated replies not close to the context or lack of topic information leads to general responses. In this work, we study the generation of multi-turn dialogues based on a large corpus and take advantage of the context information and topic information of the conversation in the process of dialogue generation to generate more coherent context-sensitive responses. We improve upon existing models and attention mechanisms and propose a new hierarchical model to better solve the problem of dialogue context (the HAT model). This method enables the model to obtain more contextual information when processing and improves the ability of the model in terms of contextual relevance to produce high-quality responses. In addition, to address the absence of topics in the responses, we pre-train the LDA(Latent Dirichlet Allocation) topic model to extract topic words of the dialogue content and retain as much topic information of dialogue as possible. Our model is extensively tested in several corpora, and the experiments illustrate that our model is superior to most hierarchical and non-hierarchical models with respect to multiple evaluation metrics. © 2021 IEEE.","Dialogue context; dialogue generation; hierarchical attention; topic","Data mining; Decoding; Hierarchical systems; Natural language processing systems; Signal encoding; Statistics; Attention mechanisms; Context information; Context models; Dialog context; Dialogue generations; Encodings; Hierarchical attention; Hierarchical model; Neural-networks; Topic; Semantics",Article,Scopus,2-s2.0-85122893071
"Ahmed R.A.E.-D., Fernández-Veiga M., Gawich M.","57413151200;6602260312;57189381027;","Neural Collaborative Filtering with Ontologies for Integrated Recommendation Systems",2022,"Sensors","22","2","700","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122859430&doi=10.3390%2fs22020700&partnerID=40&md5=f61973926a7b6d246caaaf2aabb90982","Machine learning (ML) and especially deep learning (DL) with neural networks have demonstrated an amazing success in all sorts of AI problems, from computer vision to game playing, from natural language processing to speech and image recognition. In many ways, the approach of ML toward solving a class of problems is fundamentally different than the one followed in classical engineering, or with ontologies. While the latter rely on detailed domain knowledge and almost exhaustive search by means of static inference rules, ML adopts the view of collecting large datasets and processes this massive information through a generic learning algorithm that builds up tentative solutions. Combining the capabilities of ontology-based recommendation and ML-based techniques in a hybrid system is thus a natural and promising method to enhance semantic knowledge with statistical models. This merge could alleviate the burden of creating large, narrowly focused ontologies for complicated domains, by using probabilistic or generative models to enhance the predictions without attempting to provide a semantic support for them. In this paper, we present a novel hybrid recommendation system that blends a single architecture of classical knowledge-driven recommendations arising from a tailored ontology with recommendations generated by a data-driven approach, specifically with classifiers and a neural collaborative filtering. We show that bringing together these knowledge-driven and data-driven worlds provides some measurable improvement, enabling the transfer of semantic information to ML and, in the opposite direction, statistical knowledge to the ontology. Moreover, the novel proposed system enables the extraction of the reasoning recommendation results after updating the standard ontology with the new products and user behaviors, thus capturing the dynamic behavior of the environment of our interest. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Neural collaborative filtering; Ontologies; Recommendation systems; Retail dataset","Behavioral research; Collaborative filtering; Computer games; Deep learning; Domain Knowledge; Hybrid systems; Large dataset; Learning algorithms; Natural language processing systems; Ontology; Semantics; Deep learning; Domain knowledge; Game playing; Inference rules; Large datasets; Neural collaborative filtering; Neural-networks; Ontology's; Ontology-based; Retail dataset; Recommender systems; algorithm; machine learning; natural language processing; semantics; Algorithms; Machine Learning; Natural Language Processing; Neural Networks, Computer; Semantics",Article,Scopus,2-s2.0-85122859430
"Unal U., Dag H.","57215332698;6507328166;","AnomalyAdapters: Parameter-Efficient Multi-Anomaly Task Detection",2022,"IEEE Access","10",,,"5635","5646",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122849406&doi=10.1109%2fACCESS.2022.3141161&partnerID=40&md5=1ba82b8285c783193690764834d965f4","The emergence of technological innovations brings sophisticated threats. Cyberattacks are increasing day by day aligned with these innovations and entails rapid solutions for defense mechanisms. These attacks may hinder enterprise operations or more importantly, interrupt critical infrastructure systems, that are essential to safety, security, and well-being of a society. Anomaly detection, as a protection step, is significant for ensuring a system security. Logs, which are accepted sources universally, are utilized in system health monitoring and intrusion detection systems. Recent developments in Natural Language Processing (NLP) studies show that contextual information decreases false-positives yield in detecting anomalous behaviors. Transformers and their adaptations to various language understanding tasks exemplify the enhanced ability to extract this information. Deep network based anomaly detection solutions use generally feature-based transfer learning methods. This type of learning presents a new set of weights for each log type. It is unfeasible and a redundant way considering various log sources. Also, a vague representation of model decisions prevents learning from threat data and improving model capability. In this paper, we propose AnomalyAdapters (AAs) which is an extensible multi-anomaly task detection model. It uses pretrained transformers' variant to encode a log sequences and utilizes adapters to learn a log structure and anomaly types. Adapter-based approach collects contextual information, eliminates information loss in learning, and learns anomaly detection tasks from different log sources without overuse of parameters. Lastly, our work elucidates the decision making process of the proposed model on different log datasets to emphasize extraction of threat data via explainability experiments. © 2013 IEEE.","Adapters; Anomaly detection; Cyber threat intelligence; Explainability; Log; Transfer learning","Anomaly detection; Cybersecurity; Decision making; Intrusion detection; Job analysis; Learning algorithms; Learning systems; Natural language processing systems; Adaptation models; Adapter; Anomaly detection; Cybe threat intelligence; Cyber threats; Explainability; Log; Security; Task analysis; Transfer learning; Transformer; Semantics",Article,Scopus,2-s2.0-85122849406
"Winkler S.L., Finch D., Wang X., Toyinbo P., Marszalek J., Rakoczy C.M., Rice C.E., Pollard K., Rhodes M.A., Eldred K., Llanos I., Peterson M., Williams M., Zuniga E., White H., Delikat J., Ballistrea L., White K., Cockerham G.C.","57203137975;57410622300;57410331200;23467826100;7004578023;55854005000;57410331300;57409890300;57410622400;6701731845;56444456400;57410180300;57198682496;57410034800;57205137898;55550379500;57209332056;35188191200;6701654673;","Veterans with Traumatic Brain Injury–related Ocular Injury and Vision Dysfunction: Recommendations for Rehabilitation",2022,"Optometry and Vision Science","99","1",,"9","17",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122769269&doi=10.1097%2fOPX.0000000000001828&partnerID=40&md5=3dbbfc46ba492dea3c7d6bdbb393108f","SIGNIFICANCE: We know the prevalence of traumatic brain injury (TBI)–related vision impairment and ocular injury symptoms. Lacking is an understanding of health care utilization to treat these symptoms. Utilization knowledge is important to structuring access to treatment, identifying clinical training needs, and providing evidence of the effectiveness of treatment. PURPOSE: This article reports rehabilitation, glasses/contacts, and imaging/photography/video recommendations made by optometrists and ophthalmologists as part of the Department of Veterans Affairs–mandated Performance of Traumatic Brain Injury Specific Ocular Health and Visual Functioning Examination administered to veterans with TBI at Department of Veterans Affairs polytrauma specialty facilities. METHODS: Using a retrospective design, natural language processing, and descriptive and regression statistics, data were analyzed for 2458 Operation Enduring Freedom/Operation Iraqi Freedom veterans who were administered the mandated examination between 2008 and 2017. RESULTS: Of the 2458 veterans, vision rehabilitation was recommended for 24%, glasses/contacts were recommended for 57%, and further imaging/photography/video testing was recommended for 58%. Using key words in the referral, we determined that 37% of veterans were referred to blind rehabilitation, 16% to occupational therapy, and 3% to low-vision clinics. More than 50% of the referrals could have been treated by blind rehabilitation, occupational therapy, or low-vision clinics. Rehabilitation referrals were significantly associated with younger age, floaters, photosensitivity, double vision, visual field and balance deficits, dizziness, and difficulty reading. In comparison, prescriptions for glasses and contacts were associated with older age, photosensitivity, blurred vision, decreased visual field and night vision, difficulty reading, and dry eye. Imaging/photography/video testing was associated with floaters, photosensitivity, and headache. CONCLUSIONS: Findings delineate service delivery models available to veterans with TBI-related vision impairment. The challenge these data address is the lack of clear paths from diagnosis of TBI to identification of vision dysfunction deficits to specialized vision rehabilitation, and finally to community reintegration and community based-vision rehabilitation. Copyright © 2021 American Academy of Optometry",,"Glass; Light sensitive materials; Natural language processing systems; Patient rehabilitation; Photosensitivity; Vision; Clinical training; Department of veterans affairs; Glass contact; Healthcare utilization; Low vision clinic; Training needs; Traumatic Brain Injuries; Vision impairments; Vision rehabilitation; Visual fields; Brain",Article,Scopus,2-s2.0-85122769269
"Visani E., Sebastiano D.R., Duran D., Garofalo G., Magliocco F., Silipo F., Buccino G.","14619842400;57190110680;55806625600;57194159524;56733039000;6504798973;6701802380;","The Semantics of Natural Objects and Tools in the Brain: A Combined Behavioral and MEG Study",2022,"Brain Sciences","12","1","97","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122761746&doi=10.3390%2fbrainsci12010097&partnerID=40&md5=7e8d20e2cad967a3135ab67926b81409","Current literature supports the notion that the recognition of objects, when visually pre-sented, is sub-served by neural structures different from those responsible for the semantic processing of their nouns. However, embodiment foresees that processing observed objects and their verbal labels should share similar neural mechanisms. In a combined behavioral and MEG study, we com-pared the modulation of motor responses and cortical rhythms during the processing of graspable natural objects and tools, either verbally or pictorially presented. Our findings demonstrate that conveying meaning to an observed object or processing its noun similarly modulates both motor responses and cortical rhythms; being natural graspable objects and tools differently represented in the brain, they affect in a different manner both behavioral and MEG findings, independent of presentation modality. These results provide experimental evidence that neural substrates responsible for conveying meaning to objects overlap with those where the object is represented, thus supporting an embodied view of semantic processing. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Behavioral responses; Beta rhythm; Embodiment; MEG; Object representations; Semantics","article; beta rhythm; brain; human; human experiment; recognition; semantics",Article,Scopus,2-s2.0-85122761746
"Farghaly K., Soman R.K., Collinge W., Mosleh M.H., Manu P., Cheung C.M.","57195290066;57188757365;56787735700;57226470587;35756460600;57189854261;","CONSTRUCTION SAFETY ONTOLOGY DEVELOPMENT AND ALIGNMENT WITH INDUSTRY FOUNDATION CLASSES (IFC)",2022,"Journal of Information Technology in Construction","27",,,"94","108",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122752701&doi=10.36680%2fj.itcon.2022.005&partnerID=40&md5=8f4217b3a2ea98f9d3aac1eb2be91002","A pronounced gap often exists between expected and actual safety performance in the construction industry. The multifaceted causes of this performance gap are resulting from the misalignment between design assumptions and actual construction processes that take place on-site. In general, critical factors are rooted in the lack of interoperability around the building and work-environment information due to its heterogeneous nature. To overcome the interoperability challenge in safety management, this paper represents the development of an ontological model consisting of terms and relationships between these terms, creating a conceptual information model for construction safety management and linking that ontology to IfcOWL. The developed ontology, named Safety and Health Exchange (SHE), comprises eight concepts and their relationships required to identify and manage safety risks in the design and planning stages. The main concepts of the developed ontology are identified based on reviewing accident cases from 165 Reporting of Injuries, Diseases and Dangerous Occurrences Regulations (RIDDOR) and 31 Press Releases from the database of the Health and Safety Executive (HSE) in the United Kingdom. Consequently, a semantic mapping between the developed ontology and IfcOWL (the most popular ontology and schema for interoperability in the AEC sector) is proposed. Then several SPARQL queries were developed and implemented to evaluate the semantic consistency of the developed ontology and the cross-mapping. The proposed ontology and cross-mapping gained recognition for its innovation in utilising OpenBIM and won the BuildingSMART professional research award 2020. This work could facilitate developing a knowledge-based system in the BIM environment to assist designers in addressing health and safety issues during the design and planning phases in the construction sector. © 2022 The author(s). This is an open access article distributed under the terms of the Creative Commons Attribution 4.0 International (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.","BIM; Building information modelling; Design for safety; Linked data; Ontology; Prevention through design","Accident prevention; Construction industry; Health; Health risks; Information theory; Interoperability; Knowledge based systems; Linked data; Mapping; Occupational risks; Ontology; Public relations; Semantics; BIM; Building Information Modelling; Construction safety; Design and planning; Design for safety; Ontology alignment; Ontology development; Ontology's; Prevention through design; Safety management; Architectural design",Article,Scopus,2-s2.0-85122752701
"Ma C., Sun Y., Yang Z., Huang H., Zhan D., Qu J.","57193072222;57411023900;57410437400;57410144400;57190583164;57276800600;","Content Feature Extraction-based Hybrid Recommendation for Mobile Application Services",2022,"Computers, Materials and Continua","71","2",,"6201","6217",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122734188&doi=10.32604%2fcmc.2022.022717&partnerID=40&md5=7a43ac8d2d9974faf3cabd850f860427","The number of mobile application services is showing an explosive growth trend, which makes it difficult for users to determine which ones are of interest. Especially, the new mobile application services are emerge continuously, most of them have not be rated when they need to be recommended to users. This is the typical problem of cold start in the field of collaborative filtering recommendation. This problem may makes it difficult for users to locate and acquire the services that they actually want, and the accuracy and novelty of service recommendations are also difficult to satisfy users. To solve this problem, a hybrid recommendation method for mobile application services based on content feature extraction is proposed in this paper. First, the proposed method in this paper extracts service content features through Natural Language Processing technologies such as word segmentation, part-of-speech tagging, and dependency parsing. It improves the accuracy of describing service attributes and the rationality of the method of calculating service similarity. Then, a language representation model called Bidirectional Encoder Representation from Transformers (BERT) is used to vectorize the content feature text, and an improved weighted word mover's distance algorithm based on Term Frequency-Inverse Document Frequency (TFIDF-WMD) is used to calculate the similarity of mobile application services. Finally, the recommendation process is completed by combining the item-based collaborative filtering recommendation algorithm. The experimental results show that by using the proposed hybrid recommendation method presented in this paper, the cold start problem is alleviated to a certain extent, and the accuracy of the recommendation result has been significantly improved. © 2022 Tech Science Press. All rights reserved.","Cold start; Feature extraction; Natural language processing; Service recommendation; Word mover's distance","Collaborative filtering; Extraction; Inverse problems; Mobile computing; Natural language processing systems; Text processing; Application services; Cold-start; Collaborative filtering recommendations; Features extraction; Hybrid recommendation; Mobile applications; Mover's distance; Recommendation methods; Service recommendations; Word mover distance; Feature extraction",Article,Scopus,2-s2.0-85122734188
"Jamali A., Mahdianpari M.","56909712300;57190371939;","Swin Transformer for Complex Coastal Wetland Classification Using the Integration of Sentinel-1 and Sentinel-2 Imagery",2022,"Water (Switzerland)","14","2","178","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122732902&doi=10.3390%2fw14020178&partnerID=40&md5=257b1ebdf8183d8ad54003a278db8a61","The emergence of deep learning techniques has revolutionized the use of machine learning algorithms to classify complicated environments, notably in remote sensing. Convolutional Neural Networks (CNNs) have shown considerable promise in classifying challenging high-dimensional remote sensing data, particularly in the classification of wetlands. State-of-the-art Natural Language Processing (NLP) algorithms, on the other hand, are transformers. Despite the fact that transformers have been utilized for a few remote sensing applications, they have not been compared to other well-known CNN networks in complex wetland classification. As such, for the classification of complex coastal wetlands in the study area of Saint John city, located in New Brunswick, Canada, we modified and employed the Swin Transformer algorithm. Moreover, the developed transformer classifier results were compared with two well-known deep CNNs of AlexNet and VGG-16. In terms of average accuracy, the proposed Swin Transformer algorithm outperformed the AlexNet and VGG-16 techniques by 14.3% and 44.28%, respectively. The proposed Swin Transformer classifier obtained F-1 scores of 0.65, 0.71, 0.73, 0.78, 0.82, 0.84, and 0.84 for the recognition of coastal marsh, shrub, bog, fen, aquatic bed, forested wetland, and freshwater marsh, respectively. The results achieved in this study suggest the high capability of transformers over very deep CNN networks for the classification of complex landscapes in remote sensing. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","AlexNet; CNN; Deep convolutional neural network; New Brunswick; Swin transformer; VGG-16; Wetland classification","Complex networks; Convolution; Convolutional neural networks; Image classification; Learning algorithms; Natural language processing systems; Remote sensing; Wetlands; Alexnet; Coastal wetlands; Convolutional neural network; Learning techniques; New brunswick; Remote-sensing; Sentinel-1; Swin transformer; VGG-16; Wetland classification; Deep neural networks; algorithm; artificial neural network; coastal wetland; complexity; data processing; saltmarsh; satellite imagery; Sentinel; shrub; Canada; New Brunswick",Article,Scopus,2-s2.0-85122732902
"Liu B., Yu A., Gao K., Tan X., Sun Y., Yu X.","57196891847;56337728400;57209023750;56074915200;57221679516;12141515900;","DSS-TRM: deep spatial–spectral transformer for hyperspectral image classification",2022,"European Journal of Remote Sensing","55","1",,"103","114",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122699571&doi=10.1080%2f22797254.2021.2023910&partnerID=40&md5=925c0c58696271b2fb5e58cbf011c81c","In recent years, the wide use of deep learning based methods has greatly improved the classification performance of hyperspectral image (HSI). As an effective method to improve the performance of deep convolution networks, attention mechanism is also widely used for HSI classification tasks. However, the majority of the existing attention mechanisms for HSI classification are based on the convolution layer, and the classification accuracy still has margins for improvement. Motivated by the latest self attention mechanism in natural language processing, a deep transformer is proposed for HSI classification in this paper. Specifically, deep transformer along the spectral dimension and the spatial dimension are explored respectively. Then, a deep spatial-spectral transformer (DSS-TRM) is proposed to improve the classification performance of HSI. The contribution of this paper is to make full use of self attention mechanism, that is to use transformer layer instead of convolution layer. More importantly, a DSS-TRM is proposed to realize end-to-end HSI classification. Extensive experiments are conducted on three HSI data sets. The experimental results demonstrates that the proposed DSS-TRM could outperform the traditional convolutional neural networks and attention based methods. © 2022 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","deep learning; Hyperspectral image classification; self attention; spatial-spectral transformer","Convolutional neural networks; Deep learning; Image classification; Image enhancement; Natural language processing systems; Spectroscopy; Attention mechanisms; Classification accuracy; Classification performance; Classification tasks; Deep learning; Hyperspectral image classification; Learning-based methods; Self attention; Spatial-spectral transformer; Convolution; image classification; language; learning",Article,Scopus,2-s2.0-85122699571
"Niarchos M., Stamatiadou M.E., Dimoulas C., Veglis A., Symeonidis A.","57408928900;57222375110;57105254400;6701328438;7004087388;","A Semantic Preprocessing Framework for Breaking News Detection to Support Future Drone Journalism Services",2022,"Future Internet","14","1","26","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122661764&doi=10.3390%2ffi14010026&partnerID=40&md5=193c019c32dfcd87494807cc61c36a07","Nowadays, news coverage implies the existence of video footage and sound, from which arises the need for fast reflexes by media organizations. Social media and mobile journalists assist in fulfilling this requirement, but quick on-site presence is not always feasible. In the past few years, Unmanned Aerial Vehicles (UAVs), and specifically drones, have evolved to accessible recreational and business tools. Drones could help journalists and news organizations capture and share breaking news stories. Media corporations and individual professionals are waiting for the appropriate flight regulation and data handling framework to enable their usage to become widespread. Drone journalism services upgrade the usage of drones in day-to-day news reporting operations, offering multiple benefits. This paper proposes a system for operating an individual drone or a set of drones, aiming to mediate real-time breaking news coverage. Apart from the definition of the system requirements and the architecture design of the whole system, the current work focuses on data retrieval and the semantics preprocessing framework that will be the basis of the final implementa-tion. The ultimate goal of this project is to implement a whole system that will utilize data retrieved from news media organizations, social media, and mobile journalists to provide alerts, geolocation inference, and flight planning. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Breaking news; Drone journalism; Events location estimation; Natural language processing (NLP); Semantic processing","Aircraft detection; Antennas; Data handling; Drones; Natural language processing systems; Search engines; Social networking (online); Breaking news; Drone journalism; Event location; Event location estimation; Location estimation; Natural language processing; News coverage; Semantic processing; Social media; Video footage; Semantics",Article,Scopus,2-s2.0-85122661764
"Wolna A., Durlik J., Wodniecka Z.","56636598500;57188550890;15057311400;","Pronominal anaphora resolution in Polish: Investigating online sentence interpretation using eye-tracking",2022,"PLoS ONE","17","1 January","e0262459","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122637846&doi=10.1371%2fjournal.pone.0262459&partnerID=40&md5=d0e98db5bfff6ee3d1a25be0363ab35a","The mechanism of anaphora resolution is subject to large cross-linguistic differences. The most likely reason for this is the different sensitivity of pronouns to the range of factors that determine their reference. In the current study, we explored the mechanism of anaphora resolution in Polish. First, we explored preferences in the interpretation of null and overt pronouns in ambiguous sentences. More specifically, we investigated whether Polish speakers prefer to relate overt pronouns to subject or object antecedents. Subsequently, we tested the consequences of violating this bias when tracing the online sentence-interpretation process using eye-tracking. Our results show that Polish speakers have a strong preference for interpreting null pronouns as referring to subject antecedents and interpreting overt pronouns as referring to object antecedents. However, in online sentence interpretation, only overt pronouns showed sensitivity to a violation of the speaker’s preference for a pronoun-antecedent match. This suggests that null pronoun resolution is more flexible than overt pronoun resolution. Our results indicate that it is much easier for Polish speakers to shift the reference of a null pronoun than an overt one whenever a pronoun is forced to refer to a less-preferred antecedent. These results are supported by naturalness ratings, which showed that null pronouns are considered equally natural regardless of their reference, while overt pronouns referring to subject antecedents are rated as considerably less natural than those referring to object antecedents. To explain this effect, we propose that the interpretation of null and overt pronouns is sensitive to different factors which determine their reference. © 2022 Wolna et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"article; eye tracking; adult; comprehension; female; human; language; male; online system; psycholinguistics; semantics; speech perception; young adult; Adult; Comprehension; Eye-Tracking Technology; Female; Humans; Language; Male; Online Systems; Psycholinguistics; Semantics; Speech Perception; Young Adult",Article,Scopus,2-s2.0-85122637846
"Zhang H., Qu D., Shao K., Yang X.","57227980400;57406785200;57453459400;57185881700;","DropDim: A Regularization Method for Transformer Networks",2022,"IEEE Signal Processing Letters","29",,,"474","478",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122582419&doi=10.1109%2fLSP.2022.3140693&partnerID=40&md5=331a55681ce2fdcf8759b48bbd0bd725","We introduce DropDim, a structured dropout method designed for regularizing the self-attention mechanism, which is a key component of the transformer. In contrast to the general dropout method, which randomly drops neurons, DropDim drops part of the embedding dimensions. In this way, the semantic information can be completely discarded. Thus, the excessive co-adapting between different embedding dimensions can be broken, and the self-attention is forced to encode meaningful features with a certain number of embedding dimensions erased. Experiments on a wide range of tasks executed on the MUST-C English-Germany dataset show that DropDim can effectively improve model performance, reduce over-fitting, and show complementary effects with other regularization methods. When combined with label smoothing, the WER can be reduced from 19.1% to 15.1% on the ASR task, and the BLEU value can be increased from 26.90 to 28.38 on the MT task. On the ST task, the model can reach a BLEU score of 22.99, an increase by 1.86 BLEU points compared to the strong baseline. © 1994-2012 IEEE.","dropout; End-to-end; regularization; transformer","Drops; Job analysis; Attention mechanisms; Dropout; Embedding dimensions; End to end; Regularisation; Regularization methods; Smoothing methods; Task analysis; Transformer; Semantics",Article,Scopus,2-s2.0-85122582419
"Panayiotou C.","57406972300;","Extraction of Poetic and Non-Poetic Relations From of-Prepositions Using WordNet",2022,"IEEE Access","10",,,"3469","3494",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122577384&doi=10.1109%2fACCESS.2021.3140030&partnerID=40&md5=f7e0eb9882c7ebc5b7dc6f718d91cfa3","The main goal of this paper is to extract the semantic relations underpinning the concepts of English prepositional of-constructions derived from poetic and non-poetic datasets, using Princeton WordNet. The problem is addressed by two different algorithms, which are evaluated for their ability to model the different types of resources from which the relations are derived, and for their ability to predict unseen relations. The first algorithm introduces the concept of subsumption hierarchy between relations in order to derive the most general relations associated to each type of data source and identify a set of relations specific to each dataset. The second algorithm investigates the use of a weighting scheme in order to establish the importance of each association extracted. Of particular importance are the notions of subsumption hierarchies between relations (expressed as synset pairs) and the Inverse Relation Frequency (IRF) measure, which is inspired by the Inverse Document Frequency measure used in Information Retrieval. The ontological prospects of using Princeton WordNet and the above algorithms for the creation of ontologies are also briefly discussed. Although the main interest of the proposed methods lies to the identification of conceptual relations particular to poetic resources, the methods followed can be applied and are evaluated on other domains too. © 2013 IEEE.","Knowledge representation; NLP; Ontology learning; Semantic relations extraction","Extraction; Inverse problems; Knowledge representation; Natural language processing systems; Semantics; Text processing; A-weighting; Data-source; Frequency measures; General relations; Knowledge-representation; Ontology learning; Semantic relation extractions; Semantic relations; Weighting scheme; Wordnet; Ontology",Article,Scopus,2-s2.0-85122577384
"Shahbazi Z., Byun Y.-C.","57212241946;8897891700;","Blockchain-Based Event Detection and Trust Verification Using Natural Language Processing and Machine Learning",2022,"IEEE Access","10",,,"5790","5800",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122575745&doi=10.1109%2fACCESS.2021.3139586&partnerID=40&md5=0eb54f950c610414b47bbfb9b562fed0","Information sharing is one of the huge topics in social media platform regarding the daily news related to events or disasters happens in nature or its human-made. The automatic urgent need identification and sharing posts and information delivery with a short response are essential tasks in this area. The key goal of this research is developing a solution for management of disasters and emergency response using social media platforms as a core component. This process focuses on text analysis techniques to improve the process of authorities in terms of emergency response and filter the information using the automatically gathered information to support the relief efforts. Specifically, we used state-of-art Machine Learning (ML), Deep Learning (DL), and Natural Language Processing (NLP) based on supervised and unsupervised learning using social media datasets to extract real-time content related to the emergency events to comfort the fast response in a critical situation. Similarly, the blockchain framework used in this process for trust verification of the detected events and eliminating the single authority on the system. The main reason of using the integrated system is to improve the system security and transparency to avoid sharing the wrong information related to an event in social media. © 2021 IEEE.","blockchain; deep learning; Event detection; machine learning; natural language processing","Blockchain; Blogs; Deep learning; Emergency services; Information dissemination; Interactive computer systems; Learning algorithms; Natural language processing systems; Online systems; Real time systems; Social networking (online); Block-chain; Deep learning; Events detection; Features extraction; Machine-learning; Real - Time system; Social media platforms; Social networking (online); Feature extraction",Article,Scopus,2-s2.0-85122575745
"Wang S., Yang C., Qu T., Yang K., Wan W.","57196155993;57406454000;57216990699;56707874800;57406454100;","Learning Users' Visual Preferences for Improving Recommendations",2022,"IEEE Access","10",,,"2929","2940",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122568501&doi=10.1109%2fACCESS.2022.3140215&partnerID=40&md5=1515f4d7d13914d932ff2ab494a88d50","Sequential recommender systems (SRSs) aim to predict the next item interest to a user by learning the users' dynamic preferences over items from the sequential user-item interactions. Most of existing SRSs make recommendations by only modeling a user's main preference towards the functions of items, while ignoring the user's auxiliary visual preference towards the appearances and styles of items. Although visual preference is less significant than the main preference, it may still play an important role in most of users' choice on items. On the one hand, a user often prefers to choose the item which matches her/his visual preference well from multiple items with the same function. For example, a lady may choose one clothes whose style suits her best from multiple clothes with the same function. On the other hand, some particular users (e.g., young girls) are usually very concerned about the appearances of some special items (e.g., clothes, jewelry). Therefore, the overlook of modeling users' visual preference may generate unsatisfied recommendations which can not match a user's various types of preferences and thus reduce the consumption experience. To address this gap, in this paper, we propose modeling users' visual preferences to improve the performance of sequential recommendations. Specifically, we devise a coupled Double-chain Preference learning Network (DPN) to jointly learn a user's main preference and visual preference as well as the interactions between them. In DPN, one chain is for modeling a user's main preference by taking the IDs of items as the input and the other chain is for modeling the user's visual preference by taking appearance images of items as the input. Finally, the two types of preferences are carefully integrated with an attention module for the next item prediction. Extensive experiments on two real-world transaction datasets show the superiority of our proposed DPN over the representative and state-of-the-art SRSs. © 2013 IEEE.","Deep neural networks; Recommendations; Visual preference","Markov processes; Modeling languages; Natural language processing systems; Recommender systems; Recurrent neural networks; User profile; Visual languages; Deep learning; Double chain; Learn+; Learning network; Multiple items; One chain; Performance; Preference learning; Recommendation; Visual preference; Deep neural networks",Article,Scopus,2-s2.0-85122568501
"Zhu Y., Liang X., Lin B., Ye Q., Jiao J., Lin L., Liang X.","57198948667;57223751519;57219625419;8435765900;22941049600;15061363400;55926362100;","Configurable Graph Reasoning for Visual Relationship Detection",2022,"IEEE Transactions on Neural Networks and Learning Systems","33","1",,"117","129",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122565528&doi=10.1109%2fTNNLS.2020.3027575&partnerID=40&md5=24f0fc3cdaff641cc4007e41870d0d90","Visual commonsense knowledge has received growing attention in the reasoning of long-tailed visual relationships biased in terms of object and relation labels. Most current methods typically collect and utilize external knowledge for visual relationships by following the fixed reasoning path of {subject, object $\to $ predicate} to facilitate the recognition of infrequent relationships. However, the knowledge incorporation for such fixed multidependent path suffers from the data set biased and exponentially grown combinations of object and relation labels and ignores the semantic gap between commonsense knowledge and real scenes. To alleviate this, we propose configurable graph reasoning (CGR) to decompose the reasoning path of visual relationships and the incorporation of external knowledge, achieving configurable knowledge selection and personalized graph reasoning for each relation type in each image. Given a commonsense knowledge graph, CGR learns to match and retrieve knowledge for different subpaths and selectively compose the knowledge routed path. CGR adaptively configures the reasoning path based on the knowledge graph, bridges the semantic gap between the commonsense knowledge, and the real-world scenes and achieves better knowledge generalization. Extensive experiments show that CGR consistently outperforms previous state-of-the-art methods on several popular benchmarks and works well with different knowledge graphs. Detailed analyses demonstrated that CGR learned explainable and compelling configurations of reasoning paths. © 2012 IEEE.","Graph learning; scene graph generation; visual reasoning; visual relationship detection (VRD)","Graph theory; Knowledge graph; Commonsense knowledge; External knowledge; Graph generation; Graph learning; Knowledge graphs; Scene graph generation; Scene-graphs; Semantic gap; Visual reasoning; Visual relationship detection; Semantics; article; attention; decomposition; reasoning",Article,Scopus,2-s2.0-85122565528
"Yang J., Li S., Gao S., Guo J.","57195371868;35622103200;36767575400;56168336500;","CorefDPR: A Joint Model for Coreference Resolution and Dropped Pronoun Recovery in Chinese Conversations",2022,"IEEE/ACM Transactions on Audio Speech and Language Processing","30",,,"571","581",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122560967&doi=10.1109%2fTASLP.2022.3140545&partnerID=40&md5=190e8596b261ec7ab101a7bdc141dde6","In this work, we present that coreference resolution and dropped pronoun recovery are two strongly related tasks in Chinese conversations, as recovering the dropped pronoun needs to explore the referent of the pronoun at first. Meanwhile, the omitted entity mention should be recovered before its coreferences are resolved. This motivates us to propose CorefDPR, a novel model to jointly resolve these two tasks and make them enhance each other. CorefDPR firstly utilizes a pre-trained language model to encode tokens in the conversation snippet. Then, the coreference resolution layer detects all entity mentions from the candidate text spans and groups them as coreferent mention clusters based on the contextualized token states. Furthermore, the pronoun recovery layer explores the referent of each dropped pronoun from the coreferent mention clusters and predicts the probability distribution over pronoun category for each token. Finally, a general conditional random fields (GCRF) is employed to globally optimize the pronoun recovery sequence of the snippet by modeling both intra-utterance and cross-utterance pronoun dependencies, and the recovered pronouns are further linked back to corresponding mention clusters to complete them. Experimental results on the benchmark demonstrate that our proposed model outperformed the state-of-the-art baselines of both these two tasks, and the exploratory experiments also demonstrate that these two tasks mutually benefit each other. © 2014 IEEE.","Coreference resolution; dropped pronoun recovery; natural language processing","Job analysis; Probability distributions; Recovery; Semantic Web; Speech processing; Cluster-based; Coreference; Coreference resolution; Joint models; Language model; Neural-networks; Predictive models; Probability: distributions; Proposal; Task analysis; Semantics",Article,Scopus,2-s2.0-85122560967
"Peng L., Yang Y., Wang Z., Huang Z., Shen H.T.","57197812577;57405805200;57207118649;57405805300;7404523209;","MRA-Net: Improving VQA Via Multi-Modal Relation Attention Network",2022,"IEEE transactions on pattern analysis and machine intelligence","44","1",,"318","329",,7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122546045&doi=10.1109%2fTPAMI.2020.3004830&partnerID=40&md5=c225c09ae59193f650a6a3af2628023c","Visual Question Answering (VQA) is a task to answer natural language questions tied to the content of visual images. Most recent VQA approaches usually apply attention mechanism to focus on the relevant visual objects and/or consider the relations between objects via off-the-shelf methods in visual relation reasoning. However, they still suffer from several drawbacks. First, they mostly model the simple relations between objects, which results in many complicated questions cannot be answered correctly, because of failing to provide sufficient knowledge. Second, they seldom leverage the harmony cooperation of visual appearance feature and relation feature. To solve these problems, we propose a novel end-to-end VQA model, termed Multi-modal Relation Attention Network (MRA-Net). The proposed model explores both textual and visual relations to improve performance and interpretability. In specific, we devise 1) a self-guided word relation attention scheme, which explore the latent semantic relations between words; 2) two question-adaptive visual relation attention modules that can extract not only the fine-grained and precise binary relations between objects but also the more sophisticated trinary relations. Both kinds of question-related visual relations provide more and deeper visual semantics, thereby improving the visual reasoning ability of question answering. Furthermore, the proposed model also combines appearance feature with relation feature to reconcile the two types of features effectively. Extensive experiments on five large benchmark datasets, VQA-1.0, VQA-2.0, COCO-QA, VQA-CP v2, and TDIUC, demonstrate that our proposed model outperforms state-of-the-art approaches.",,"algorithm; natural language processing; semantics; Algorithms; Natural Language Processing; Semantics",Article,Scopus,2-s2.0-85122546045
"Ghorbani A., Berenbaum D., Ivgi M., Dafna Y., Zou J.Y.","57207776014;57350176900;57221150585;56829620500;26326649300;","Beyond Importance Scores: Interpreting Tabular ML by Visualizing Feature Semantics",2022,"Information (Switzerland)","13","1","15","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122545160&doi=10.3390%2finfo13010015&partnerID=40&md5=660289af0a9e2b8cb1095838bd8cfbc0","Interpretability is becoming an active research topic as machine learning (ML) models are more widely used to make critical decisions. Tabular data are one of the most commonly used modes of data in diverse applications such as healthcare and finance. Much of the existing interpretability methods used for tabular data only report feature-importance scores—either locally (per example) or globally (per model)—but they do not provide interpretation or visualization of how the features interact. We address this limitation by introducing Feature Vectors, a new global interpretability method designed for tabular datasets. In addition to providing feature-importance, Feature Vectors discovers the inherent semantic relationship among features via an intuitive feature visualization technique. Our systematic experiments demonstrate the empirical utility of this new method by applying it to several real-world datasets. We further provide an easy-to-use Python package for Feature Vectors. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Explainability; Interpretability; Tabular ML","Data visualization; Semantics; Visualization; Diverse applications; Explainability; Features vector; Interpretability; Machine learning models; Research topics; Semantic relationships; Tabular data; Tabular machine learning; Visualization technique; Python",Article,Scopus,2-s2.0-85122545160
"Suat-Rojas N., Gutierrez-Osorio C., Pedraza C.","57405661400;57209267130;56825517500;","Extraction and Analysis of Social Networks Data to Detect Traffic Accidents",2022,"Information (Switzerland)","13","1","26","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122519154&doi=10.3390%2finfo13010026&partnerID=40&md5=11b12ca9b80803d9f839eed316a60e1e","Traffic accident detection is an important strategy governments can use to implement policies intended to reduce accidents. They usually use techniques such as image processing, RFID devices, among others. Social network mining has emerged as a low-cost alternative. However, social networks come with several challenges such as informal language and misspellings. This paper proposes a method to extract traffic accident data from Twitter in Spanish. The method consists of four phases. The first phase establishes the data collection mechanisms. The second consists of vectorially representing the messages and classifying them as accidents or non-accidents. The third phase uses named entity recognition techniques to detect the location. In the fourth phase, locations pass through a geocoder that returns their geographic coordinates. This method was applied to Bogota city and the data on Twitter were compared with the official traffic information source; comparisons showed some influence of Twitter on the commercial and industrial area of the city. The results reveal how effective the information on accidents reported on Twitter can be. It should therefore be considered as a source of information that may complement existing detection methods. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Classification; Intelligent transportation system; Machine learning; Named entity recognition; Natural language processing; Social media; Social sensors; Text mining; Traffic accident","Accidents; Character recognition; Data mining; Image processing; Learning algorithms; Machine learning; Natural language processing systems; Social networking (online); Text processing; Accident data; Accident detections; Images processing; Intelligent transportation systems; Low-costs; Named entity recognition; Network data; Social media; Social networks minings; Social sensors; Intelligent systems",Article,Scopus,2-s2.0-85122519154
"Garat D., Wonsever D.","15074095800;55097108100;","Automatic Curation of Court Documents: Anonymizing Personal Data",2022,"Information (Switzerland)","13","1","27","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122509539&doi=10.3390%2finfo13010027&partnerID=40&md5=918502e648011c5ae0e5fd9cb8117c4e","In order to provide open access to data of public interest, it is often necessary to perform several data curation processes. In some cases, such as biological databases, curation involves quality control to ensure reliable experimental support for biological sequence data. In others, such as medical records or judicial files, publication must not interfere with the right to privacy of the persons involved. There are also interventions in the published data with the aim of generating metadata that enable a better experience of querying and navigation. In all cases, the curation process constitutes a bottleneck that slows down general access to the data, so it is of great interest to have automatic or semi-automatic curation processes. In this paper, we present a solution aimed at the automatic curation of our National Jurisprudence Database, with special focus on the process of the anonymization of personal information. The anonymization process aims to hide the names of the participants involved in a lawsuit without losing the meaning of the narrative of facts. In order to achieve this goal, we need, not only to recognize person names but also resolve co-references in order to assign the same label to all mentions of the same person. Our corpus has significant differences in the spelling of person names, so it was clear from the beginning that pre-existing tools would not be able to reach a good performance. The challenge was to find a good way of injecting specialized knowledge about person names syntax while taking profit of previous capabilities of pre-trained tools. We fine-tuned an NER analyzer and we built a clusterization algorithm to solve co-references between named entities. We present our first results, which, for both tasks, are promising: We obtained a 90.21% of F1-micro in the NER task—from a 39.99% score before retraining the same analyzer in our corpus—and a 95.95% ARI score in clustering for co-reference resolution. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Clusterization; Co-reference resolution; De-identification; NER; Transfer learning","Data privacy; Natural language processing systems; Open Data; Anonymization; Clusterization; Coreference resolution; Curation; Data curation; De-identification; NER; OpenAccess; Public interest; Transfer learning; Query processing",Article,Scopus,2-s2.0-85122509539
"Lombardo G., Tomaiuolo M., Mordonini M., Codeluppi G., Poggi A.","57201158248;6507142395;6603120253;57212269027;7101947836;","Mobility in Unsupervised Word Embeddings for Knowledge Extraction—The Scholars’ Trajectories across Research Topics",2022,"Future Internet","14","1","25","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122509024&doi=10.3390%2ffi14010025&partnerID=40&md5=5dea335b1f358e57abf1576fafa38901","In the knowledge discovery field of the Big Data domain the analysis of geographic positioning and mobility information plays a key role. At the same time, in the Natural Language Processing (NLP) domain pre-trained models such as BERT and word embedding algorithms such as Word2Vec enabled a rich encoding of words that allows mapping textual data into points of an arbitrary multi-dimensional space, in which the notion of proximity reflects an association among terms or topics. The main contribution of this paper is to show how analytical tools, traditionally adopted to deal with geographic data to measure the mobility of an agent in a time interval, can also be effectively applied to extract knowledge in a semantic realm, such as a semantic space of words and topics, looking for latent trajectories that can benefit the properties of neural network latent representations. As a case study, the Scopus database was queried about works of highly cited researchers in recent years. On this basis, we performed a dynamic analysis, for measuring the Radius of Gyration as an index of the mobility of researchers across scientific topics. The semantic space is built from the automatic analysis of the paper abstracts of each author. In particular, we evaluated two different methodologies to build the semantic space and we found that Word2Vec embeddings perform better than the BERT ones for this task. Finally, The scholars’ trajectories show some latent properties of this model, which also represent new scientific contributions of this work. These properties include (i) the correlation between the scientific mobility and the achievement of scientific results, measured through the H-index; (ii) differences in the behavior of researchers working in different countries and subjects; and (iii) some interesting similarities between mobility patterns in this semantic realm and those typically observed in the case of human mobility. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Bert; Human mobility; Knowledge discovery; Radius of Gyration; Semantic space; Word embedding; Word2vec","Behavioral research; Data mining; Natural language processing systems; Semantics; Trajectories; Bert; Embeddings; Human mobility; Knowledge extraction; Property; Radius of gyration; Research topics; Semantic Space; Word embedding; Word2vec; Embeddings",Article,Scopus,2-s2.0-85122509024
"Liu Q., Guan W., Li S., Cheng F., Kawahara D., Kurohashi S.","57204393030;57216693875;53984734300;56900462600;6701580216;8935020500;","RODA: Reverse Operation Based Data Augmentation for Solving Math Word Problems",2022,"IEEE/ACM Transactions on Audio Speech and Language Processing","30",,,"1","11",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122410464&doi=10.1109%2fTASLP.2021.3126932&partnerID=40&md5=08f48672ff7121b53c42fdd491dba946","Automatically solving math word problems is a critical task in the field of natural language processing. Recent models have reached their performance bottleneck and require more high-quality data for training. We propose a novel data augmentation method that reverses the mathematical logic of math word problems to produce new high-quality math problems and introduce new knowledge points that can benefit learning the mathematical reasoning logic. We apply the augmented data on two SOTA math word problem solving models and compare our results with a strong data augmentation baseline. Experimental results show the effectiveness of our approach (we release our code and data at https://github.com/yiyunya/RODA). © 2014 IEEE.","data augmentation; Math word problems; question answering","Computer circuits; Problem solving; Augmentation methods; Critical tasks; Data augmentation; High quality data; Math word problem; New high; Performance bottlenecks; Question Answering; Reverse operations; Word problem; Natural language processing systems",Article,Scopus,2-s2.0-85122410464
"Vo T.","57204163635;","An integrated fuzzy neural network with topic-aware auto-encoding for sentiment analysis",2022,"Soft Computing","26","2",,"677","693",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122401242&doi=10.1007%2fs00500-021-06520-8&partnerID=40&md5=4080f1f75f18c1db9c533702ffa6669e","Recent advanced deep learning architectures, such as neural seq2seq and transformer, have demonstrated remarkable improvements in multi-typed sentiment classification tasks. Even though recent transformer-based and seq2seq-based models have successfully enabled to capture rich contextual information of texts, they still lacked attention on incorporating global semantic information which enables to sufficiently leverage the performance of downstream SA tasks. Moreover, emotional expressions of users are normally in the form of natural human-written textual data which contains a lot of noises and ambiguities that impose great challenges on the processes of textual representation learning as well as sentiment polarity prediction. To meet these challenges, we propose a novel integrated fuzzy neural architecture with a topic-driven textual representation learning approach for handling the SA task, called as: TopFuzz4SA. Specifically, in the proposed TopFuzz4SA model, we first apply a topic-driven neural encoder–decoder architecture with the incorporation of latent topic embedding and attention mechanism to sufficiently learn both rich contextual and global semantic information of the given textual data. Then, the achieved rich semantic representations of texts are fed into a fused deep fuzzy neural network to effectively reduce the feature ambiguity and noise, forming the final textual representations for sentiment classification task. Extensive experiments in benchmark datasets demonstrate the effectiveness of our proposed TopFuzz4SA model compared with contemporary state-of-the-art baselines. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Fuzzy neural network; Sentiment analysis; Seq2seq; Topic modelling","Deep learning; Fuzzy inference; Network architecture; Network coding; Semantics; Sentiment analysis; Classification tasks; Fuzzy-neural-networks; Learning architectures; Semantics Information; Sentiment analysis; Sentiment classification; Seq2seq; Textual data; Textual representation; Topic Modeling; Fuzzy neural networks",Article,Scopus,2-s2.0-85122401242
"Frisoni G., Moro G., Carlassare G., Carbonaro A.","57219265807;8702087400;57397768300;57211468869;","Unsupervised event graph representation and similarity learning on biomedical literature",2022,"Sensors","22","1","3","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122315152&doi=10.3390%2fs22010003&partnerID=40&md5=4340054322032374f176f7f6533af7aa","The automatic extraction of biomedical events from the scientific literature has drawn keen interest in the last several years, recognizing complex and semantically rich graphical interactions otherwise buried in texts. However, very few works revolve around learning embeddings or similarity metrics for event graphs. This gap leaves biological relations unlinked and prevents the application of machine learning techniques to promote discoveries. Taking advantage of recent deep graph kernel solutions and pre-trained language models, we propose Deep Divergence Event Graph Kernels (DDEGK), an unsupervised inductive method to map events into low-dimensional vectors, preserving their structural and semantic similarities. Unlike most other systems, DDEGK operates at a graph level and does not require task-specific labels, feature engineering, or known correspondences between nodes. To this end, our solution compares events against a small set of anchor ones, trains cross-graph attention networks for drawing pairwise alignments (bolstering interpretability), and employs transformer-based models to encode continuous attributes. Extensive experiments have been done on nine biomedical datasets. We show that our learned event representations can be effectively employed in tasks such as graph classification, clustering, and visualization, also facilitating downstream semantic textual similarity. Empirical results demonstrate that DDEGK significantly outperforms other state-of-the-art methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Biomedical text mining; Event embedding; Event extraction; Graph kernels; Graph neural networks; Graph representation learning; Graph similarity learning; Metric learning","Data mining; Extraction; Graph neural networks; Semantics; Biomedical text minings; Embeddings; Event embedding; Events extractions; Graph kernels; Graph neural networks; Graph representation; Graph representation learning; Graph similarity; Graph similarity learning; Metric learning; Similarity learning; Embeddings; cluster analysis; machine learning; publication; semantics; Cluster Analysis; Machine Learning; Publications; Semantics",Article,Scopus,2-s2.0-85122315152
"Jiang J.-Y., Zhou Y., Chen X., Jhou Y.-R., Zhao L., Liu S., Yang P.-C., Ahmar J., Wang W.","55536831700;57210635158;57203390885;57397013100;57397013200;57397584900;57398335500;57397954900;55157954300;","COVID-19 Surveiller: Toward a robust and effective pandemic surveillance system based on social media mining",2022,"Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences","380","2214","20210125","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122303149&doi=10.1098%2frsta.2021.0125&partnerID=40&md5=e6f77a46fbbd8212809a24b400f944b3","The outbreak of the novel coronavirus, COVID-19, has become one of the most severe pandemics in human history. In this paper, we propose to leverage social media users as social sensors to simultaneously predict the pandemic trends and suggest potential risk factors for public health experts to understand spread situations and recommend proper interventions. More precisely, we develop novel deep learning models to recognize important entities and their relations over time, thereby establishing dynamic heterogeneous graphs to describe the observations of social media users. A dynamic graph neural network model can then forecast the trends (e.g. newly diagnosed cases and death rates) and identify high-risk events from social media. Based on the proposed computational method, we also develop a web-based system for domain experts without any computer science background to easily interact with. We conduct extensive experiments on large-scale datasets of COVID-19 related tweets provided by Twitter, which show that our method can precisely predict the new cases and death rates. We also demonstrate the robustness of our web-based pandemic surveillance system and its ability to retrieve essential knowledge and derive accurate predictions across a variety of circumstances. Our system is also available at http://scaiweb.cs.ucla.edu/covidsurveiller/. This article is part of the theme issue 'Data science approachs to infectious disease surveillance'. © 2021 The Authors.","knowledge graph; natural language processing; pandemic surveillance; social media mining","Deep learning; Forecasting; Health risks; Large dataset; Monitoring; Natural language processing systems; Population statistics; Security systems; Coronaviruses; Death rates; Knowledge graphs; Pandemic surveillance; Potential risks; Risk factors; Social media; Social media minings; Social sensors; Surveillance systems; Social networking (online)",Article,Scopus,2-s2.0-85122303149
"Zhang Z., Zhang Y., Zhao H.","57203781650;57216359386;55715822700;","Syntax-Aware Multi-Spans Generation for Reading Comprehension",2022,"IEEE/ACM Transactions on Audio Speech and Language Processing","30",,,"260","268",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122299137&doi=10.1109%2fTASLP.2021.3138679&partnerID=40&md5=f86a68764373a2edabac8df3883d45c4","This paper presents a novel method to generate answers for non-extraction machine reading comprehension (MRC) tasks whose answers cannot be simply extracted as one span from the given passages. Using a pointer network-style extractive decoder for such type of MRC may result in unsatisfactory performance when the ground-truth answers are given by human annotators or highly re-paraphrased from parts of the passages. On the other hand, using a generative decoder cannot well guarantee the resulted answers with well-formed syntax and semantics when encountering long sentences. Therefore, to alleviate the obvious drawbacks of both sides, we propose an answer making-up method from extracted multi-spans that are learned by our model as highly confident $n$-gram candidates in the given passage. That is, the returned answers are composed of discontinuous multi-spans but not just one consecutive span in the given passages anymore. The proposed method is simple but effective: empirical experiments on MS MARCO show that the proposed method has a better performance on accurately generating long answers and substantially outperforms two typical competitive one-span and Seq2Seq baseline decoders. © 2014 IEEE.","Answer generation; encoder-decoder mechanism; machine reading comprehension; syntactic parsing","Decoding; Job analysis; Speech processing; Syntactics; Comprehension tasks; Computational modelling; Ground truth; Multi-spans; N-grams; Novel methods; Performance; Predictive models; Reading comprehension; Task analysis; Semantics",Article,Scopus,2-s2.0-85122299137
"Jiang H., Song L., Ge Y., Meng F., Yao J., Su J.","57221281450;57198355269;57211756281;55847567500;57313513000;55157800300;","An AST Structure Enhanced Decoder for Code Generation",2022,"IEEE/ACM Transactions on Audio Speech and Language Processing","30",,,"468","476",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122298746&doi=10.1109%2fTASLP.2021.3138717&partnerID=40&md5=d18a05b4a9fa82f3fd358ac2678a9d7e","Currently, the most dominant neural code generation modelsare often equipped with a tree-structured LSTM decoder, which outputs a sequence of actions to construct an Abstract Syntax Tree (AST) via pre-order traversal. However, such a decoder has two obvious drawbacks. First, except for the parent action, other faraway and important history actions rarely contribute to the current decision. Second, it also neglects future actions, which may be crucial for the prediction of the current action. To deal with these issues, in this paper, we propose a novel AST structure enhanced decoder for code generation, which significantly extends the decoder with respect to the above two aspects. First, we introduce an AST information enhanced attention mechanism to fully exploit history actions, of which impacts are further distinguished according to their syntactic distances, action types and relative positions; Second, we jointly model the predictions of current action and its important future action via multi-task learning, where the learned hidden state of the latter can be further leveraged to improve the former. Experimental results on commonly-used datasets demonstrate the effectiveness of our proposed decoder.1 © 2014 IEEE.","abstract syntax tree; attention mechanism; Code generation; future action prediction","Decoding; Forecasting; Long short-term memory; Semantics; Speech processing; Trees (mathematics); 'current; Abstract Syntax Trees; Action prediction; Attention mechanisms; Code; Codegeneration; Future action prediction; Grammar; Predictive models; Tree structures; Syntactics",Article,Scopus,2-s2.0-85122298746
"Zhang S., Li B., Yin C.","57189492379;57211429853;56217759500;","Cross-modal sentiment sensing with visual-augmented representation and diverse decision fusion",2022,"Sensors","22","1","74","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122298151&doi=10.3390%2fs22010074&partnerID=40&md5=889c4a382440971252689bbfb28f3364","The rising use of online media has changed the social customs of the public. Users have become accustomed to sharing daily experiences and publishing personal opinions on social networks. Social data carrying emotion and attitude has provided significant decision support for numerous tasks in sentiment analysis. Conventional methods for sentiment classification only concern textual modality and are vulnerable to the multimodal scenario, while common multimodal approaches only focus on the interactive relationship among modalities without considering unique intra-modal information. A hybrid fusion network is proposed in this paper to capture both inter-modal and intra-modal features. Firstly, in the stage of representation fusion, a multi-head visual attention is proposed to extract accurate semantic and sentimental information from textual contents, with the guidance of visual features. Then, multiple base classifiers are trained to learn independent and diverse discriminative information from different modal representations in the stage of decision fusion. The final decision is determined based on fusing the decision supports from base classifiers via a decision fusion method. To improve the generalization of our hybrid fusion network, a similarity loss is employed to inject decision diversity into the whole model. Empiric results on five multimodal datasets have demonstrated that the proposed model achieves higher accuracy and better generalization capacity for multimodal sentiment analysis. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Decision fusion; Multimodal learning; Representation fusion; Social network","Behavioral research; Classification (of information); Decision support systems; Semantics; Cross-modal; Decision supports; Decisions fusion; Hybrid fusions; Multi-modal; Multi-modal learning; Online medium; Representation fusion; Sentiment analysis; Social network; Sentiment analysis; attitude; machine learning; semantics; social network; Attitude; Machine Learning; Semantics; Sentiment Analysis; Social Networking",Article,Scopus,2-s2.0-85122298151
"Kumar A.S., Nair J.J.","57201946779;37049291400;","Scene Graph Generation Using Depth, Spatial, and Visual Cues in 2D Images",2022,"IEEE Access","10",,,"1968","1978",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122297073&doi=10.1109%2fACCESS.2021.3139000&partnerID=40&md5=372b6201091e78b877709ec90ae4802c","To understand an image or a scene properly, it is necessary to identify objects participating in the scene, their relationships, and various attributes that describe their properties. A scene graph is a high-level representation that confines all these features in a structured manner. Scene graph generation includes multiple challenges like the semantics of relationships considered and the availability of a well-balanced dataset with sufficient training examples. We tried to mitigate these problems by extracting two subsets, VG-R10 and VG-A16, from the popular Visual Genome dataset. Also, a framework (S2G) is proposed for generating scene graphs directly from images using depth and spatial information of object pairs. Evaluations on the scene graph generation model reveal that the proposed framework achieves better results on our data than the state-of-the-art. © 2013 IEEE.","Attribute prediction; relation prediction; scene graph generation; Visual Genome","Bioinformatics; Feature extraction; Object recognition; Semantics; Attribute prediction; Depth cue; Features extraction; Graph generation; Objects recognition; Relation prediction; Scene graph generation; Scene-graphs; Spatial cues; Visual genome; Genes",Article,Scopus,2-s2.0-85122297073
"Xie J., Peng N., Cai Y., Wang T., Huang Q.","57211275233;57224477810;57209921113;57398191100;8307744400;","Diverse Distractor Generation for Constructing High-Quality Multiple Choice Questions",2022,"IEEE/ACM Transactions on Audio Speech and Language Processing","30",,,"280","291",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122294067&doi=10.1109%2fTASLP.2021.3138706&partnerID=40&md5=0bfa4ea6010df5aa729b0e4bc7c240d0","Distractor generation task aims to generate incorrect options (i.e., distractors) for multiple choice questions from an article.Existing methods for this task often utilize a standard encoder-decoder framework. However, these methods often tend to generate semantically similar distractors, since the same article representations are used to generate different distractors. Multiple generated distractors with similar semantics are considered equivalent. Because the correct answer is unique, students can eliminate these distractors even without reading the article. In this paper, we propose a multi-selector generation network (MSG-Net) that generates distractors with rich semantics based on different sentences in an article. MSG-Net adopts a multi-selector mechanism to select multiple different sentences in an article that are useful to generate diverse distractors. Specifically, a question-aware and answer-aware mechanism are introduced to assist in selecting useful key sentences, where each key sentence is coherent with the question and not equivalent to the answer. MSG-Net can generate diverse distractors based on each selected key sentence with different semantics. Extensive experiments on the RACE dataset and Cosmos QA dataset show that the proposed model outperforms the state-of-the-art models in generating diverse distractors. © 2014 IEEE.","Distractor generation; diversity generation task; multiple question generation","Decoding; Job analysis; Speech processing; Distractor generation; Diversity generation task; Diversity generations; Encoder-decoder; High quality; Multiple question generation; Multiple-choice questions; Ontology's; Search method; Task analysis; Semantics",Article,Scopus,2-s2.0-85122294067
"Li Q., Peng H., Li J., Wu J., Ning Y., Wang L., Yu P.S., Wang Z.","57221243818;57190014707;55720560100;23971568900;57211751494;57007714500;7402366049;35111811300;","Reinforcement Learning-Based Dialogue Guided Event Extraction to Exploit Argument Relations",2022,"IEEE/ACM Transactions on Audio Speech and Language Processing","30",,,"520","533",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122289846&doi=10.1109%2fTASLP.2021.3138670&partnerID=40&md5=bcd44c1fa1608547409ac121e0420d64","Event extraction is a ftask for natural language processing. Finding the roles of event arguments like event participants is essential for event extraction. However, doing so for real-life event descriptions is challenging because an argument’s role often varies in different contexts. While the relationship and interactions between multiple arguments are useful for settling the argument roles, such information is largely ignored by existing approaches. This paper presents a better approach for event extraction by explicitly utilizing the relationships of event arguments. We achieve this through a carefully designed task-oriented dialogue system. To model the argument relation, we employ reinforcement learning and incremental learning to extract multiple arguments via a multi-turned, iterative process. Our approach leverages knowledge of the already extracted arguments of the same sentence to determine the role of arguments that would be difficult to decide individually. It then uses the newly obtained information to improve the decisions of previously extracted arguments. This two-way feedback process allows us to exploit the argument relations to effectively settle argument roles, leading to better sentence understanding and event extraction. Experimental results show that our approach consistently outperforms seven state-of-the-art event extraction methods for the classification of events and argument role and argument identification. 2329-9290 © 2021 IEEE.","Data mining; Explosives; Generators; Instruments; Speech processing; Task analysis; Weapons","Extraction; Iterative methods; Job analysis; Natural language processing systems; Reinforcement learning; Speech processing; Event description; Events extractions; Generator; Incremental learning; Life events; Multi-turned; Reinforcement learnings; Task analysis; Task-oriented; Weapon; Data mining",Article,Scopus,2-s2.0-85122289846
"Zhao Y., Komachi M., Kajiwara T., Chu C.","57216846748;24168747300;56903744700;55916219700;","Word-Region Alignment-Guided Multimodal Neural Machine Translation",2022,"IEEE/ACM Transactions on Audio Speech and Language Processing","30",,,"244","259",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122285959&doi=10.1109%2fTASLP.2021.3138719&partnerID=40&md5=f77eebdd186bbabc951075e6b6936623","We propose word-region alignment-guided multimodal neural machine translation (MNMT), a novel model for MNMT that links the semantic correlation between textual and visual modalities using word-region alignment (WRA). Existing studies on MNMT have mainly focused on the effect of integrating visual and textual modalities. However, they do not leverage the semantic relevance between the two modalities. We advance the semantic correlation between textual and visual modalities in MNMT by incorporating WRA as a bridge. This proposal has been implemented on two mainstream architectures of neural machine translation (NMT): the recurrent neural network (RNN) and the transformer. Experiments on two public benchmarks, English-German and English-French translation tasks using the Multi30k dataset and English-Japanese translation tasks using the Flickr30kEnt-JP dataset prove that our model has a significant improvement with respect to the competitive baselines across different evaluation metrics and outperforms most of the existing MNMT models. For example, 1.0 BLEU scores are improved for the English-German task and 1.1 BLEU scores are improved for the English-French task on the Multi30k test2016 set; and 0.7 BLEU scores are improved for the English-Japanese task on the Flickr30kEnt-JP test set. Further analysis demonstrates that our model can achieve better translation performance by integrating WRA, leading to better visual information use. © 2014 IEEE.","Multi30k; multimodal machine translation; semantic correlation; vision and language; word-region alignment","Alignment; Computational linguistics; Computer aided language translation; Information use; Magnetostatics; Neural machine translation; Recurrent neural networks; Semantics; Enter key word or phrase in alphabetical order; Graphic; Guideline; Image color analysis; Key words or phrases in alphabetical order; Multi-modal; Permeability; Region alignments; Separated by commas; Visual modalities; Speech processing",Article,Scopus,2-s2.0-85122285959
"Yang K., Yu H., Fan G., Yang X., Huang Z.","57208582284;55494828500;24829046300;57202391638;57210743971;","A graph sequence neural architecture for code completion with semantic structure features",2022,"Journal of Software: Evolution and Process","34","1","e2414","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122136280&doi=10.1002%2fsmr.2414&partnerID=40&md5=970b6c7fa148f561f4d2147c0e4ad322","Code completion plays an important role in intelligent software development for accelerating coding efficiency. Recently, the prediction models based on deep learning have achieved good performance in code completion task. However, the existing models cannot avoid three drawbacks: (i) In the existing models, the code representation loses the information (parent–child information between nodes) and lacks many effective features (orientation between nodes). (ii) The known code structure information is not fully utilized, which will cause the model to generate completely irrelevant results. (iii) Simple sequence modeling ignores repeated patterns and structural information. Besides, previous works cannot capture the characteristics of correlation and directionality between nodes. In this paper, we propose a Code Completion approach named CC-GGNN, which is graph model based on Gated Graph Neural Networks (GGNNs) to address the problems. We introduce a new architecture to obtain the effective code features from code representation. In order to utilize the known information, we propose Classification Mechanism, which classifies the representation of the node using the known parent node and constructs training graph in the model. The experimental results show that our model outperforms the state-of-the-art methods MRR@5 at most 9.2% and ACC at most 11.4% in datasets. © 2022 John Wiley & Sons, Ltd.",,"Classification (of information); Deep learning; Graph neural networks; Network architecture; Semantics; Software design; Code completions; Code representation; Coding efficiency; Graph neural networks; Graph sequences; Model-based OPC; Neural architectures; Prediction modelling; Semantic structures; Structure features; Graph theory",Article,Scopus,2-s2.0-85122136280
"Luo Y., Tang N., Li G., Tang J., Chai C., Qin X.","57202576432;55570310100;55800543300;57394013100;57190390247;57202577375;","Natural Language to Visualization by Neural Machine Translation",2022,"IEEE Transactions on Visualization and Computer Graphics","28","1",,"217","226",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122122541&doi=10.1109%2fTVCG.2021.3114848&partnerID=40&md5=afcb2721cb494d3f69726d6802c127c7","Supporting the translation from natural language (NL) query to visualization (NL2VIS) can simplify the creation of data visualizations because if successful, anyone can generate visualizations by their natural language from the tabular data. The state-of-the-art NL2VIS approaches (e.g., NL4DV and FlowSense) are based on semantic parsers and heuristic algorithms, which are not end-to-end and are not designed for supporting (possibly) complex data transformations. Deep neural network powered neural machine translation models have made great strides in many machine translation tasks, which suggests that they might be viable for NL2VIS as well. In this paper, we present ncNet, a Transformer-based sequence-to-sequence model for supporting NL2VIS, with several novel visualization-aware optimizations, including using attention-forcing to optimize the learning process, and visualization-aware rendering to produce better visualization results. To enhance the capability of machine to comprehend natural language queries, ncNet is also designed to take an optional chart template (e.g., a pie chart or a scatter plot) as an additional input, where the chart template will be served as a constraint to limit what could be visualized. We conducted both quantitative evaluation and user study, showing that ncNet achieves good accuracy in the nvBench benchmark and is easy-to-use. © 1995-2012 IEEE.","chart template; data visualization; Natural language interface; neural machine translation","Computational linguistics; Computer aided language translation; Deep neural networks; Heuristic algorithms; Natural language processing systems; Neural machine translation; Optimization; Semantics; Visualization; Chart template; Complex data; Datum transformation; End to end; Heuristics algorithm; Natural language interfaces; Natural language queries; Natural languages; State of the art; Tabular data; Data visualization; article; attention; human; human experiment; language; learning; quantitative analysis",Article,Scopus,2-s2.0-85122122541
"Deng Y., Xie Y., Li Y., Yang M., Lam W., Shen Y.","57200607347;57208258053;56273199400;56349712700;57203073460;56763084800;","Contextualized Knowledge-Aware Attentive Neural Network: Enhancing Answer Selection with Knowledge",2022,"ACM Transactions on Information Systems","40","1","2","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122121615&doi=10.1145%2f3457533&partnerID=40&md5=bdc946f899ff3ee822d9fb5c3dc42a65","Answer selection, which is involved in many natural language processing applications, such as dialog systems and question answering (QA), is an important yet challenging task in practice, since conventional methods typically suffer from the issues of ignoring diverse real-world background knowledge. In this article, we extensively investigate approaches to enhancing the answer selection model with external knowledge from knowledge graph (KG). First, we present a context-knowledge interaction learning framework, Knowledge-Aware Neural Network, which learns the QA sentence representations by considering a tight interaction with the external knowledge from KG and the textual information. Then, we develop two kinds of knowledge-Aware attention mechanism to summarize both the context-based and knowledge-based interactions between questions and answers. To handle the diversity and complexity of KG information, we further propose a Contextualized Knowledge-Aware Attentive Neural Network, which improves the knowledge representation learning with structure information via a customized Graph Convolutional Network and comprehensively learns context-based and knowledge-based sentence representation via the multi-view knowledge-Aware attention mechanism. We evaluate our method on four widely used benchmark QA datasets, including WikiQA, TREC QA, InsuranceQA, and Yahoo QA. Results verify the benefits of incorporating external knowledge from KG and show the robust superiority and extensive applicability of our method. © 2021 Association for Computing Machinery.","Answer selection; attention mechanism; graph convolutional network; knowledge graph","Convolution; Convolutional neural networks; Natural language processing systems; Answer selection; Attention mechanisms; Contextualized knowledge; Convolutional networks; External knowledge; Graph convolutional network; Knowledge graphs; Learn+; Neural-networks; Question Answering; Knowledge graph",Article,Scopus,2-s2.0-85122121615
"Althobaiti M.J.","57198790950;","A Simple Yet Robust Algorithm for Automatic Extraction of Parallel Sentences: A Case Study on Arabic-English Wikipedia Articles",2022,"IEEE Access","10",,,"401","420",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122091517&doi=10.1109%2fACCESS.2021.3137830&partnerID=40&md5=30fd3f0a24287aca38f050b2459d8a7b","Parallel corpora are vital components in several applications of Natural Language Processing (NLP), particularly in machine translation. In this paper, we present a novel method for automatically creating parallel sentences from comparable corpora. The method requires a bilingual dictionary as well as an adequate word-vectorisation method. We use Arabic and English Wikipedia as a comparable corpus to apply our proposed method and construct a parallel corpus between Arabic and English. The created Arabic-English corpus consists of 105,010 parallel sentences with a total number of 4.6M words. During our study, we compared two methods of word vectorisation, word embedding and term frequency-inverse document frequency, in terms of their usefulness in computing similarities between well-formed and syntactically ill-formed sentences. We also quantitatively and qualitatively examined the parallel corpus produced by our proposed method and compared it with other available Arabic-English parallel corpora counterparts: GlobalVoices, TED, and Wiki-OPUS. We explored the main advantages and shortcomings of these corpora when used for NLP applications, such as word semantic similarity identification and Neural Machine Translation (NMT). The word semantic similarity models trained on our parallel corpus outperformed models trained on other corpora in the task of English non-similar word identification. Our parallel corpus also proved competitive when building Arabic-English NMT systems, yielding results comparable to those of the automatically created Wiki-OPUS corpus and of the manually created TED corpus, while achieving results superior to the smaller GlobalVoices corpus. © 2013 IEEE.","Automatic creation of parallel corpus; automatic sentence alignment; deep learning; neural machine translation; transformer model; word embedding","Computational linguistics; Computer aided language translation; Deep learning; Inverse problems; Machine components; Natural language processing systems; Neural machine translation; Text processing; Automatic creation of parallel corpus; Automatic creations; Automatic sentence alignment; Computational modelling; Deep learning; Embeddings; Encyclopedia; On-line service; Parallel corpora; Sentence alignment; Transformer modeling; Word embedding; Semantics",Article,Scopus,2-s2.0-85122091517
"Ahmed T., Suffian M., Khan M.Y., Bogliolo A.","36162927700;55014130300;57203095674;7006342709;","Discovering Lexical Similarity Using Articulatory Feature-Based Phonetic Edit Distance",2022,"IEEE Access","10",,,"1533","1544",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122088561&doi=10.1109%2fACCESS.2021.3137905&partnerID=40&md5=09039c4abadfbc0150365314463420fd","Lexical Similarity (LS) between two languages uncovers many interesting linguistic insights such as phylogenetic relationship, mutual intelligibility, common etymology, and loan words. There are various methods through which LS is evaluated. This paper presents a method of Phonetic Edit Distance (PED) that uses a soft comparison of letters using the articulatory features associated with their International Phonetic Alphabet (IPA) transcription. In particular, the comparison between the articulatory features of two letters taken from words belonging to different languages is used to compute the cost of replacement in the inner loop of edit distance computation. As an example, PED gives edit distance of 0.82 between German word 'vater' ([fa:tr]) and Persian word ' ' ([pedær]), meaning 'father,' and, similarly, PED of 0.93 between Hebrew word ' ' ([am]) and Arabic word ' ' ([sa:m], meaning 'peace,' whereas classical edit distances would be 4 and 2, respectively. We report the results of systematic experiments conducted on six languages: Arabic, Hindi, Marathi, Persian, Sanskrit, and Urdu. Universal Dependencies (UD) corpora were used to restrict the comparison to lists of words belonging to the same part of speech. The LS based on the average PED between pair of words was then computed for each pair of languages, unveiling similarities otherwise masked by the adoption of different alphabets, grammars, and pronunciations rules. © 2013 IEEE.","Articulatory features; cognates; computational linguistics; edit distance; lexical similarity; natural language processing; phonetic matching","Computational linguistics; Job analysis; Articulatory features; Cognate; Edit distance; Lexical similarity; License; Matchings; Persians; Phonetic matching; Task analysis; Vocabulary; Natural language processing systems",Article,Scopus,2-s2.0-85122088561
"Dai P., Chen P., Wu Q., Hong X., Ye Q., Tian Q., Lin C.-W., Ji R.","35321895800;57209823631;57204471674;35179692900;8435765900;7102891959;57393647600;23134935200;","Disentangling Task-Oriented Representations for Unsupervised Domain Adaptation",2022,"IEEE Transactions on Image Processing","31",,,"1012","1026",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122058201&doi=10.1109%2fTIP.2021.3136615&partnerID=40&md5=6447099233c5f63cfdd29ddb57c7f1d7","Unsupervised domain adaptation (UDA) aims to address the domain-shift problem between a labeled source domain and an unlabeled target domain. Many efforts have been made to eliminate the mismatch between the distributions of training and testing data by learning domain-invariant representations. However, the learned representations are usually not task-oriented, i.e., being class-discriminative and domain-transferable simultaneously. This drawback limits the flexibility of UDA in complicated open-set tasks where no labels are shared between domains. In this paper, we break the concept of task-orientation into task-relevance and task-irrelevance, and propose a dynamic task-oriented disentangling network (DTDN) to learn disentangled representations in an end-to-end fashion for UDA. The dynamic disentangling network effectively disentangles data representations into two components: the task-relevant ones embedding critical information associated with the task across domains, and the task-irrelevant ones with the remaining non-transferable or disturbing information. These two components are regularized by a group of task-specific objective functions across domains. Such regularization explicitly encourages disentangling and avoids the use of generative models or decoders. Experiments in complicated, open-set scenarios (retrieval tasks) and empirical benchmarks (classification tasks) demonstrate that the proposed method captures rich disentangled information and achieves superior performance. © 1992-2012 IEEE.","Deep learning; Image retrieval; Person re-identification; Unsupervised domain adaptation","Benchmarking; Classification (of information); Computer vision; Deep learning; Image analysis; Image retrieval; Job analysis; Linear programming; Adaptation models; Deep learning; Domain adaptation; Image color analysis; Linear-programming; Person re identifications; Task analysis; Task-oriented; Unsupervised domain adaptation; Vehicle's dynamics; Semantics; article; embedding; information retrieval",Article,Scopus,2-s2.0-85122058201
"Xie F.","57274830500;","rethnicity: An R package for predicting ethnicity from names",2022,"SoftwareX","17",,"100965","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122013583&doi=10.1016%2fj.softx.2021.100965&partnerID=40&md5=c79546bc9e434eb4e26e95ae9b62acb0","In this study, a new R package, rethnicity 1 is provided for predicting ethnicity based on names. The Bidirectional Long Short-Term Memory (Bi-LSTM), a recurrent neural network architecture commonly used for natural language processing, was chosen as the model for our study. The Florida Voter Registration was used as the training and testing data. Special care was given for the accuracy of minority groups by adjusting the imbalance in the dataset. The models were trained and exported to C++ and then integrated with R using Rcpp. Additionally, the availability, accuracy, and performance of the package were compared with other solutions. © 2021 The Author(s)","Ethnicity prediction; LSTM; R","Forecasting; Natural language processing systems; Network architecture; Ethnicity predictions; Florida; LSTM; Minority groups; Performance; R; Recurrent neural network architectures; Testing data; Training and testing; Training data; Long short-term memory",Article,Scopus,2-s2.0-85122013583
"Li P., Yu Z., Zhan Y.","55643279800;55730410900;56100491900;","Deep relational self-Attention networks for scene graph generation",2022,"Pattern Recognition Letters","153",,,"200","206",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121965427&doi=10.1016%2fj.patrec.2021.12.013&partnerID=40&md5=1c482c3df20333cc997780a15e8f665c","Scene graph generation (SGG) aims to simultaneously detect objects in an image and predict relations for these detected objects. SGG is challenging that requires modeling the contextualized relationships among objects rather than only considering relationships between paired objects. Most existing approaches address this problem by using a CNN or RNN framework, which can not explicitly and effectively models the dense interactions among objects. In this paper, we exploit the attention mechanism and introduce a relational self-attention (RSA) module to simultaneously model the object and relation contexts. By stacking such RSA modules in depth, we obtain a deep relational self-attention network (RSAN), which is able to characterize complex interactions thus facilitating the understanding of object and relation semantics. Extensive experiments on the benchmark Visual Genome dataset demonstrate the effectiveness of RSAN. © 2021 Elsevier B.V.","Deep neural networks; Image understanding; Scene graph generation","Object detection; Semantics; Attention mechanisms; Graph generation; Object semantic; Relation semantics; Scene graph generation; Scene-graphs; Stackings; Deep neural networks",Article,Scopus,2-s2.0-85121965427
"Li J., Wang D., Liu X., Shi Z., Wang M.","57214207642;57387935400;36100195100;57388157700;55533321600;","Two-Branch Attention Network via Efficient Semantic Coupling for One-Shot Learning",2022,"IEEE Transactions on Image Processing","31",,,"341","351",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121878999&doi=10.1109%2fTIP.2021.3124668&partnerID=40&md5=dd4163483de21fb4d5734f6237864457","Over the past few years, Convolutional Neural Networks (CNNs) have achieved remarkable advancement for the tasks of one-shot image classification. However, the lack of effective attention modeling has limited its performance. In this paper, we propose a Two-branch (Content-aware and Position-aware) Attention (CPA) Network via an Efficient Semantic Coupling module for attention modeling. Specifically, we harness content-aware attention to model the characteristic features (e.g., color, shape, texture) as well as position-aware attention to model the spatial position weights. In addition, we exploit support images to improve the learning of attention for the query images. Similarly, we also use query images to enhance the attention model of the support set. Furthermore, we design a local-global optimizing framework that further improves the recognition accuracy. The extensive experiments on four common datasets (miniImageNet, tieredImageNet, CUB-200-2011, CIFAR-FS) with three popular networks (DPGN, RelationNet and IFSL) demonstrate that our devised CPA module equipped with local-global Two-stream framework (CPAT) can achieve state-of-the-art performance, with a significant improvement in accuracy of 3.16% on CUB-200-2011 in particular. © 1992-2012 IEEE.","Content-aware attention; Efficient semantic coupling; Local-global optimizing framework; One-shot learning; Position-aware attention; Two-branch attention network","Image enhancement; Neural networks; Semantics; Attention model; Content-aware; Content-aware attention; Efficient semantic coupling; Local-global optimizing framework; One-shot learning; Position-aware attention; Query images; Semantic couplings; Two-branch attention network; Global optimization; article; attention network; learning",Article,Scopus,2-s2.0-85121878999
"Panboonyuen T., Thongbai S., Wongweeranimit W., Santitamnont P., Suphan K., Charoenphon C.","57194783124;57387830500;57387980400;19934411000;57388130400;57210447541;","Object detection of road assets using transformer-based yolox with feature pyramid decoder on thai highway panorama",2022,"Information (Switzerland)","13","1","5","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121864198&doi=10.3390%2finfo13010005&partnerID=40&md5=75587e6b6807c6aa5ab5d7e8f1a6359b","Due to the various sizes of each object, such as kilometer stones, detection is still a challenge, and it directly impacts the accuracy of these object counts. Transformers have demonstrated impressive results in various natural language processing (NLP) and image processing tasks due to long-range modeling dependencies. This paper aims to propose an exceeding you only look once (YOLO) series with two contributions: (i) We propose to employ a pre-training objective to gain the original visual tokens based on the image patches on road asset images. By utilizing pre-training Vision Transformer (ViT) as a backbone, we immediately fine-tune the model weights on downstream tasks by joining task layers upon the pre-trained encoder. (ii) We apply Feature Pyramid Network (FPN) decoder designs to our deep learning network to learn the importance of different input features instead of simply summing up or concatenating, which may cause feature mismatch and performance degradation. Conclusively, our proposed method (Transformer-Based YOLOX with FPN) learns very general representations of objects. It significantly outperforms other state-of-the-art (SOTA) detectors, including YOLOv5S, YOLOv5M, and YOLOv5L. We boosted it to 61.5% AP on the Thailand highway corpus, surpassing the current best practice (YOLOv5L) by 2.56% AP for the test-dev data set. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Highway scenes; Vision Transformer; YOLO; YOLOX","Decoding; Deep learning; Modeling languages; Natural language processing systems; Roads and streets; Statistical tests; Deep learning; Feature pyramid; Highway scene; Learn+; Pre-training; Pyramid network; Stone detection; Vision transformer; YOLOX; You only look once; Object detection",Article,Scopus,2-s2.0-85121864198
"Wu Z., Zhang H., Liu W., Li Z., Zheng W.","57207937374;57386888400;56106147600;57216860950;57216869389;","Machine identification of potential manufacturing process failure modes based on process constituent elements",2022,"Advanced Engineering Informatics","51",,"101491","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121799871&doi=10.1016%2fj.aei.2021.101491&partnerID=40&md5=309e46a744d212021ff7084cfce8f4cc","It is urgent to solve the problems of low efficiency, high cost and lack of system integrity when PFMEA identifies potential process failure modes in the process of multi-variety and small batch production. Current study innovatively proposes a machine identification method of potential process failure modes based on the combination of process constituent elements (PCE) model and feature case-based reasoning represented by extension. Firstly, the specific description of the process content is developed with the PCE and the way of expression is standardized through the method of data mining. Then, the PCE of the normative knowledge representation of extension feature cases were constructed, and natural language processing (NLP) technology is employed to solve the feature similarity between the same PCE after the specification of the sentence and chunk features respectively, and case-based reasoning and extension operator are applied to carry out analogical reasoning and calculation for the feature cases with high similarity degree of feature attributes, so as to realize the machine identification of the potential process failure modes in the manufacturing process. Finally, the proposed method is specifically applied to the three parts assembly process of an aircraft assembly to verify the effectiveness and applicability of the proposed method. © 2021 Elsevier Ltd","Extension operator; PFMEA; Process constituent elements; Process failure modes; Similarity","Case based reasoning; Data mining; Knowledge representation; Natural language processing systems; Constituent elements; Extension operators; Machine identification; Manufacturing process; Mode-based; PFMEA; Process constituent element; Process failure; Process failure mode; Similarity; Failure modes",Article,Scopus,2-s2.0-85121799871
"Alimova I., Tutubalina E., Nikolenko S.I.","57034290000;56441621500;13608710100;","Cross-Domain Limitations of Neural Models on Biomedical Relation Classification",2022,"IEEE Access","10",,,"1432","1439",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121795127&doi=10.1109%2fACCESS.2021.3135381&partnerID=40&md5=0ad8a6a13c88b80d18b5a4dd8ca0bf1a","Relation extraction (RE) aims to extract relational facts from plain text, which is essential to the biomedical research field with the rapid growth of biomedical literature and generally large volumes of biomedicine-related text coming from various sources. Numerous annotated corpora and state-of-the-art models have been introduced in the past five years. However, there are no general guidelines about evaluating models on these corpora in single- and cross-domain settings with diverse entities and relation types. We aim to fill this gap for the task of detecting whether a relation holds between two biomedical entities given a text span. In this work, we present a fine-grained evaluation intended to perform a comparative evaluation of four biomedical benchmarks and understand the efficiency of state-of-the-art neural architectures based on Long Short-Term Memory (LSTM) with cross-attention and Bidirectional Encoder Representations from Transformers (BERT) for relation extraction across two main domains, namely scientific abstracts and electronic health records. We present a comparative evaluation of biomedical RE datasets, including the PHAEDRA, i2b2/VA, BC5CDR, and MADE corpora. Our evaluation of BioBERT and LSTM for binary classification shows significant divergence in in-domain and out-of-domain performance, finding an average drop in F1-measure of 34.2% for BioBERT. The cross-attention LSTM model developed in this work exhibits better cross-domain performance, with a drop of only 27.6% in F-measure. © 2013 IEEE.","bioinformatics; natural language processing; Relation extraction","Bioinformatics; Biological systems; Computational linguistics; Diagnosis; Drops; Extraction; Modeling languages; Natural language processing systems; User interfaces; Biological system modeling; Comparative evaluations; Cross-domain; Drug; Medical diagnostic imaging; Neural modelling; Performance; Relation extraction; State of the art; Task analysis; Long short-term memory",Article,Scopus,2-s2.0-85121795127
"Pastoriza-Domínguez P., Torre I.G., Diéguez-Vide F., Gómez-Ruiz I., Geladó S., Bello-López J., Ávila-Rivera A., Matías-Guiu J.A., Pytel V., Hernández-Fernández A.","57222128827;57201809098;7801413126;40461197500;57222121640;57222120060;56048992000;35584677700;57190977858;52364053900;","Speech pause distribution as an early marker for Alzheimer's disease",2022,"Speech Communication","136",,,"107","117",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121785532&doi=10.1016%2fj.specom.2021.11.009&partnerID=40&md5=edd742b8dd1ee298a1485cafd0e08ff7","Background: Pause duration analysis is a common feature in the study of discourse in Alzheimer's disease (AD) since this patient group has shown a consistent trend for longer pauses in comparison to healthy controls. This speech feature may also be helpful for early detection; however, studies involving patients at the pre-clinical, high-risk phase of amnestic mild cognitive impairment (aMCI) have yielded varying results. Objective: To characterize the probability density distribution of speech pause durations in 26 patients with AD, 57 amnestic multi-domain amnestic MCI patients (29 with memory encoding deficits, a-mdMCI-E, and 28 with retrieval impairment only, a-mdMCI-R) and 29 healthy controls (HC) in order assess whether there are significant differences between them. To explore the potential differences in pause production between patients with a-mdMCI-E and a-mdMCI-R, as the former are considered to be at higher risk of progressing to dementia. Methods: The 112 picture-based oral narratives obtained were manually transcribed and annotated for the automatic extraction and analysis of pause durations. Different probability distributions were tested for the fitting of pause durations while truncating shorter ranges. Recent findings in the field of Statistics were considered in order to avoid the inherent methodological uncertainty that this type of analysis entails by addressing the question of temporal thresholding and its potential repercussions on inter-annotator reliability in manual transcriptions. Results: A lognormal distribution (LND) explained the distribution of pause duration for all groups. Its fitted parameters (μ, σ) followed a gradation from the group with shorter durations and a higher tendency to produce short pauses (HC) to the group with longer pause durations and a considerably higher tendency to produce long pauses with greater variance (AD). Importantly, a-mdMCI-E produced significantly longer pauses and with greater variability than their a-mdMCI-R counterparts (α=0.05). Conclusion: We report significant differences at the group level in pause distribution across all groups of study that could be used in future diagnostic tools and discuss the clinical implications of these findings, particularly regarding the characterization of a-mdMCI. © 2021","Alzheimer's disease; Fluency; Mild Cognitive Impairment; Natural language processing; Speech; Statistical distributions","Diagnosis; Natural language processing systems; Probability distributions; Reliability analysis; Speech; Uncertainty analysis; Alzheimers disease; Cognitive impairment; Common features; Fluency; Healthy controls; Mild cognitive impairment; Pre-clinical; Probability density distribution; Speech features; Statistical distribution; Neurodegenerative diseases",Article,Scopus,2-s2.0-85121785532
"Shi J., Gao X., Kinsman W.C., Ha C., Gao G.G., Chen Y.","57195933507;57219796666;57203285172;56414149400;57226161352;55115475000;","DI++: A deep learning system for patient condition identification in clinical notes",2022,"Artificial Intelligence in Medicine","123",,"102224","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121759610&doi=10.1016%2fj.artmed.2021.102224&partnerID=40&md5=5d9401fe8a5a12df499b7393181f919e","Accurately recording a patient's medical conditions in an EHR system is the basis of effectively documenting patient health status, coding for billing, and supporting data-driven clinical decision making. However, patient conditions are often not fully captured in structured EHR systems, but may be documented in unstructured clinical notes. The challenge is that not all disease mentions in clinical notes actually refer to a patient's conditions. We developed a two-step workflow for identifying patient's conditions from clinical notes: disease mention extraction and disease mention classification. We implemented this workflow in a prototype system, DI++, for Disease Identification. An advanced deep learning model, CLSTM-Attention model, is developed for disease mention classification in DI++. Extensive empirical evaluation on about one million pages of de-identified clinical notes demonstrates that DI++ has significant performance advantage over existing systems on F1 Score, Area Under the Curve metrics, and efficiency. The proposed CLSTM-Attention model outperforms the existing deep learning models for disease mention classification. © 2021","Clinical notes; Concept extraction; Deep learning; Deep neural network; Disease mention extraction; Natural language processing (NLP); Patient condition classification","Decision making; Deep neural networks; Natural language processing systems; Clinical notes; Concept extraction; Deep learning; Disease mention extraction; EHR systems; Learning models; Natural language processing; Patient condition classification; Patients' conditions; Work-flows; Extraction",Article,Scopus,2-s2.0-85121759610
"Dai H., Peng X., Shi X., He L., Xiong Q., Jin H.","57215208610;57215335171;8935128100;57385719600;57215927320;57225858507;","Reveal training performance mystery between TensorFlow and PyTorch in the single GPU environment",2022,"Science China Information Sciences","65","1","112103","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121710442&doi=10.1007%2fs11432-020-3182-1&partnerID=40&md5=42633450e7842a38a6329b0c2de57a7a","Deep learning has gained tremendous success in various fields while training deep neural networks (DNNs) is very compute-intensive, which results in numerous deep learning frameworks that aim to offer better usability and higher performance to deep learning practitioners. TensorFlow and PyTorch are the two most popular frameworks. TensorFlow is more promising within the industry context, while PyTorch is more appealing in academia. However, these two frameworks differ much owing to the opposite design philosophy: static vs dynamic computation graph. TensorFlow is regarded as being more performance-friendly as it has more opportunities to perform optimizations with the full view of the computation graph. However, there are also claims that PyTorch is faster than TensorFlow sometimes, which confuses the end-users on the choice between them. In this paper, we carry out the analytical and experimental analysis to unravel the mystery of comparison in training speed on single-GPU between TensorFlow and PyTorch. To ensure that our investigation is as comprehensive as possible, we carefully select seven popular neural networks, which cover computer vision, speech recognition, and natural language processing (NLP). The contributions of this work are two-fold. First, we conduct the detailed benchmarking experiments on TensorFlow and PyTorch and analyze the reasons for their performance difference. This work provides the guidance for the end-users to choose between these two frameworks. Second, we identify some key factors that affect the performance, which can direct the end-users to write their models more efficiently. © 2021, Science China Press and Springer-Verlag GmbH Germany, part of Springer Nature.","comparison; deep learning; performance; PyTorch; TensorFlow","Benchmarking; Deep neural networks; Natural language processing systems; Speech recognition; Comparison; Deep learning; Design philosophy; Dynamics computation; End-users; Industry contexts; Learning frameworks; Performance; Pytorch; Tensorflow; Graphics processing unit",Article,Scopus,2-s2.0-85121710442
"Zhang Z., Cui P., Zhu W.","57202260633;34568700100;7404232311;","Deep Learning on Graphs: A Survey",2022,"IEEE Transactions on Knowledge and Data Engineering","34","1",,"249","270",,76,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121706096&doi=10.1109%2fTKDE.2020.2981333&partnerID=40&md5=435587ef339f3747945c56ef3dfc4f1d","Deep learning has been shown to be successful in a number of domains, ranging from acoustics, images, to natural language processing. However, applying deep learning to the ubiquitous graph data is non-trivial because of the unique characteristics of graphs. Recently, substantial research efforts have been devoted to applying deep learning methods to graphs, resulting in beneficial advances in graph analysis techniques. In this survey, we comprehensively review the different types of deep learning methods on graphs. We divide the existing methods into five categories based on their model architectures and training strategies: graph recurrent neural networks, graph convolutional networks, graph autoencoders, graph reinforcement learning, and graph adversarial methods. We then provide a comprehensive overview of these methods in a systematic manner mainly by following their development history. We also analyze the differences and compositions of different methods. Finally, we briefly outline the applications in which they have been used and discuss potential future research directions. © 1989-2012 IEEE.","deep learning; graph autoencoder; graph convolutional network; Graph data; graph neural network","Convolution; Graph neural networks; Graphic methods; Natural language processing systems; Recurrent neural networks; Reinforcement learning; Auto encoders; Convolutional networks; Deep learning; Graph autoencoder; Graph convolutional network; Graph data; Graph neural networks; Learning methods; Networks/graphs; Non-trivial; Surveys",Article,Scopus,2-s2.0-85121706096
"Fedotova A., Romanov A., Kurtukova A., Shelupanov A.","57214763715;55372881400;57207346456;6505540251;","Authorship attribution of social media and literary russian-language texts using machine learning methods and feature selection",2022,"Future Internet","14","1","4","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121691246&doi=10.3390%2ffi14010004&partnerID=40&md5=b617159626d9b14b0989afc0d302379f","Authorship attribution is one of the important fields of natural language processing (NLP). Its popularity is due to the relevance of implementing solutions for information security, as well as copyright protection, various linguistic studies, in particular, researches of social networks. The article is a continuation of the series of studies aimed at the identification of the Russian-language text’s author and reducing the required text volume. The focus of the study was aimed at the attribution of textual data created as a product of human online activity. The effectiveness of the models was evaluated on the two Russian-language datasets: literary texts and short comments from users of social networks. Classical machine learning (ML) algorithms, popular neural networks (NN) architectures, and their hybrids, including convolutional neural network (CNN), networks with long short-term memory (LSTM), Bidirectional Encoder Representations from Transformers (BERT), and fastText, that have not been used in previous studies, were applied to solve the problem. A particular experiment was devoted to the selection of informative features using genetic algorithms (GA) and evaluation of the classifier trained on the optimal feature space. Using fastText or a combination of support vector machine (SVM) with GA reduced the time costs by half in comparison with deep NNs with comparable accuracy. The average accuracy for literary texts was 80.4% using SVM combined with GA, 82.3% using deep NNs, and 82.1% using fastText. For social media comments, results were 66.3%, 73.2%, and 68.1%, respectively. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Authorship identification; Deep neural networks; FastText; Genetic algorithms; Machine learning; Natural language processing; Support vector machine","Copyrights; Deep neural networks; Feature extraction; Genetic algorithms; Learning algorithms; Long short-term memory; Natural language processing systems; Authorship attribution; Authorship identification; Fasttext; Features selection; Literary texts; Machine learning methods; Russian languages; Security protection; Social media; Support vectors machine; Support vector machines",Article,Scopus,2-s2.0-85121691246
"Zhang X., Rao Y., Li Q.","57217279695;55753260800;57199178903;","Lifelong topic modeling with knowledge-enhanced adversarial network",2022,"World Wide Web","25","1",,"219","238",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121587821&doi=10.1007%2fs11280-021-00984-2&partnerID=40&md5=2d4d102fc81e7fe12f669b355f6b1f5e","Lifelong topic modeling has attracted much attention in natural language processing (NLP), since it can accumulate knowledge learned from past for the future task. However, the existing lifelong topic models often require complex derivation or only utilize part of the context information. In this study, we propose a knowledge-enhanced adversarial neural topic model (KATM) and extend it to LKATM for lifelong topic modeling. KATM employs a knowledge extractor to encourage the generator to learn interpretable document representations and retrieve knowledge from the generated documents. LKATM incorporates knowledge from the previous trained KATM into the current model to learn from prior models without catastrophic forgetting. Experiments on four benchmark text streams validate the effectiveness of our KATM and LKATM in topic discovery and document classification. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Knowledge distillation; Lifelong learning; Neural topic modeling","Distillation; Information retrieval systems; Modeling languages; Text processing; Adversarial networks; Context information; Current modeling; Document Representation; Knowledge distillation; Learn+; Life long learning; Neural topic modeling; Prior modeling; Topic Modeling; Natural language processing systems",Article,Scopus,2-s2.0-85121587821
"Li J., Sun A., Han J., Li C.","56609767400;7202552214;57189214315;48761579500;","A Survey on Deep Learning for Named Entity Recognition",2022,"IEEE Transactions on Knowledge and Data Engineering","34","1",,"50","70",,58,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121413226&doi=10.1109%2fTKDE.2020.2981314&partnerID=40&md5=44f691f7321e524f1276494d1289d4e7","Named entity recognition (NER) is the task to identify mentions of rigid designators from text belonging to predefined semantic types such as person, location, organization etc. NER always serves as the foundation for many natural language applications such as question answering, text summarization, and machine translation. Early NER systems got a huge success in achieving good performance with the cost of human engineering in designing domain-specific features and rules. In recent years, deep learning, empowered by continuous real-valued vector representations and semantic composition through nonlinear processing, has been employed in NER systems, yielding stat-of-the-art performance. In this paper, we provide a comprehensive review on existing deep learning techniques for NER. We first introduce NER resources, including tagged NER corpora and off-the-shelf NER tools. Then, we systematically categorize existing works based on a taxonomy along three axes: distributed representations for input, context encoder, and tag decoder. Next, we survey the most representative methods for recent applied techniques of deep learning in new NER problem settings and applications. Finally, we present readers with the challenges faced by NER systems and outline future directions in this area. © 1989-2012 IEEE.","Deep learning; Named entity recognition; Natural language processing; Survey","Character recognition; Cost engineering; Deep learning; Semantics; Surveys; Deep learning; Machine translations; Named entity recognition; Natural language applications; Performance; Pre-defined semantics; Question Answering; Recognition systems; Semantic types; Text Summarisation; Natural language processing systems",Article,Scopus,2-s2.0-85121413226
"Chu C., Oliveira V., Virgo F.G., Otani M., Garcia N., Nakashima Y.","55916219700;57376410300;57376556700;56095441600;57201849958;57376410400;","The semantic typology of visually grounded paraphrases",2022,"Computer Vision and Image Understanding","215",,"103333","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121398384&doi=10.1016%2fj.cviu.2021.103333&partnerID=40&md5=072c28f9b4229cbe6e7c6c5a7d6a2eaf","Visually grounded paraphrases (VGPs) are different phrasal expressions describing the same visual concept in an image. Previous studies treat VGP identification as a binary classification task, which ignores various phenomena behind VGPs (i.e., different linguistic interpretation of the same visual concept) such as linguistic paraphrases and VGPs from different aspects. In this paper, we propose semantic typology for VGPs, aiming to elucidate the VGP phenomena and deepen the understanding about how human beings interpret vision with language. We construct a large VGP dataset that annotates the class to which each VGP pair belongs according to our typology. In addition, we present a classification model that fuses language and visual features for VGP classification on our dataset. Experiments indicate that joint language and vision representation learning is important for VGP classification. We further demonstrate that our VGP typology can boost the performance of visually grounded textual entailment. © 2021 The Author(s)","Dataset; Image interpretation; Semantic typology; Vision and language; Visual grounded paraphrases","Classification (of information); Large dataset; Visual languages; Binary classification; Classification tasks; Dataset; Human being; Image interpretation; Paraphrase identifications; Semantic typology; Vision and language; Visual concept; Visual grounded paraphrase; Semantics",Article,Scopus,2-s2.0-85121398384
"Ma B., Sun H., Wang J., Qi Q., Liao J.","57191853544;57190948696;57285766000;57226881061;57226839870;","Extractive Dialogue Summarization without Annotation Based on Distantly Supervised Machine Reading Comprehension in Customer Service",2022,"IEEE/ACM Transactions on Audio Speech and Language Processing","30",,,"87","97",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121381339&doi=10.1109%2fTASLP.2021.3133206&partnerID=40&md5=2a1439f532e1c95ef58d08261417e6de","Given a long dialogue, the dialogue summarization system aims to obtain a shorter highlight which retains the important information in the original text. For the customer service scenarios, the summaries of most dialogues between an agent and a user focus on several fixed key points, such as users' question, users' purpose, the agent's solution, and so on. Traditional extractive methods are difficult to extract all predefined key points exactly. Furthermore, there is a lack of large-scale and high-quality extractive summarization datasets containing the annotation for key points. Moreover, the speaker's role information is ignored or not fully utilized in previous work. In order to solve the above challenges, we propose a Distant Supervision based Machine Reading Comprehension model for extractive Summarization (DSMRC-S). DSMRC-S transforms the summarization task into the machine reading comprehension problem, to fetch key points from the original text exactly according to the predefined questions. In addition, a distant supervision method is proposed to alleviate the lack of eligible extractive summarization datasets. What's more, a speaker's role token and the solver classification task are proposed to make full use of speaker's role information. We conduct experiments on a real-world summarization dataset collected in customer service scenarios, and the results show that the proposed method outperforms the strong baseline methods by 6 percentage points on ROUGE$_L$. © 2014 IEEE.","customer service; distant supervision; Extractive summarization; machine reading comprehension","Classification (of information); Data mining; Job analysis; Sales; Semantics; Context models; Customer-service; Distant supervision; Extractive summarizations; Keypoints; Machine reading comprehension; Reading comprehension; Summarization systems; Task analysis; Speech processing",Article,Scopus,2-s2.0-85121381339
"Flamholz Z.N., Crane-Droesch A., Ungar L.H., Weissman G.E.","57209987251;55998767800;57374903900;53464532400;","Word embeddings trained on published case reports are lightweight, effective for clinical tasks, and free of protected health information",2022,"Journal of Biomedical Informatics","125",,"103971","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121303546&doi=10.1016%2fj.jbi.2021.103971&partnerID=40&md5=7cc33b08254e1564471928dab58299cd","Objective: Quantify tradeoffs in performance, reproducibility, and resource demands across several strategies for developing clinically relevant word embeddings. Materials and methods: We trained separate embeddings on all full-text manuscripts in the Pubmed Central (PMC) Open Access subset, case reports therein, the English Wikipedia corpus, the Medical Information Mart for Intensive Care (MIMIC) III dataset, and all notes in the University of Pennsylvania Health System (UPHS) electronic health record. We tested embeddings in six clinically relevant tasks including mortality prediction and de-identification, and assessed performance using the scaled Brier score (SBS) and the proportion of notes successfully de-identified, respectively. Results: Embeddings from UPHS notes best predicted mortality (SBS 0.30, 95% CI 0.15 to 0.45) while Wikipedia embeddings performed worst (SBS 0.12, 95% CI −0.05 to 0.28). Wikipedia embeddings most consistently (78% of notes) and the full PMC corpus embeddings least consistently (48%) de-identified notes. Across all six tasks, the full PMC corpus demonstrated the most consistent performance, and the Wikipedia corpus the least. Corpus size ranged from 49 million tokens (PMC case reports) to 10 billion (UPHS). Discussion: Embeddings trained on published case reports performed as least as well as embeddings trained on other corpora in most tasks, and clinical corpora consistently outperformed non-clinical corpora. No single corpus produced a strictly dominant set of embeddings across all tasks and so the optimal training corpus depends on intended use. Conclusion: Embeddings trained on published case reports performed comparably on most clinical tasks to embeddings trained on larger corpora. Open access corpora allow training of clinically relevant, effective, and reproducible embeddings. © 2021 Elsevier Inc.","Clinical informatics; Natural language processing; Protected health information; Word embeddings","Embeddings; Health; Medical informatics; Medical information systems; Case reports; Clinical informatics; Clinical tasks; Embeddings; Health systems; Pennsylvania; Performance; Protected health informations; Wikipedia; Word embedding; Natural language processing systems; article; electronic health record; embedding; human; intensive care; medical informatics; Medline; mortality; natural language processing; Pennsylvania; prediction; systematic review; case report; electronic health record; natural language processing; publication; reproducibility; Electronic Health Records; Humans; Natural Language Processing; Publications; PubMed; Reproducibility of Results",Article,Scopus,2-s2.0-85121303546
"Moon S., Lee G., Chi S.","57193772900;57219325492;15073984400;","Automated system for construction specification review using natural language processing",2022,"Advanced Engineering Informatics","51",,"101495","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121228089&doi=10.1016%2fj.aei.2021.101495&partnerID=40&md5=13904bb816478be33b390f1a0295eabf","Existing attempts to automate construction document analysis are limited in understanding the varied semantic properties of different documents. Due to the semantic conflicts, the construction specification review process is still conducted manually in practice despite the promising performance of the existing approaches. This research aimed to develop an automated system for reviewing construction specifications by analyzing the different semantic properties using natural language processing techniques. The proposed method analyzed varied semantic properties of 56 different specifications from five different countries in terms of vocabulary, sentence structure, and the organizing styles of provisions. First, the authors developed a semantic thesaurus for construction terms including 208 word-replacement rules based on Word2Vec embedding to understand the different vocabularies. Second, the authors developed a named entity recognition model based on bi-directional long short-term memory with a conditional random field layer, which identified the required keywords from given provisions with an averaged F1 score of 0.928. Third, the authors developed a provision-pairing model based on Doc2Vec embedding, which identified the most relevant provisions with an average accuracy of 84.4%. The web-based prototype demonstrated that the proposed system can facilitate the construction specification review process by reducing the time spent, supplementing the reviewer's experience, enhancing accuracy, and achieving consistency. The results contribute to risk management in the construction industry, with practitioners being able to review construction specifications thoroughly in spite of tight schedules and few available experts. © 2021 The Authors","Automated review; Construction specification; Machine learning; Natural language processing; Text mining","Construction industry; Embeddings; Machine learning; Natural language processing systems; Risk management; Semantics; Specifications; Text processing; Automated review; Automated systems; Construction documents; Construction specifications; Documents analysis; Embeddings; Model-based OPC; Review process; Semantic conflict; Semantic properties; Automation",Article,Scopus,2-s2.0-85121228089
"Baldini L., Martino A., Rizzi A.","57215116312;57194493625;7101771404;","A class-specific metric learning approach for graph embedding by information granulation",2022,"Applied Soft Computing","115",,"108199","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121127035&doi=10.1016%2fj.asoc.2021.108199&partnerID=40&md5=9c1834d531835f61d630196bf4652ee2","Graphs have gained a lot of attention in the pattern recognition community thanks to their ability to encode both topological and semantic information. Despite their invaluable descriptive power, their arbitrarily complex structured nature poses serious challenges when they are involved in learning systems. Typical approaches aim at building a vectorial representation of the graph in a suitable embedding space by leveraging on the selection of relevant prototypes that enable the use of common pattern recognition methods. An emerging paradigm able to synthesize prototypes in a data-driven fashion can be found in Granular Computing. Nonetheless, these methods often require a core dissimilarity measure defined directly in the graph domain that usually relies on a set of suitable parameters which are heavily problem-dependent. The automatic selection of these parameters is of utmost importance for building embedding spaces able to preserve the semantic contents between the structured and vector domains. In this paper, we propose an evolutionary-based approach for learning multiple dissimilarity measures tailored on each of the problem-related classes for the classification problem at hand. The learnt class-specific metrics contribute in synthesizing prototypes with high informative content related to each class by means of a Granular Computing approach. Such prototypes induce an embedding space where the graph classification can take place with common pattern recognition techniques for vector data. Tests conducted on publicly available datasets corroborate the effectiveness of the proposed approach both in terms of learning performances and interpretability of the model, as measured by the classification accuracy and number of meaningful prototypes considered in the synthesized model. © 2021 Elsevier B.V.","Granular Computing; Graph classification; Graph embedding; Inexact graph matching; Metric learning; Structural pattern recognition; Supervised learning","Classification (of information); Graph embeddings; Historic preservation; Pattern matching; Topology; Vector spaces; Dissimilarity measures; Embeddings; Graph classification; Graph embeddings; Inexact graph matching; Information granulation; Learning approach; Metric learning; Structural pattern recognition; Topological information; Semantics",Article,Scopus,2-s2.0-85121127035
"Jiang A., Yang X., Liu Y., Zubiaga A.","57211492138;57189372807;57264548700;35089138800;","SWSR: A Chinese dataset and lexicon for online sexism detection",2022,"Online Social Networks and Media","27",,"100182","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121114830&doi=10.1016%2fj.osnem.2021.100182&partnerID=40&md5=72c2f498456edfb4de34a97b2bfe3f66","Online sexism has become an increasing concern in social media platforms as it has affected the healthy development of the Internet and can have negative effects in society. While research in the sexism detection domain is growing, most of this research focuses on English as the language and on Twitter as the platform. Our objective here is to broaden the scope of this research by considering the Chinese language on Sina Weibo. We propose the first Chinese sexism dataset – Sina Weibo Sexism Review (SWSR) dataset –, as well as a large Chinese lexicon SexHateLex made of abusive and gender-related terms. We introduce our data collection and annotation process, and provide an exploratory analysis of the dataset characteristics to validate its quality and to show how sexism is manifested in Chinese. The SWSR dataset provides labels at different levels of granularity including (i) sexism or non-sexism, (ii) sexism category and (iii) target type, which can be exploited, among others, for building computational methods to identify and investigate finer-grained gender-related abusive language. We conduct experiments for the three sexism classification tasks making use of state-of-the-art machine learning models. Our results show competitive performance, providing a benchmark for sexism detection in the Chinese language, as well as an error analysis highlighting open challenges needing more research in Chinese NLP. The SWSR dataset and SexHateLex lexicon are publicly available.1 © 2021","Abusive language detection; Chinese sexism dataset; Chinese sexist lexicon; Hate speech detection; Natural language processing; Sexism detection","Benchmarking; Large dataset; Learning algorithms; Quality control; Social networking (online); Speech recognition; Abusive language detection; Chinese language; Chinese sexism dataset; Chinese sexist lexicon; Hate speech detection; Language detection; Sexism detection; Sina-weibo; Social media platforms; Speech detection; Natural language processing systems",Article,Scopus,2-s2.0-85121114830
"Martínez-deMiguel C., Segura-Bedmar I., Chacón-Solano E., Guerrero-Aspizua S.","57226950100;35303400800;57200920080;36677301900;","The RareDis corpus: A corpus annotated with rare diseases, their signs and symptoms",2022,"Journal of Biomedical Informatics","125",,"103961","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120964120&doi=10.1016%2fj.jbi.2021.103961&partnerID=40&md5=05a071f4ecaff437b659337051039493","Rare diseases affect a small number of people compared to the general population. However, more than 6,000 different rare diseases exist and, in total, they affect more than 300 million people worldwide. Rare diseases share as part of their main problem, the delay in diagnosis and the sparse information available for researchers, clinicians, and patients. Finding a diagnostic can be a very long and frustrating experience for patients and their families. The average diagnostic delay is between 6–8 years. Many of these diseases result in different manifestations among patients, which hampers even more their detection and the correct treatment choice. Therefore, there is an urgent need to increase the scientific and medical knowledge about rare diseases. Natural Language Processing (NLP) can help to extract relevant information about rare diseases to facilitate their diagnosis and treatments, but most NLP techniques require manually annotated corpora. Therefore, our goal is to create a gold standard corpus annotated with rare diseases and their clinical manifestations. It could be used to train and test NLP approaches and the information extracted through NLP could enrich the knowledge of rare diseases, and thereby, help to reduce the diagnostic delay and improve the treatment of rare diseases. The paper describes the selection of 1,041 texts to be included in the corpus, the annotation process and the annotation guidelines. The entities (disease, rare disease, symptom, sign and anaphor) and the relationships (produces, is a, is acron, is synon, increases risk of, anaphora) were annotated. The RareDis corpus contains more than 5,000 rare diseases and almost 6,000 clinical manifestations are annotated. Moreover, the Inter Annotator Agreement evaluation shows a relatively high agreement (F1-measure equal to 83.5% under exact match criteria for the entities and equal to 81.3% for the relations). Based on these results, this corpus is of high quality, supposing a significant step for the field since there is a scarcity of available corpus annotated with rare diseases. This could open the door to further NLP applications, which would facilitate the diagnosis and treatment of these rare diseases and, therefore, would improve dramatically the quality of life of these patients. © 2021 The Author(s)","Gold-standard corpus; Named Entity Recognition; Rare Diseases; Relation Extraction","Data mining; Diagnosis; Natural language processing systems; Patient treatment; Clinical manifestation; General population; Gold standards; Gold-standard corpus; Medical knowledge; Named entity recognition; Number of peoples; Rare disease; Relation extraction; Scientific knowledge; Diseases; Article; clinical feature; delayed diagnosis; human; information processing; medical informatics; natural language processing; quality of life; rare disease; symptomatology; quality of life; Delayed Diagnosis; Humans; Natural Language Processing; Quality of Life; Rare Diseases",Article,Scopus,2-s2.0-85120964120
"Trigueros O., Blanco A., Lebeña N., Casillas A., Pérez A.","57264129500;57210149702;57370186400;22633528200;57208931888;","Explainable ICD multi-label classification of EHRs in Spanish with convolutional attention",2022,"International Journal of Medical Informatics","157",,"104615","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120959162&doi=10.1016%2fj.ijmedinf.2021.104615&partnerID=40&md5=fb65bd6f7cd8792225972ab48dc937c5","Background: This work deals with Natural Language Processing applied to Electronic Health Records (EHRs). EHRs are coded following the International Classification of Diseases (ICD) leading to a multi-label classification problem. Previously proposed approaches act as black-boxes without giving further insights. Explainable Artificial Intelligence (XAI) helps to clarify what brought the model to make the predictions. Goal: This work aims to obtain explainable predictions of the diseases and procedures contained in EHRs. As an application, we show visualizations of the attention stored and propose a prototype of a Decision Support System (DSS) that highlights the text that motivated the choice of each of the proposed ICD codes. Methods: Convolutional Neural Networks (CNNs) with attention mechanisms were used. Attention mechanisms allow to detect which part of the input (EHRs) motivate the output (medical codes), producing explainable predictions. Results: We successfully applied methods in a Spanish corpus getting challenging results. Finally, we presented the idea of extracting the chronological order of the ICDs in a given EHR by anchoring the codes to different stages of the clinical admission. Conclusions: We found that explainable deep learning models applied to predict medical codes store helpful information that could be used to assist medical experts while reaching a solid performance. In particular, we show that the information stored in the attention mechanisms enables DSS and a shallow chronology of diagnoses. © 2021 Elsevier B.V.","Clinical Language Processing; Decision Support Systems; Deep Neural Understanding; Electronic Health Records; International Classification of Diseases","Classification (of information); Convolution; Deep learning; Diagnosis; Forecasting; Medical computing; Natural language processing systems; Records management; Anchorings; Attention mechanisms; Black boxes; Chronological order; Clinical language processing; Convolutional neural network; Deep neural understanding; International classification of disease; Language processing; Spanish corpora; Decision support systems; artificial intelligence; electronic health record; International Classification of Diseases; natural language processing; Artificial Intelligence; Electronic Health Records; International Classification of Diseases; Natural Language Processing; Neural Networks, Computer",Article,Scopus,2-s2.0-85120959162
"Kumari R., Ashok N., Ghosal T., Ekbal A.","57224125181;57224117799;57191853279;23093674100;","What the fake? Probing misinformation detection standing on the shoulder of novelty and emotion",2022,"Information Processing and Management","59","1","102740","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120942200&doi=10.1016%2fj.ipm.2021.102740&partnerID=40&md5=d436157b8f90a122c85eff4d0579cff9","One of the most time-critical challenges for the Natural Language Processing (NLP) community is to combat the spread of fake news and misinformation. Existing approaches for misinformation detection use neural network models, statistical methods, linguistic traits, fact-checking strategies, etc. However, the menace of fake news seems to grow more vigorous with the advent of humongous and unusually creative language models. Relevant literature reveals that one major characteristic of the virality of fake news is the presence of an element of surprise in the story, which attracts immediate attention and invokes strong emotional stimulus in the reader. In this work, we leverage this idea and propose textual novelty detection and emotion prediction as the two tasks relating to automatic misinformation detection. We re-purpose textual entailment for novelty detection and use the models trained on large-scale datasets of entailment and emotion to classify fake information. Our results correlate with the idea as we achieve state-of-the-art (SOTA) performance (7.92%, 1.54%, 17.31% and 8.13% improvement in terms of accuracy) on four large-scale misinformation datasets. We hope that our current probe will motivate the community to explore further research on misinformation detection along this line. The source code is available at the GitHub. © 2021 Elsevier Ltd","Deep learning; Emotion prediction; Fake news detection; Novelty prediction","Classification (of information); Deep learning; Fake detection; Large dataset; Natural language processing systems; Community IS; Critical challenges; Deep learning; Emotion predictions; Fake news detection; Language model; Neural network model; Novelty detection; Novelty prediction; Time-critical; Forecasting",Article,Scopus,2-s2.0-85120942200
"Chen Z., Qian T.","57211393145;17135487000;","Description and demonstration guided data augmentation for sequence tagging",2022,"World Wide Web","25","1",,"175","194",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120914872&doi=10.1007%2fs11280-021-00978-0&partnerID=40&md5=0b4077ce8e419f0812a946f0c96490bd","Fine-grained annotations are indispensable for sequence tagging tasks like named entity recognition and aspect-based sentiment analysis, which may incur extremely high time and labor costs. Recent efforts are towards data augmentation which aims to generate synthetic labeled instances. However, most existing methods adopt the random replacement or perturbation strategy under pre-defined constraints, and thus often lead to unstable performance. More importantly, these methods focus on producing more artificial samples yet neglect to make good use of real training samples. In this paper, we propose a novel description and demonstration guided data augmentation (D3A) approach for sequence tagging. On one hand, we collect dependency paths as descriptions to supervise the instance-level augmentation process, such that we can consistently generate high-quality synthetic data. On the other hand, we retrieve semantic or syntactic related features as demonstrations to enhance the learning capability of neural networks under limited training data. We conduct extensive experiments on four sequence tagging datasets with various sizes of training data. The results demonstrate that our proposed D3A approach can significantly improve the performance of sequence tagging, especially in low-resource scenarios. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Aspect-based sentiment analysis; Data augmentation; Named entity recognition; Sequence tagging","Semantics; Sentiment analysis; Wages; Aspect-based sentiment analyse; Data augmentation; Fine grained; Labor costs; Named entity recognition; Performance; Random replacements; Sentiment analysis; Sequence tagging; Time cost; Demonstrations",Article,Scopus,2-s2.0-85120914872
"Le T.-N., Cao Y., Nguyen T.-C., Le M.-Q., Nguyen K.-D., Do T.-T., Tran M.-T., Nguyen T.V.","55203995200;57223750709;57217246052;57218529044;57212419052;56647672700;35176729700;24480257100;","Camouflaged Instance Segmentation In-the-Wild: Dataset, Method, and Benchmark Suite",2022,"IEEE Transactions on Image Processing","31",,,"287","300",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120899736&doi=10.1109%2fTIP.2021.3130490&partnerID=40&md5=70f9444c4bc92bd3ffbf8b589f1388ae","This paper pushes the envelope on decomposing camouflaged regions in an image into meaningful components, namely, camouflaged instances. To promote the new task of camouflaged instance segmentation of in-the-wild images, we introduce a dataset, dubbed CAMO++, that extends our preliminary CAMO dataset (camouflaged object segmentation) in terms of quantity and diversity. The new dataset substantially increases the number of images with hierarchical pixel-wise ground truths. We also provide a benchmark suite for the task of camouflaged instance segmentation. In particular, we present an extensive evaluation of state-of-the-art instance segmentation methods on our newly constructed CAMO++ dataset in various scenarios. We also present a camouflage fusion learning (CFL) framework for camouflaged instance segmentation to further improve the performance of state-of-the-art methods. The dataset, model, evaluation suite, and benchmark will be made publicly available on our project page. © 1992-2012 IEEE.","benchmark suite; camouflage dataset; Camouflaged instance segmentation; in-the-wild image; multimodal learning","Benchmarking; Instance Segmentation; Job analysis; Semantic Segmentation; Statistical tests; Benchmark suites; Benchmark testing; Camouflage dataset; Camouflaged instance segmentation; Image color analysis; Images segmentations; In-the-wild image; Multi-modal learning; Objects segmentation; Task analysis; Urban areas; Semantics; article; decomposition; learning",Article,Scopus,2-s2.0-85120899736
"Fouad K.M., Sabbeh S.F., Medhat W.","57060094900;56521713200;56173896900;","Arabic fake news detection using deep learning",2022,"Computers, Materials and Continua","71","2",,"3647","3665",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120808570&doi=10.32604%2fcmc.2022.021449&partnerID=40&md5=4fd7a51dfdc02f79e8e07308d112c3a8","Nowadays, an unprecedented number of users interact through social media platforms and generate a massive amount of content due to the explosion of online communication. However, because user-generated content is unregulated, it may contain offensive content such as fake news, insults, and harassment phrases. The identification of fake news and rumors and their dissemination on social media has become a critical requirement. They have adverse effects on users, businesses, enterprises, and even political regimes and governments. State of the art has tackled the English language for news and used feature-based algorithms. This paper proposes a model architecture to detect fake news in the Arabic language by using only textual features.Machine learning and deep learning algorithms were used. The deep learning models are used depending on conventional neural nets (CNN), long short-term memory (LSTM), bidirectional LSTM (BiLSTM), CNN+LSTM, and CNN + BiLSTM. Three datasets were used in the experiments, each containing the textual content of Arabic news articles; one of them is reallife data. The results indicate that the BiLSTM model outperforms the other models regarding accuracy rate when both simple data split and recursive training modes are used in the training process. © 2022 Tech Science Press. All rights reserved.","Deep learning; Fake news detection; Machine learning; Natural language processing","Fake detection; Learning algorithms; Natural language processing systems; Social networking (online); Adverse effect; Business enterprise; Deep learning; English languages; Fake news detection; On-line communication; Social media; Social media platforms; State of the art; User-generated; Long short-term memory",Article,Scopus,2-s2.0-85120808570
"Shukla A.K., Das S.","57211267360;56298977200;","Deep neural network and pseudo relevance feedback based query expansion",2022,"Computers, Materials and Continua","71","2",,"3557","3570",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120808280&doi=10.32604%2fcmc.2022.022411&partnerID=40&md5=570a1f63f35a7f58aef78b1bfd921ca1","The neural network has attracted researchers immensely in the last couple of years due to its wide applications in various areas such as Data mining, Natural language processing, Image processing, and Information retrieval etc. Word embedding has been applied by many researchers for Information retrieval tasks. In this paper word embedding-based skip-gram model has been developed for the query expansion task. Vocabulary terms are obtained from the top ""k"" initially retrieved documents using the Pseudo relevance feedback model and then they are trained using the skip-grammodel to find the expansion terms for the user query. The performance of the model based on mean average precision is 0.3176. The proposed model compares with other existing models. An improvement of 6.61%, 6.93%, and 9.07% on MAP value is observed compare to the Original query, BM25 model, and query expansion with the Chi-Square model respectively. The proposed model also retrieves 84, 25, and 81 additional relevant documents compare to the original query, query expansion with Chi-Square model, and BM25 model respectively and thus improves the recall value also. The per query analysis reveals that the proposed model performs well in 30, 36, and 30 queries compare to the original query, query expansion with Chi-square model, and BM25 model respectively. © 2022 Tech Science Press. All rights reserved.","Deep neural network; Information retrieval; Neural network; Query expansion; Word embedding","Data mining; Deep neural networks; Embeddings; Expansion; Image processing; Natural language processing systems; Embeddings; Feed-back based; Gram models; Image information; Images processing; Neural-networks; Pseudo-relevance feedbacks; Query expansion; Retrieved documents; Word embedding; Information retrieval",Article,Scopus,2-s2.0-85120808280
"Uprety S.P., Jeong S.R.","57277784900;7402425254;","The impact of semi-supervised learning on the performance of intelligent chatbot system",2022,"Computers, Materials and Continua","71","2",,"3937","3952",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120791768&doi=10.32604%2fcmc.2022.023127&partnerID=40&md5=3c1b2fd963148c149f305edb1f073b6f","Artificial intelligent based dialog systems are getting attention from both business and academic communities. The key parts for such intelligent chatbot systems are domain classification, intent detection, and named entity recognition. Various supervised, unsupervised, and hybrid approaches are used to detect each field. Such intelligent systems, also called natural language understanding systems analyze user requests in sequential order: Domain classification, intent, and entity recognition based on the semantic rules of the classified domain. This sequential approach propagates the downstream error; i.e., if the domain classification model fails to classify the domain, intent and entity recognition fail. Furthermore, training such intelligent system necessitates a large number of user-annotated datasets for each domain. This study proposes a single joint predictive deep neural network framework based on long short-term memory using only a small user-annotated dataset to address these issues. It investigates value added by incorporating unlabeled data from user chatting logs intomulti-domain spoken language understanding systems. Systematic experimental analysis of the proposed joint frameworks, along with the semi-supervised multi-domain model, using open-source annotated and unannotated utterances shows robust improvement in the predictive performance of the proposed multi-domain intelligent chatbot over a base joint model and joint model based on adversarial learning. © 2022 Tech Science Press. All rights reserved.","Chatbot; Dialog system; Joint learning; LSTM; Natural language understanding; Semi-supervised learning","Deep neural networks; Large dataset; Long short-term memory; Semantics; Annotated datasets; Chatbots; Dialogue systems; Entity recognition; Intent recognition; Joint learning; Joint models; LSTM; Natural language understanding; Performance; Intelligent systems",Article,Scopus,2-s2.0-85120791768
"Abas A.R., Elhenawy I., Zidan M., Othman M.","36098943300;54915283100;57366233100;57196846048;","Bert-cnn: A deep learning model for detecting emotions from text",2022,"Computers, Materials and Continua","71","2",,"2943","2961",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120790983&doi=10.32604%2fcmc.2022.021671&partnerID=40&md5=a07b99b9e7da6573c47f62f12e61d612","Due to the widespread usage of social media in our recent daily lifestyles, sentiment analysis becomes an important field in pattern recognition andNatural Language Processing (NLP). In this field, users' feedback data on a specific issue are evaluated and analyzed.Detecting emotions within the text is therefore considered one of the important challenges of the current NLP research. Emotions have been widely studied in psychology and behavioral science as they are an integral part of the human nature. Emotions describe a state of mind of distinct behaviors, feelings, thoughts and experiences. The main objective of this paper is to propose a new model named BERT-CNN to detect emotions from text. This model is formed by a combination of the Bidirectional Encoder Representations from Transformer (BERT) and the Convolutional Neural networks (CNN) for textual classification. This model embraces the BERT to train the word semantic representation language model. According to the word context, the semantic vector is dynamically generated and then placed into the CNN to predict the output. Results of a comparative study proved that the BERT-CNN model overcomes the state-of-art baseline performance produced by different models in the literature using the semeval 2019 task3 dataset and ISEAR datasets. The BERTCNN model achieves an accuracy of 94.7% and an F1-score of 94% for semeval2019 task3 dataset and an accuracy of 75.8% and an F1-score of 76% for ISEAR dataset. © 2022 Tech Science Press. All rights reserved.","BERT-CNN; Deep learning; Emotion detection; Semeval2019; Text classification","Behavioral research; Classification (of information); Convolutional neural networks; Deep learning; Pattern recognition; Semantics; Bidirectional encoder representation from transformer-convolutional neural network; Convolutional neural network; Deep learning; Emotion detection; F1 scores; Learning models; Semeval2019; Sentiment analysis; Social media; Text classification; Sentiment analysis",Article,Scopus,2-s2.0-85120790983
"Croft R., Xie Y., Zahedi M., Babar M.A., Treude C.","57219534056;57226598098;55910177200;6602842620;23135531900;","An empirical study of developers’ discussions about security challenges of different programming languages",2022,"Empirical Software Engineering","27","1","27","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120735240&doi=10.1007%2fs10664-021-10054-w&partnerID=40&md5=9e81c71cfa70e21ae28e1f8bc47cbaeb","Given programming languages can provide different types and levels of security support, it is critically important to consider security aspects while selecting programming languages for developing software systems. Inadequate consideration of security in the choice of a programming language may lead to potential ramifications for secure development. Whilst theoretical analysis of the supposed security properties of different programming languages has been conducted, there has been relatively little effort to empirically explore the actual security challenges experienced by developers. We have performed a large-scale study of the security challenges of 15 programming languages by quantitatively and qualitatively analysing the developers’ discussions from Stack Overflow and GitHub. By leveraging topic modelling, we have derived a taxonomy of 18 major security challenges for 6 topic categories. We have also conducted comparative analysis to understand how the identified challenges vary regarding the different programming languages and data sources. Our findings suggest that the challenges and their characteristics differ substantially for different programming languages and data sources, i.e., Stack Overflow and GitHub. The findings provide evidence-based insights and understanding of security challenges related to different programming languages to software professionals (i.e., practitioners or researchers). The reported taxonomy of security challenges can assist both practitioners and researchers in better understanding and traversing the secure development landscape. This study highlights the importance of the choice of technology, e.g., programming language, in secure software engineering. Hence, the findings are expected to motivate practitioners to consider the potential impact of the choice of programming languages on software security. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Empirical software engineering; Natural language processing; Repository mining; Software security","Computer programming languages; Taxonomies; Data-source; Empirical Software Engineering; Empirical studies; Repository mining; Security aspects; Security challenges; Security support; Software security; Software-systems; Stack overflow; Natural language processing systems",Article,Scopus,2-s2.0-85120735240
"Karthik K., Kamath S. S.","57113290300;57364063000;","Deep neural models for automated multi-task diagnostic scan management - Quality enhancement, view classification and report generation",2022,"Biomedical Physics and Engineering Express","8","1","015011","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120695098&doi=10.1088%2f2057-1976%2fac3add&partnerID=40&md5=66a994a61ed5a5ac82cf5ab1952daba4","The detailed physiological perspectives captured by medical imaging provides actionable insights to doctors to manage comprehensive care of patients. However, the quality of such diagnostic image modalities is often affected by mismanagement of the image capturing process by poorly trained technicians and older/poorly maintained imaging equipment. Further, a patient is often subjected to scanning at different orientations to capture the frontal, lateral and sagittal views of the affected areas. Due to the large volume of diagnostic scans performed at a modern hospital, adequate documentation of such additional perspectives is mostly overlooked, which is also an essential key element of quality diagnostic systems and predictive analytics systems. Another crucial challenge affecting effective medical image data management is that the diagnostic scans are essentially stored as unstructured data, lacking a well-defined processing methodology for enabling intelligent image data management for supporting applications like similar patient retrieval, automated disease prediction etc. One solution is to incorporate automated diagnostic image descriptions of the observation/findings by leveraging computer vision and natural language processing. In this work, we present multi-task neural models capable of addressing these critical challenges. We propose ESRGAN, an image enhancement technique for improving the quality and visualization of medical chest x-ray images, thereby substantially improving the potential for accurate diagnosis, automatic detection and region-of-interest segmentation. We also propose a CNN-based model called ViewNet for predicting the view orientation of the x-ray image and generating a medical report using Xception net, thus facilitating a robust medical image management system for intelligent diagnosis applications. Experimental results are demonstrated using standard metrics like BRISQUE, PIQE and BLEU scores, indicating that the proposed models achieved excellent performance. Further, the proposed deep learning approaches enable diagnosis in a lesser time and their hybrid architecture shows significant potential for supporting many intelligent diagnosis applications. © 2021 IOP Publishing Ltd.","deep learning; enhancement; ESRGAN; medical report; natural language processing; orientation; ViewNet","Automation; Deep neural networks; Diagnosis; Image enhancement; Image segmentation; Medical imaging; Natural language processing systems; Predictive analytics; Deep learning; Diagnostic images; Diagnostic scans; Enhancement; ESRGAN; Medical report; Multi tasks; Neural modelling; Orientation; Viewnet; Information management",Article,Scopus,2-s2.0-85120695098
"El-allaly E.-D., Sarrouti M., En-Nahnahi N., Ouatik El Alaoui S.","57207994980;57170906800;57221862377;26431917800;","An attentive joint model with transformer-based weighted graph convolutional network for extracting adverse drug event relation",2022,"Journal of Biomedical Informatics","125",,"103968","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120673667&doi=10.1016%2fj.jbi.2021.103968&partnerID=40&md5=6775c5d621d40822130529bd3b3e9094","Adverse drug event (ADE) relation extraction is a crucial task for drug safety surveillance which aims to discover potential relations between ADE mentions from unstructured medical texts. To date, the graph convolutional networks (GCN) have been the state-of-the-art solutions for improving the ability of relation extraction task. However, there are many challenging issues that should be addressed. Among these, the syntactic information is not fully exploited by GCN-based methods, especially the diversified dependency edges. Still, these methods fail to effectively extract complex relations that include nested, discontinuous and overlapping mentions. Besides, the task is primarily regarded as a classification problem where each candidate relation is treated independently which neglects the interaction between other relations. To deal with these issues, in this paper, we propose an attentive joint model with transformer-based weighted GCN for extracting ADE Relations, called ADERel. Firstly, the ADERel system formulates the ADE relation extraction task as an N-level sequence labelling so as to model the complex relations in different levels and capture greater interaction between relations. Then, it exploits our neural joint model to process the N-level sequences jointly. The joint model leverages the contextual and structural information by adopting a shared representation that combines a bidirectional encoder representation from transformers (BERT) and our proposed weighted GCN (WGCN). The latter assigns a score to each dependency edge within a sentence so as to capture rich syntactic features and determine the most influential edges for extracting ADE relations. Finally, the system employs a multi-head attention to exchange boundary knowledge across levels. We evaluate ADERel on two benchmark datasets from TAC 2017 and n2c2 2018 shared tasks. The experimental results show that ADERel is superior in performance compared with several state-of-the-art methods. The results also demonstrate that incorporating a transformer model with WGCN makes the proposed system more effective for extracting various types of ADE relations. The evaluations further highlight that ADERel takes advantage of joint learning, showing its effectiveness in recognizing complex relations. © 2021 Elsevier Inc.","Adverse drug events; Joint learning; Natural language processing; Relation extraction; Transfer learning; Weighted graph convolutional network","Complex networks; Convolution; Convolutional neural networks; Extraction; Knowledge management; Learning systems; Natural language processing systems; Syntactics; Adverse drug event; Convolutional networks; Drug safety; Joint learning; Joint models; Level sequence; Relation extraction; Transfer learning; Weighted graph; Weighted graph convolutional network; Graphic methods; tacrolimus; warfarin; adverse drug reaction; Article; attentive joint model; calculation; convolutional neural network; drug labeling; information processing; liver disease; machine learning; mathematical analysis; mathematical model; neutropenia; randomization; scoring system; steatohepatitis; support vector machine; thrombocytopenia; human; Drug-Related Side Effects and Adverse Reactions; Humans",Article,Scopus,2-s2.0-85120673667
"Zhuang Y., Liu Z., Liu T.-T., Hung C.-C., Chai Y.-J.","57220199206;56263344500;55727711100;57363480300;40760977200;","Implicit sentiment analysis based on multi-feature neural network model",2022,"Soft Computing","26","2",,"635","644",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120628829&doi=10.1007%2fs00500-021-06486-7&partnerID=40&md5=58b48fc0293da77cbc6baf25d5b44122","As social media has become a ubiquitous part of daily life, researchers made a great progress in identifying the emotion in user-generated texts. However, it is a challenging task as people express their emotion in explicit and implicit ways. This paper focuses on the problem of identifying sentiments from implicit sentences which contain no emotional word or phrase. Most of the existing sentiment classification models cannot identify the sentiments accurately since they usually focus on extracting features from grammatical information without taking contextual information into account. In this paper, we argue that the contextual information is the key to identify sentiments in implicit sentences. Moreover, multiple features extracting from different aspects should be taken into account to improve sentiment identification. This paper proposes a multi-feature neural network model considering three aspects: contextual information, syntactic information and semantic information. To better get the semantic information of the sentence, we propose an attention mechanism based on contextual affective space. The experimental results on the SMP2019-ECISA dataset demonstrate that our model outperforms the previous systems and strong neural baselines. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Contextual affective space; Contextual information; Implicit sentiment analysis; Multi-feature; Semantic information; Syntactic information","Classification (of information); Semantic Web; Semantics; Syntactics; Contextual affective space; Contextual information; Daily lives; Implicit sentiment analyse; Multifeatures; Neural network model; Semantics Information; Sentiment analysis; Social media; Syntactic information; Sentiment analysis",Article,Scopus,2-s2.0-85120628829
"Narayanan S., Mannam K., Achan P., Ramesh M.V., Rangan P.V., Rajan S.P.","57195328435;57363691200;57241154700;57363111200;57188754415;7102282609;","A contextual multi-task neural approach to medication and adverse events identification from clinical text",2022,"Journal of Biomedical Informatics","125",,"103960","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120616157&doi=10.1016%2fj.jbi.2021.103960&partnerID=40&md5=55a070400629d99923856765b7950031","Effective wide-scale pharmacovigilance calls for accurate named entity recognition (NER) of medication entities such as drugs, dosages, reasons, and adverse drug events (ADE) from clinical text. The scarcity of adverse event annotations and underlying semantic ambiguities make accurate scope identification challenging. The current research explores integrating contextualized language models and multi-task learning from diverse clinical NER datasets to mitigate this challenge. We propose a novel multi-task adaptation method to refine the embeddings generated by the Bidirectional Encoder Representations from Transformers (BERT) language model to improve inter-task knowledge sharing. We integrated the adapted BERT model into a unique hierarchical multi-task neural network comprised of the medication and auxiliary clinical NER tasks. We validated the model using two different versions of BERT on diverse well-studied clinical tasks: Medication and ADE (n2c2 2018/n2c2 2009), Clinical Concepts (n2c2 2010/n2c2 2012), Disorders (ShAReCLEF 2013). Overall medication extraction performance enhanced by up to +1.19 F1 (n2c2 2018) while generalization enhanced by +5.38 F1 (n2c2 2009) as compared to standalone BERT baselines. ADE recognition enhanced significantly (McNemar's test), out-performing prior baselines. Similar benefits were observed on the auxiliary clinical and disorder tasks. We demonstrate that combining multi-dataset BERT adaptation and multi-task learning out-performs prior medication extraction methods without requiring additional features, newer training data, or ensembling. Taken together, the study contributes an initial case study towards integrating diverse clinical datasets in an end-to-end NER model for clinical decision support. © 2021 Elsevier Inc.","Adverse Drug Events; Biomedical Named Entity Recognition; Clinical Decision Support; Medication Extraction; Multi-task Learning; Pharmacovigilance","Character recognition; Clinical research; Decision support systems; Drug dosage; Learning systems; Semantics; Adverse drug event; Adverse events; Biomedical named entity recognition; Clinical decision support; Language model; Medication extraction; Multi tasks; Multitask learning; Named entity recognition; Pharmacovigilance; Extraction; adverse drug reaction; Article; artificial neural network; decision support system; embedding; language; machine learning; pharmacovigilance; clinical decision support system; drug surveillance program; natural language processing; semantics; Decision Support Systems, Clinical; Natural Language Processing; Neural Networks, Computer; Pharmacovigilance; Semantics",Article,Scopus,2-s2.0-85120616157
"Lin C.-C., Kuo C.-H., Chiang H.-T.","57204780769;12762153200;57204766819;","CNN-Based Classification for Point Cloud Object with Bearing Angle Image",2022,"IEEE Sensors Journal","22","1",,"1003","1011",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120579544&doi=10.1109%2fJSEN.2021.3130268&partnerID=40&md5=26b9934af236d8d10d4f14aefe32ce11","Convolutional neural network (CNN), one of the branches of deep neural networks, has been widely used in image recognition, natural language processing, and other related fields with great success recently. This paper proposes a novel framework with CNN to classify objects in a point cloud captured by LiDAR on urban streets. The proposed BA-CNN algorithm is composed of five steps: (i) removing ground points, (ii) clustering objects, (iii) transforming to bearing angle images, (iv) ROI selection, and (V) identifying objects by CNN. In the first step, ground points are removed by the multi-threshold-based ground detection to reduce the processing time. Then, a flood-fill-based clustering method is used for object segmentation. Those individual point cloud objects are converted to bearing angle (BA) images. Then, a well-trained CNN is used to classify objects with BA images. The main contribution of this paper is proposing an efficient recognition method that uses the information from point clouds only. In contrast, because most 3D object classifiers use the fusion of point clouds and color images, their models are very complicated and take a colossal amount of memory to store the parameters. Since the ground point detection and object clustering process all points along with the scanline-major order and layer-major order, the proposed algorithm performs better in terms of time consumption and memory consumption. In the experiment, three scenes from KITTI dataset are used for training and testing the proposed BA-CNN classifier, and the proposed BA-CNN achieves high classification accuracy. © 2001-2012 IEEE.","Bearing angle image; Convolutional neural network (CNN); Instance segmentation; Light detection and ranging (LiDAR); Object classification; Point cloud segmentation","Classification (of information); Cluster analysis; Clustering algorithms; Convolution; Convolutional neural networks; Edge detection; Feature extraction; Image recognition; Image segmentation; Natural language processing systems; Object detection; Object recognition; Optical character recognition; Optical radar; Statistical tests; Three dimensional displays; Tracking radar; Bearing angle image; Bearing angles; Convolutional neural network; Features extraction; Light detection and ranging; Object classification; Objects recognition; Point cloud segmentation; Three-dimensional display; Deep neural networks",Article,Scopus,2-s2.0-85120579544
"Lin L., Rao Y., Xie H., Lau R.Y.K., Yin J., Wang F.L., Li Q.","57217174366;55753260800;57219619828;12794272900;35316639800;7501312845;57199178903;","Copula Guided Parallel Gibbs Sampling for Nonparametric and Coherent Topic Discovery",2022,"IEEE Transactions on Knowledge and Data Engineering","34","1",,"219","235",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120373073&doi=10.1109%2fTKDE.2020.2976945&partnerID=40&md5=672dd6f8362bc590b85fb6177a4e1e90","Hierarchical Dirichlet Process (HDP) has attracted much attention in the research community of natural language processing. Given a corpus, HDP is able to determine the number of topics automatically, possessing an important feature dubbed nonparametric that overcomes the challenging issue of manually specifying a suitable topic number in parametric topic models, such as Latent Dirichlet Allocation (LDA). Nevertheless, HDP requires a much higher computational cost than LDA for parameter estimation. By taking the advantage of multi-threading, a parallel Gibbs sampling algorithm is proposed to estimate parameters for HDP based on the equivalence between HDP and Gamma-Gamma Poisson Process (G2PP) in terms of the generative process. Unfortunately, the above parallel Gibbs sampling algorithm requires to apply the finite approximation on the number of topics manually (i.e., predefine the topic number), thus can not retain the nonparametric feature of HDP. Another drawback of the above models is the lack of capturing the semantic dependencies between words, because the topic assignment of words is independent with each other. Although some works have been done in phrase-based topic modelling, these existing methods are still limited by either enforcing the entire phrase to share a common topic or requiring much complex and time-consuming phrase mining methods. In this paper, we aim to develop a copula guided parallel Gibbs sampling algorithm for HDP which can adjust the number of topics dynamically and capture the latent semantic dependencies between words that compose a coherent segment. Extensive experiments on real-world datasets indicate that our method achieves low perplexities and high topic coherence scores with a small time cost. In addition, we validate the effectiveness of our method on the modelling of word semantic dependencies by comparing the extracted topical phrases with those learned by state-of-the-art phrase-based baselines. © 1989-2012 IEEE.","Copulas; Parallel gibbs sampling; Topic modelling","Approximation algorithms; Data mining; Mining; Natural language processing systems; Parameter estimation; Semantics; Statistics; Copula; Gibbs sampling; Hierarchical Dirichlet process; Latent Dirichlet allocation; Nonparametrics; Parallel gibbs sampling; Sampling algorithm; Semantic dependency; Topic Discovery; Topic Modeling; Learning algorithms",Article,Scopus,2-s2.0-85120373073
"Stewart M., Liu W.","57196713433;36077178500;","E2EET: from pipeline to end-to-end entity typing via transformer-based embeddings",2022,"Knowledge and Information Systems","64","1",,"95","113",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120321362&doi=10.1007%2fs10115-021-01626-9&partnerID=40&md5=fdb145fe18a08b00c6a9fc99abc74e2f","Entity typing (ET) is the process of identifying the semantic types of every entity within a corpus. ET involves labelling each entity mention with one or more class labels. As a multi-class, multi-label task, it is considerably more challenging than named entity recognition. This means existing entity typing models require pre-identified mentions and cannot operate directly on plain text. Pipeline-based approaches are therefore used to join a mention extraction model and an entity typing model to process raw text. Another key limiting factor is that these mention-level ET models are trained on fixed context windows, which makes the entity typing results sensitive to window size selection. In light of these drawbacks, we propose an end-to-end entity typing model (E2EET) using a Bi-GRU to remove the dependency on window size. To demonstrate the effectiveness of our E2EET model, we created a stronger baseline mention-level model by incorporating the latest contextualised transformer-based embeddings (BERT). Extensive ablative studies demonstrate the competitiveness and simplicity of our end-to-end model for entity typing. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Bidirectional GRU; Bidirectional LSTM; Entity typing; Natural language processing; Neural language models; Sequence labelling","Embeddings; Ion beams; Long short-term memory; Natural language processing systems; Semantics; Bidirectional GRU; Bidirectional LSTM; Embeddings; End to end; Entity typing; Language model; Neural language model; Semantic types; Sequence Labeling; Window Size; Pipelines",Article,Scopus,2-s2.0-85120321362
"Wang L., Foer D., MacPhaul E., Lo Y.-C., Bates D.W., Zhou L.","57155540700;40361142100;57223226747;57233279300;57208855489;57226593855;","PASCLex: A comprehensive post-acute sequelae of COVID-19 (PASC) symptom lexicon derived from electronic health record clinical notes",2022,"Journal of Biomedical Informatics","125",,"103951","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120179249&doi=10.1016%2fj.jbi.2021.103951&partnerID=40&md5=a9f78d5df3472e5197e7525e6c2f540d","Objective: To develop a comprehensive post-acute sequelae of COVID-19 (PASC) symptom lexicon (PASCLex) from clinical notes to support PASC symptom identification and research. Methods: We identified 26,117 COVID-19 positive patients from the Mass General Brigham's electronic health records (EHR) and extracted 328,879 clinical notes from their post-acute infection period (day 51–110 from first positive COVID-19 test). PASCLex incorporated Unified Medical Language System® (UMLS) Metathesaurus concepts and synonyms based on selected semantic types. The MTERMS natural language processing (NLP) tool was used to automatically extract symptoms from a development dataset. The lexicon was iteratively revised with manual chart review, keyword search, concept consolidation, and evaluation of NLP output. We assessed the comprehensiveness of PASCLex and the NLP performance using a validation dataset and reported the symptom prevalence across the entire corpus. Results: PASCLex included 355 symptoms consolidated from 1520 UMLS concepts of 16,466 synonyms. NLP achieved an averaged precision of 0.94 and an estimated recall of 0.84. Symptoms with the highest frequency included pain (43.1%), anxiety (25.8%), depression (24.0%), fatigue (23.4%), joint pain (21.0%), shortness of breath (20.8%), headache (20.0%), nausea and/or vomiting (19.9%), myalgia (19.0%), and gastroesophageal reflux (18.6%). Discussion and conclusion: PASC symptoms are diverse. A comprehensive lexicon of PASC symptoms can be derived using an ontology-driven, EHR-guided and NLP-assisted approach. By using unstructured data, this approach may improve identification and analysis of patient symptoms in the EHR, and inform prospective study design, preventative care strategies, and therapeutic interventions for patient care. © 2021 Elsevier Inc.","Coronavirus; Electronic health records; Natural language processing; Outcomes; Prognosis; SARS-CoV-2","Clinical research; Diagnosis; Iterative methods; Natural language processing systems; Patient treatment; Records management; Search engines; Semantics; Clinical notes; Coronaviruses; Keyword search; Metathesaurus; Natural Language Processing Tools; Outcome; Prognose; SARS-CoV-2; Semantic types; Unified medical language systems; Coronavirus; SARS; electronic health record; human; natural language processing; prospective study; COVID-19; Electronic Health Records; Humans; Natural Language Processing; Prospective Studies; SARS-CoV-2",Article,Scopus,2-s2.0-85120179249
"De Angeli K., Gao S., Danciu I., Durbin E.B., Wu X.-C., Stroup A., Doherty J., Schwartz S., Wiggins C., Damesyn M., Coyle L., Penberthy L., Tourassi G.D., Yoon H.-J.","57219721766;57201077784;22940242900;22633969400;7407064425;6602112934;57356863500;57357009800;57356933300;6508143408;6701910617;7004055226;7003845683;34874205500;","Class imbalance in out-of-distribution datasets: Improving the robustness of the TextCNN for the classification of rare cancer types",2022,"Journal of Biomedical Informatics","125",,"103957","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120156672&doi=10.1016%2fj.jbi.2021.103957&partnerID=40&md5=8ca81874bd9ee8ad8af28a6cf3f5e10a","In the last decade, the widespread adoption of electronic health record documentation has created huge opportunities for information mining. Natural language processing (NLP) techniques using machine and deep learning are becoming increasingly widespread for information extraction tasks from unstructured clinical notes. Disparities in performance when deploying machine learning models in the real world have recently received considerable attention. In the clinical NLP domain, the robustness of convolutional neural networks (CNNs) for classifying cancer pathology reports under natural distribution shifts remains understudied. In this research, we aim to quantify and improve the performance of the CNN for text classification on out-of-distribution (OOD) datasets resulting from the natural evolution of clinical text in pathology reports. We identified class imbalance due to different prevalence of cancer types as one of the sources of performance drop and analyzed the impact of previous methods for addressing class imbalance when deploying models in real-world domains. Our results show that our novel class-specialized ensemble technique outperforms other methods for the classification of rare cancer types in terms of macro F1 scores. We also found that traditional ensemble methods perform better in top classes, leading to higher micro F1 scores. Based on our findings, we formulate a series of recommendations for other ML practitioners on how to build robust models with extremely imbalanced datasets in biomedical NLP applications. © 2021","Class Imbalance; CNN; Deep Learning; Ensemble; NLP; Text Classification","Classification (of information); Clinical research; Convolutional neural networks; Data mining; Deep learning; Learning algorithms; Natural language processing systems; Pathology; Text processing; Class imbalance; Clinical notes; Convolutional neural network; Deep learning; Ensemble; F1 scores; Information mining; Language processing techniques; Performance; Text classification; Diseases; Article; cancer classification; cancer registry; convolutional neural network; evolution; intermethod comparison; machine learning; natural language processing; out of distribution; prevalence; statistical analysis; statistical distribution; text classification; electronic health record; human; neoplasm; Electronic Health Records; Humans; Machine Learning; Natural Language Processing; Neoplasms; Neural Networks, Computer",Article,Scopus,2-s2.0-85120156672
"Douven I., Elqayam S., Gärdenfors P., Mirabile P.","16401429600;22984837000;6602498566;57200406818;","Conceptual spaces and the strength of similarity-based arguments",2022,"Cognition","218",,"104951","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119365687&doi=10.1016%2fj.cognition.2021.104951&partnerID=40&md5=3e9c9bee4d8ab2c7f8fbbb52743ebdfc","Central to the conceptual spaces framework is the thought that concepts can be studied mathematically, by geometrical and topological means. Various applications of the framework have already been subjected to empirical testing, mostly with excellent results, demonstrating the framework's usefulness. So far untested is the suggestion that conceptual spaces may help explain certain inferences people are willing to make. The experiment reported in this paper focused on similarity-based arguments, testing the hypothesis that the strength of such arguments can be predicted from the structure of the conceptual space in which the items being reasoned about are represented. A secondary aim of the experiment concerned a recent inferentialist semantics for indicative conditionals, according to which the truth of a conditional requires the presence of a sufficiently strong inferential connection between its antecedent and consequent. To the extent that the strength of similarity-based inferences can be predicted from the geometry and topology of the relevant conceptual space, such spaces should help predict truth ratings of conditionals embodying a similarity-based inferential link. The results supported both hypotheses. © 2021 Elsevier B.V.","Argument strength; Concepts; Conceptual spaces; Conditionals; Inference; Inferentialism; Similarity","article; geometry; human; semantics; problem solving; semantics; suggestion; Humans; Problem Solving; Semantics; Suggestion",Article,Scopus,2-s2.0-85119365687
"Ayala D., Hernández I., Ruiz D., Rahm E.","57204530984;42961380400;16317312600;6603724515;","LEAPME: Learning-based Property Matching with Embeddings",2022,"Data and Knowledge Engineering","137",,"101943","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119247697&doi=10.1016%2fj.datak.2021.101943&partnerID=40&md5=ece373d7f094aed47adaf5519537a957","Data integration tasks such as the creation and extension of knowledge graphs involve the fusion of heterogeneous entities from many sources. Matching and fusion of such entities require to also match and combine their properties (attributes). However, previous schema matching approaches mostly focus on two sources only and often rely on simple similarity measurements. They thus face problems in challenging use cases such as the integration of heterogeneous product entities from many sources. We therefore present a new machine learning-based property matching approach called LEAPME (LEArning-based Property Matching with Embeddings) that utilizes numerous features of both property names and instance values. The approach heavily makes use of word embeddings to better utilize the domain-specific semantics of both property names and instance values. The use of supervised machine learning helps exploit the predictive power of word embeddings. Our comparative evaluation against five baselines for several multi-source datasets with real-world data shows the high effectiveness of LEAPME. We also show that our approach is even effective when training data from another domain (transfer learning) is used. © 2021 The Author(s)","Data integration; Knowledge graphs; Machine learning","Embeddings; Knowledge graph; Semantics; Supervised learning; Domain specific semantics; Embeddings; Heterogeneous products; Knowledge graphs; Matchings; Property; Schema matching; Similarity measurements; Simple++; Two sources; Data integration",Article,Scopus,2-s2.0-85119247697
"Kim J., Wohn D.Y., Cha M.","57338738000;36096100000;24069708400;","Understanding and identifying the use of emotes in toxic chat on Twitch",2022,"Online Social Networks and Media","27",,"100180","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119059166&doi=10.1016%2fj.osnem.2021.100180&partnerID=40&md5=f33f22003d541545bcb87a5ce1d52280","The latest advances in NLP (natural language processing) have led to the launch of the much needed machine-driven toxic chat detection. Nevertheless, people continuously find new forms of hateful expressions that are easily identified by humans, but not by machines. One such common expression is the mix of text and emotes, a type of visual toxic chat that is increasingly used to evade algorithmic moderation and a trend that is an under-studied aspect of the problem of online toxicity. This research analyzes chat conversations from the popular streaming platform Twitch to understand the varied types of visual toxic chat. Emotes were sometimes used to replace a letter, seek attention, or for emotional expression. We created a labeled dataset that contains 29,721 cases of emotes replacing letters. Based on the dataset, we built a neural network classifier and identified visual toxic chat that would otherwise be undetected through traditional methods and caught an additional 1.3% examples of toxic chat out of 15 million chat utterances. © 2021 The Authors","Algorithmic moderation; Detection; Emotes; Live streaming; Twitch; Usages; Visual toxic chat","Classification (of information); Algorithmic moderation; Algorithmics; Detection; Emotes; Live streaming; New forms; Research analysis; Twitch; Usage; Visual toxic chat; Natural language processing systems",Article,Scopus,2-s2.0-85119059166
"Kaczmarek I., Iwaniak A., Świetlicka A., Piwowarczyk M., Nadolny A.","56539582800;25635709500;55540538900;57195939191;57217248754;","A machine learning approach for integration of spatial development plans based on natural language processing",2022,"Sustainable Cities and Society","76",,"103479","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119055558&doi=10.1016%2fj.scs.2021.103479&partnerID=40&md5=4e2e3fd7d8756dbbcf5ecca58bf822e1","Spatial development plans are the basic tool for shaping spatial policy and have an impact on the implementation of the concept of sustainable development. Monitoring the implementation of plans can be difficult where no standard of plans exists that allows for obtaining comprehensive information on the arrangements of the plans, including future land development. The purpose of the research is to integrate spatial development plans by analyzing and classifying their textual content. We use machine learning methods for the processing of the text of plans and their classification. The result is a model, that classifies the texts of findings for individual areas in the plan into defined land use categories. We use machine learning methods in natural language processing for the analyzing of the text part of plans and their classification. The results indicate the best quality of the model when using neural networks. The proposed approach allows for obtaining comprehensive information on the planned land use of the area, derived from many heterogeneous planning documents. Due to the combination of textual arrangements with spatial data, it allows both for the unification of land use classification and then integration of multiple spatial development plans in spatial dimension. © 2021","GRU; LSTM; Neural networks; Spatial development plan; Spatial planning; Text classification; Unsupervised machine learning","Classification (of information); Learning algorithms; Long short-term memory; Natural language processing systems; Text processing; Comprehensive information; Development plans; GRU; LSTM; Neural-networks; Spatial development; Spatial development plan; Spatial planning; Text classification; Unsupervised machine learning; Land use",Article,Scopus,2-s2.0-85119055558
"Sharma A., Kabra A., Jain M.","57205444094;57221146371;56452251900;","Ceasing hate with MoH: Hate Speech Detection in Hindi–English code-switched language",2022,"Information Processing and Management","59","1","102760","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118884761&doi=10.1016%2fj.ipm.2021.102760&partnerID=40&md5=060b1c07021c367f73a6e4a92ce3c6e2","Warning: This manuscript may contain upsetting language. Social media has become a bedrock for people to voice their opinions worldwide. Due to the greater sense of freedom with the anonymity feature, it is possible to disregard social etiquette online and attack others without facing severe consequences, inevitably propagating hate speech. The current measures to sift the online content and offset the hatred spread do not go far enough. One factor contributing to this is the prevalence of regional languages in social media and the paucity of language flexible hate speech detectors. The proposed work focuses on analyzing hate speech in Hindi–English code-switched language. Our method explores transformation techniques to capture precise text representation. To contain the structure of data and yet use it with existing algorithms, we developed ‘MoH’ or (Map Only Hindi), which means ‘Love’ in Hindi. ‘MoH’ pipeline which consists of language identification, Roman to Devanagari Hindi transliteration using a knowledge base of Roman Hindi words, and finally employs the fine-tuned Multilingual Bert, and MuRIL language models. We conducted several quantitative experiment studies on three datasets, and evaluated performance using Precision, Recall and F1 metrics. The first experiment studies ‘MoH’ mapped text's performance with classical machine learning models and shows an average increase of 13% in F1 scores. The second compares the proposed work's scores with those of the baseline models and shows a rise in performance by 6%. Finally, the third compares the proposed ‘MoH’ technique with various data simulations using the existing transliteration library. Here, ‘MoH’ outperforms the rest by 15%. Our results demonstrate a significant improvement in the state-of-the-art scores on all three datasets. © 2021 Elsevier Ltd","And machine learning; Bert; Cyber hate; Data simulations; MuRIL; Social media; Text classification; Transfer learning","Classification (of information); Codes (symbols); Computational linguistics; Knowledge based systems; Machine learning; Natural language processing systems; Speech; Speech recognition; Text processing; And machine learning; Bert; Cybe hate; Data simulation; Experiment study; MuRIL; Performance; Social media; Text classification; Transfer learning; Social networking (online)",Article,Scopus,2-s2.0-85118884761
"Le Guilly M., Petit J.-M., Scuturici V.-M.","57211937184;57205867724;15756127900;","SQL query extensions for imprecise questions",2022,"Data and Knowledge Engineering","137",,"101944","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118883346&doi=10.1016%2fj.datak.2021.101944&partnerID=40&md5=88b180f73c91fdaae1faefae7196fd4a","Within the big data tsunami, relational databases and SQL remain inescapable in most cases for accessing data. If SQL is easy-to-use and has proved its robustness over the years, it is not always easy to formulate SQL queries as it is more and more frequent to have databases with hundreds of tables and/or attributes. Identifying the pertinent conditions to select the desired data, or even the relevant attributes, is not trivial, especially when the user only has an imprecise question in mind, and is not sure of how to translate its conditions directly into SQL. To make it easier to write SQL queries when the initial question is imprecise, we propose SQL query extensions: given a query, it suggests several possible additional selection clauses, to complete the Where clause of the query, as a form of SQL query semantic autocompletion. This is helpful for both understanding the initial query's results, and refining the query to reach the desired tuples. The process is iterative, as a query constructed using an extension can also be completed. It is also adaptable, as the number of extensions to compute is flexible. A prototype has been implemented in a SQL editor on top of a database management system, and two types of evaluation are proposed. A first one looks at the scaling of the system with a large number of tuples. Then a user study examines two questions: does the extension tool speed up the writing of SQL queries? And is it easily adopted by users? A thorough experiment was conducted on a group of 70 computer science students divided in two groups (one with the extension tool and the other one without) to answer those questions. In the end, the results showed a faster answering time for students that could use the extensions: 32 min on average to complete the test for the group with extensions, against 48 min for the others. © 2021 Elsevier B.V.","Imprecise questions; Query extensions; SQL","Relational database systems; Semantics; Students; Accessing data; Condition; Desired datum; Imprecise question; Query extension; Query results; Query semantics; Relational Database; SQL; SQL query; Query processing",Article,Scopus,2-s2.0-85118883346
"Firebanks-Quevedo D., Planas J., Buckingham K., Taylor C., Silva D., Naydenova G., Zamora-Cristales R.","57330315900;57328973300;36699307800;57329737700;57329737800;57329737900;56015151800;","Using machine learning to identify incentives in forestry policy: Towards a new paradigm in policy analysis",2022,"Forest Policy and Economics","134",,"102624","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118854601&doi=10.1016%2fj.forpol.2021.102624&partnerID=40&md5=cbeed89e773361a52a8cc2cca4cd50c4","As 2021 saw the launch of the United Nations Decade on Ecosystem Restoration, it highlighted the need to prepare for success over the decade and to understand what public economic and financial incentives exist to support sustainable forest and landscape restoration. To date, Initiative 20 × 20, a coalition of 18 Latin American countries, has committed to place 50 million hectares under restoration and conservation by 2030. Understanding the public policies in these countries that turn those commitments into action, however, is very labor-intensive, requiring decision makers to read and analyze thousands of pages of documents that span multiple sectors, ministries, and scales that lie outside of their areas of expertise. To address this, we developed a semi-automated policy analysis tool that uses state-of-the-art Natural Language Processing (NLP) methods to mine policy documents, assist the labeling process carried out by policy experts, automatically identify policies that contain incentives and classify them by incentive instrument from the following categories: direct payments, fines, credit, tax deduction, technical assistance and supplies. Our best model achieves an F1 score of 93–94% in both identifying an incentive and its policy instrument, as well as an accuracy of above 90% for 5 out of 6 policy instruments, reducing multiple weeks of policy analysis work to a matter of minutes. In particular, the model properly identified the relative frequency of credits, direct payments, and fines that exist as the primary policy instruments in these countries. We also found that tax deductions, supplies, and technical assistance are much less used among most of the countries and that, oftentimes, the policy documents describe economic incentives for restoration in vague and intangible terms. In addition, our model is designed to constantly improve its performance with more data and feedback from policy experts. Furthermore, while our experiments were run on Spanish policy documents, we designed our framework to be widely scalable to policies from different countries and multiple languages, limited only by the number of languages supported by current multilingual NLP models. Using a standardized approach to generate incentives data could provide an evidence-based and transparent system to find complementarity between policies and help remove barriers for implementers and policymakers and enable a more informed decision-making process. © 2021 The Authors","Data-science; Economic-incentives; Environmental-policy; Machine learning; Politics","Conservation; Decision making; Environmental protection; Forestry; Learning algorithms; Natural language processing systems; Public policy; Restoration; Economic incentive; Ecosystem restoration; Environmental policy; Forestry policy; Policy analysis; Policy documents; Policy instruments; Tax deduction; Technical assistance; United Nations; Machine learning; Analysis; Conservation; Decision Making; Documents; Forestry; Incentives; Restoration; United Nations",Article,Scopus,2-s2.0-85118854601
"Wang P., Brown C., Jennings J.A., Stolee K.T.","56903232900;57213322400;57219534176;35273571700;","Demystifying regular expression bugs: A comprehensive study on regular expression bug causes, fixes, and testing",2022,"Empirical Software Engineering","27","1","21","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118722258&doi=10.1007%2fs10664-021-10033-1&partnerID=40&md5=610820c3285f753b32c8b1331cbf016c","Regular expressions cause string-related bugs and open security vulnerabilities for DOS attacks. However, beyond ReDoS (Regular expression Denial of Service), little is known about the extent to which regular expression issues affect software development and how these issues are addressed in practice. We conduct an empirical study of 356 regex-related bugs from merged pull requests in Apache, Mozilla, Facebook, and Google GitHub repositories. We identify and classify the nature of the regular expression problems, the fixes, and the related changes in the test code. The most important findings in this paper are as follows: 1) incorrect regular expression semantics is the dominant root cause of regular expression bugs (165/356, 46.3%). The remaining root causes are incorrect API usage (9.3%) and other code issues that require regular expression changes in the fix (29.5%), 2) fixing regular expression bugs is nontrivial as it takes more time and more lines of code to fix them compared to the general pull requests, 3) most (51%) of the regex-related pull requests do not contain test code changes. Certain regex bug types (e.g., compile error, performance issues, regex representation) are less likely to include test code changes than others, and 4) the dominant type of test code changes in regex-related pull requests is test case addition (75%). The results of this study contribute to a broader understanding of the practical problems faced by developers when using, fixing, and testing regular expressions. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Bug fixes; Pull requests; Regular expression bug characteristics; Test code","Application programming interfaces (API); Denial-of-service attack; Semantics; Software design; Testing; Bug characteristics; Bug fixes; Code changes; Denial of Service; Pull request; Regular expression bug characteristic; Regular expressions; Root cause; Security vulnerabilities; Test code; Pattern matching",Article,Scopus,2-s2.0-85118722258
"Xiao S., Wang S., Dai Y., Guo W.","57315896900;36601774500;57200559318;13606768700;","Graph neural networks in node classification: survey and evaluation",2022,"Machine Vision and Applications","33","1","4","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118717700&doi=10.1007%2fs00138-021-01251-0&partnerID=40&md5=015bf9a2b8e21c2e978721924c87a3b9","Neural networks have been proved efficient in improving many machine learning tasks such as convolutional neural networks and recurrent neural networks for computer vision and natural language processing, respectively. However, the inputs of these deep learning paradigms all belong to the type of Euclidean structure, e.g., images or texts. It is difficult to directly apply these neural networks to graph-based applications such as node classification since graph is a typical non-Euclidean structure in machine learning domain. Graph neural networks are designed to deal with the particular graph-based input and have received great developments because of more and more research attention. In this paper, we provide a comprehensive review about applying graph neural networks to the node classification task. First, the state-of-the-art methods are discussed and divided into three main categories: convolutional mechanism, attention mechanism and autoencoder mechanism. Afterward, extensive comparative experiments are conducted on several benchmark datasets, including citation networks and co-author networks, to compare the performance of different methods with diverse evaluation metrics. Finally, several suggestions are provided for future research based on the experimental results. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Attention mechanism; Autoencoder mechanism; Convolutional mechanism; Deep learning; Graph neural networks; Node classification","Benchmarking; Convolution; Convolutional neural networks; Graph theory; Graphic methods; Learning algorithms; Natural language processing systems; Recurrent neural networks; Attention mechanisms; Auto encoders; Autoencoder mechanism; Convolutional mechanism; Deep learning; Euclidean structure; Graph neural networks; Graph-based; Neural-networks; Node classification; Graph neural networks",Article,Scopus,2-s2.0-85118717700
"Boussakssou M., Ezzikouri H., Erritali M.","57216969485;55988022700;55375629800;","Chatbot in Arabic language using seq to seq model",2022,"Multimedia Tools and Applications","81","2",,"2859","2871",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118678170&doi=10.1007%2fs11042-021-11709-y&partnerID=40&md5=f4ed53c08dc9b43ae89d196197d62fdd","A conversational agent (chatbot) is a software that can communicate with humans using natural language. Conversation modeling is an extremely important topic in natural language processing and artificial intelligence (AI). Indeed, since the birth of AI, creating a good chatbot remains one of the most difficult challenges in this field. Although chatbots can be used for a variety of tasks, they generally need to understand what users are saying and to provide appropriate answers to their questions. In this paper, we present midoBot: a deep learning Arabic chatbot based on the seq2seq model. midoBot is capable of conversing with humans on popular conversation topics through text. We built the model and tested it in the Tensorflow 2 deep learning framework using the most seq 2 seq Model architectures. We use a dataset of ~81,659 pairs of conversations created manually and without any handcrafted rules. Our algorithm was trained on a VM on google cloud (GPU TESLA K80 10 GO). The results obtained are significant, In most questions the chatbot was able to reproduce good answers. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Arabic chatbot dialogue; Seq2seq; System conversation agents","Deep learning; Modeling languages; Software agents; Arabic chatbot dialog; Arabic languages; Chatbots; Conversation agents; Conversational agents; Learning frameworks; Modeling architecture; Natural languages; Seq2seq; System conversation agent; Natural language processing systems",Article,Scopus,2-s2.0-85118678170
"Strobelt H., Kinley J., Krueger R., Beyer J., Pfister H., Rush A.M.","24766708300;57326603300;55571324900;16318861700;23028620700;51665746500;","GenNI: Human-AI Collaboration for Data-Backed Text Generation",2022,"IEEE Transactions on Visualization and Computer Graphics","28","1",,"1106","1116",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118642859&doi=10.1109%2fTVCG.2021.3114845&partnerID=40&md5=7b5ec23fa0edadb077ecf776835450b2","Table2Text systems generate textual output based on structured data utilizing machine learning. These systems are essential for fluent natural language interfaces in tools such as virtual assistants; however, left to generate freely these ML systems often produce misleading or unexpected outputs. GenNI (Generation Negotiation Interface) is an interactive visual system for high-level human-AI collaboration in producing descriptive text. The tool utilizes a deep learning model designed with explicit control states. These controls allow users to globally constrain model generations, without sacrificing the representation power of the deep learning models. The visual interface makes it possible for users to interact with AI systems following a Refine-Forecast paradigm to ensure that the generation system acts in a manner human users find suitable. We report multiple use cases on two experiments that improve over uncontrolled generation approaches, while at the same time providing fine-grained control. A demo and source code are available at https://genni.vizhub.ai. © 1995-2012 IEEE.","Machine Learning; Modelling; Simulation Applications; Statistics; Tabular Data; Text/Document Data","Data visualization; Job analysis; Learning algorithms; Natural language processing systems; Collaboration; Computational modelling; Deep learning; Machine-learning; Modeling; Simulation applications; Tabular data; Task analysis; Text document; Text/document data; Deep learning; article; deep learning; human; human experiment",Article,Scopus,2-s2.0-85118642859
"Latif S., Zhou Z., Kim Y., Beck F., Kim N.W.","57204297580;57232739000;57223901343;25824730200;57226407607;","Kori: Interactive Synthesis of Text and Charts in Data Documents",2022,"IEEE Transactions on Visualization and Computer Graphics","28","1",,"184","194",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118622924&doi=10.1109%2fTVCG.2021.3114802&partnerID=40&md5=c78f94f9d9dec3a2ca1b51722ffe9a81","Charts go hand in hand with text to communicate complex data and are widely adopted in news articles, online blogs, and academic papers. They provide graphical summaries of the data, while text explains the message and context. However, synthesizing information across text and charts is difficult; it requires readers to frequently shift their attention. We investigated ways to support the tight coupling of text and charts in data documents. To understand their interplay, we analyzed the design space of chart-text references through news articles and scientific papers. Informed by the analysis, we developed a mixed-initiative interface enabling users to construct interactive references between text and charts. It leverages natural language processing to automatically suggest references as well as allows users to manually construct other references effortlessly. A user study complemented with algorithmic evaluation of the system suggests that the interface provides an effective way to compose interactive data documents. © 1995-2012 IEEE.","authoring; Data-driven storytelling; interaction design; interactive documents; mixed-initiative interface; visualization-text linking","Data visualization; Visualization; Authoring; Data driven; Data-driven storytelling; Interaction design; Interactive documents; Mixed-initiative; Mixed-initiative interface; Programming; Text linking; Visualization&#x2013;; Natural language processing systems; algorithm; article; human; natural language processing; synthesis",Article,Scopus,2-s2.0-85118622924
"Xiong C., Setlur V., Bach B., Koh E., Lin K., Franconeri S.","57211998174;21743848500;37103551800;14035733000;57238882600;6603307356;","Visual Arrangements of Bar Charts Influence Comparisons in Viewer Takeaways",2022,"IEEE Transactions on Visualization and Computer Graphics","28","1",,"955","965",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118598836&doi=10.1109%2fTVCG.2021.3114823&partnerID=40&md5=a1d5e762604b0b51aa5c99ccc8120e1a","Well-designed data visualizations can lead to more powerful and intuitive processing by a viewer. To help a viewer intuitively compare values to quickly generate key takeaways, visualization designers can manipulate how data values are arranged in a chart to afford particular comparisons. Using simple bar charts as a case study, we empirically tested the comparison affordances of four common arrangements: vertically juxtaposed, horizontally juxtaposed, overlaid, and stacked. We asked participants to type out what patterns they perceived in a chart and we coded their takeaways into types of comparisons. In a second study, we asked data visualization design experts to predict which arrangement they would use to afford each type of comparison and found both alignments and mismatches with our findings. These results provide concrete guidelines for how both human designers and automatic chart recommendation systems can make visualizations that help viewers extract the 'right' takeaway. © 1995-2012 IEEE.","bar charts; Comparison; natural language interaction; perception; recommendation systems; visual grouping","Data visualization; Job analysis; Recommender systems; Visual languages; Visualization; Affordances; Bar chart; Case-studies; Comparison; Data values; Natural language interaction; Simple++; Task analysis; Visual grouping; Visualization designs; Semantics; adult; article; data visualization; human; practice guideline",Article,Scopus,2-s2.0-85118598836
"Yang J., Yang Z., Zhang S., Tu H., Huang Y.","57219737267;57191362143;57219385670;57324833500;14627673100;","SeSy: Linguistic Steganalysis Framework Integrating Semantic and Syntactic Features",2022,"IEEE Signal Processing Letters","29",,,"31","35",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118595668&doi=10.1109%2fLSP.2021.3122901&partnerID=40&md5=eb9d1204dc643de981eaffbfcd1485a3","With the rapid development of natural language processing technology and linguistic steganography, linguistic steganalysis gains considerable interest in recent years. Current advanced methods dominantly focus on statistical features in semantic view yet ignore syntax structure of text, which leads to limited performance to some newly statistically indistinguishable steganography algorithms. To fill this gap, in this paper, we propose a novel linguistic steganalysis framework named SeSy to integrate both semantic and syntactic features. Specifically, we propose to employ transformer-architecture language model as semantics extractor and leverage a graph attention network to retain syntactic features. Extensive experimental results show that owing to additional syntactic information, the SeSy framework effectively brings about remarkable improvement to current advanced linguistic steganalysis methods. © 1994-2012 IEEE.","graph network; Linguistic steganalysis; syntactic feature","Natural language processing systems; Semantic Web; Semantics; Steganography; Syntactics; 'current; Correlation; Features extraction; Graph networks; Linguistic steganalyse; Processing technologies; Semantic features; Steganalysis; Syntactic features; Transformer; Graph neural networks",Article,Scopus,2-s2.0-85118595668
"Cui W., Wang J., Huang H., Wang Y., Lin C.-Y., Zhang H., Zhang D.","24587013600;57324647100;57202043133;57189645214;23009406500;54586310400;55717568500;","A Mixed-Initiative Approach to Reusing Infographic Charts",2022,"IEEE Transactions on Visualization and Computer Graphics","28","1",,"173","183",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118595457&doi=10.1109%2fTVCG.2021.3114856&partnerID=40&md5=c3bd7c0dfc304917bfb96ddb58903cf6","Infographic bar charts have been widely adopted for communicating numerical information because of their attractiveness and memorability. However, these infographics are often created manually with general tools, such as PowerPoint and Adobe Illustrator, and merely composed of primitive visual elements, such as text blocks and shapes. With the absence of chart models, updating or reusing these infographics requires tedious and error-prone manual edits. In this paper, we propose a mixed-initiative approach to mitigate this pain point. On one hand, machines are adopted to perform precise and trivial operations, such as mapping numerical values to shape attributes and aligning shapes. On the other hand, we rely on humans to perform subjective and creative tasks, such as changing embellishments or approving the edits made by machines. We encapsulate our technique in a PowerPoint add-in prototype and demonstrate the effectiveness by applying our technique on a diverse set of infographic bar chart examples. © 1995-2012 IEEE.","Automatic visualization; Graphic design; Infographics; Reusable templates","Data visualization; Semantics; Visualization; Automatic visualization; Bar chart; Graphic design; Image color analysis; Infographic; Mixed-initiative; Numerical information; Powerpoint; Reusable templates; Shape; Data mining",Article,Scopus,2-s2.0-85118595457
"Correia F.A., Almeida A.A.A., Nunes J.L., Santos K.G., Hartmann I.A., Silva F.A., Lopes H.","57211296575;57211291268;57211294320;57325205600;56644412200;57139569700;7006023298;","Fine-grained legal entity annotation: A case study on the Brazilian Supreme Court",2022,"Information Processing and Management","59","1","102794","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118578654&doi=10.1016%2fj.ipm.2021.102794&partnerID=40&md5=a9d7df90dfe89c7c1e11108290d348e2","The exploration of legal documents in the Brazilian Judiciary context lacks reliable annotated corpus to support the development of new Natural Language Process (NLP) applications. Therefore, this paper presents a step toward exploring legal decisions with Named Entity Recognition (NER) in the Brazilian Supreme Court (STF) context. We aim to present a case study on the fine-grained annotation task of legal decisions, performed by law students as annotators where two levels of nested legal entities were annotated. Nested entities mapped in a preliminary study composed of four coarser legal named entities and twenty-four nested ones (fine-grained). The final result is a corpus of 594 decisions published by the STF annotated by the 76 law students, those with the highest average inter-annotator agreement score. We also present two baselines for NER based on Conditional Random Fields (CRFs) and Bidirectional Long-Short Term Memory Networks (BiLSTMs). This corpus is the first of its kind, the most extensive corpus known in Portuguese dedicated for legal named entity recognition, open and available to better support further research studies in a similar context. © 2021 Elsevier Ltd","Annotated corpus in Portuguese; Brazilian Supreme Court; Legal documents; Manual annotation task; Named Entity Recognition","Law enforcement; Natural language processing systems; Annotated corpus in portuguese; Brazilian supreme court; Case-studies; Fine grained; Legal documents; Legal entities; Manual annotation; Manual annotation task; Named entity recognition; Supreme Court; Authentication",Article,Scopus,2-s2.0-85118578654
"Park W., Qureshi N.M.F., Shin D.R.","57194058144;57191380223;57325190700;","Pseudo nlp joint spam classification technique for big data cluster",2022,"Computers, Materials and Continua","71","1",,"517","535",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118576468&doi=10.32604%2fcmc.2022.021421&partnerID=40&md5=4be485e5632a7f02ae9dbf1e51e590ac","Spammail classification considered complex and error-prone task in the distributed computing environment. There are various available spammail classification approaches such as the naive Bayesian classifier, logistic regression and support vector machine and decision tree, recursive neural network, and long short-term memory algorithms. However, they do not consider the document when analyzing spam mail content. These approaches use the bagof- words method, which analyzes a large amount of text data and classifies features with the help of term frequency-inverse document frequency. Because there are many words in a document, these approaches consume a massive amount of resources and become infeasible when performing classification on multiple associated mail documents together. Thus, spam mail is not classified fully, and these approaches remain with loopholes. Thus, we propose a term frequency topic inverse document frequency model that considers the meaning of text data in a larger semantic unit by applying weights based on the document's topic. Moreover, the proposed approach reduces the scarcity problem through a frequency topic-inverse document frequency in singular value decomposition model. Our proposed approach also reduces the dimensionality, which ultimately increases the strength of document classification. Experimental evaluations show that the proposed approach classifies spam mail documents with higher accuracy using individual document-independent processing computation. Comparative evaluations show that the proposed approach performs better than the logistic regression model in the distributed computing environment, with higher document word frequencies of 97.05%, 99.17% and 96.59%. © 2022 Tech Science Press. All rights reserved.","Big data; Machine learning; NLP; Spam mail; TFT-IDF","Big data; Classification (of information); Decision trees; Information retrieval systems; Inverse problems; K-means clustering; Natural language processing systems; Neural networks; Semantics; Singular value decomposition; Text processing; Classification technique; Data clusters; Distributed computing environment; Error prone tasks; Inverse Document Frequency; Mail documents; Spam classification; Spam mails; Text data; TFT-IDF; Support vector machines",Article,Scopus,2-s2.0-85118576468
"Zhang G., Pan Y., Zhang L.","57218129987;57205735838;55271615200;","Deep learning for detecting building façade elements from images considering prior knowledge",2022,"Automation in Construction","133",,"104016","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118567340&doi=10.1016%2fj.autcon.2021.104016&partnerID=40&md5=02a040f771e120715de98a03bcda8703","Building façades elements detection plays a key point role in façade defects detection and street scene reconstruction tasks for sustainable city development. Although the artificial intelligence technology has made a breakthrough in image segmentation, it is nontrivial to directly apply standard deep learning approaches for building façade element detection. The main reason is that the existing semantic segmentation networks have a bad performance in predicting highly regularized shapes. This research develops a hieratical deep learning framework with a symmetric loss function to automatically detect building façade elements from images. The new framework contains two types of attention modules, namely, the spatial attention module, and the channel attention module. A new loss function is designed to integrate prior engineering knowledge, which can be used to force the detection of façade elements (e.g., windows, doors, concrete walls, and sunshades) to have a highly proportionate shape. The effectiveness of the developed approach is demonstrated in two public datasets. Experimental results indicate that the developed deep learning framework with a new loss function outperforms state-of-the-art models significantly, where the achieved a Mean Intersection over Union (IoU) on the Ecole Centrale Paris(ECP) dataset (81.9%) brings an improvement of 11.3% over Fully Convolutional Network (FCN) and 4.0% over Deepfaçade, respectively, and the achieved a Mean IoU on the ArtDeco dataset (85.6%) yields an improvement of 11.4% over FCN and 5.8% over Deepfaçade, respectively. Moreover, the developed approach is more practical and effective to detect regularized façade elements, where the detection of the wall components has an IoU of 93.6% on the ECP dataset and 88.6% on the ArtDeco dataset, respectively. Overall, the contribution from the technical aspect is to develop a hieratical deep learning framework consisting of attention modules together with the newly designed loss function and the prior engineering knowledge. The contribution from the practical aspect is to realize the automatic and accurate detection for various building façade elements in complex environments, which can be potentially helpful for the infrastructure monitoring and maintenance operation. © 2021 Elsevier B.V.","Deep learning; Façade elements detection; Objective function; Semantic segmentation","Deep learning; Facades; Image segmentation; Semantics; Urban growth; Walls (structural partitions); Building facades; Deep learning; Elements detection; Engineering knowledge; Facade element detection; Facade elements; Learning frameworks; Loss functions; Objective functions; Semantic segmentation; Semantic Segmentation",Article,Scopus,2-s2.0-85118567340
"Gao L., Lei Y., Zeng P., Song J., Wang M., Shen H.T.","56611089900;57325110500;57204475579;57205085174;55533321600;7404523209;","Hierarchical Representation Network with Auxiliary Tasks for Video Captioning and Video Question Answering",2022,"IEEE Transactions on Image Processing","31",,,"202","215",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118527354&doi=10.1109%2fTIP.2021.3120867&partnerID=40&md5=b778072f63780694de2e47c0ce0a8eca","Recently, integrating vision and language for in-depth video understanding e.g., video captioning and video question answering, has become a promising direction for artificial intelligence. However, due to the complexity of video information, it is challenging to extract a video feature that can well represent multiple levels of concepts i.e., objects, actions and events. Meanwhile, content completeness and syntactic consistency play an important role in high-quality language-related video understanding. Motivated by these, we propose a novel framework, named Hierarchical Representation Network with Auxiliary Tasks (HRNAT), for learning multi-level representations and obtaining syntax-aware video captions. Specifically, the Cross-modality Matching Task enables the learning of hierarchical representation of videos, guided by the three-level representation of languages. The Syntax-guiding Task and the Vision-assist Task contribute to generating descriptions which are not only globally similar to the video content, but also syntax-consistent to the ground-truth description. The key components of our model are general and they can be readily applied to both video captioning and video question answering tasks. Performances for the above tasks on several benchmark datasets validate the effectiveness and superiority of our proposed method compared with the state-of-the-art methods. Codes and models are also released https://github.com/riesling00/HRNAT. © 1992-2012 IEEE.","auxiliary task; hierarchical structure; representation learning; Video captioning; video question answering","Computer vision; Job analysis; Neural networks; Semantic Web; Syntactics; Auxiliary task; Hierarchical representation; Hierarchical structures; Question Answering; Representation learning; Task analysis; Video captioning; Video information; Video question answering; Video understanding; Semantics; article; artificial intelligence; human; human experiment; language; learning; videorecording; vision",Article,Scopus,2-s2.0-85118527354
"Kim L., Smith D.S., Hofstra B., McFarland D.A.","57226682251;57322934500;56688609800;57203382561;","Gendered knowledge in fields and academic careers",2022,"Research Policy","51","1","104411","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118499360&doi=10.1016%2fj.respol.2021.104411&partnerID=40&md5=def5a81f2b407e37da97bf03ca96f566","Women and men often contribute differently to research knowledge. Do differences in these contributions partially explain disparities in academic career outcomes? We explore this by looking at how gender is embodied in research language, and then ascertain whether the adoption of more gendered research language affects career outcomes beyond the researcher's attributes. We identify different forms of gendered knowledge—gender referents (explicit references to sex and gender) and gender-associated terms (words that are implicitly associated with women or men researchers)—by applying natural language processing techniques to nearly one million doctoral dissertations published in the United States between 1980 and 2010. We then determine whether employing gender referents and gender-associated terms affects the course of PhDs’ ensuing careers. We find women researchers have lower chances of securing academic positions than men in every field; explicit references to women as research subjects are modestly rewarded in comparison to references to men; and more career opportunities are afforded to research knowledge associated with men. These results suggest that academia is slowly correcting the traditional and explicit bias of studying men at the exclusion of women. Still, there remains a stronger implicit bias against knowledge associated with women scholars. We discuss relative differences between humanities and social sciences versus natural sciences, technology, engineering, and math, as well as potential treatments for offsetting bias in those fields. © 2021 Elsevier B.V.","Academic career; Gender; Gendered knowledge; Inequality; Natural language processing; Science of science","Social sciences computing; Academic careers; Career opportunities; Doctoral dissertations; Gendered knowledge; Humanities and social science; In-field; Inequality; Language processing techniques; Research subjects; Science of science; Natural language processing systems",Article,Scopus,2-s2.0-85118499360
"Lyu Y., Wang Z., Ren Z., Ren P., Chen Z., Liu X., Li Y., Li H., Song H.","57323261100;57253673900;53985046100;56181249600;15749859500;57241489600;57205346557;36175959900;57224727334;","Improving legal judgment prediction through reinforced criminal element extraction",2022,"Information Processing and Management","59","1","102780","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118481517&doi=10.1016%2fj.ipm.2021.102780&partnerID=40&md5=75831b489e6cc026d81f9f52f0fd5d8b","Legal text mining is targeted at automatically analyzing the texts in the legal domain by employing various natural language processing techniques and has attracted enormous attention from the NLP community. As one of the most crucial tasks of legal text mining, Legal Judgment Prediction (LJP) aims to automatically predict judgment results (e.g., applicable law articles, charges, and terms of penalty) according to fact descriptions on law cases and becomes a promising application of artificial intelligence techniques. Unfortunately, ambiguous fact descriptions and law articles often appear due to a great number of shared words and legal concepts. Prior works are proposed to partially address these problems, focusing on introducing additional attributes to distinguish similar fact descriptions, or differentiating confusing law articles by grouping and distilling law articles. However, existing works still face two severe challenges: (1) indistinguishable fact descriptions with different criminals and targets and (2) misleading law articles with highly similar TF–IDF representations, both of which lead to serious misjudgments for the LJP task. In this paper, we present a novel reinforcement learning (RL) based framework, named Criminal Element Extraction Network (CEEN), to handle above challenges simultaneously. In CEEN, we propose four types of discriminative criminal elements, including the criminal, target, intentionality, and criminal behavior. To discriminate ambiguous fact descriptions, an reinforcement learning based extractor is designed to accurately locate elements for different cases. To enhance law article predictions, distinctive element representations are constructed for each type of criminal element. Finally, with the input of element representations, a multi-task predictor is adopted for the judgment predictions. Experimental results on real-world datasets show that extracting criminal elements is highly useful for predicting the judgment results. © 2021 Elsevier Ltd","Attention mechanism; Criminal element extraction; Legal judgment prediction; Legal text mining; Neural networks; Reinforcement learning","Crime; Data mining; Extraction; Forecasting; Learning algorithms; Natural language processing systems; Attention mechanisms; Criminal element extraction; Element extraction; Fact descriptions; Legal judgements; Legal judgment prediction; Legal text mining; Legal texts; Neural-networks; Text-mining; Reinforcement learning",Article,Scopus,2-s2.0-85118481517
"Bhatti M.S., Ullah A., Latip R., Sohail A., Riaz A., Hassan R.","55652486800;57322127600;6506513485;36459180300;57322365800;56575985100;","Benchmarking performance of document level classification and topic modeling",2022,"Computers, Materials and Continua","71","1",,"125","141",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118445438&doi=10.32604%2fcmc.2022.020083&partnerID=40&md5=4ac10a6a1df97fbc762f8b29f7fa0f4e","Text classification of low resource language is always a trivial and challenging problem. This paper discusses the process of Urdu news classification and Urdu documents similarity. Urdu is one of the most famous spoken languages in Asia. The implementation of computational methodologies for text classification has increased over time. However, Urdu language has not much experimented with research, it does not have readily available datasets, which turn out to be the primary reason behind limited research and applying the latest methodologies to the Urdu. To overcome these obstacles, a mediumsized dataset having six categories is collected from authentic Pakistani news sources. Urdu is a rich but complex language. Text processing can be challenging for Urdu due to its complex features as compared to other languages. Term frequency-inverse document frequency (TFIDF) based term weighting scheme for extracting features, chi-2 for selecting essential features, and Linear discriminant analysis (LDA) for dimensionality reduction have been used. TFIDF matrix and cosine similarity measure have been used to identify similar documents in a collection and find the semantic meaning of words in a document FastText model has been applied. The training-test split evaluation methodology is used for this experimentation, which includes 70% for training data and 30% for testing data. State-of-the-art machine learning and deep dense neural network approaches forUrdu news classification have been used. Finally, we trained Multinomial Naïve Bayes, XGBoost, Bagging, and Deep dense neural network. Bagging and deep dense neural network outperformed the other algorithms. The experimental results show that deep dense achieves 92.0% mean f1 score, and Bagging 95.0% f1 score. © 2022 Tech Science Press. All rights reserved.","Classification; Cosine similarity; Deep neural network; Gradient boosting; Linear discriminant analysis; Machine learning; Natural language processing; Sparse matrix; TFIDF","Benchmarking; Classification (of information); Complex networks; Deep neural networks; Information retrieval systems; Inverse problems; Learning algorithms; Matrix algebra; Semantics; Text processing; Classification models; Cosine similarity; F1 scores; Gradient boosting; Linear discriminant analyze; Neural-networks; Performance; Sparse matrices; Term frequencyinverse document frequency (TF-IDF); Topic Modeling; Natural language processing systems",Article,Scopus,2-s2.0-85118445438
"Li S., Ao X., Pan F., He Q.","57211409102;57197191379;57204465575;26643590900;","Learning policy scheduling for text augmentation",2022,"Neural Networks","145",,,"121","127",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118362146&doi=10.1016%2fj.neunet.2021.09.028&partnerID=40&md5=762f5ccf94a8f570d446f098e2a015e5","When training deep learning models, data augmentation is an important technique to improve the performance and alleviate overfitting. In natural language processing (NLP), existing augmentation methods often use fixed strategies. However, it might be preferred to use different augmentation policies in different stage of training, and different datasets may require different augmentation policies. In this paper, we take dynamic policy scheduling into consideration. We design a search space over augmentation policies by integrating several common augmentation operations. Then, we adopt a population based training method to search the best augmentation schedule. We conduct extensive experiments on five text classification and two machine translation tasks. The results show that the optimized dynamic augmentation schedules achieve significant improvements against previous methods. © 2021 Elsevier Ltd","Data augmentation; Text classification","Classification (of information); Deep learning; Natural language processing systems; Text processing; Augmentation methods; Data augmentation; Different stages; Fixed strategy; Learning models; Learning policy; Modeling data; Overfitting; Performance; Text classification; Scheduling; article; learning; human; machine learning; natural language processing; policy; Humans; Machine Learning; Natural Language Processing; Policy",Article,Scopus,2-s2.0-85118362146
"Pearce C., McLeod A., Supple J., Gardner K., Proposch A., Ferrigi J.","7101785869;8309403500;57320307500;37063277700;57215021081;57207250553;","Responding to COVID-19 with real-time general practice data in Australia",2022,"International Journal of Medical Informatics","157",,"104624","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118338040&doi=10.1016%2fj.ijmedinf.2021.104624&partnerID=40&md5=1ba6184808a8cfee9b3991458c67ec5f","Introduction: As SARS-CoV-2 spread around the world, Australia was no exception. Part of the Australian response was a robust primary care approach, involving changes to care models (including telehealth) and the widespread use of data to inform the changes. This paper outlines how a large primary care database responded to provide real-time data to inform policy and practice. Simply extracting the data is not sufficient. Understanding the data is. The POpulation Level Analysis and Reporting (POLAR) program is designed to use GP data for multiple objectives and is built on a pre-existing engagement framework established over a fifteen-year period. Initially developed to provide QA activities for general practices and population level data for General Practice support organisations, the POLAR platform has demonstrated the critical ability to design and deploy real-time data analytics solutions during the COVID-19 pandemic for a variety of stakeholders including state and federal government agencies. Methods: The system extracts and processes data from over 1,300 general practices daily. Data is de-identified at the point of collection and encrypted before transfer. Data cleaning for analysis uses a variety of techniques, including Natural Language Processing and coding of free text information. The curated dataset is then distilled into several analytic solutions designed to address specific areas of investigation of interest to various stakeholders. One such analytic solution was a model we created that used multiple data inputs to rank patient geographic areas by the likelihood of a COVID-19 outbreak. The model utilised pathology ordering, COVID-19 related diagnoses, indication of COVID-19 related concern (via progress notes) and also incorporated state based actual confirmed case figures. Results: Using the methods described, we were able to deliver real-time data feeds to practices, Primary Health Networks (PHN) and other agencies. In addition, we developed a COVID-19 geographic risk stratification based on local government areas (LGAs) to pro-actively inform the primary care response. Providing PHNs with a list of geographic priority hotspots allowed for better targeting and response of Personal Protective Equipment allocation and pop-up clinic placement. Conclusions: The program summarised here demonstrates the ability of a well-designed system underpinned by accurate and reliable data, to respond in real-time to a rapidly evolving public health emergency in a way which supports and enhances the health system response. © 2021","COVID-19; Data quality; Predictive tools; Primary care; SNOMED","Data Analytics; Decision trees; Diseases; Medical computing; Natural language processing systems; Population statistics; Protective clothing; SARS; Analytic solution; Australia; COVID-19; Data quality; Population levels; Predictive tools; Primary care; Real- time; Real-time data; SNOMED; Data mining; dipeptidyl carboxypeptidase inhibitor; hydroxychloroquine; ivermectin; Article; Australia; coronavirus disease 2019; data quality; general practice; government; human; natural language processing; pandemic; patient monitoring; primary medical care; risk assessment; Systematized Nomenclature of Medicine; time series analysis; Australia; epidemiology; Australia; COVID-19; General Practice; Humans; Pandemics; SARS-CoV-2",Article,Scopus,2-s2.0-85118338040
"Eligüzel N., Çetinkaya C., Dereli T.","57218846692;57190388669;7003495303;","Application of named entity recognition on tweets during earthquake disaster: a deep learning-based approach",2022,"Soft Computing","26","1",,"395","421",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118215772&doi=10.1007%2fs00500-021-06370-4&partnerID=40&md5=7d6c4f4773148ba7e82231d6e9ecc145","Twitter is an intensely utilized platform for disaster events and emergencies. Therefore, Twitter is an important resource for providing the essential information. Named entity recognition (NER), which is the process of determining the elementary units in a text and classifying them with pre-defined categories, plays a significant role to extract essential and usefulness information. However, NER is a challenging task due to the utilized informal text in the Twitter platform such as grammatical errors and nonstandard abbreviations. In this paper, recurrent neural network (RNN)-based approaches considering diversity of activation functions and optimization functions with NER tools are utilized to extract named entities such as organization, person, and location from the tweets. Inputs for RNN models are provided via two different NER tools which are natural language toolkit (NLTK) and general architecture for text engineering (Gate). Then, pre-labeled data are trained via GloVe word embedding technique, and RNN model variants such as LSTM, BLSTM, and GRU are demonstrated. Therefore, outperforming models among RNN variants are presented for predicting named entities. Yellowbrick interpreter is used for evaluation of the proposed method and Wilcoxon signed-rank test are applied on results of two different data sets to demonstrate consistency of the proposed method. In addition, comparison is made with existing machine learning methods. The experiments by utilizing the Nepal earthquake Twitter data set show that the RNN-based approaches achieve good results in finding named entities. In emergencies, the results of this paper can help in reducing the efforts of event location detection and provide better disaster management. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Disaster; Earthquake; Named entity recognition; Recurrent Neural Network; Twitter","Character recognition; Classification (of information); Disaster prevention; Earthquakes; Long short-term memory; Natural language processing systems; Social networking (online); Text processing; Activation functions; Data set; Earthquake; Earthquake disaster; Grammatical errors; Learning-based approach; Named entities; Named entity recognition; Network-based approach; Recurrent neural network model; Disasters",Article,Scopus,2-s2.0-85118215772
"Balasubramanian V., Vivekanandhan S., Mahadevan V.","57221282102;36606870800;57312627800;","Pandemic tele-smart: a contactless tele-health system for efficient monitoring of remotely located COVID-19 quarantine wards in India using near-field communication and natural language processing system",2022,"Medical and Biological Engineering and Computing","60","1",,"61","79",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117934835&doi=10.1007%2fs11517-021-02456-1&partnerID=40&md5=c7f852cf9afb94f9bbc3bee3a8693927","Efficient remote monitoring of the patient infected with coronavirus without spread to healthcare workers is the need of the hour. An effectual and faster communication system must be established wherein the healthcare workers at the remote quarantine ward can communicate with healthcare professionals present in specialty hospitals. Incidentally, there is a need to establish a contactless smart cloud-based connection between a specialty hospital and quarantine wards during pandemic situation. This paper proposes an initial contactless web-based tele-health clinical decision support system that integrates near-field communication (NFC) tags and a smart cloud-based structuring tool that enables the quick diagnosis of patients with COVID-19 symptoms and monitors the remotely located quarantine wards during the recent pandemic. The proposed framework consists of three-stages: (i) contactless health parameter extraction from the patient using an NFC tag; (ii) converting medical report into digital text using optical character recognition algorithm and extracting values of relevant medical-parameters using natural language processing; and (iii) smart visualization of key medical parameters. The accuracy of the proposed system from NFC reader until analysis using a novel structuring algorithm deployed in the cloud is more than 94%. Several capabilities of the proposed web-based system were compared with similar systems and tested in an authentic mock clinical setup, and the physicians found that the system is reliable and user friendly. Graphical abstract: [Figure not available: see fulltext.] © 2021, International Federation for Medical and Biological Engineering.","Clinical decision support systems; Electronic medical records; Mobile health; Remote consultation; Telemedicine","Abstracting; Artificial intelligence; Decision support systems; Hospitals; Medical computing; mHealth; Natural language processing systems; Optical character recognition; Telemedicine; Websites; Clinical decision support systems; Cloud-based; Communication languages; Contact less; Efficient monitoring; Health systems; Healthcare workers; Near-field communication; Remote consultation; Telehealth; Diagnosis",Article,Scopus,2-s2.0-85117934835
"Lu Z., Li X., Liu Y., Zhou C., Cui J., Wang B., Zhang M., Su J.","57209394254;57221401285;57196311263;57211746947;57217248343;57161232700;15019988000;55157800300;","Exploring Multi-Stage Information Interactions for Multi-Source Neural Machine Translation",2022,"IEEE/ACM Transactions on Audio Speech and Language Processing","30",,,"562","570",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117854570&doi=10.1109%2fTASLP.2021.3120592&partnerID=40&md5=6249248b2df77ae4383e2f32494530a2","Existing studies for multi-source neural machine translation (NMT) either separately model different source sentences or resort to the conventional single-source NMT by simply concatenating all source sentences. However, there exist two drawbacks in these approaches. First, they ignore the explicit word-level semantic interactions between source sentences, which have been shown effective in the embeddings of multilingual texts. Second, multiple source sentences are simultaneously encoded by an NMT model, which is unable to fully exploit the semantic information of each source sentence. In this paper, we explore multi-stage information interactions for multi-source NMT. Specifically, we first propose a multi-source NMT model that performs information interactions at the encoding stage. Its encoder contains multiple semantic interaction layers, each of which sequentially consists of (1) monolingual semantic interaction sub-layer, which is based on the self-attention mechanism and used to learn word-level monolingual contextual representations of source sentences, and (2) cross-lingual semantic interaction sub-layer, which leverages word alignments to perform fine-grained semantic transitions among hidden states of different source sentences. Furthermore, at the training stage, we introduce a mutual distillation based training framework, where single-source models and ours perform information interactions. Such framework can fully exploit the semantic information of each source sentence to enhance our model. Extensive experimental results on the WMT14 English-German-French dataset show our method exhibits significant improvements upon competitive baselines. © 2014 IEEE.","Information interactions; Mutual distillation; Natural language processing; Neural machine translation","Computational linguistics; Computer aided language translation; Distillation; Encoding (symbols); Natural language processing systems; Semantics; Signal encoding; Speech processing; Adaptation models; Encodings; Information interaction; Machine translation models; Multi-Sources; Multi-stages; Mutual distillation; Semantic interactions; Transformer; Word level; Machine translation; Neural machine translation",Article,Scopus,2-s2.0-85117854570
"Kobayashi M., Fujita S., Wada T.","55496615900;57310108000;57310108100;","Aesthetic design based on the analysis of questionnaire results using deep learning techniques",2022,"Computer-Aided Design and Applications","19","3",,"602","611",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117820667&doi=10.14733%2fCADAPS.2022.602-611&partnerID=40&md5=e21453cdc5471a585d2029189d6878ba","In recent years, deep learning is attracting a lot of attention. Deep learning is a class of machine learning algorithms based on artificial neural networks. Deep learning uses multiple layers to extract higher-level features progressively and automatically from the raw input. Various types of deep learning models have been developed and applied to computer vision, speech recognition, natural language processing, audio recognition, self-driving car, board game, etc. In the field of sensitivity engineering, research that takes advantage of the features of deep learning has also been carried out. In this paper, a new aesthetic design method that creates new designs by analyzing customer's preferences using deep learning methods is proposed. More specifically, questionnaire investigations are carried out to collect customer’s preferences for existing products and a CNN network that infer customer's preferences from product images is trained using questionnaire results. Aesthetic elements closely related to the customer's “like” evaluations are then identified and catalogued by analyzing the trained network using Grad-CAM and semantic segmentation. New designs are finally created by selecting and combining the favorite aesthetic elements from the catalog. In the case study, the proposed method was applied to a chair design. A lot of chair images were collected from the internet and subjects rated their preferences for them. Favorite aesthetic elements were then extracted and catalogued from the evaluation results and new chair designs were created by combining them. Created chair designs were finally rated by subjects and the effectiveness of the proposed method was confirmed. © 2022 CAD Solutions, LLC, http://www.cad-journal.net.","Aesthetic Design; Deep Learning; Grad-CAM; Kansei Engineering; Semantic Segmentation","Computer games; Deep learning; Design; Learning algorithms; Natural language processing systems; Neural networks; Sales; Semantics; Speech recognition; Surveys; Aesthetics designs; Analysis of questionnaire; Customer preferences; Deep learning; Grad-CAM; Kansei Engineering; Learning techniques; Machine learning algorithms; Multiple layers; Semantic segmentation; Semantic Segmentation",Article,Scopus,2-s2.0-85117820667
"Rebuffel C., Roberti M., Soulier L., Scoutheeten G., Cancelliere R., Gallinari P.","57194442733;56492520000;55301159700;57216581845;6603417372;57207514183;","Controlling hallucinations at word level in data-to-text generation",2022,"Data Mining and Knowledge Discovery","36","1",,"318","354",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117731062&doi=10.1007%2fs10618-021-00801-4&partnerID=40&md5=3ae8751132518ac9edb034103a421b7a","Data-to-Text Generation (DTG) is a subfield of Natural Language Generation aiming at transcribing structured data in natural language descriptions. The field has been recently boosted by the use of neural-based generators which exhibit on one side great syntactic skills without the need of hand-crafted pipelines; on the other side, the quality of the generated text reflects the quality of the training data, which in realistic settings only offer imperfectly aligned structure-text pairs. Consequently, state-of-art neural models include misleading statements –usually called hallucinations—in their outputs. The control of this phenomenon is today a major challenge for DTG, and is the problem addressed in the paper. Previous work deal with this issue at the instance level: using an alignment score for each table-reference pair. In contrast, we propose a finer-grained approach, arguing that hallucinations should rather be treated at the word level. Specifically, we propose a Multi-Branch Decoder which is able to leverage word-level labels to learn the relevant parts of each training instance. These labels are obtained following a simple and efficient scoring procedure based on co-occurrence analysis and dependency parsing. Extensive evaluations, via automated metrics and human judgment on the standard WikiBio benchmark, show the accuracy of our alignment labels and the effectiveness of the proposed Multi-Branch Decoder. Our model is able to reduce and control hallucinations, while keeping fluency and coherence in generated texts. Further experiments on a degraded version of ToTTo show that our model could be successfully used on very noisy settings. © 2021, The Author(s).","Controlled text generation; Data-to-text generation; Hallucinations","Decoding; Natural language processing systems; Controled text generation; Data-to-text generation; Hallucination; Natural language generation; Natural languages; Structured data; Subfields; Text generations; Word level; Syntactics",Article,Scopus,2-s2.0-85117731062
"Lin J., Liu Y., Cleland-Huang J.","57200513163;57200512589;6506741859;","Information retrieval versus deep learning approaches for generating traceability links in bilingual projects",2022,"Empirical Software Engineering","27","1","5","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117709914&doi=10.1007%2fs10664-021-10050-0&partnerID=40&md5=fb18745b51cfea856892a90ae4156f24","Software traceability links are established between diverse artifacts of the software development process in order to support tasks such as compliance analysis, safety assurance, and requirements validation. However, practice has shown that it is difficult and costly to create and maintain trace links in non-trivially sized projects. For this reason, many researchers have proposed and evaluated automated approaches based on information retrieval and deep-learning. Generating trace links automatically can also be challenging – especially in multi-national projects which include artifacts written in multiple languages. The intermingled language use can reduce the efficiency of automated tracing solutions. In this work, we analyze patterns of intermingled language that we observed in several different projects, and then comparatively evaluate different tracing algorithms. These include Information Retrieval techniques, such as the Vector Space Model (VSM), Latent Semantic Indexing (LSI), Latent Dirichlet Allocation (LDA), and various models that combine mono- and cross-lingual word embeddings with the Generative Vector Space Model (GVSM), and a deep-learning approach based on a BERT language model. Our experimental analysis of trace links generated for 14 Chinese-English projects indicates that our MultiLingual Trace-BERT approach performed best in large projects with close to 2-times the accuracy of the best IR approach, while the IR-based GVSM with neural machine translation and a monolingual word embedding performed best on small projects. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","BERT; Cross-lingual information retrieval; Generalized Vector Space Model; Software traceability","Deep learning; Embeddings; Information retrieval; Neural machine translation; Semantics; Software design; Statistics; BERT; Bilinguals; Cross-lingual information retrieval; Embeddings; Generalized vector space model; Learning approach; Software development process; Software traceability; Traceability links; Vector space models; Vector spaces",Article,Scopus,2-s2.0-85117709914
"Rabassa V., Sabri O., Spaletta C.","57303764500;36451215800;57303764600;","Conversational commerce: Do biased choices offered by voice assistants’ technology constrain its appropriation?",2022,"Technological Forecasting and Social Change","174",,"121292","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117608828&doi=10.1016%2fj.techfore.2021.121292&partnerID=40&md5=00489843a1b39d6c3452a5d08b6ce66e","Conversational commerce, which relies on algorithm-based voice assistants, is still an emerging technology that changes how consumers shop. Based on natural language processing (NLP) technology and artificial intelligence (AI) systems, consumers can now purchase products and services online by making use of voice assistants such as Google Assistant, Amazon's Alexa, and Apple's Siri. However, the economic literature and international organization reports have identified some problems with conversational commerce technology that may constrain its appropriation, demonstrating that algorithm-based voice assistants can lead to exclusionary conduct and nonoptimal choices for consumers. In that context, the research explores consumers’ perception of conversational commerce and product choice offers delivered by voice assistants. The paper considers how algorithm-based voice assistants can lead to perceived biased offers and identifies different strategies that could be implemented by consumers to overcome the negative side effects of algorithms and support their appropriation. The study has strong implications for policymakers and conversational commerce platform owners. © 2021","Algorithm; Biased choice; Conversational commerce; Technology appropriation; Voice assistant","Natural language processing systems; Online systems; Artificial intelligence systems; Biased choice; Conversational commerce; Emerging technologies; Google+; International organizations; Processing technologies; Product and services; Technology appropriations; Voice assistant; Commerce; algorithm; artificial intelligence; consumption behavior; electronic commerce; information and communication technology; perception; policy making; technology adoption",Article,Scopus,2-s2.0-85117608828
"Tan S., Guo D., Liu H., Zhang X., Sun F.","57219732915;57274515600;35272617500;57221516078;55555423500;","Embodied scene description",2022,"Autonomous Robots","46","1",,"21","43",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117441052&doi=10.1007%2fs10514-021-10014-9&partnerID=40&md5=79994fcd408ac6c4d2eb328fcdbc201c","Embodiment is an important characteristic for all intelligent agents, while existing scene description tasks mainly focus on analyzing images passively and the semantic understanding of the scenario is separated from the interaction between the agent and the environment. In this work, we propose the Embodied Scene Description, which exploits the embodiment ability of the agent to find an optimal viewpoint in its environment for scene description tasks. A learning framework with the paradigms of imitation learning and reinforcement learning is established to teach the intelligent agent to generate corresponding sensorimotor activities. The proposed framework is tested on both the AI2Thor dataset and a real-world robotic platform for different scene description tasks, demonstrating the effectiveness and scalability of the developed method. Also, a mobile application is developed, which can be used to assist visually-impaired people to better understand their surroundings. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Embodied learning; Embodied perception; Environment exploration; Scene description","Reinforcement learning; Semantics; Embodied learning; Embodied perceptions; Environment exploration; Imitation learning; Learning frameworks; Real-world; Robotic platforms; Scene description; Semantics understanding; Sensorimotor activities; Intelligent agents",Article,Scopus,2-s2.0-85117441052
"Zuo R., Zhang G., Zhang R., Jia X.","57299417700;57217155399;56420791800;7201933692;","A Deformable Attention Network for High-Resolution Remote Sensing Images Semantic Segmentation",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117266967&doi=10.1109%2fTGRS.2021.3119537&partnerID=40&md5=1e01558c6edf605c0f73db7654fde6ab","Deformable convolutional networks (DCNs) can mitigate the inherent limited geometric transformation. We reformulate the spatialwise attention mechanism using DCNs in this article for semantic segmentation of high-resolution remote sensing (HRRS) images. It combines the sparse spatial sampling strategy and the long-range relationship modeling capability, namely, deformable attention module (DAM). Such locality awareness, more adaptable to HRRS image structures, can capture each pixel's neighboring structural information. A reasonable multiscale deformable attention net (MDANet) is designed for the HRRS image semantic segmentation with a slightly increased computational cost based on the proposed DAM. Specifically, standard convolutional layers in the raw ResNet50 are equipped with a DAM to control sampling over a broader range of feature levels and aggregate multiscale context information. The experimental results evaluated on Vaihingen and DeepGlobe Land Cover Classification datasets show that the performance accuracy of MDANet is improved by 7.77% and 8.45% compared with the backbone network (ResNet50) in terms of Miou evaluation, respectively. Furthermore, a DAM can perform better than a global spatial attention mechanism with less computation on the 3 × 64 × 64 feature map. In addition, the added ablation studies demonstrate the effectiveness and efficiency of the DAM and multiscale strategy, respectively. Moreover, the sensitivity of critical hyperparameters is analyzed. © 1980-2012 IEEE.","Deformable convolutional networks (DCNs); high-resolution remote sensing (HRRS) images; multiscale; spatialwise attention mechanism","Classification (of information); Convolution; Dams; Deformation; Image segmentation; Mathematical transformations; Remote sensing; Semantic Web; Semantics; Attention mechanisms; Convolutional networks; Deformable convolutional network; Features extraction; High-resolution remote sensing images; Images segmentations; Multi-scales; Neural-networks; Residual neural network; Spatial-wise attention mechanism; Task analysis; Semantic Segmentation; image analysis; remote sensing; sampling; semantic standardization",Article,Scopus,2-s2.0-85117266967
"Mittal A., Shivakumara P., Pal U., Lu T., Blumenstein M.","57218700913;8277294200;57200742116;27169293700;56243577200;","A new method for detection and prediction of occluded text in natural scene images",2022,"Signal Processing: Image Communication","100",,"116512","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117151152&doi=10.1016%2fj.image.2021.116512&partnerID=40&md5=36bbacc9775e41380dbbd1adcfc0cc88","Text detection from natural scene images is an active research area for computer vision, signal, and image processing because of several real-time applications such as driving vehicles automatically and tracing person behaviors during sports or marathon events. In these situations, there is a high probability of missing text information due to the occlusion of different objects/persons while capturing images. Unlike most of the existing methods, which focus only on text detection by ignoring the effect of missing texts, this work detects and predicts missing texts so that the performance of the OCR improves. The proposed method exploits the property of DCT for finding significant information in images by selecting multiple channels. For chosen DCT channels, the proposed method studies texture distribution based on statistical measurement to extract features. We propose to adopt Bayesian classifier for categorizing text pixels using extracted features. Then a deep learning model is proposed for eliminating false positives to improve text detection performance. Further, the proposed method employs a Natural Language Processing (NLP) model for predicting missing text information by using detected and recognition texts. Experimental results on our dataset, which contains texts occluded by objects, show that the proposed method is effective in predicting missing text information. To demonstrate the effectiveness and objectiveness of the proposed method, we also tested it on the standard datasets of natural scene images, namely, ICDAR 2017-MLT, Total-Text, and CTW1500. © 2021","Bayesian classifier; DCT channels; Natural language processing; Text detection; Text prediction; Text restoration","Character recognition; Deep learning; Image classification; Image compression; Image reconstruction; Natural language processing systems; Textures; Bayesian classifier; DCT channel; Natural scene images; Research areas; Signal and image processing; Text detection; Text information; Text prediction; Text restoration; Vision processing; Forecasting",Article,Scopus,2-s2.0-85117151152
"Al-Besher A., Kumar K., Sangeetha M., Butsa T.","57211663260;57207948054;57315861900;57295188400;","BERT for conversational question answering systems using semantic similarity estimation",2022,"Computers, Materials and Continua","70","3",,"4763","4780",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117028253&doi=10.32604%2fcmc.2022.021033&partnerID=40&md5=974186f8797fc2569da2c7ee9e1d0122","Most of the questions from users lack the context needed to thoroughly understand the problem at hand, thus making the questions impossible to answer. Semantic Similarity Estimation is based on relating user’s questions to the context from previous Conversational Search Systems (CSS) to provide answers without requesting the user’s context. It imposes constraints on the time needed to produce an answer for the user. The proposed model enables the use of contextual data associated with previous Conversational Searches (CS). While receiving a question in a new conversational search, the model determines the question that refers to more past CS. The model then infers past contextual data related to the given question and predicts an answer based on the context inferred without engaging in multi-turn interactions or requesting additional data from the user for context. This model shows the ability to use the limited information in user queries for best context inferences based on Closed-Domain-based CS and Bidirectional Encoder Representations from Transformers for textual representations. © 2022 Tech Science Press. All rights reserved.","BERT; Context inference; Conversational search; Multi-turn interactions; Semantic similarity estimation; User intent","Semantic Web; BERT; Context inference; Conversational search; Multi-turn; Multi-turn interaction; Question answering systems; Semantic similarity; Semantic similarity estimation; Similarity estimation; User intent; Semantics",Article,Scopus,2-s2.0-85117028253
"Kumar S., Sastry H.G., Marriboyina V., Alshazly H., Idris S.A., Verma M., Kaur M.","57301710800;57221199086;57209251058;56537040500;57226267577;57226164201;57209860887;","Semantic information extraction from multi-corpora using deep learning",2022,"Computers, Materials and Continua","70","3",,"5021","5038",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117026691&doi=10.32604%2fcmc.2022.021149&partnerID=40&md5=f626133c9ee7051018a0d5770853a96d","Information extraction plays a vital role in natural language processing, to extract named entities and events from unstructured data. Due to the exponential data growth in the agricultural sector, extracting significant information has become a challenging task. Though existing deep learning-based techniques have been applied in smart agriculture for crop cultivation, crop disease detection, weed removal, and yield production, still it is difficult to find the semantics between extracted information due to unswerving effects of weather, soil, pest, and fertilizer data. This paper consists of two parts. An initial phase, which proposes a data preprocessing technique for removal of ambiguity in input corpora, and the second phase proposes a novel deep learning-based long short-term memory with rectification in Adam optimizer and multilayer perceptron to find agricultural-based named entity recognition, events, and relations between them. The proposed algorithm has been trained and tested on four input corpora i.e., agriculture, weather, soil, and pest & fertilizers. The experimental results have been compared with existing techniques and it was observed that the proposed algorithm outperforms Weighted-SOM, LSTM+RAO, PLR-DBN, KNN, and Naïve Bayes on standard parameters like accuracy, sensitivity, and specificity. © 2022 Tech Science Press. All rights reserved.","Agriculture; Deep learning; Information extraction; Soil; Weather","Computational linguistics; Crops; Cultivation; Data mining; Information retrieval; Long short-term memory; Natural language processing systems; Semantics; Soils; Agricultural sector; Crop cultivation; Deep learning; Exponentials; Information extraction; Named entities; Semantic information extractions; Smart agricultures; Unstructured data; Weather; Fertilizers",Article,Scopus,2-s2.0-85117026691
"Alrumiah S.S., Al-Shargabi A.A.","57221496350;57221589260;","Educational videos subtitles’ summarization using latent dirichlet allocation and length enhancement",2022,"Computers, Materials and Continua","70","3",,"6205","6221",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117008186&doi=10.32604%2fcmc.2022.021780&partnerID=40&md5=acd19436ce8618098ea7a98c861d8fa5","Nowadays, people use online resources such as educational videos and courses. However, such videos and courses are mostly long and thus, summarizing them will be valuable. The video contents (visual, audio, and subtitles) could be analyzed to generate textual summaries, i.e., notes. Videos’ subtitles contain significant information. Therefore, summarizing subtitles is effective to concentrate on the necessary details. Most of the existing studies used Term Frequency–Inverse Document Frequency (TF-IDF) and Latent Semantic Analysis (LSA) models to create lectures’ summaries. This study takes another approach and applies Latent Dirichlet Allocation (LDA), which proved its effectiveness in document summarization. Specifically, the proposed LDA summarization model follows three phases. The first phase aims to prepare the subtitle file for modelling by performing some preprocessing steps, such as removing stop words. In the second phase, the LDA model is trained on subtitles to generate the keywords list used to extract important sentences. Whereas in the third phase, a summary is generated based on the keywords list. The generated summaries by LDA were lengthy; thus, a length enhancement method has been proposed. For the evaluation, the authors developed manual summaries of the existing “EDUVSUM” educational videos dataset. The authors compared the generated summaries with the manual-generated outlines using two methods, (i) Recall-Oriented Understudy for Gisting Evaluation (ROUGE) and (ii) human evaluation. The performance of LDA-based generated summaries outperforms the summaries generated by TF-IDF and LSA. Besides reducing the summaries’ length, the proposed length enhancement method did improve the summaries’ precision rates. Other domains, such as news videos, can apply the proposed method for video summarization. © 2022 Tech Science Press. All rights reserved.","Educational videos; Extractive summarization; LDA; Subtitle summarization; Topic modelling","Inverse problems; Semantics; Text processing; Video recording; Analysis models; Educational videos; Extractive summarizations; Latent Dirichlet allocation; Latent Semantic Analysis; Online resources; Subtitle summarization; Term frequencyinverse document frequency (TF-IDF); Topic Modeling; Video contents; Statistics",Article,Scopus,2-s2.0-85117008186
"Elmitwally N.S., Kanwal A., Abbas S., Khan M.A., Khan M.A., Ahmad M., Alanazi S.","57209421201;57189664755;57202833192;57215096761;57226797214;57226132376;57211018616;","Personality detection using context based emotions in cognitive agents",2022,"Computers, Materials and Continua","70","3",,"4947","4964",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116984120&doi=10.32604%2fcmc.2022.021104&partnerID=40&md5=dbd32c593ca857d2d14f6d47e356afe4","Detection of personality using emotions is a research domain in artificial intelligence. At present, some agents can keep the human’s profile for interaction and adapts themselves according to their preferences. However, the effective method for interaction is to detect the person’s personality by understanding the emotions and context of the subject. The idea behind adding personality in cognitive agents begins an attempt to maximize adaptability on the basis of behavior. In our daily life, humans socially interact with each other by analyzing the emotions and context of interaction from audio or visual input. This paper presents a conceptual personality model in cognitive agents that can determine personality and behavior based on some text input, using the context subjectivity of the given data and emotions obtained from a particular situation/context. The proposed work consists of Jumbo Chatbot, which can chat with humans. In this social interaction, the chatbot predicts human personality by understanding the emotions and context of interactive humans. Currently, the Jumbo chatbot is using the BFI technique to interact with a human. The accuracy of proposed work varies and improve through getting more experiences of interaction. © 2022 Tech Science Press. All rights reserved.","Contextual analysis; Emotions; Fuzzy; Personality detection; Semantic analysis","Intelligent agents; Chatbots; Cognitive agents; Context-based; Contextual analysis; Daily lives; Emotion; Fuzzy; Personality detections; Research domains; Semantic analysis; Semantics",Article,Scopus,2-s2.0-85116984120
"Lai K., Porter J.R., Amodeo M., Miller D., Marston M., Armal S.","57226140430;25652039900;57221929221;57213017788;57194719307;57217901340;","A Natural Language Processing Approach to Understanding Context in the Extraction and GeoCoding of Historical Floods, Storms, and Adaptation Measures",2022,"Information Processing and Management","59","1","102735","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116941635&doi=10.1016%2fj.ipm.2021.102735&partnerID=40&md5=50294e36999c20a1eb3d6eb018a026d0","Despite the known financial, economical, and humanitarian impacts of hurricanes and the floods that follow, datasets consisting of flood and flood risk reduction projects are either small in scope, lack in details, or held privately by commercial holders. However, with the amount of online data growing exponentially, we have seen a rise of information extraction techniques on unstructured text to drive insights. On one hand, social media in particular has seen a tremendous increase in popularity. On the other hand, despite this popularity, social media has proven to be unreliable and difficult to extract full information from. In contrast, online newspapers are often vetted by a journalist, and consist of more fine details. As a result, in this paper we leverage Natural Language Processing (NLP) to create a hybrid Named-Entity Recognition (NER) model that employs a domain-specific machine learning model, linguistic features, and rule-based matching to extract information from newspapers. To the knowledge of the authors, this model is the first of its kind to extract detailed flooding information and risk reduction projects over the entire contiguous United States. The approach used in this paper expands upon previous similar works by widening the geographical location and applying techniques to extract information over large documents, with minimal accuracy loss from the previous methods. Specifically, our model is able to extract information such as street closures, project costs, and metrics. Our validation indicates an F1 score of 72.13% for the NER model entity extraction, a binary classification location filter with a score of 73%, and an overall performance only 8.4% lower than a human validator against a gold-standard. Through this process, we find the location of 27,444 streets, 181,076 flood risk reduction projects, and 435,353 storm locations throughout the United States in the past two decades. © 2021","Floods; Information Extraction; Machine Learning; Natural language processing; Newspapers","Classification (of information); Digital storage; Extraction; Information retrieval; Learning algorithms; Linguistics; Location; Machine learning; Natural language processing systems; Newsprint; Risk assessment; Social networking (online); Storms; Extract informations; Flood risk reduction; Geo coding; Historical floods; Information extraction; Machine-learning; Named entity recognition; Processing approach; Recognition models; Social media; Floods",Article,Scopus,2-s2.0-85116941635
"Gerard L., Linn M.C., Berkeley U.C.","23488541200;7006538185;57291960300;","Computer-based guidance to support students’ revision of their science explanations",2022,"Computers and Education","176",,"104351","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116888509&doi=10.1016%2fj.compedu.2021.104351&partnerID=40&md5=47f096e803f01fc85071499845691fc8","As they encounter new ideas, students need to make integrated revisions to their science explanations, a key aspect of science learning. This involves filling gaps, resolving inconsistencies with evidence, and strengthening connections among ideas. Rather than making integrated revisions, even after automated, adaptive guidance, students typically add disconnected ideas or fix mechanical errors. The knowledge integration framework, supported by new technologies including natural language processing, guided the design of the Annotator, a tool that models the revision process for students’ written explanations. This research investigates the added value of the Annotator compared to automated, adaptive guidance to support students to make integrated revisions to their science explanations and to strengthen knowledge integration. 798 6th and 7th-grade students from 4 schools participated in a study featuring pretests, posttests, embedded student explanations, student interviews and observations. Students using the Annotator who initially displayed unintegrated ideas were more likely to make integrated revisions to their explanations, than students receiving automated, adaptive guidance. These students also made greater knowledge integration revisions on the posttest one week later. Thus, modeling revision with the Annotator strengthened the ability of students who started with unintegrated ideas to explain scientific phenomena. © 2021 The Authors",,"Automation; Integration; Natural language processing systems; Adaptive guidance; Added values; Filling gaps; Knowledge integration; Knowledge integration frameworks; Mechanical errors; Model revisions; Science learning; Student interviews; Students' observations; Students",Article,Scopus,2-s2.0-85116888509
"Morkner P., Bauer J., Creason C.G., Sabbatino M., Wingo P., Greenburg R., Walker S., Yeates D., Rose K.","57202274592;56699291500;57198434589;57291810100;57210315556;57291810200;57292501700;57292268600;35254432300;","Distilling data to drive carbon storage insights",2022,"Computers and Geosciences","158",,"104945","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116887766&doi=10.1016%2fj.cageo.2021.104945&partnerID=40&md5=0c282095dabb5149206a2a78d7e72f5a","Wide-spread implementation of carbon capture and storage has the potential to decrease carbon emissions and aid in meeting global climate change mitigation goals. Data availability is one of the biggest challenges faced by the carbon capture and storage (CCS) community for modeling risks associated with CCS, necessary for wide-spread implementation in coming years. Collecting, integrating, and intuitively managing data is a time-consuming process, but one which is fundamental to establishing necessary access to carbon storage data. The US Department of Energy (US DOE) has been a major supporter of energy research in the US, including significant investment into carbon capture and storage research and technology development over the last ten years. The US DOE investments into the Regional Carbon Sequestration Partnerships, the National Risk Assessment Partnership, and other CCS related research has resulted in a large volume of data, of which much has been made public through the National Energy Technology Laboratories data repository, the Energy Data eXchange (EDX). Researchers at the National Energy Technology Laboratory have developed workflows, tools, and other methods that leverage EDX, open-source software, machine learning, and natural language processing to discover, curate, label, organize and visualize available data. This paper describes the available data on EDX for carbon storage applications, describes the results of a spatial and temporal analysis of the data, describes where it is most geographically available, makes a general assessment of the quality of the available data, and discusses visualization tools and natural language processing tools developed for understanding, discovering and reusing the data. © 2021 Elsevier Ltd","Data availability; Energy data exchange; FAIR (Findable, Accessible, Interoperable, Reusable); Geologic carbon sequestration; Natural language processing; Spatial data density analysis","Climate change; Data visualization; Digital storage; Electronic data interchange; Investments; Natural language processing systems; Open Data; Open source software; Open systems; Quality control; Risk assessment; Spatial variables measurement; Carbon storage; Data availability; Data density; Energy data; Energy data exchange; Findable, accessible, interoperable, reusable; Geologic carbon sequestrations; Spatial data; Spatial data density analyse; Wide spreads; Carbon capture; accessibility; carbon emission; carbon sequestration; carbon storage; data acquisition; data processing; energy market; environmental policy; mitigation; partnership approach; policy implementation; pollution policy; power generation; research work; spatial data; United States",Article,Scopus,2-s2.0-85116887766
"Fukumura K., Pozniak C., Alario F.-X.","35092402900;57204533544;6603938925;","Avoiding gender ambiguous pronouns in French",2022,"Cognition","218",,"104909","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116864665&doi=10.1016%2fj.cognition.2021.104909&partnerID=40&md5=d91a2f624e2b4d8f1d6ff8edbf529588","Across many languages, pronouns are the most frequently produced referring expressions. We examined whether and how speakers avoid referential ambiguity that arises when the gender of a pronoun is compatible with more than one entity in the context in French. Experiment 1 showed that speakers use fewer pronouns when human referents have the same gender than when they had different genders, but grammatical gender congruence between inanimate referents did not result in fewer pronouns. Experiment 2 showed that semantic similarity between non-human referents can enhance the likelihood that speakers avoid grammatical-gender ambiguous pronouns. Experiment 3 pitched grammatical gender ambiguity avoidance against the referents' competition in the non-linguistic context, showing that when speakers can base their pronoun choice on non-linguistic competition, they ignore the pronoun's grammatical gender ambiguity even when the referents are semantically related. The results thus indicated that speakers preferentially produce referring expressions based on non-linguistic information; they are more likely to be affected by the referents' non-linguistic similarity than by the linguistic ambiguity of a pronoun. © 2021 Elsevier B.V.","Ambiguity avoidance; Grammatical gender; Language production; Pronoun; Referential communication; Semantic similarity","article; avoidance behavior; competition; female; gender; human; language; male; linguistics; semantics; Female; Humans; Language; Linguistics; Male; Semantics",Article,Scopus,2-s2.0-85116864665
"Huang S., Chen Y.","57291388900;55985403200;","Generative Adversarial Networks with Adaptive Semantic Normalization for text-to-image synthesis",2022,"Digital Signal Processing: A Review Journal","120",,"103267","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116860986&doi=10.1016%2fj.dsp.2021.103267&partnerID=40&md5=46549b80750db51ab1f1d8f1d0b94e7e","Transforming human natural language into photo-realistic images has always been a challenging issue. Batch Normalization (BN) is used in current text-to-image models to accelerate and stabilize the training process. However, the BN ignores feature differences between individuals and semantic relationship between modalities, which is negative for text-to-image tasks. To solve the problems, a novel module called Adaptive Semantic Instance Normalization (ASIN) is proposed. The ASIN considers the individuality of generated images and introduces text semantic information to the image normalization process, establishing a consistent and semantically close correlation between generated images and given text. Extensive experiments and ablation studies are carried out on two types of datasets. The results demonstrate the superiority of the proposed method in comparison of previous methods. © 2021 Elsevier Inc.","Coarse-to-fine image generation; Cross-modal information fusion; Generative Adversarial Network; Text-to-image synthesis","Image processing; Semantic Web; Semantics; Coarse to fine; Coarse-to-fine image generation; Cross-modal; Cross-modal information fusion; Fine images; Image generations; Images synthesis; Natural languages; Normalisation; Text-to-image synthesis; Generative adversarial networks",Article,Scopus,2-s2.0-85116860986
"Montobbio F., Staccioli J., Virgillito M.E., Vivarelli M.","6506436826;57203959171;56653457000;55671877300;","Robots and the origin of their labour-saving impact",2022,"Technological Forecasting and Social Change","174",,"121122","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116474336&doi=10.1016%2fj.techfore.2021.121122&partnerID=40&md5=8dab66c279c57eff82c916af9f7c7391","This paper investigates the presence of explicit labour-saving heuristics within robotic patents. It analyses innovative actors engaged in robotic technology and their economic environment (identity, location, industry), and identifies the technological fields more exposed to labour-saving innovations. It exploits advanced natural language processing and probabilistic topic modelling techniques applied to the universe of USPTO patent applications between 2009 and 2018, matched with the ORBIS (Bureau van Dijk) firm-level dataset. The results show that labour-saving patent holders comprise not only robot producers, but mainly adopters. Consequently, labour-saving robotic patents appear along the entire supply chain. Additionally, labour-saving innovations are directed towards manual activities in services (e.g. in the logistics sector), activities entailing social intelligence (e.g. in the healthcare sector) and cognitive skills (e.g. learning and predicting). © 2021 Elsevier Inc.","Labour-saving technology; Probabilistic topic models; Robotic patents; Search heuristics","Heuristic algorithms; Modeling languages; Natural language processing systems; Robotics; Statistics; Supply chains; Economic environment; Exposed to; Labor saving; Labor-saving technology; Modelling techniques; Patent applications; Probabilistic topic models; Robotic patent; Robotic technologies; Search heuristics; Patents and inventions; data set; heuristics; numerical model; probability; robotics; supply chain management",Article,Scopus,2-s2.0-85116474336
"Zhang J., Zhu L.","22634492200;57222139407;","Citation recommendation using semantic representation of cited papers’ relations and content",2022,"Expert Systems with Applications","187",,"115826","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116334407&doi=10.1016%2fj.eswa.2021.115826&partnerID=40&md5=d9c31eb6f706a65e26371fd98fbcaeb2","Citation recommendation can help researchers quickly find supplementary or alternative references in massive academic resources. Current research on citation recommendation mainly focuses on the citing papers, resulting in the enormous cited papers are ignored, including the relations among cited papers and their citation context cited in citing papers. Moreover, cited paper's content is often denoted with its original title and abstract, which is hard to acquire and rarely considers different citation motivations. Furthermore, the most appropriate method for semantic representation of cited papers’ relations and content is uncertain. Therefore, this paper studies citation recommendation from the perspective of semantic representation of cited papers’ relations and content. Firstly, four forms of citation context are designed and extracted as cited papers’ content considering citation motivations, as well as co-citation relationships are extracted as cited papers’ relations. Secondly, 132 methods are designed for generating semantic vector of cited paper, including four network embedding methods, 16 methods by combining four text representation algorithms with four forms of citation content, and 112 fusion methods. Finally, similarity among cited papers is calculated for citation recommendation and a quantitative evaluation method based on link prediction is designed, to find the most appropriate form of citation content and the optimal method. The result shows that doc2vecC (Document to Vector through Corruption) with the form of CS&SS (Current Sentences and Surrounding Sentences) performs best, in which the AUC (Area Under Curve) and MAP (Macro Average Precision) reach 0.877 and 0.889 and have increased by 0.462 and 0.370 compared with the worst-performing method. This performance is slightly improved by parameters adjustment, and a case study is performed whose results have further proved the effectiveness of this method. In addition, among four forms of cited papers’ content, CS&SS performs best in almost all methods. Furthermore, the fusion methods not always perform better than the single methods, where doc2vecC (CS&SS) performs better than the best fusion method GCN (Graph Convolutional Network). These results not only prove the effectiveness of citation recommendation from the perspective of cited paper, but also provide helpful and useful suggestions for method selection and citation content selection. The data and conclusions can be extended to other text mining-related tasks. Simultaneously, it is a preliminary research which needs to be further studied in other domains using emerging semantic representation methods. © 2021 Elsevier Ltd","Citation content; Citation recommendation; Cited paper; Co-citation; Semantic representation","Abstracting; Network embeddings; Semantics; 'current; Academic resources; Citation content; Citation contexts; Citation recommendation; Cited papers; Cocitation; Fusion methods; Semantic representation; Semantic vectors; Motivation",Article,Scopus,2-s2.0-85116334407
"Alsudais A., Alotaibi W., Alomary F.","56323952400;57224631106;57224625603;","Similarities between Arabic dialects: Investigating geographical proximity",2022,"Information Processing and Management","59","1","102770","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116067593&doi=10.1016%2fj.ipm.2021.102770&partnerID=40&md5=cdb7cf4a1845c7ff59d4744021ab38d8","The automatic classification of Arabic dialects is an ongoing research challenge, which has been explored in recent work that defines dialects based on increasingly limited geographic areas like cities and provinces. This paper focuses on a related, yet relatively unexplored topic: the effects of the geographical proximity of cities located in Arab countries on their dialectal similarity. Our work is twofold, reliant on: (1) comparing the textual similarities between dialects using cosine similarity and (2) measuring the geographical distance between locations. We study MADAR and NADI, two established datasets with Arabic dialects from many cities and provinces. Our results indicate that cities located in different countries may in fact have more dialectal similarity than cities within the same country, depending on their geographical proximity. The correlation between dialectal similarity and city proximity suggests that cities that are closer together are more likely to share dialectal attributes, regardless of country borders. This nuance provides the potential for important advancements in Arabic dialect research because it indicates that a more granular approach to dialect classification is essential to understanding how to frame the problem of Arabic dialect identification. © 2021 Elsevier Ltd","Arabic dialects; Arabic natural language processing; Geolocation; Textual similarity","Arab countries; Arabic dialects; Arabic natural language processing; Automatic classification; Cosine similarity; Geographic areas; Geographical proximity; Geolocations; Research challenges; Textual similarities; Natural language processing systems",Article,Scopus,2-s2.0-85116067593
"Gad W., Alokla A., Nazih W., Aref M., Salem A.-B.","26632826600;57278702600;24344470300;35576465900;36762342200;","DLBT: Deep learning-based transformer to generate pseudo-code from source code",2022,"Computers, Materials and Continua","70","2",,"3117","3132",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116056060&doi=10.32604%2fcmc.2022.019884&partnerID=40&md5=82f94f6e4986d88e0cf9c696b3b22d1f","Understanding the content of the source code and its regular expression is very difficult when they are written in an unfamiliar language. Pseudo-code explains and describes the content of the code without using syntax or programming language technologies. However, writing Pseudo-code to each code instruction is laborious. Recently, neural machine translation is used to generate textual descriptions for the source code. In this paper, a novel deep learning-based transformer (DLBT) model is proposed for automatic Pseudo-code generation from the source code. The proposed model uses deep learning which is based on Neural Machine Translation (NMT) to work as a language translator. The DLBT is based on the transformer which is an encoder-decoder structure. There are three major components: tokenizer and embeddings, transformer, and post-processing. Each code line is tokenized to dense vector. Then transformer captures the relatedness between the source code and the matching Pseudo-code without the need of Recurrent Neural Network (RNN). At the post-processing step, the generated Pseudo-code is optimized. The proposed model is assessed using a real Python dataset, which contains more than 18,800 lines of a source code written in Python. The experiments show promising performance results compared with other machine translation methods such as Recurrent Neural Network (RNN). The proposed DLBT records 47.32, 68. 49 accuracy and BLEU performance measures, respectively. © 2022 Tech Science Press. All rights reserved.","Deep learning-based transformer; Long short-term memory; Natural language processing; Neural machine translation; Pseudo-code generation","Codes (symbols); Computational linguistics; Computer aided language translation; High level languages; Natural language processing systems; Python; Recurrent neural networks; Codegeneration; Deep learning-based transformer; Model use; Post-processing; Pseudo codes; Pseudo-code generation; Regular expressions; Source codes; Textual description; Transformer modeling; Neural machine translation",Article,Scopus,2-s2.0-85116056060
"Baral C., Gelfond G., Pontelli E., Son T.C.","7004928552;13007498400;7004203882;7006447691;","An action language for multi-agent domains",2022,"Artificial Intelligence","302",,"103601","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116055184&doi=10.1016%2fj.artint.2021.103601&partnerID=40&md5=4406bc94af6d73465e3417b6414ae5e0","The goal of this paper is to investigate an action language, called mA⁎, for representing and reasoning about actions and change in multi-agent domains. The language, as designed, can also serve as a specification language for epistemic planning, thereby addressing an important issue in the development of multi-agent epistemic planning systems. The mA⁎ action language is a generalization of the single-agent action languages, extensively studied in the literature, to the case of multi-agent domains. The language allows the representation of different types of actions that an agent can perform in a domain where many other agents might be present—such as world-altering actions, sensing actions, and communication actions. The action language also allows the specification of agents' dynamic awareness of action occurrences—which has implications on what agents' know about the world and other agents' knowledge about the world. These features are embedded in a language that is simple, yet powerful enough to address a large variety of knowledge manipulation scenarios in multi-agent domains. The semantics of mA⁎ relies on the notion of state, which is described by a pointed Kripke model and is used to encode the agents' knowledge1 and the real state of the world. The semantics is defined by a transition function that maps pairs of actions and states into sets of states. The paper presents a number of properties of the action theories and relates mA⁎ to other relevant formalisms in the area of reasoning about actions in multi-agent domains. © 2021 Elsevier B.V.","Action languages; Epistemic planning; Reasoning about knowledge","Semantics; Specification languages; Specifications; Action language; Agent domains; Agent knowledge; Epistemic planning; Generalisation; Multi agent; Planning systems; Reasoning about action and changes; Reasoning about knowledge; Single-agent; Multi agent systems",Article,Scopus,2-s2.0-85116055184
"Cao N., Ji S., Chiu D.K.W., Gong M.","57216840808;55320543100;23388109000;8933846400;","A deceptive reviews detection model: Separated training of multi-feature learning and classification",2022,"Expert Systems with Applications","187",,"115977","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116049688&doi=10.1016%2fj.eswa.2021.115977&partnerID=40&md5=765800ae3232ce4afed3585411e2b16b","The increasing online reviews play an essential role in the e-commerce platform, which profoundly affects the purchase decisions of consumers. However, rampant dishonest sellers manipulate other buyers or robots to post deceptive reviews for profit. Recently, the detection of deceptive reviews has attracted general research attention, which mainly comprises two directions, traditional methods based on statistics and intelligent methods based on neural networks. These methods use a single feature or multiple features for classifier design. To make full use of different features for better feature representation of detecting deceptive reviews, this paper proposes a new feature fusion strategy and verifies its performance by comparing it with other feature fusion strategies. First, we utilize three independent models for feature extraction: the TextCNN, the Bidirectional Gated Recurrent Unit (GRU), and the Self-Attention are used to learn local semantic features, temporal semantic features, and weighted semantic features of reviews, respectively. Secondly, after obtaining different feature representations from the fully connected layers of these three models, we concatenate them together to form the final documental representation. Finally, we use a full connection layer and the sigmoid function to further learn and complete deceptive review detection. Experiments on three balanced and unbalanced in-domain small datasets (hotel, restaurant, doctor) and mixed-domain datasets show that our model is superior to baselines. Experiments on large-scale data with various imbalanced proportions verify the effectiveness of our method. We also analyze the results of different datasets from the perspective of part of speech to improve the model's interpretability. © 2021","Convolutional neural network; Deceptive reviews detection; Recurrent neural network; Self attention; Separated training","Classification (of information); Convolutional neural networks; Feature extraction; Large dataset; Semantics; Convolutional neural network; Deceptive review detection; Detection models; Feature representation; Features fusions; Fusion strategies; Learn+; Self attention; Semantic features; Separated training; Recurrent neural networks",Article,Scopus,2-s2.0-85116049688
"Bayrak S., Yucel E., Takci H.","56526714700;7006490870;18134705700;","Epilepsy radiology reports classification using deep learning networks",2022,"Computers, Materials and Continua","70","2",,"3589","3607",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116030512&doi=10.32604%2fcmc.2022.018742&partnerID=40&md5=143525736ca09794d53afca215100166","The automatic and accurate classification of Magnetic Resonance Imaging (MRI) radiology report is essential for the analysis and interpretation epilepsy and non-epilepsy. Since the majority of MRI radiology reports are unstructured, the manual information extraction is time-consuming and requires specific expertise. In this paper, a comprehensive method is proposed to classify epilepsy and non-epilepsy real brain MRI radiology text reports automatically. This method combines the Natural Language Processing technique and statistical Machine Learning methods. 122 real MRI radiology text reports (97 epilepsy, 25 non-epilepsy) are studied by our proposed method which consists of the following steps: (i) for a given text report our systems first cleans HTML/XML tags, tokenize, erase punctuation, normalize text, (ii) then it converts into MRI text reports numeric sequences by using index-based word encoding, (iii) then we applied the deep learning models that are uni-directional long short-term memory (LSTM) network, bidirectional long short-term memory (BiLSTM) network and convolutional neural network (CNN) for the classifying comparison of the data, (iv) finally, we used 70% of used for training, 15% for validation, and 15% for test observations. Unlike previous methods, this study encompasses the following objectives: (a) to extract significant text features from radiologic reports of epilepsy disease; (b) to ensure successful classifying accuracy performance to enhance epilepsy data attributes. Therefore, our study is a comprehensive comparative study with the epilepsy dataset obtained from numeric sequences by using index-based word encoding method applied for the deep learning models. The traditional method is numeric sequences by using index-based word encoding which has been made for the first time in the literature, is successful feature descriptor in the epilepsy data set. The BiLSTM network has shown a promising performance regarding the accuracy rates. We show that the larger sized medical text reports can be analyzed by our proposed method. © 2022 Tech Science Press. All rights reserved.","Deep learning networks-based text classification; Epilepsy; Feature engineering; Index-based word encoding; Natural language processing; Radiology text report analysis","Brain; Classification (of information); Long short-term memory; Magnetic resonance imaging; Natural language processing systems; Network coding; Neurology; Radiation; Radiology; Deep learning network-based text classification; Feature engineerings; Index-based word encoding; Learning models; Learning network; Memory network; Network-based; Performance; Radiology reports; Radiology text report analyse; Text processing",Article,Scopus,2-s2.0-85116030512
"Deorukhkar K., Ket S.","57279813200;34880320400;","A detailed review of prevailing image captioning methods using deep learning techniques",2022,"Multimedia Tools and Applications","81","1",,"1313","1336",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116006499&doi=10.1007%2fs11042-021-11293-1&partnerID=40&md5=d2aa1e68f931dad060b876ad58032ab5","Image captioning is a challenging task of computer vision and natural language processing. The big challenge lies in obtaining semantic information from images and translating that into the human language using machines. The interaction of computer vision and natural language processing further increases the complexity of image captioning. Notably, research has been carried out in image captioning to narrow down the semantic gap using deep learning techniques effectively. Deep learning techniques are proficient in dealing with the complexities of image captioning. A detailed study is carried out to identify the various state-of-the-art techniques for image captioning. The working algorithm of technique, positive highlights, and weakness of every technique is discussed in this paper. We also discussed the quantitative evaluation measures used for deep learning techniques and available datasets. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Attention Mechanism; CNN; GAN; Image Captioning; RNN","Computer vision; Deep learning; Natural language processing systems; Semantics; Attention mechanisms; CNN; GAN; Human language; Image captioning; Learning techniques; RNN; Semantic gap; Semantics Information; State-of-the-art techniques; Learning algorithms",Article,Scopus,2-s2.0-85116006499
"Pan M., Wang J., Huang J.X., Huang A.J., Chen Q., Chen J.","57203523631;57217280454;57189975304;57275487800;57275556100;57275581100;","A probabilistic framework for integrating sentence-level semantics via BERT into pseudo-relevance feedback",2022,"Information Processing and Management","59","1","102734","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115888585&doi=10.1016%2fj.ipm.2021.102734&partnerID=40&md5=cb595c11ccdbaa35c1080943bbfc0ca9","Existing pseudo-relevance feedback (PRF) methods often divide an original query into individual terms for processing and select expansion terms based on the term frequency, proximity, position, etc. This process may lose some contextual semantic information from the original query. In this work, based on the classic Rocchio model, we propose a probabilistic framework that incorporates sentence-level semantics via Bidirectional Encoder Representations from Transformers (BERT) into PRF. First, we obtain the importance of terms at the term level. Then, we use BERT to interactively encode the query and sentences in the feedback document to acquire the semantic similarity score of a sentence and the query. Next, the semantic scores of different sentences are summed as the term score at the sentence level. Finally, we balance the term-level and sentence-level weights by adjusting factors and combine the terms with the top-k scores to form a new query for the next-round processing. We apply this method to three Rocchio-based models (Rocchio, PRoc2, and KRoc). A series of experiments are conducted based on six official TREC data sets. Various evaluation indicators suggest that the improved models achieve a significant improvement over the corresponding baseline models. Our proposed models provide a promising avenue for incorporating sentence-level semantics into PRF, which is feasible and robust. Through comparison and analysis of a case study, expansion terms obtained from the proposed models are shown to be more semantically consistent with the query. © 2021","Information retrieval; Latent semantic information; Pseudo-relevance feedback; Query expansion; Text similarity","Information retrieval; Latent semantic information; Latent semantics; Probabilistic framework; Pseudo-relevance feedbacks; Query expansion; Relevance feedback method; Rocchio; Semantics Information; Sentence level; Text similarity; Semantics",Article,Scopus,2-s2.0-85115888585
"Frederiksen A.T., Mayberry R.I.","57189064167;7003819842;","Pronoun production and comprehension in American Sign Language: the interaction of space, grammar, and semantics",2022,"Language, Cognition and Neuroscience","37","1",,"80","102",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115835654&doi=10.1080%2f23273798.2021.1968013&partnerID=40&md5=6873876413d8f89adad5f365f6050119","Spoken language research has investigated how pronouns are influenced by grammar and semantics/pragmatics. In contrast, sign language research has focused on unambiguous pronominal reference arising from spatial co-reference. However, understanding signed pronouns contributes to cross-linguistically valid models of pronoun production and comprehension. In two sentence-continuation experiments, the present study investigated how linguistic use of space (modality-specific), antecedent grammatical role and verb implicit causality bias (modality-independent) affect American Sign Language (ASL) pronouns. Production of pronouns was determined by antecedent grammatical role, and overt pronouns were marginally more frequent for referents articulated in specific areas of signing space compared to neutral space. Signers interpreted pronouns using spatial information and, notably, verb bias, despite spatial co-reference supposedly removing the ambiguity that verb bias resolves. These findings demonstrate that ASL pronouns are subject to modality-independent factors, despite their use of space, and lend support to models of pronominal reference positing a production/comprehension asymmetry. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","American Sign Language; implicit causality biases; pronoun production and comprehension; Pronouns; spatial localisation","article; comprehension; grammar; human; human experiment; semantics; sign language",Article,Scopus,2-s2.0-85115835654
"Briskilal J., Subalalitha C.N.","56483236600;55356546000;","An ensemble model for classifying idioms and literal texts using BERT and RoBERTa",2022,"Information Processing and Management","59","1","102756","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115776710&doi=10.1016%2fj.ipm.2021.102756&partnerID=40&md5=b8e05fc4565637740c8af44f1f7e844e","An idiom is a common phrase that means something other than its literal meaning. Detecting idioms automatically is a serious challenge in natural language processing (NLP) domain applications like information retrieval (IR), machine translation and chatbot. Automatic detection of Idioms plays an important role in all these applications. A fundamental NLP task is text classification, which categorizes text into structured categories known as text labeling or categorization. This paper deals with idiom identification as a text classification task. Pre-trained deep learning models have been used for several text classification tasks; though models like BERT and RoBERTa have not been exclusively used for idiom and literal classification. We propose a predictive ensemble model to classify idioms and literals using BERT and RoBERTa, fine-tuned with the TroFi dataset. The model is tested with a newly created in house dataset of idioms and literal expressions, numbering 1470 in all, and annotated by domain experts. Our model outperforms the baseline models in terms of the metrics considered, such as F-score and accuracy, with a 2% improvement in accuracy. © 2021 Elsevier Ltd","BERT; Ensemble model; Idiom; Literal classification; RoBERTa","Deep learning; Natural language processing systems; Text processing; Automatic Detection; BERT; Chatbots; Ensemble models; Idiom; Labelings; Literal classification; Literals; Machine translations; RoBERTa; Classification (of information)",Article,Scopus,2-s2.0-85115776710
"Khurana A., Bhatnagar V.","57212571311;7102660553;","Investigating Entropy for Extractive Document Summarization",2022,"Expert Systems with Applications","187",,"115820","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115638128&doi=10.1016%2fj.eswa.2021.115820&partnerID=40&md5=a9d996a3fd6a07703072e007f11e796c","Automatic text summarization aims to cut down readers’ time and cognitive effort by reducing the content of a text document without compromising on its essence. Ergo, informativeness is the prime attribute of document summary generated by an algorithm, and selecting sentences that capture the essence of a document is the primary goal of extractive document summarization. In this paper, we employ Shannon's entropy to capture informativeness of sentences. We employ Non-negative Matrix Factorization (NMF) to reveal probability distributions for computing entropy of terms, topics, and sentences in latent space. We present an information theoretic interpretation of the computed entropy, which is the bedrock of the proposed E-Summ algorithm, an unsupervised method for extractive document summarization. The algorithm systematically applies information theoretic principle for selecting informative sentences from important topics in the document. The proposed algorithm is generic and fast, and hence amenable to use for summarization of documents in real time. Furthermore, it is domain-, collection-independent and agnostic to the language of the document. Benefiting from strictly positive NMF factor matrices, E-Summ algorithm is transparent and explainable too. We use standard ROUGE toolkit for performance evaluation of the proposed method on four well known public data-sets. We also perform quantitative assessment of E-Summ summary quality by computing its semantic similarity w.r.t the original document. Our investigation reveals that though using NMF and information theoretic approach for document summarization promises efficient, explainable, and language independent text summarization, it needs to be bolstered to match the performance of deep neural methods. © 2021 Elsevier Ltd","Entropy; Extractive summarization; Language independent; Non-negative Matrix Factorization; Semantic similarity","Entropy; Factorization; Information theory; Matrix algebra; Matrix factorization; Petroleum reservoir evaluation; Probability distributions; Semantics; Automatic text summarization; Cognitive efforts; Document summarization; Extractive summarizations; Informativeness; Language independents; Matrix factorizations; Nonnegative matrix factorization; Semantic similarity; Text document; Non-negative matrix factorization",Article,Scopus,2-s2.0-85115638128
"Jafarzadeh P., Ensan F.","57265283400;14053832600;","A semantic approach to post-retrieval query performance prediction",2022,"Information Processing and Management","59","1","102746","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115358193&doi=10.1016%2fj.ipm.2021.102746&partnerID=40&md5=9c90f7cf63d26d0f9491ffd89bc3bd4b","The importance of query performance prediction has been widely acknowledged in the literature, especially for query expansion, refinement, and interpolating different retrieval approaches. This paper proposes a novel semantics-based query performance prediction approach based on estimating semantic similarities between queries and documents. We introduce three post-retrieval predictors, namely (1) semantic distinction, (2) semantic query drift, and (3) semantic cohesion based on (1) the semantic similarity of a query to the top-ranked documents compared to the whole collection, (2) the estimation of non-query related aspects of the retrieved documents using semantic measures, and (3) the semantic cohesion of the retrieved documents. We assume that queries and documents are modeled as sets of entities from a knowledge graph, e.g., DBPedia concepts, instead of bags of words. With this assumption, semantic similarities between two texts are measured based on the relatedness between entities, which are learned from the contextual information represented in the knowledge graph. We empirically illustrate these predictors’ effectiveness, especially when term-based measures fail to quantify query performance prediction hypotheses correctly. We report our findings on the proposed predictors’ performance and their interpolation on three standard collections, namely ClueWeb09-B, ClueWeb12-B, and Robust04. We show that the proposed predictors are effective across different datasets in terms of Pearson and Kendall correlation coefficients between the predicted performance and the average precision measured by relevance judgments. © 2021 Elsevier Ltd","Post-retrieval prediction; Query performance prediction; Semantic information retrieval; Semantic-enabled prediction","Information retrieval; Interpolation; Knowledge graph; Semantics; Knowledge graphs; Post-retrieval prediction; Query expansion; Query-performance predictions; Retrieval query; Retrieved documents; Semantic approach; Semantic information retrieval; Semantic similarity; Semantic-enabled prediction; Forecasting",Article,Scopus,2-s2.0-85115358193
"Jana A., Haldar S., Goyal P.","57195420295;57205610222;57119093300;","Network embeddings from distributional thesauri for improving static word representations",2022,"Expert Systems with Applications","187",,"115868","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115325419&doi=10.1016%2fj.eswa.2021.115868&partnerID=40&md5=6b477e94420092773195bba31b15a67d","Word representations obtained from text using the distributional hypothesis have proved to be useful for various natural language processing tasks. To prepare vector representation from the text, some researchers use predictive model (Word2vec) or dense count-based model (GloVe), whereas others attempt to explore network structure obtained from text namely, distributional thesaurus network where the neighborhood of a word is a set of words having adequate context feature overlap. Being inspired by the successful application of network embedding techniques (DeepWalk, LINE, node2vec, etc.) in various tasks, we attempt to apply network embedding techniques to turn a distributional thesaurus network into dense word vectors and investigate the usefulness of distributional thesaurus embedding in improving the overall word vector representation. This is the first attempt where we show that combining the proposed word representation obtained by distributional thesaurus embedding with the state-of-the-art word representations helps in improving the performance by a significant margin when evaluated against several NLP tasks which include intrinsic tasks like word similarity and relatedness, subspace alignment, synonym detection, analogy detection; extrinsic tasks like noun compound interpretation, sentence pair similarity task as well as subconscious intrinsic evaluation methods using neural activation pattern in the brain, etc. © 2021","Distributional hypothesis; Distributional thesaurus; Network embedding; Word level semantics; Word representation","Embeddings; Natural language processing systems; Petroleum reservoir evaluation; Semantic Web; Semantics; Thesauri; Distributional hypothesis; Distributional thesaurus; Embedding technique; Embeddings; Network embedding; Vector representations; Word level; Word level semantic; Word representations; Word vectors; Network embeddings",Article,Scopus,2-s2.0-85115325419
"Grubišić A., Žitko B., Gašpar A., Vasić D., Dodaj A.","24485216400;24485775500;57218312001;57126955700;55515088600;","Evaluation of split-and-rephrase output of the knowledge extraction tool in the intelligent tutoring system",2022,"Expert Systems with Applications","187",,"115900","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115248455&doi=10.1016%2fj.eswa.2021.115900&partnerID=40&md5=3312194c9c1b4143a0ab6f4dde7365b1","Various approaches to text simplification have been proposed in an attempt to increase text readability. The rephrasing of syntactically and semantically complex structures is still challenging. A pedagogically motivated simplified version of the same text can have both positive and negative side effects. On the one hand, it can facilitate reading comprehension because of much shorter sentences and a limited vocabulary, but on the other hand, the simplified text often lacks coherence, unity and style. Therefore, reasonable trade-offs among linguistic simplicity, naturalness and informativeness are highly needed. This is a survey paper that discusses state-of-the-art approaches to sentence/text simplification and evaluation methods, along with an empirical evaluation of our approach. The quality of sentence splitting, using the knowledge extraction tool SAAT was compared to state-of-the-art syntactic simplification systems. The research was carried out on the WikiSplit, the HSplit and MinWikiSplit simplification corpora. Automatic metrics for the HSplit showed that the SAAT outperformed other TS systems in all categories. For the WikiSplit dataset, automatic metrics scores were slightly lower than that of the baseline system DisSim. However, the human evaluation showed that DisSim outperformed the SAAT in terms of simplicity and grammar. The quality of AG18copy output corresponded to that of the SAAT. The inter-annotator agreement was calculated. Research limitations as well as suggestions for future research were also provided. © 2021 Elsevier Ltd","Intelligent tutoring system; Knowledge extraction; Natural language processing; Text simplification","Computer aided instruction; Economic and social effects; Education computing; Extraction; Intelligent vehicle highway systems; Automatic metrics; Complexes structure; Intelligent tutoring; Intelligent tutoring system; Knowledge extraction; Negative side effects; Positive sides; Reading comprehension; Text simplification; Tutoring system; Natural language processing systems",Article,Scopus,2-s2.0-85115248455
"Ma Y., Liu X., Zhao L., Liang Y., Zhang P., Jin B.","8348553300;57206738120;57216952498;57263767400;36040790900;7201511565;","Hybrid embedding-based text representation for hierarchical multi-label text classification",2022,"Expert Systems with Applications","187",,"115905","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115136310&doi=10.1016%2fj.eswa.2021.115905&partnerID=40&md5=ad5d8dfcde1782a2ab533dd17fbdec4b","Many real-world text classification tasks often deal with a large number of closely related categories organized in a hierarchical structure or taxonomy. Hierarchical multi-label text classification (HMTC) has become rather challenging when it requires handling large sets of closely related categories. The structural features of all categories in the entire hierarchy and the word semantics of their category labels are very helpful in improving text classification accuracy over large sets of closely related categories, which has been neglected in most of existing HMTC approaches. In this paper, we present a hybrid embedding-based text representation for HMTC with high accuracy. First, the hybrid embedding consists of both graph embedding of categories in the hierarchy and their word embedding of category labels. The Structural Deep Network Embedding-based graph embedding model is used to simultaneously encode the global and local structural features of a given category in the whole hierarchy for making the category structurally discriminable. We further use the word embedding technique to encode the word semantics of each category label in the hierarchy for making different categories semantically discriminable. Second, we presented a level-by-level HMTC approach based on the bidirectional Gated Recurrent Unit network model together with the hybrid embedding that is used to learn the representation of the text level-by-level. Last but not least, extensive experiments were made over five large-scale real-world datasets in comparison with the state-of-the-art hierarchical and flat multi-label text classification approaches, and the experimental results show that our approach is very competitive to the state-of-the-art approaches in classification accuracy, in particular maintaining computational costs while achieving superior performance. © 2021 Elsevier Ltd","Graph embedding; Hierarchical classification; Hybrid embedding; Multi-label classification; Text classification","Classification (of information); Embeddings; Encoding (symbols); Large dataset; Semantics; Text processing; Classification approach; Embeddings; Graph embeddings; Hierarchical classification; Hybrid embedding; Multi-label text classification; Structural feature; Text classification; Text representation; Word Semantics; Graph embeddings",Article,Scopus,2-s2.0-85115136310
"Chen J., Qian Y.","57207877740;7402872685;","Hierarchical Multilabel Ship Classification in Remote Sensing Images Using Label Relation Graphs",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115128337&doi=10.1109%2fTGRS.2021.3111117&partnerID=40&md5=e3be195a2994a0ae9398b24b43ba1f85","Hierarchical multilabel classification (HMC) assigns multiple labels to each instance with the labels organized under hierarchical relations. In ship classification in remote sensing images, depending on the expert knowledge and image quality, the same type of ships in different remote sensing images may be annotated with different class labels from coarse to fine levels such as merchant ship (MS) or container ship (CTS). In this article, we propose a novel deep network with two output channels and their associated loss functions to learn an HMC classifier using samples labeled at different levels in the hierarchy. In the proposed network, a hierarchy and exclusion (HEX) graph is introduced to model the label hierarchy, which satisfies hierarchical constraints by encoding semantic relations between any two labels. The output nodes of the first channel are organized according to the HEX graph, and its corresponding probabilistic classification loss is built to reflect the hierarchical structure of the HEX graph. On the other hand, the output nodes of the second channel only represent the finest grained (last level in the hierarchy) classes, and its multiclass cross-entropy loss is designed to enhance the discriminative power of the HMC classifier on the last level labels, which is also compatible with constraints in the HEX graph. The combination of these two losses from two output channels can effectively transfer the hierarchical information of ship taxonomy during network training. Experimental results on two commonly used ship datasets demonstrate that the proposed method outperforms the state-of-the-art HMC approaches, and is especially advantageous when trained with fewer fine-grained samples. © 2021 IEEE.","Feature extraction; Marine vehicles; Probabilistic logic; Remote sensing; Semantics; Task analysis; Taxonomy","Graph theory; Remote sensing; Semantics; Ships; Discriminative power; Hierarchical information; Hierarchical multi-label classifications; Hierarchical relations; Hierarchical structures; Probabilistic classification; Remote sensing images; Ship classification; Image classification; container ship; extraction method; hierarchical system; image analysis; merchant ship; remote sensing; semantic standardization; taxonomy",Article,Scopus,2-s2.0-85115128337
"Ha S., Geum Y.","57259232000;25654780100;","Identifying new innovative services using M&A data: An integrated approach of data-driven morphological analysis",2022,"Technological Forecasting and Social Change","174",,"121197","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114952249&doi=10.1016%2fj.techfore.2021.121197&partnerID=40&md5=23d34931e512ec4e9e21f3cb4450d488","This study suggests a concrete framework for generating new service ideas using an M&A dataset. Addressing the limitations of previous works that neglected service-specific characteristics, we suggest methods to extract service-specific keywords and phrases from the text and restructure them to provide clear evidence for new service development. Therefore, we propose a process for building data-driven quality function deployment (QFD) and data-driven morphological analysis (MA). First, M&A transactions were collected from CrunchBase, which is an open platform that provides start-up information. Service actions and service contents are then extracted from the text using natural language processing. For each extracted keyword, a clustering analysis was performed to identify the new service patterns. For clustered service actions and contents, MA is employed to generate new service ideas. This study contributes to the technology management field by first employing M&A records for the data-driven morphological matrix and suggests how to extract service actions and service contents from the text. We also suggested a new systematic way of identifying new services using an integrated approach of QFD and MA. This work is expected to help managers in new service development by providing practical guidance and tools for utilizing textual data. © 2021 Elsevier Inc.","Big data; Data analytics; M&A; MA; New service development; QFD","Big data; Integrated control; Linguistics; Natural language processing systems; Quality control; Quality function deployment; Action contents; Data analytics; Data driven; Integrated approach; M&A; Morphological analysis; New service development; New services; Service actions; Service content; Data Analytics; data set; innovation; integrated approach",Article,Scopus,2-s2.0-85114952249
"Iwatsuki K., Boudin F., Aizawa A.","57195965364;23088264000;6701312731;","Extraction and evaluation of formulaic expressions used in scholarly papers",2022,"Expert Systems with Applications","187",,"115840","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114922561&doi=10.1016%2fj.eswa.2021.115840&partnerID=40&md5=f9e96bf68f67b3e62c0dbe5e64647cf6","Formulaic expressions, such as ‘in this paper we propose’, are helpful for authors of scholarly papers because they convey communicative functions; in the above, it is ‘showing the aim of this paper’. Thus, resources of formulaic expressions, such as a dictionary, that could be looked up easily would be useful. However, forms of formulaic expressions can often vary to a great extent. For example, ‘in this paper we propose’, ‘in this study we propose’ and ‘in this paper we propose a new method to’ are all regarded as formulaic expressions. Such a diversity of spans and forms causes problems in both extraction and evaluation of formulaic expressions. In this paper, we propose a new approach that is robust to variation of spans and forms of formulaic expressions. Our approach regards a sentence as consisting of a formulaic part and non-formulaic part. Then, instead of trying to extract formulaic expressions from a whole corpus, by extracting them from each sentence, different forms can be dealt with at once. Based on this formulation, to avoid the diversity problem, we propose evaluating extraction methods by how much they convey specific communicative functions rather than by comparing extracted expressions to an existing lexicon. We also propose a new extraction method that utilises named entities and dependency structures to remove the non-formulaic part from a sentence. Experimental results show that the proposed extraction method achieved the best performance compared to other existing methods. © 2021 The Authors","English for academic purposes; Formulaic expressions; Multi-word expressions; Natural language processing; Writing assistance","Extraction; Function evaluation; Communicative functions; Dependency structures; English for academic purpose; Extraction method; Formulaic expression; Multi-word expressions; Named entities; New approaches; Performance; Writing assistance; Natural language processing systems",Article,Scopus,2-s2.0-85114922561
"Gao K., Wang H., Cao Y., Inoue K.","57258033400;14055082900;55470280700;35748800500;","Learning from interpretation transition using differentiable logic programming semantics",2022,"Machine Learning","111","1",,"123","145",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114889047&doi=10.1007%2fs10994-021-06058-8&partnerID=40&md5=6b3157b233949a72dcee193517afa606","The combination of learning and reasoning is an essential and challenging topic in neuro-symbolic research. Differentiable inductive logic programming is a technique for learning a symbolic knowledge representation from either complete, mislabeled, or incomplete observed facts using neural networks. In this paper, we propose a novel differentiable inductive logic programming system called differentiable learning from interpretation transition (D-LFIT) for learning logic programs through the proposed embeddings of logic programs, neural networks, optimization algorithms, and an adapted algebraic method to compute the logic program semantics. The proposed model has several characteristics, including a small number of parameters, the ability to generate logic programs in a curriculum-learning setting, and linear time complexity for the extraction of trained neural networks. The well-known bottom clause positionalization algorithm is incorporated when the proposed system learns from relational datasets. We compare our model with NN-LFIT, which extracts propositional logic rules from retuned connected networks, the highly accurate rule learner RIPPER, the purely symbolic LFIT system LF1T, and CILP++, which integrates neural networks and the propositionalization method to handle first-order logic knowledge. From the experimental results, we conclude that D-LFIT yields comparable accuracy with respect to the baselines when given complete, incomplete, and mislabeled data. Our experimental results indicate that D-LFIT not only learns symbolic logic programs quickly and precisely but also performs robustly when processing mislabeled and incomplete datasets. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.","Differentiable inductive logic programming; Explainability; Learning from interpretation transition; Machine learning; Neuro-symbolic method","Algebra; Computer circuits; Knowledge representation; Learning algorithms; Learning systems; Neural networks; Semantics; Temporal logic; Linear time complexity; Logic program semantics; Logic programming semantics; Optimization algorithms; Propositional logic; Propositionalization; Symbolic knowledge; Trained neural networks; Inductive logic programming (ILP)",Article,Scopus,2-s2.0-85114889047
"Federmeier K.D.","6701548060;","Connecting and considering: Electrophysiology provides insights into comprehension",2022,"Psychophysiology","59","1","e13940","","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114882063&doi=10.1111%2fpsyp.13940&partnerID=40&md5=9403c1d80587d791d3d77b2b296a76d0","The ability to rapidly and systematically access knowledge stored in long-term memory in response to incoming sensory information—that is, to derive meaning from the world—lies at the core of human cognition. Research using methods that can precisely track brain activity over time has begun to reveal the multiple cognitive and neural mechanisms that make this possible. In this article, I delineate how a process of connecting affords an effortless, continuous infusion of meaning into human perception. In a relatively invariant time window, uncovered through studies using the N400 component of the event-related potential, incoming sensory information naturally induces a graded landscape of activation across long-term semantic memory, creating what might be called “proto-concepts”. Connecting can be (but is not always) followed by a process of further considering those activations, wherein a set of more attentionally demanding “active comprehension” mechanisms mediate the selection, augmentation, and transformation of the initial semantic representations. The result is a limited set of more stable bindings that can be arranged in time or space, revised as needed, and brought to awareness. With this research, we are coming closer to understanding how the human brain is able to fluidly link sensation to experience, to appreciate language sequences and event structures, and, sometimes, to even predict what might be coming up next. © 2021 Society for Psychophysiological Research","attention; ERPs; language comprehension; meaning; N400","attention; brain; cognition; comprehension; electroencephalography; electrophysiology; evoked response; human; language; memory; physiology; semantics; Attention; Brain; Cognition; Comprehension; Electroencephalography; Electrophysiology; Evoked Potentials; Humans; Language; Memory; Semantics",Article,Scopus,2-s2.0-85114882063
"Das A., Saha D.","57201450005;8675512600;","Deep learning based Bengali question answering system using semantic textual similarity",2022,"Multimedia Tools and Applications","81","1",,"589","613",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114838915&doi=10.1007%2fs11042-021-11228-w&partnerID=40&md5=6b4cd0a42663b1111191ae812bfa4a00","Recently, Question answering system is a major research area in language processing. Bengali isone of the most popular spoken languages in India. Still, it has faced difficulties in natural language processing.Among the semantic based systems, word mapping and keyword based approaches achieved the best results and got better attention on the user side. These systems are already implemented in various languages but not much in Indian language like Bengali. This work presents an efficient question answering system for retrieving Bengali language text. This system includes word embedding clustering and deep level feature representation for providing better grammatical similarities for retrieving the Bengali textual contents relevant to user queries. The pre-trained word embedding module is created by the help of a deep belief network. The modified density peak algorithm is employed to perform word embedding clustering.The presented work has been tested on a dataset from the Bengali corpus developed by TDIL and synthetic Bengali translated datasets accessible in English called SQuAD 2.0. This question answering system is implemented in python with NLTK tool kit and got good performance while retrieving the Bengali textual data. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Bengali question answering system; Deep Belief Network; Modified density peak clustering; NLP; Semantic search","Embeddings; Natural language processing systems; Semantics; Bengali language; Deep belief networks; Feature representation; Language processing; Natural languages; Question answering systems; Semantic based system; Textual similarities; Deep learning",Article,Scopus,2-s2.0-85114838915
"Xu Z., Zhang B., Li D., Yue X.","57214467042;56026241300;55704079400;55180828700;","Hierarchical multilabel classification by exploiting label correlations",2022,"International Journal of Machine Learning and Cybernetics","13","1",,"115","131",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114834793&doi=10.1007%2fs13042-021-01371-z&partnerID=40&md5=d8243f60bc696aaddd4aedf1e91ab068","Hierarchical multilabel classification (HMC) aims to classify the complex data such as text with multiple topics and image with multiple semantics, in which the multiple labels are organized in hierarchical structures such as trees and direct acyclic graphs (DAG). To reduce the computational complexity, HMC methods generally assume that the class labels of different branches in a hierarchical structure are conditional independent. However, these class-independent HMC methods neglect the correlation between labels and thereby the precision of classification is affected. To tackle the problem, in this paper we propose a hierarchical multilabel classification method with class label correlation (HMC-CLC) which exploits the label correlations of different branches to benefit the discrimination of HMC. Specifically, in the training stage, for each label in the hierarchy, we use feature incremental learning to encode the labels of different branches into the input space. Based on this, the label correlations of different branches are reflected by the weights of classification model in corresponding dimensions. Then in the test stage, considering that the different samples have different label distributions, we propose a greedy label selection method to dynamically decide the correlated labels of different branches for each label. Therefore, for the same label in the hierarchy, the correlated labels could be different in different samples. Experimental results on a number of real-world data sets show that the proposed method outperforms the state-of-the-art HMC methods. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Greedy strategy; Hierarchical multilabel classification; Incremental learning; Label correlations","Semantics; Text processing; Trees (mathematics); Classification models; Direct acyclic graphs; Hierarchical multi-label classifications; Hierarchical structures; Incremental learning; Label correlations; Label distribution; Selection methods; Classification (of information)",Article,Scopus,2-s2.0-85114834793
"Phan T.H.V., Do P.","57218366609;55120354700;","NER2QUES: combining named entity recognition and sequence to sequence to automatically generating Vietnamese questions",2022,"Neural Computing and Applications","34","2",,"1593","1612",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114814096&doi=10.1007%2fs00521-021-06477-7&partnerID=40&md5=924212c6861526e2ae83694f89a56461","Named entity recognition (NER) is an important task in natural language processing. NER is usually used to classify documents, extract information, and translate languages. However, few studies have used NER types to automatically generate questions. In this paper, we proposed a method named NER2QUES to solve the above problem for a low-resource language such as Vietnamese. NER2QUES was the combining pre-trained language model and sequence-to-sequence model. Specifically, we used BERT to detect NERs in a sentence and then applied a sequence-to-sequence model to automatically generate questions that corresponded to NER’s types. We compared the accuracy of the proposed method to PhoBERT and spaCy on the NER task. Also, we used F1, BLEU, ROUGE, and METEOR to measure the effectiveness of this approach with the rules-based method, T5, and BERT on question generation tasks. The experiment results show that the accuracy of our method is more improved than previous methods’ accuracy of 94% on SQuAD, 89% on XQuAD, and 95% on MLQA. This indicates that using NER to automatically generate questions may enrich question answering systems. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","BERT; Named entity recognition; Question answering; Question generation; Sequence to sequence","Classification (of information); Extract informations; Language model; Low resource languages; Named entity recognition; NAtural language processing; Question answering systems; Rules based; Sequence modeling; Natural language processing systems",Article,Scopus,2-s2.0-85114814096
"Lu H.-C., Tseng H.-Y., Yao L.","30467793700;57221167285;24081898500;","Neutrino-like particle for particle swarm optimization",2022,"International Journal of Intelligent Systems","37","1",,"859","913",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114723149&doi=10.1002%2fint.22650&partnerID=40&md5=d90032d0192c2071677745d96aaf8c28","The real-world optimal problems frequently encountered by various industries are the nonlinear constrained optimization problems (NCOPs), where the constraints represent the limitations of practical resources. Many researchers have attempted to improve particle swarm optimization (PSO) in the past decades; however, in solving the NCOPs, the PSO-based approaches often cause premature convergences. The problem-specific constraints frequently generate many infeasible regions that block the movements of particles. The particles' behavior causes the exploration abilities of particles that tend to weaken along with time. The decreasing of exploration ability often comes from the particle becoming stagnant or moving unusefully. This study proposes a neutrino-like particle (NLP) with adaptive NLP hyperparameters that simulate the natural neutrino behavior. The proposed NLPs can be embedded in the PSO-based approaches for overcoming premature convergence. The experiment results demonstrate that all referenced PSO-based methods with the NLPs improved significantly compared with those without the NLPs to solve the NCOPs. All referenced PSO-based methods that embedded the NLPs also significantly outperform four recent strong algorithms in most IEEE CEC 2020 benchmark problems. Therefore, the proposed NLPs with adaptive NLP hyperparameters can effectively solve the premature convergences, reinforce the exploration ability, and maintain the exploitation capability for solving the NCOPs over the whole evolution process. © 2021 Wiley Periodicals LLC","exploration; neutrino-like particle; nonlinear constrained optimization problems; particle swarm optimization; premature convergances","Constrained optimization; Natural language processing systems; Neutrons; Bench-mark problems; Evolution process; Hyperparameters; Nonlinear constrained optimization problems; Optimal problems; Pre-mature convergences; Real-world; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-85114723149
"Krishna Siva Prasad M., Sharma P.","57204723760;57248414900;","Exploring intrinsic information content models for addressing the issues of traditional semantic measures to evaluate verb similarity",2022,"Computer Speech and Language","71",,"101280","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114357312&doi=10.1016%2fj.csl.2021.101280&partnerID=40&md5=dbf206822627b5e7260bf041224caf78","Semantic similarity measures play an important role in many natural language processing and information retrieval activities. It is highly challenging to measure semantic similarity with higher accuracy. A notable branch of semantic similarity evaluation based on information content (IC) is popular in this aspect. Intrinsic information content (IIC) models are another wing of IC based evaluation. Both IC based and IIC based approaches majorly handled similarity evaluation of nouns. Research related to semantic similarity assessment of verb pairs are rarely discussed. To bridge this gap, this work examines various IC based, IIC based approaches on verb pairs. A detailed discussion of the existing measures and their drawbacks are mentioned in this work. Strategies based on information content, length and depth of the concepts are discussed and tested on benchmark datasets. Existing intrinsic information content models are enhanced by addressing various issues like (a) dealing concepts with no path in WordNet and (b) handling the synonym sets of verb concepts. Measures based on path length, intrinsic information content, combined strategies and non-linear strategies for verb pairs are thoroughly inspected. This paper also presents novel strategies to understand novel aspects that are not addressed before. The strategies are experimented by generating the synonym sets of required parts-of-speech which proved very effective in improving the correlation with human judgment. Results on benchmark datasets specify that the proposed approaches for verb similarity will be a guiding factor for understanding the natural language processing tasks. © 2021 Elsevier Ltd","Depth; Information content; Intrinsic information content model; Path length; Semantic similarity","Integrated circuits; Semantics; Syntactics; Benchmark datasets; Information contents; NAtural language processing; Novel strategies; Semantic measures; Semantic similarity; Semantic similarity measures; Similarity evaluation; Natural language processing systems",Article,Scopus,2-s2.0-85114357312
"Adikari A., de Silva D., Moraliyage H., Alahakoon D., Wong J., Gancarz M., Chackochan S., Park B., Heo R., Leung Y.","57200703350;55242193900;57220061394;6602546111;57247536200;57221471528;57247965300;57247100000;57247100100;57247965400;","Empathic conversational agents for real-time monitoring and co-facilitation of patient-centered healthcare",2022,"Future Generation Computer Systems","126",,,"318","329",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114349934&doi=10.1016%2fj.future.2021.08.015&partnerID=40&md5=52b6029917fa536150663d09e086de3d","Healthcare systems across the world are transitioning into patient-centered healthcare models to ensure improved health outcomes, increased operational efficiencies and respectful patient engagement. Digital health technologies are at the forefront of this transition in facilitating a role for the patient in the clinical dimensions of the healthcare trajectory, from diagnosis and interventions to treatment and recovery. Despite this prevalence in the clinical space, the non-clinical needs of patient mental health and wellbeing are frequently overlooked by contemporary patient-centered healthcare models. Conversational agents (or chatbots) are digital dialogue systems that are widespread and widely used in sequential information provision and information acquisition tasks. Given the intimate nature of this human-machine interaction, conversational agents can be effectively utilized to support and sustain patient mental health and wellbeing. In this paper, we propose an empathic conversational agent framework based on an ensemble of natural language processing techniques and artificial intelligence algorithms for real-time monitoring and co-facilitation of patient-centered healthcare for improved mental health and wellbeing outcomes. The technical contributions of this framework are; detection of patient emotions, prediction of patient emotion transitions, detection of group emotions, formulation of patient behavioral metrics, and resource recommendations based on patient concerns. The architectural contributions of the framework are intelligent communication channels that stream empathic conversational elements and resource recommendations for the multi-user conversations and co-facilitation updates for the human healthcare provider interface. The framework was empirically evaluated on a benchmark dataset and further validated based on a clinical protocol designed for its application in an online support group setting for cancer patients and caregivers in Canada. The results of these experiments confirm the effectiveness of this framework, its contributory role and practical value in realizing a patient-centered healthcare model for improved mental health and wellbeing outcomes. © 2021 Elsevier B.V.","Artificial intelligence; Cancer care; Chatbots; Co-facilitation; Conversational agents; Empathic AI; Group emotions; Human-centric AI; Markov models; Patient emotions; Patient-centered care; Real-time monitoring","Artificial intelligence; Benchmarking; Diagnosis; Natural language processing systems; Patient rehabilitation; Patient treatment; Real time systems; Speech processing; Cancer care; Chatbots; Co-facilitation; Conversational agents; Empathic AI; Group emotions; Human-centric AI; Markov model; Patient emotion; Patient-centered care; Real time monitoring; Diseases",Article,Scopus,2-s2.0-85114349934
"Dutta H., Gupta A.","8217634300;57219640095;","PNRank: Unsupervised ranking of person name entities from noisy OCR text",2022,"Decision Support Systems","152",,"113662","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114232870&doi=10.1016%2fj.dss.2021.113662&partnerID=40&md5=428737da3bb5ca0dcf2261081069dca8","Text databases have grown tremendously in number, size, and volume over the last few decades. Optical Character Recognition (OCR) software is used to scan the text and make them available in online repositories. The OCR transcription process is often not accurate resulting in large volumes of garbled text in the repositories. Spell correction and other post-processing of OCR text often prove to be very expensive and time-consuming. While it is possible to rely on the OCR model to assess the quality of text in a corpus, many natural language processing and information retrieval tasks prefer the extrinsic evaluation of the effect of noise on the task at hand. This paper examines the effect of noise on the unsupervised ranking of person name entities by first populating a list of person names using an out-of-the-box Named Entity Recognition (NER) software, extracting content-based features for the identified entities, and ranking them using a novel unsupervised Kernel Density Estimation (KDE) based ranking algorithm. This generative model has the ability to learn rankings using the data distribution and therefore requires limited manual intervention. Empirical results are presented on a carefully curated parallel corpus of OCR and clean text and “in the wild” using a large real-world corpus. Experiments on the parallel corpus reveals that even with a reasonable degree of noise in the dataset, it is possible to generate ranked lists using the KDE algorithm with a high degree of precision and recall. Furthermore, since the KDE algorithm has comparable performance to state-of-the-art unsupervised rankers, using it on real-world corpora is feasible. The paper concludes by reflecting on other methods for enhancing the performance of the unsupervised algorithm on OCR text such as cleaning entity names, disambiguating names concatenated to one another and correcting OCR errors that are statistically significant in the corpus. © 2021 Elsevier B.V.","Kernel density estimation; Named entity recognition; OCR noise; Unsupervised ranking","Linguistics; Natural language processing systems; Optical character recognition; Content-based features; Kernel Density Estimation; Manual intervention; Named entity recognition; NAtural language processing; Optical character recognition (OCR); Transcription process; Unsupervised algorithms; Learning to rank",Article,Scopus,2-s2.0-85114232870
"Wu F., Cheng J., Wang X., Wang L., Tao D.","57214770266;57344944600;54406086400;36987001200;54411283900;","Image Hallucination From Attribute Pairs",2022,"IEEE Transactions on Cybernetics","52","1",,"568","581",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114130432&doi=10.1109%2fTCYB.2020.2979258&partnerID=40&md5=2481c3d32a08fc35f201452152c9e912","Recent image-generation methods have demonstrated that realistic images can be produced from captions. Despite the promising results achieved, existing caption-based generation methods confront a dilemma. On the one hand, the image generator should be provided with sufficient details for realistic hallucination, meaning that longer sentences with rich content are preferred, but on the other hand, the generator is meanwhile fragile to long sentences due to their complex semantics and syntax like long-range dependencies and the combinatorial explosion of object visual features. Toward alleviating this dilemma, a novel approach is proposed in this article to hallucinate images from attribute pairs, which can be extracted from natural language processing (NLP) toolsets in the presence of complex semantics and syntax. Attribute pairs, therefore, enable our image generator to tackle long sentences handily and alleviate the combinatorial explosion, and at the same time, allow us to enlarge the training dataset and to produce hallucinations from randomly combined attribute pairs at ease. Experiments on widely used datasets demonstrate that the proposed approach yields results superior to the state of the art. © 2013 IEEE.","Attentional generative adversarial network (GAN); attribute pair; text-to-image synthesis","Complex networks; Generative adversarial networks; Image processing; Natural language processing systems; Semantics; Attentional generative adversarial network; Attribute pair; Combinatorial explosion; Generation method; Image generations; Image generators; Images synthesis; Long-range dependencies; Realistic images; Text-to-image synthesis; Syntactics; diagnostic imaging; hallucination; human; natural language processing; semantics; Hallucinations; Humans; Natural Language Processing; Semantics",Article,Scopus,2-s2.0-85114130432
"Irshad M., Börstler J., Petersen K.","57190225560;6603198801;57234608700;","Supporting refactoring of BDD specifications—An empirical study",2022,"Information and Software Technology","141",,"106717","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113689195&doi=10.1016%2fj.infsof.2021.106717&partnerID=40&md5=239cf67a7d78813f33693a1702143169","Context: Behavior-driven development (BDD) is a variant of test-driven development where specifications are described in a structured domain-specific natural language. Although refactoring is a crucial activity of BDD, little research is available on the topic. Objective: To support practitioners in refactoring BDD specifications by (1) proposing semi-automated approaches to identify refactoring candidates; (2) defining refactoring techniques for BDD specifications; and (3) evaluating the proposed identification approaches in an industry context. Method: Using Action Research, we have developed an approach for identifying refactoring candidates in BDD specifications based on two measures of similarity and applied the approach in two projects of a large software organization. The accuracy of the measures for identifying refactoring candidates was then evaluated against an approach based on machine learning and a manual approach based on practitioner perception. Results: We proposed two measures of similarity to support the identification of refactoring candidates in a BDD specification base; (1) normalized compression similarity (NCS) and (2) similarity ratio (SR). A semi-automated approach based on NCS and SR was developed and applied to two industrial cases to identify refactoring candidates. Our results show that our approach can identify candidates for refactoring 6o times faster than a manual approach. Our results furthermore showed that our measures accurately identified refactoring candidates compared with a manual identification by software practitioners and outperformed an ML-based text classification approach. We also described four types of refactoring techniques applicable to BDD specifications; merging candidates, restructuring candidates, deleting duplicates, and renaming specification titles. Conclusion: Our results show that NCS and SR can help practitioners in accurately identifying BDD specifications that are suitable candidates for refactoring, which also decreases the time for identifying refactoring candidates. © 2021 The Authors","BDD; Behavior-driven development; Normalized Compression Distance (NCD); Normalized Compression Similarity (NCS); Refactoring; Reuse; Similarity ratio (SR); Specifications; Testing","Automation; Boolean functions; Classification (of information); Natural language processing systems; Text processing; Automated approach; Empirical studies; Identification approach; Manual identification; Software organization; Software practitioners; Test driven development; Text classification; Specifications",Article,Scopus,2-s2.0-85113689195
"Maimaiti M., Liu Y., Luan H., Sun M.","57204634175;56023483700;23035489400;7403180987;","Data augmentation for low-resource languages NMT guided by constrained sampling",2022,"International Journal of Intelligent Systems","37","1",,"30","51",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113371052&doi=10.1002%2fint.22616&partnerID=40&md5=9d4b3a9076722e544797788719f0edf4","Data augmentation (DA) is a ubiquitous approach for several text generation tasks. Intuitively, in the machine translation paradigm, especially in low-resource languages scenario, many DA methods have appeared. The most commonly used methods are building pseudocorpus by randomly sampling, omitting, or replacing some words in the text. However, previous approaches hardly guarantee the quality of augmented data. In this study, we try to augment the corpus by introducing a constrained sampling method. Additionally, we also build the evaluation framework to select higher quality data after augmentation. Namely, we use the discriminator submodel to mitigate syntactic and semantic errors to some extent. Experimental results show that our augmentation method consistently outperforms all the previous state-of-the-art methods on both small and large-scale corpora in eight language pairs from four corpora by 2.38–4.18 bilingual evaluation understudy points. © 2021 Wiley Periodicals LLC","artificial intelligence; constrained sampling; data augmentation; low-resource languages; natural language processing; neural machine translation","Computer aided language translation; Semantics; Augmentation methods; Constrained sampling; Data augmentation; Evaluation framework; Low resource languages; Machine translations; State-of-the-art methods; Ubiquitous approach; Quality control",Article,Scopus,2-s2.0-85113371052
"Zhang H., Zhang H., Lu X., Gao Q.","57227980400;9735209400;57227980500;57228348700;","Attention-based overall enhance network for chinese semantic textual similarity measure",2022,"Journal of Applied Science and Engineering (Taiwan)","25","2",,"287","295",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113300090&doi=10.6180%2fjase.202204_25%282%29.0005&partnerID=40&md5=1b703379db095e7ef9258cf917a16a31","Semantic text similarity(STS) measure plays an important role in the practical application of natural language processing. However, due to the complexity of Chinese semantic comprehension and the lack of currently available Chinese text similarity dataset, present research on Chinese semantic text similarity still exists many limitations. In this paper, we construct a new private self-built Chinese semantic similarity (NCSS) dataset and propose a new method called Attention-based Overall Enhance Network (ABOEN) for measuring semantic textual similarity. This model takes advantage of convolutional neural network upon soft attention layers to capture more fine-grained interactive features between two sentences. Besides, inspired by the channel attention mechanism in image classification, we adopt a channel attention mechanism to enhance the critical overall interactive features between two sentences. The experimental results show that compared with other baseline models, the accuracy based on our model on the NCSS and LCQMC datasets has increased by 1.38% and 1.49%, respectively, which proves the effectiveness of our proposed model. © 2022 Tamkang University. All rights reserved.","ABOEN; Attention mechanism; Chinese semantic textual similarity; Convolutional neural network","Convolutional neural networks; Image enhancement; Multilayer neural networks; Natural language processing systems; Semantic Web; Attention mechanisms; Baseline models; Interactive features; NAtural language processing; Semantic comprehension; Semantic similarity; Text similarity; Textual similarities; Semantics",Article,Scopus,2-s2.0-85113300090
"Li Y., Zhang X., Gu J., Li C., Wang X., Tang X., Jiao L.","57215604730;55802358000;57015938500;56564807100;57219556130;57020306700;7102491544;","Recurrent Attention and Semantic Gate for Remote Sensing Image Captioning",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113206889&doi=10.1109%2fTGRS.2021.3102590&partnerID=40&md5=119508d4660d9d9776e94f5fa469e198","The remote sensing image captioning has attracted wide spread attention in remote sensing field due to its application potentiality. However, most existing approaches model limited interactions between image content and sentence and fail to exploit special characteristics of the remote sensing images. We introduce a novel recurrent attention and semantic gate (RASG) framework to facilitate the remote sensing image captioning in this article, which integrates competitive visual features and a recurrent attention mechanism to generate a better context vector for the images every time as well as enhances the representations of the current word state. Specifically, we first project each image into competitive visual features by taking the advantage of both static visual features and multiscale features. Then, a novel recurrent attention mechanism is developed to extract the high-level attentive maps from encoded features and nonvisual features, which can help the decoder recognize and focus on the effective information for understanding the complex content of the remote sensing images. Finally, the hidden states from the long short-term memory (LSTM) and other semantic references are incorporated into a semantic gate, which contributes to more comprehensive and precise semantic understanding. Comprehensive experiments on three widely used datasets, Sydney-Captions, UCM-Captions, and Remote Sensing Image Captioning Dataset, have demonstrated the superiority of the proposed RASG over a series of attentive models based on image captioning methods. © 2021 IEEE.","Decoding; Feature extraction; Logic gates; Neural networks; Remote sensing; Semantics; Visualization","Image enhancement; Long short-term memory; Semantics; Attention mechanisms; Context vector; Image captioning; ITS applications; Multi-scale features; Remote sensing images; Semantic understanding; Visual feature; Remote sensing; artificial neural network; data set; image processing; remote sensing; visualization; Sydney [New South Wales]",Article,Scopus,2-s2.0-85113206889
"Chang C.-Y., Hwang G.-J., Gau M.-L.","57195596036;57226126184;6701848435;","Promoting students' learning achievement and self-efficacy: A mobile chatbot approach for nursing training",2022,"British Journal of Educational Technology","53","1",,"171","188",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113179194&doi=10.1111%2fbjet.13158&partnerID=40&md5=c9ce982bbb75eb1f860a5ddac0bbde8b","The aims of nursing training include not only mastering skills but also fostering the competence to make decisions for problem solving. In prenatal education, cultivating nurses' knowledge and competence of vaccine administration is a crucial issue for protecting pregnant women and newborns from infection. Therefore, obstetric vaccination knowledge has become a basic and essential training program for nursing students. However, most of these training programs are given via the lecture-based teaching approach with skills practice, providing students with few opportunities to think deeply about the relevant issues owing to the lack of interaction and context. This could have a negative impact on their learning effectiveness and clinical judgment. To address this problem, a mobile chatbot-based learning approach is proposed in this study to enable students to learn and think deeply in the contexts of handling obstetric vaccine cases via interacting with the chatbot. In order to verify the effectiveness of the proposed approach, an experiment was implemented. Two classes of 36 students from a university in northern Taiwan were recruited as participants. One class was the experimental group learning with the proposed approach, while the other class was the control group learning with the conventional approach (ie, giving lectures to explain the instructional content and training cases). The results indicate that applying a mobile chatbot for learning can enhance nursing students' learning achievement and self-efficacy. In addition, based on the analysis of the interview results, students generally believed that learning through the mobile chatbot was able to promote their self-efficacy as well as their learning engagement and performance. Practitioner notes What is already known about this topic Issues relevant to AI technology in education have been extensively discussed and explored around the world. Among the various AI systems, the potential of chatbots has been highlighted by researchers owing to the user-friendly interface developed using the natural language processing (NLP) technology. Few studies using AI chatbots in professional training have been conducted. What this paper adds In this study, a mobile chatbot was used in a nursing training program to enhance students' learning achievement and self-efficacy for handling vaccine cases. The mobile chatbot significantly improved the students' learning achievement and self-efficacy in comparison with the conventional learning approach in the vaccine training program. From the interview results, it was found that the students generally believed that the mobile chatbot was able to promote their self-efficacy as well as learning engagement and performances in the vaccine training program. Implications for practice and/or policy Mobile chatbots have great potential for professional training owing to their convenient and user-friendly features. It would be worth applying mobile chatbots as well as other NLP-based applications to other professional training programs in the future. © 2021 British Educational Research Association",,"Application programs; Artificial intelligence; Curricula; Natural language processing systems; Nursing; Personnel training; Professional aspects; User interfaces; Vaccines; Conventional approach; Experimental groups; Learning achievement; Learning effectiveness; NAtural language processing; Professional training; Teaching approaches; User friendly interface; Students",Article,Scopus,2-s2.0-85113179194
"Yu X., Zhou Q., Wang S., Zhang Y.-D.","57210165932;57217028405;36544650700;35786830100;","A systematic survey of deep learning in breast cancer",2022,"International Journal of Intelligent Systems","37","1",,"152","216",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113142019&doi=10.1002%2fint.22622&partnerID=40&md5=c1315ba29cdce35d4a2834f9a73e2fa8","In recent years, we witnessed a speeding development of deep learning in computer vision fields like categorization, detection, and semantic segmentation. Within several years after the emergence of AlexNet, the performance of deep neural networks has already surpassed human being experts in certain areas and showed great potential in applications such as medical image analysis. The development of automated breast cancer detection systems that integrate deep learning has received wide attention from the community. Breast cancer, a major killer of females that results in millions of deaths, can be controlled even be cured given that it is detected at an early stage with sophisticated systems. In this paper, we reviewed breast cancer diagnosis, detection, and segmentation computer-aided (CAD) systems based on state-of-the-art deep convolutional neural networks. The available data sets also indirectly determine CAD systems' performance, so we introduced and discussed the details of public data sets. The challenges remaining in CAD systems for breast cancer are discussed at the end of this paper. The highlights of this survey mainly come from three following aspects. First, we covered a wide range of the basics of breast cancer from imaging modalities to popular databases in the community; Second, we presented the key elements in deep learning to form the compactness for methods mentioned in reviewed papers; Third and lastly, the summative details in each reviewed paper are provided so that interested readers can have a refined version of these works without referring to original papers. Therefore, this systematic survey suits readers with varied backgrounds and will be beneficial to them. © 2021 Wiley Periodicals LLC","breast cancer; CAD systems; deep learning; systematic review","Computer aided diagnosis; Convolutional neural networks; Deep neural networks; Diseases; Learning systems; Medical imaging; Semantics; Surveys; Breast Cancer; Breast cancer detection; Breast cancer diagnosis; Computer aided; Imaging modality; Key elements; Semantic segmentation; Sophisticated system; Deep learning",Article,Scopus,2-s2.0-85113142019
"Röchert D., Neubaum G., Ross B., Stieglitz S.","57218164247;56047144800;57195061365;8982225800;","Caught in a networked collusion? Homogeneity in conspiracy-related discussion networks on YouTube",2022,"Information Systems","103",,"101866","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112810119&doi=10.1016%2fj.is.2021.101866&partnerID=40&md5=af8a41fae61a8ffe93771a60e9495ab2","In many instances, misinformation among the population manifests itself in the form of conspiracy theories. Services such as YouTube, which allow the publication of audiovisual material in juxtaposition with peer responses (e.g., comments), function as ideal forums to disseminate such conspiracy theories and reach a massive audience. While previous research provided initial evidence about the prevalence of conspiracy theories in social media, it remains unclear how online networks discussing conspiracist content are structured. Knowledge about the network structure, however, could indicate to what extent people discussing conspiracist ideas face the risk of becoming caught in homogeneous communication cocoons. This work presents an approach combining natural language processing and network analysis to measure opinion-based homogeneity of discussion networks of three conspiracy theories (Hollow Earth, Chemtrails, and New World Order) on YouTube. A classification model was used to identify conspiracy and counter-conspiracy videos and associated user-generated comments (N = 123,642), as well as the interconnections between them. Although classification accuracy varied between the investigated conspiracy theories, our results indicated that people who expressed a favorable stance toward the conspiracy theory tended to respond to content or interact with users that shared the same opinion. In contrast, for two out of three conspiracy theories, people who advocated against the theory in their comments were more willing to engage in cross-cutting interactions. Findings are interpreted in light of the widely discussed fragmentation of homogeneous online networks. © 2021 Elsevier Ltd","Conspiracy theories; Machine learning; Opinion- based homogeneity; Social network analysis; YouTube","International cooperation; Audio-visual material; Classification accuracy; Classification models; NAtural language processing; Network structures; New world order; On-line network; User-generated; Natural language processing systems",Article,Scopus,2-s2.0-85112810119
"Grissette H., Nfaoui E.H.","57207995939;55793822300;","Affective Concept-Based Encoding of Patient Narratives via Sentic Computing and Neural Networks",2022,"Cognitive Computation","14","1",,"274","299",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112762055&doi=10.1007%2fs12559-021-09903-z&partnerID=40&md5=5840080566e146eea7aa76b831897d11","The automatic generation of features without human intervention is the most critical task for biomedical sentiment analysis. Regarding the high dynamicity of shared patient narrative data, the lack of formal medical language sentiment dictionaries prevents retrieval of the appropriate sentiment, which is unapproachable and can be prone to annotator bias. We propose a novel affective biomedical concept-based encoding via sentic computing and neural networks. The main contributions include four aspects. First, a biomedical embedding, in which a medical entity is defined, normalized, and synthesized from a text, is built using online patient narratives after being combined with label propagation from a widely used comprehensive biomedical vocabulary. Second, considering the dependence on biomedical definitions, drug reaction sample selection based on general matching is suggested. These feature settings are then used to build and recognize affective semantics and sentics based on an extreme learning machine. Finally, a semisupervised LSTM-BiLSTM model for biomedical sentiment analysis is constructed. There was a massive influx of patient self-reports related to the COVID-19 pandemic. A study was conducted in this direction, and we tested the validity, medical language familiarity, and transferability of our approach by analyzing millions of COVID-19 tweets. Comparisons to affective lexicons also indicate that integrating extreme learning machine cognitive capabilities has advantages over biomedical sentiment analysis. By considering sentics vectors on top of the formed embeddings, our semisupervised LSTM-BiLSTM achieved an accuracy of 87.5%. The evaluations of unsupervised learning approximated the results of the previous model when dealing with a serious loss of biomedical data. In this paper, we demonstrate the effectiveness of integrating deep-learning-based cognitive capabilities for both enhancing distributed biomedical definitions and inferring sentiment compositions from many patient self-reports on social networks. The relevant encoding of affective information conveyed regarding medication subjects clearly reveals defined roles and expectations that can have a positive impact on public health. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Affective computing; Biomedical sentiment analysis; Distributed biomedical vocabularies; Pandemic COVID-19; Sentic computing; Social networks","Backpropagation; Deep learning; Embeddings; Encoding (symbols); Knowledge acquisition; Long short-term memory; Network coding; Semantics; Sentiment analysis; Affective concepts; Automatic Generation; Biomedical vocabularies; Cognitive capability; Extreme learning machine; Human intervention; Label propagation; Sentiment dictionaries; Learning systems",Article,Scopus,2-s2.0-85112762055
"Liu Y., Wang L., Shi T., Li J.","57226804644;57268118400;57223082905;8920756500;","Detection of spam reviews through a hierarchical attention architecture with N-gram CNN and Bi-LSTM",2022,"Information Systems","103",,"101865","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112731633&doi=10.1016%2fj.is.2021.101865&partnerID=40&md5=9b2caaa9198e5862acda4c3d48db5a41","Spam reviews misguide decision makings of consumers and may seriously affect fair trading in the online markets. Existing methods for detecting spam reviews mainly focus on feature designs from linguistic and psychological clues, but they hardly reveal the potential semantics. Recent research works apply deep learning to capture semantics features, while these models fail to extract multi-granularity information of the text structures nor consider the mutual influence among the sentences. We propose a hierarchical attention network in which distinct attentions are purposely used at the two layers to capture important, comprehensive, and multi-granularity semantic information. At the first layer, we especially use an N-gram CNN to extract the multi-granularity semantics of the sentences. We then use a combination of convolution structure and Bi-LSTM to extract important and comprehensive semantics in a document at the second layer. Extensive experiments on public datasets demonstrate that our model has superior detection performance over the state-of-the-art baselines, improving F1 score in the mixed-domain to 89.3% (with 4.8 points absolute improvement), F1 score in the Doctor domain to 92.8% (with 9.9 points absolute improvement), F1 score in the Hotel domain to 86.1% (with 2.4 points absolute improvement) and F1 score in the cross-domain to 84.7% (with 10.4 points absolute improvement). © 2021 Elsevier Ltd","Bi-LSTM; Deceptive review detection; Document representation; Hierarchical attention; N-gram CNN","Bismuth compounds; Computational linguistics; Decision making; Deep learning; Semantics; Convolution structure; Detection performance; Multi-granularity; Online markets; Recent researches; Semantic information; State of the art; Text structure; Long short-term memory",Article,Scopus,2-s2.0-85112731633
"Bao Q., Zhao G., Yu Y., Dai S.","57191981777;56517310000;56996337400;57203987116;","Ontology-based assembly process modeling with element extraction and reasoning",2022,"Computer-Aided Design and Applications","19","2",,"280","292",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112720251&doi=10.14733%2fCADAPS.2022.280-292&partnerID=40&md5=fc2b4edc570d41b397f075c2c33999f9","In the context of collaborative manufacturing and cloud manufacturing, an ontology-based semantic modeling method of assembly process is studied to solve the prob-lem of non-standard assembly process formulation. Firstly, assembly process is divided into worksteps according to the assembly topology relationships, which correspond to the concept of event ontology as basic units. Secondly, natural language processing technology is used to process workstep contents in assembly documents, further realize the transformation from human language to machine language, which constitutes a significant step of the extraction of workstep event elements. In addition, the workstep event elements is supplemented with ontology reasoning of assembly resources without directly specifying the relatedness with the worksteps as well. Finally, a case study is illustrated to show the entire process of element extraction and reasoning of assembly worksteps, indicating the availability and feasibility of method proposed. © 2022 CAD Solutions, LLC, http://www.cad-journal.net.","Assembly process; Element extraction; Element reasoning; Ontology","Assembly; Extraction; Manufacture; Natural language processing systems; Semantics; Assembly process modeling; Assembly resources; Cloud Manufacturing; Collaborative manufacturing; Element extraction; NAtural language processing; Ontology reasonings; Topology relationships; Ontology",Article,Scopus,2-s2.0-85112720251
"Nys B.L., Brisson J., Schaeken W.","57226767624;55802524200;6604051737;","Find extra options or reason badly: An investigation of children's reasoning with incompatibility statements",2022,"Journal of Experimental Child Psychology","213",,"105258","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112641897&doi=10.1016%2fj.jecp.2021.105258&partnerID=40&md5=99c07e4d85d0009718571483e02f43b3","The false dilemma or dichotomy is a logical fallacy that occurs when interlocuters accept the premises in an incompatibility statement as being jointly exhaustive (i.e., leaving no third option), whereas that is in fact not the case. Brisson et al. [Memory & Cognition (2018), Vol. 46, pp. 657–670] investigated this fallacy in an adult sample and discovered a content effect that influenced participants’ performance. The current study aimed to elaborate on these findings by establishing whether similar patterns could be observed with children. A number of age-appropriate incompatibility premises were constructed. For every item, four different inferential problems were presented (Affirm First, Affirm Second, Deny First, and Deny Second) with three potential answers to choose from (X, not X, or uncertainty regarding X). A sample of 192 volunteer children, with ages ranging from 8 to 13 years, was collected. Statistical analysis showed no significant effect for participants’ age but did reveal main effects for premise validity and the amount of available “third options” (possibilities outside of the presented dichotomy). These results are a clear replication of the general effects on adults found by Brisson et al. Affirm inferences were also easy for children, Deny inferences were difficult (even more so than for adults), and content had a profound effect on participants’ performance. Whenever more third options could be generated, children were less likely to fall into the false dilemma fallacy. Our findings thus further support the idea that reasoning with incompatibilities is influenced by the same semantic retrieval processes that have been previously related to human conditional reasoning. © 2021 Elsevier Inc.","Cognitive development; Content effect; Deductive reasoning; False dichotomy; False dilemma; Logical fallacy","adolescent; adult; article; child; cognitive development; deductive reasoning; female; human; human experiment; human tissue; information retrieval; major clinical study; male; uncertainty; validity; cognition; logic; problem solving; semantics; Adolescent; Adult; Child; Cognition; Humans; Logic; Problem Solving; Semantics; Uncertainty",Article,Scopus,2-s2.0-85112641897
"Ye F., Hu J., Huang T.-Q., You L.-J., Weng B., Gao J.-Y.","57222114259;57226776574;57226787103;57211169710;14072088600;57308799500;","Transformer for EI Niño-Southern Oscillation Prediction",2022,"IEEE Geoscience and Remote Sensing Letters","19",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112612053&doi=10.1109%2fLGRS.2021.3100485&partnerID=40&md5=ef7e46d45335009fead3195cb7ba1c12","Accurate prediction of EI Niño-southern oscillation (ENSO) is of great significance to seasonal climate forecast. Recently, a convolutional neural network (CNN) has shown an optimal skill for ENSO prediction. However, it is difficult for the convolutional kernel to capture long-range precursors of ENSO due to its build-in local property. The transformer model has long been used in natural language processing (NLP) for its ability to focus on global features. Here, we introduce it to the ENSO research community and propose the ENSO transformer (ENSOTR). We show that using the ENSOTR model, the monthly average Niño3.4 index can be skillfully predicted up to one and a half years ahead. The model can also predict strong EI Niño cases more than a year ahead, such as 1997-1998. Experimental results show that our model achieves better skill than CNN for ENSO prediction. © 2004-2012 IEEE.","Convolutional neural network (CNN); deep learning; EI Niño southern oscillation (ENSO); ENSO transformer (ENSOTR)","Atmospheric pressure; Convolution; Convolutional neural networks; Forecasting; Natural language processing systems; Accurate prediction; Convolutional kernel; Local property; NAtural language processing; Research communities; Seasonal climate forecast; Southern oscillation; Transformer modeling; Climatology; air-sea interaction; El Nino-Southern Oscillation; prediction",Article,Scopus,2-s2.0-85112612053
"Kiefer S.","57226699703;","CaSE: Explaining Text Classifications by Fusion of Local Surrogate Explanation Models with Contextual and Semantic Knowledge",2022,"Information Fusion","77",,,"184","195",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112361230&doi=10.1016%2fj.inffus.2021.07.014&partnerID=40&md5=fe5b08b88d7c0e712ea1484ad5a15f7f","Generating explanations within a local and model-agnostic explanation scenario for text classification is often accompanied by a local approximation task. In order to create a local neighborhood for a document, whose classification shall be explained, sampling techniques are used that most often treat the according features at least semantically independent from each other. Hence, contextual as well as semantic information is lost and therefore cannot be used to update a human's mental model within the according explanation task. In case of dependent features, such explanation techniques are prone to extrapolation to feature areas with low data density, therefore causing misleading interpretations. Additionally, the ”the whole is greater than the sum of its parts” phenomenon is disregarded when using explanations that treat the according words independently from each other. In this paper, an architecture named CaSE is proposed that either uses Semantic Feature Arrangements or Semantic Interrogations to overcome these drawbacks. Combined with a modified version of Local interpretable model-agnostic explanations (LIME), a state of the art local explanation framework, it is capable of generating meaningful and coherent explanations. The approach utilizes contextual and semantic knowledge from unsupervised topic models in order to enable realistic and semantic sampling and based on that generate understandable explanations for any text classifier. The key concepts of CaSE that are deemed essential for providing humans with high quality explanations are derived from findings of psychology. In a nutshell, CaSE shall enable Semantic Alignment between humans and machines and thus further improve the basis for Interactive Machine Learning. An extensive experimental validation of CaSE is conducted, showing its effectiveness by generating reliable and meaningful explanations whose elements are made of contextually coherent words and therefore are suitable to update human mental models in an appropriate way. In the course of a quantitative analysis, the proposed architecture is evaluated w.r.t. a consistency property and to Local Fidelity of the resulting explanation models. According to that, CaSE generates more realistic explanation models leading to higher Local Fidelity compared to LIME. © 2021 Elsevier B.V.","Contextual and semantic interrogations; Explainable Artificial Intelligence; Explanatory understanding; Human-like explanations; Interpretable Machine Learning; Topic modeling","Classification (of information); Cognitive systems; Information retrieval systems; Lime; Text processing; Consistency property; Experimental validations; Interactive machine learning; Local approximation; Local neighborhoods; Proposed architectures; Semantic information; Text classification; Semantics",Article,Scopus,2-s2.0-85112361230
"Cocquyt E.M., Santens P., van Mierlo P., Duyck W., Szmalec A., De Letter M.","57211945850;7006128792;15121889200;6602607503;6508142599;6602734656;","Age- and gender-related differences in verbal semantic processing: the development of normative electrophysiological data in the Flemish population",2022,"Language, Cognition and Neuroscience","37","2",,"241","267",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111856567&doi=10.1080%2f23273798.2021.1957137&partnerID=40&md5=40597592d56809cc43e7c255b0d158f1","Categorical and associative relationships among words are two key forms of semantic knowledge. In this study, we examined ageing and gender effects on the processing of both types of semantic relationships by using the event-related potential technique. Moreover, we aimed to develop normative electrophysiological data for clinical purposes. One hundred and ten healthy subjects were divided among three age groups and subjected to two auditory word priming paradigms. Early auditory processing was influenced by increasing age as shown by larger P1 amplitudes and by delayed onsets of the N1 and P2. Conversely, ageing effects on the main N400 effect were limited to an increased right hemispheric lateralisation pattern for associative relationships. Gender effects could be demonstrated, with women showing larger P2 amplitudes and larger semantic priming effects in comparison to men. The interpretation of these findings is discussed and the practical utility of the obtained normative data is emphasised. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","Ageing; event-related potentials; gender; normative data; semantics","adult; aging; article; controlled study; event related potential; female; gender; groups by age; human; human experiment; major clinical study; male; semantics",Article,Scopus,2-s2.0-85111856567
"Yang Z., Liu Y., Chen Y., Zhou J.T.","57207982406;56454483900;7601438779;57230242900;","Deep Learning for Latent Events Forecasting in Content Caching Networks",2022,"IEEE Transactions on Wireless Communications","21","1",,"413","428",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111599307&doi=10.1109%2fTWC.2021.3096747&partnerID=40&md5=415a9ec80b823eeec5e889a323b83010","A novel Twitter context aided content caching (TAC) framework is proposed for enhancing the caching efficiency by taking advantage of the legibility and massive volume of Twitter data. For the purpose of promoting the caching efficiency, three machine learning models are proposed to predict latent events and events popularity, utilizing collected Twitter data with geo-tags and geographic information of the adjacent base stations (BSs). Firstly, we propose a latent Dirichlet allocation (LDA) model for latent events forecasting because of the superiority of LDA model in natural language processing (NLP). Then, we conceive long short-term memory (LSTM) with skip-gram embedding approach and LSTM with continuous skip-gram-Geo-aware embedding approach for the events popularity forecasting. Furthermore, we associate the predict latent events and the popularity of the events with the caching strategy. Lastly, we propose a non-orthogonal multiple access (NOMA) based content transmission scheme. Extensive practical experiments demonstrate that: 1) the proposed TAC framework outperforms conventional caching framework and is capable of being employed in practical applications thanks to the associating ability with public interests; 2) the proposed LDA approach conserves superiority for natural language processing (NLP) in Twitter data; 3) the perplexity of the proposed skip-gram based LSTM is lower compared with conventional LDA approach; and 4) evaluation of the model demonstrates that the hit rates of tweets of the model vary from 50% to 65% and the hit rate of the caching contents is up to approximately 75% with smaller caching space compared to conventional algorithms. Simulation results also shows that the proposed NOMA-enabled caching scheme outperforms conventional least frequently used (LFU) scheme by 25%. © 2002-2012 IEEE.","edge computing; Machine learning (ML); neural networks; supervised learning","Deep learning; Efficiency; Embeddings; Forecasting; Natural language processing systems; Social networking (online); Statistics; Content caching networks; Content transmission; Conventional algorithms; Geographic information; Latent dirichlet allocations; Least frequently used; Machine learning models; NAtural language processing; Long short-term memory",Article,Scopus,2-s2.0-85111599307
"Hernández-Castañeda Á., García-Hernández R.A., Ledeneva Y., Millán-Hernández C.E.","57191863544;56000830800;57156702900;57214349429;","Language-independent extractive automatic text summarization based on automatic keyword extraction",2022,"Computer Speech and Language","71",,"101267","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111539507&doi=10.1016%2fj.csl.2021.101267&partnerID=40&md5=fe970a9822f3aa463f04ad97ac1f51d7","This study proposes a language and domain independent approach for automatic extractive text summarization (EATS) tasks, which is based on a clustering scheme supported by a genetic algorithm (GA), to find an optimal grouping of sentences. Furthermore, our approach includes a topic modeling algorithm to find the key sentences in clusters based on automatically generated keywords. Our experimental results show that our system outperforms previous methods through the application of two general steps: clustering, which helps to increase coverage, and the addition of semantic information to the model, which facilitates the detection of the key sentences in the clusters and improves precision. © 2021","Automatic summarization; Extractive summaries; Genetic algorithm; Keywords; Topic modeling","Semantics; Text processing; Automatic text summarization; Automatically generated; Domain-independent approach; Keyword extraction; Language independents; Semantic information; Text summarization; Topic modeling algorithms; Genetic algorithms",Article,Scopus,2-s2.0-85111539507
"Liu Y., Li P., Hu X.","57226430332;56048282400;23090636200;","Combining context-relevant features with multi-stage attention network for short text classification",2022,"Computer Speech and Language","71",,"101268","","",,4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111488858&doi=10.1016%2fj.csl.2021.101268&partnerID=40&md5=8a98e406bfc67405a23ceee4eb567f38","Short text classification is a challenging task in natural language processing. Existing traditional methods using external knowledge to deal with the sparsity and ambiguity of short texts have achieved good results, but accuracy still needs to be improved because they ignore the context-relevant features. Deep learning methods based on RNN or CNN are hence becoming more and more popular in short text classification. However, RNN based methods cannot perform well in the parallelization which causes the lower efficiency, while CNN based methods ignore sequences and relationships between words, which causes the poorer effectiveness. Motivated by this, we propose a novel short text classification approach combining Context-Relevant Features with multi-stage Attention model based on Temporal Convolutional Network (TCN) and CNN, called CRFA. In our approach, we firstly use Probase as external knowledge to enrich the semantic representation for the solution to the data sparsity and ambiguity of short texts. Secondly, we design a multi-stage attention model based on TCN and CNN, where TCN is introduced to improve the parallelization of the proposed model for higher efficiency, and discriminative features are obtained at each stage through the fusion of attention and different-level CNN for a higher accuracy. Specifically, TCN is adopted to capture context-related features at word and concept levels, and meanwhile, in order to measure the importance of features, Word-level TCN (WTCN) based attention, Concept-level TCN (CTCN) based attention and different-level CNN are used at each stage to focus on the information of more important features. Finally, experimental studies demonstrate the effectiveness and efficiency of our approach in the short text classification compared to several well-known short text classification approaches based on CNN and RNN. © 2021","Attention mechanism; Deep learning; Short text classification; Temporal Convolutional Network (TCN)","Classification (of information); Convolutional neural networks; Efficiency; Learning systems; Natural language processing systems; Recurrent neural networks; Semantics; Convolutional networks; Discriminative features; Effectiveness and efficiencies; External knowledge; Important features; NAtural language processing; Semantic representation; Short text classifications; Text processing",Article,Scopus,2-s2.0-85111488858
"Jia Y., Jeong J.-H.","57204696093;7402045818;","Deep learning for quantile regression under right censoring: DeepQuantreg",2022,"Computational Statistics and Data Analysis","165",,"107323","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111328071&doi=10.1016%2fj.csda.2021.107323&partnerID=40&md5=20f047d6692948e688cac583358828f4","The computational prediction algorithm of neural network, or deep learning, has drawn much attention recently in statistics as well as in image recognition and natural language processing. Particularly in statistical application for censored survival data, the loss function used for optimization has been mainly based on the partial likelihood from Cox's model and its variations to utilize existing neural network library such as Keras, which was built upon the open source library of TensorFlow. As a novel contribution to the literature, an extension of the neural network to the quantile regression is proposed for survival data with right censoring, which is adjusted by the inverse of the estimated censoring distribution in the check function. The main purpose is to show that the deep learning method could be flexible enough to predict nonlinear patterns more accurately compared to existing quantile regression methods such as traditional linear quantile regression and nonparametric quantile regression with total variation regularization, emphasizing practicality of the method for censored survival data. Simulation studies were performed to generate nonlinear censored survival data and compare the deep learning method with existing quantile regression methods in terms of prediction accuracy. The proposed method is illustrated with two publicly available breast cancer data sets with gene signatures. The method has been built into a package and is freely available at https://github.com/yicjia/DeepQuantreg. © 2021 Elsevier B.V.","Huber check function; Inverse probability censoring weights (IPCW); Neural network; Survival analysis; Time to event","Forecasting; Image recognition; Inverse problems; Learning systems; Natural language processing systems; Neural networks; Regression analysis; Censoring distribution; Computational predictions; NAtural language processing; Nonparametric quantile regressions; Open-source libraries; Partial likelihoods; Prediction accuracy; Total variation regularization; Deep learning",Article,Scopus,2-s2.0-85111328071
"Jang M., Kang P.","57207995787;14034324500;","Sentence transition matrix: An efficient approach that preserves sentence semantics",2022,"Computer Speech and Language","71",,"101266","","",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111056478&doi=10.1016%2fj.csl.2021.101266&partnerID=40&md5=134fca9df179470970ffa4e6f39fe6cc","Sentence embedding is an influential research topic in natural language processing (NLP). Generation of sentence vectors that reflect the intrinsic meaning of sentences is crucial for improving performance in various NLP tasks. Therefore, numerous supervised and unsupervised sentence-representation approaches have been proposed since the advent of the distributed representation of words. These approaches have been evaluated on semantic textual similarity (STS) tasks designed to measure the degree of semantic information preservation; neural network-based supervised embedding models typically deliver state-of-the-art performance. However, these models have limitations in that they have numerous learnable parameters and thus require large amounts of specific types of labeled training data. Pretrained language model-based approaches, which have become a predominant trend in the NLP field, alleviate this issue to some extent; however, it is still necessary to collect sufficient labeled data for the fine-tuning process is still necessary. Herein, we propose an efficient approach that learns a transition matrix tuning a sentence embedding vector to capture the latent semantic meaning. Our proposed method has two practical advantages: (1) it can be applied to any sentence embedding method, and (2) it can deliver robust performance in STS tasks with only a few training examples. © 2021","Natural language processing; Paraphrase; Sentence embedding; Sentence semantics; Transition matrix","Embeddings; Labeled data; Matrix algebra; Semantics; Distributed representation; Improving performance; Labeled training data; NAtural language processing; Semantic information; State-of-the-art performance; Textual similarities; Transition matrices; Natural language processing systems",Article,Scopus,2-s2.0-85111056478
"Imbriaco R., Sebastian C., Bondarev E., De With P.H.N.","57207829924;57204589683;8106824200;7003945229;","Toward Multilabel Image Retrieval for Remote Sensing",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111047798&doi=10.1109%2fTGRS.2021.3095957&partnerID=40&md5=3df5644fb127b429f4e08a45995ae554","The availability of large-scale remote sensing (RS) data facilitates a wide range of applications, such as disaster management and urban planning. An approach for such problems is image retrieval, where, given a query image, the goal is to find the most relevant match from a database. Most RS literature has been focused on single-label retrieval, where we assume an image has a single label. The primary challenge in single-label RS retrieval is that performance in most datasets is saturated, and it has become difficult to compare the performance of different methods. In this work, we extend the major multilabel classification datasets to the multilabel retrieval problem. We also define protocols, provide evaluation metrics, and study the impact of commonly used loss functions and reranking methods for multilabel retrieval. To this end, a novel multilabel loss function and a reranking technique are proposed, which circumvent the challenges present in conventional single-label image retrieval. The developed loss function considers both class and feature similarity. The proposed reranking technique achieves high performance with computation cost that is well-suited for fast online retrieval. © 1980-2012 IEEE.","Image retrieval; loss function; multilabel; query expansion; remote sensing (RS)","Classification (of information); Disaster prevention; Disasters; Natural language processing systems; Query processing; Remote sensing; Computation costs; Disaster management; Evaluation metrics; Feature similarities; Loss functions; Multi-label classifications; Re-ranking techniques; Remote sensing data; Image retrieval; data set; disaster management; remote sensing; satellite imagery; urban planning",Article,Scopus,2-s2.0-85111047798
"Taghizadeh N., Faili H.","55676344000;27367722500;","Cross-lingual transfer learning for relation extraction using Universal Dependencies",2022,"Computer Speech and Language","71",,"101265","","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111004456&doi=10.1016%2fj.csl.2021.101265&partnerID=40&md5=142f7eb4a3acd2cd227f43e102ce2f69","This paper focuses on the task of cross-language relation extraction, which aims to identify the semantic relations holding between entities in the text. The goal of the task is to train classifiers for low-resource languages by means of the annotated data from high-resource languages. Related methods usually employ parallel data or Machine Translator (MT) to project annotated data from a source to a target language. However, the availability and the quality of parallel data and MT are big challenges for low-resource languages. In this paper, a novel transfer learning method is presented for this task. The key idea is to utilize a tree-based representation of data, which is highly informative for classifying semantic relations, and also shared among different languages. All the training and test data are shown using this representation. We propose to use the Universal Dependency (UD) parsing, which is a language-agnostic formalism for representation of syntactic structures. Equipping UD parse trees with multi-lingual word embeddings makes an ideal representation for the cross-language relation extraction task. We propose two deep networks to use this representation. The first one utilizes the Shortest Dependency Path of UD trees, while the second employs the UD-based positional embeddings. Experiments are performed using SemEval 2010-task 8 training data, whereas French and Farsi are the test languages. The results show 63.9% and 56.2% F1 scores, for French and Farsi test data, respectively, which are 14.4% and 17.9% higher than the baseline. This work can be considered a simple yet powerful baseline for further investigation into the cross-language tasks. © 2021","Dependency context; Relation extraction; Tree-based models; Universal Dependency Parsing","Embeddings; Extraction; Learning systems; Semantics; Syntactics; Transfer learning; Cross languages; Low resource languages; Parallel data; Relation extraction; Semantic relations; Syntactic structure; Target language; Transfer learning methods; Trees (mathematics)",Article,Scopus,2-s2.0-85111004456
"Li R., Zheng S., Zhang C., Duan C., Su J., Wang L., Atkinson P.M.","57061176600;8265381500;56605657100;57215424832;57219790529;57222867791;7201906181;","Multiattention Network for Semantic Segmentation of Fine-Resolution Remote Sensing Images",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110895550&doi=10.1109%2fTGRS.2021.3093977&partnerID=40&md5=6a443c30de2a42514a11fddf1977fd4d","Semantic segmentation of remote sensing images plays an important role in a wide range of applications, including land resource management, biosphere monitoring, and urban planning. Although the accuracy of semantic segmentation in remote sensing images has been increased significantly by deep convolutional neural networks, several limitations exist in standard models. First, for encoder-decoder architectures such as U-Net, the utilization of multiscale features causes the underuse of information, where low-level features and high-level features are concatenated directly without any refinement. Second, long-range dependencies of feature maps are insufficiently explored, resulting in suboptimal feature representations associated with each semantic class. Third, even though the dot-product attention mechanism has been introduced and utilized in semantic segmentation to model long-range dependencies, the large time and space demands of attention impede the actual usage of attention in application scenarios with large-scale input. This article proposed a multiattention network (MANet) to address these issues by extracting contextual dependencies through multiple efficient attention modules. A novel attention mechanism of kernel attention with linear complexity is proposed to alleviate the large computational demand in attention. Based on kernel attention and channel attention, we integrate local feature maps extracted by ResNet-50 with their corresponding global dependencies and reweight interdependent channel maps adaptively. Numerical experiments on two large-scale fine-resolution remote sensing datasets demonstrate the superior performance of the proposed MANet. Code is available at https://github.com/lironui/Multi-Attention-Network. © 1980-2012 IEEE.","Attention mechanism; fine-resolution remote sensing images; semantic segmentation","Convolutional neural networks; Deep neural networks; Image segmentation; Large dataset; Mobile ad hoc networks; Semantic Web; Semantics; Space optics; Application scenario; Computational demands; Encoder-decoder architecture; Feature representation; Long-range dependencies; Numerical experiments; Remote sensing images; Semantic segmentation; Remote sensing; artificial neural network; image resolution; remote sensing; segmentation; semantic standardization",Article,Scopus,2-s2.0-85110895550
"Alwaneen T.H., Azmi A.M., Aboalsamh H.A., Cambria E., Hussain A.","57226165805;6603711563;6508339935;56140547500;19734290900;","Arabic question answering system: a survey",2022,"Artificial Intelligence Review","55","1",,"207","253",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110677244&doi=10.1007%2fs10462-021-10031-1&partnerID=40&md5=16eabaf1c7989ed33f51245c06429153","Question answering is a subfield of information retrieval. It is a task of answering a question posted in a natural language. A question answering system (QAS) may be considered a good alternative to search engines that return a set of related documents. The QAS system is composed of three main modules; question analysis, passage retrieval, and answer extraction. Over the years, numerous QASs have been presented for use in different languages. However, the the development of Arabic QASs has been slowed by linguistic challenges and the lack of resources and tools available to researchers. In this survey, we start with the challenges due to the language and how these challenges make the development of new Arabic QAS more difficult. Next, we do a detailed review of several Arabic QASs. This is followed by an in-depth analysis of the techniques and approaches in the three modules of a QAS. We present an overview of important and recent tools that were developed to help the researchers in this field. We also cover the available Arabic and multilingual datasets, and a look at the different measures used to assess QASs. Finally, the survey delves into the future direction of Arabic QAS systems based on the current state-of-the-art techniques developed for question answering in other languages. © 2021, The Author(s), under exclusive licence to Springer Nature B.V.","Answer extraction; Arabic question answering system; Information retrieval; Passage retrieval; Question analysis; Survey","Search engines; Surveys; Answer extraction; In-depth analysis; Natural languages; Passage retrieval; Question analysis; Question Answering; Question answering systems; State-of-the-art techniques; Natural language processing systems",Article,Scopus,2-s2.0-85110677244
"Zhou Y., Ye Q., Lv J.","57219795316;57218994836;57215374165;","Communication-Efficient Federated Learning with Compensated Overlap-FedAvg",2022,"IEEE Transactions on Parallel and Distributed Systems","33","1","9459540","192","205",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110582345&doi=10.1109%2fTPDS.2021.3090331&partnerID=40&md5=a52e48d0d3d1fb05e1e40b05e65f8c12","While petabytes of data are generated each day by a number of independent computing devices, only a few of them can be finally collected and used for deep learning (DL) due to the apprehension of data security and privacy leakage, thus seriously retarding the extension of DL. In such a circumstance, federated learning (FL) was proposed to perform model training by multiple clients' combined data without the dataset sharing within the cluster. Nevertheless, federated learning with periodic model averaging (FedAvg) introduced massive communication overhead as the synchronized data in each iteration is about the same size as the model, and thereby leading to a low communication efficiency. Consequently, variant proposals focusing on the communication rounds reduction and data compression were proposed to decrease the communication overhead of FL. In this article, we propose Overlap-FedAvg, an innovative framework that loosed the chain-like constraint of federated learning and paralleled the model training phase with the model communication phase (i.e., uploading local models and downloading the global model), so that the latter phase could be totally covered by the former phase. Compared to vanilla FedAvg, Overlap-FedAvg was further developed with a hierarchical computing strategy, a data compensation mechanism, and a nesterov accelerated gradients (NAG) algorithm. In Particular, Overlap-FedAvg is orthogonal to many other compression methods so that they could be applied together to maximize the utilization of the cluster. Besides, the theoretical analysis is provided to prove the convergence of the proposed framework. Extensive experiments conducting on both image classification and natural language processing tasks with multiple models and datasets also demonstrate that the proposed framework substantially reduced the communication overhead and boosted the federated learning process. © 1990-2012 IEEE.","Distributed computing; efficient communication; federated learning; overlap","Classification (of information); Cost reduction; Deep learning; Iterative methods; Natural language processing systems; Privacy by design; Communication efficiency; Communication overheads; Communication rounds; Compression methods; Computing strategies; Data compensation; Data security and privacy; NAtural language processing; Learning systems",Article,Scopus,2-s2.0-85110582345
"Ocaña M., Chapela-Campa D., Álvarez P., Hernández N., Mucientes M., Fabra J., Llamazares Á., Lama M., Revenga P.A., Bugarín A., García-Garrido M.A., Alonso J.M.","56410080500;57203988184;35271271700;35760976400;6506530208;15019133700;36632755700;12239276700;12790320200;55910844200;13005796000;55618144400;","Automatic linguistic reporting of customer activity patterns in open malls",2022,"Multimedia Tools and Applications","81","3",,"3369","3395",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109612951&doi=10.1007%2fs11042-021-11186-3&partnerID=40&md5=6b57179b8c8370c47b0ffffab7f95cfa","In this work, we present a complete system to produce an automatic linguistic reporting about the customer activity patterns inside open malls, a mixed distribution of classical malls joined with the shops on the street. These reports can assist to design marketing campaigns by means of identifying the best places to catch the attention of customers. Activity patterns are estimated with process mining techniques and the key information of localization. Localization is obtained with a parallelized solution based on WiFi fingerprint system to speed up the solution. In agreement with the best practices for human evaluation of natural language generation systems, the linguistic quality of the generated report was evaluated by 41 experts who filled in an online questionnaire. Results are encouraging, since the average global score of the linguistic quality dimension is 6.17 (0.76 of standard deviation) in a 7-point Likert scale. This expresses a high degree of satisfaction of the generated reports and validates the adequacy of automatic natural language textual reports as a complementary tool to process model visualization. © 2021, The Author(s).","Automatic linguistic reporting; Data mining techniques; Localization; Parallelization strategies; Social workflows","Data mining; Electronic assessment; Linguistics; Natural language processing systems; Online systems; Sales; Complementary tools; Degree of satisfaction; Fingerprint systems; Marketing campaign; Mixed distribution; Natural language generation systems; Online questionnaire; Standard deviation; Quality control",Article,Scopus,2-s2.0-85109612951
"Hettiarachchi H., Adedoyin-Olowe M., Bhogal J., Gaber M.M.","57190174249;55859916800;55291053500;8927664800;","Embed2Detect: temporally clustered embedded words for event detection in social media",2022,"Machine Learning","111","1",,"49","87",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108607674&doi=10.1007%2fs10994-021-05988-7&partnerID=40&md5=75499a9b25e7a56b68d3a82a4e042b38","Social media is becoming a primary medium to discuss what is happening around the world. Therefore, the data generated by social media platforms contain rich information which describes the ongoing events. Further, the timeliness associated with these data is capable of facilitating immediate insights. However, considering the dynamic nature and high volume of data production in social media data streams, it is impractical to filter the events manually and therefore, automated event detection mechanisms are invaluable to the community. Apart from a few notable exceptions, most previous research on automated event detection have focused only on statistical and syntactical features in data and lacked the involvement of underlying semantics which are important for effective information retrieval from text since they represent the connections between words and their meanings. In this paper, we propose a novel method termed Embed2Detect for event detection in social media by combining the characteristics in word embeddings and hierarchical agglomerative clustering. The adoption of word embeddings gives Embed2Detect the capability to incorporate powerful semantical features into event detection and overcome a major limitation inherent in previous approaches. We experimented our method on two recent real social media data sets which represent the sports and political domain and also compared the results to several state-of-the-art methods. The obtained results show that Embed2Detect is capable of effective and efficient event detection and it outperforms the recent event detection methods. For the sports data set, Embed2Detect achieved 27% higher F-measure than the best-performed baseline and for the political data set, it was an increase of 29%. © 2021, The Author(s).","Dendrogram; Hierarchical clustering; Social media; Vocabulary; Word embedding","Data streams; Embeddings; Hierarchical clustering; Semantics; Social networking (online); Sports; Data production; Dynamic nature; Event detection; Hierarchical agglomerative clustering; High volumes; Social media datum; Social media platforms; State-of-the-art methods; Feature extraction",Article,Scopus,2-s2.0-85108607674
"Mohammed H.S.","35225155200;","Bayesian Prediction Bounds from a Family of Exponentiated Distributions in the Presence of Outliers",2022,"American Journal of Mathematical and Management Sciences","41","1",,"88","99",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107614989&doi=10.1080%2f01966324.2021.1931587&partnerID=40&md5=3783ed00c1014f1cbeb651edef3ac21c","In this paper, Bayesian prediction bounds for order statistics of future observations from a family of exponentiated distributions are obtained in the presence of a single outlier arising from different members of the same family of distributions. During an experimentation, we come across circumstances where one or more observations may not be homogeneous to rest of the observations and hence can be treated as outliers. Nowadays, the classification for outlier prediction are applied in various fields like bioinformatics, natural language processing, military application, geographical domains etc. We consider single outliers of two types in future observations when the sample size of the future sample is a random variable. The exponentiated exponential distribution has been used as a special case from the suggested family. We introduce numerical examples and compute Bayesian prediction bounds based on the real data, by using Markov chain Monte Carlo (MCMC) algorithm. © 2021 Taylor & Francis Group, LLC.","Markov chain Monte Carlo procedure; exponentiated exponential distribution; Single outliers","Computer aided diagnosis; Forecasting; Markov chains; Natural language processing systems; Bayesian predictions; Exponential distributions; Exponentiated distributions; Future observations; Markov chain monte carlo algorithms; NAtural language processing; Order statistics; Sample sizes; Statistics",Article,Scopus,2-s2.0-85107614989
"Li J., Feng C., Lin X., Qian X.","57221572457;36451839400;55260317200;56492087400;","Utilizing GCN and Meta-Learning Strategy in Unsupervised Domain Adaptation for Pancreatic Cancer Segmentation",2022,"IEEE Journal of Biomedical and Health Informatics","26","1",,"79","89",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107381955&doi=10.1109%2fJBHI.2021.3085092&partnerID=40&md5=9eba2c3738f3a3f44e719000179b1f02","Automated pancreatic cancer segmentation is highly crucial for computer-assisted diagnosis. The general practice is to label images from selected modalities since it is expensive to label all modalities. This practice brought about a significant interest in learning the knowledge transfer from the labeled modalities to unlabeled ones. However, the imaging parameter inconsistency between modalities leads to a domain shift, limiting the transfer learning performance. Therefore, we propose an unsupervised domain adaptation segmentation framework for pancreatic cancer based on GCN and meta-learning strategy. Our model first transforms the source image into a target-like visual appearance through the synergistic collaboration between image and feature adaptation. Specifically, we employ encoders incorporating adversarial learning to separate domain-invariant features from domain-specific ones to achieve visual appearance translation. Then, the meta-learning strategy with good generalization capabilities is exploited to strike a reasonable balance in the training of the source and transformed images. Thus, the model acquires more correlated features and improve the adaptability to the target images. Moreover, a GCN is introduced to supervise the high-dimensional abstract features directly related to the segmentation outcomes, and hence ensure the integrity of key structural features. Extensive experiments on four multi-parameter pancreatic-cancer magnetic resonance imaging datasets demonstrate improved performance in all adaptation directions, confirming our model's effectiveness for unlabeled pancreatic cancer images. The results are promising for reducing the burden of annotation and improving the performance of computer-aided diagnosis of pancreatic cancer. Our source codes will be released at https://github.com/SJTUBME-QianLab/UDAseg, once this manuscript is accepted for publication. © 2013 IEEE.","graph convolutional network; meta-learning; Pancreatic cancer segmentation; unsupervised domain adaptation","Computer aided diagnosis; Diseases; Knowledge management; Learning systems; Magnetic resonance imaging; Natural language processing systems; Transfer learning; Visualization; Adversarial learning; Computer assisted diagnosis; Feature adaptation; Generalization capability; Invariant features; Learning performance; Meta learning strategy; Pancreatic cancers; Image enhancement; Article; functional connectivity; image segmentation; mathematical model; nerve cell network; pancreas cancer; prediction; qualitative analysis; quantitative analysis; root mean squared error; transfer of learning",Article,Scopus,2-s2.0-85107381955
"Zheng X., Wang B., Du X., Lu X.","56022876500;57200180033;57221663724;57214860554;","Mutual Attention Inception Network for Remote Sensing Visual Question Answering",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107335044&doi=10.1109%2fTGRS.2021.3079918&partnerID=40&md5=6d7c7ba0d1c3f7ac6318774c4d3cfc62","Remote sensing images (RSIs) containing various ground objects have been applied in many fields. To make semantic understanding of RSIs objective and interactive, the task remote sensing visual question answering (VQA) has appeared. Given an RSI, the goal of remote sensing VQA is to make an intelligent agent answer a question about the remote sensing scene. Existing remote sensing VQA methods utilized a nonspatial fusion strategy to fuse the image features and question features, which ignores the spatial information of images and word-level information of questions. A novel method is proposed to complete the task considering these two aspects. First, convolutional features of the image are included to represent spatial information, and the word vectors of questions are adopted to present semantic word information. Second, attention mechanism and bilinear technique are introduced to enhance the feature considering the alignments between spatial positions and words. Finally, a fully connected layer with softmax is utilized to output an answer from the perspective of the multiclass classification task. To benchmark this task, a RSIVQA dataset is introduced in this article. For each of more than 37 000 RSIs, the proposed dataset contains at least one or more questions, plus corresponding answers. Experimental results demonstrate that the proposed method can capture the alignments between images and questions. The code and dataset are available at https://github.com/spectralpublic/RSIVQA. © 1980-2012 IEEE.","Attention mechanism; Feature fusion; Remote sensing visual question answering (RSVQA); Semantic understanding","Alignment; Semantics; Attention mechanisms; Fusion strategies; Multi-class classification; Question Answering; Remote sensing images; Semantic understanding; Spatial informations; Spatial positions; Remote sensing; data set; image analysis; image classification; remote sensing",Article,Scopus,2-s2.0-85107335044
"Ma M., Na S., Wang H., Chen C., Xu J.","57225713355;57213065510;55840085200;56941056300;57209290378;","The graph-based behavior-aware recommendation for interactive news",2022,"Applied Intelligence","52","2",,"1913","1929",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107324174&doi=10.1007%2fs10489-021-02497-x&partnerID=40&md5=b7d80aebd6d3c936836569684eaf1cfc","Interactive news recommendation has been launched and attracted much attention recently. In this scenario, user’s behavior evolves from single click behavior to multiple behaviors including like, comment, share etc. However, most of the existing methods still use single click behavior as the unique criterion of judging user’s preferences. Further, although heterogeneous graphs have been applied in different areas, a proper way to construct a heterogeneous graph for interactive news data with an appropriate learning mechanism on it is still desired. To address the above concerns, we propose a graph-based behavior-aware network, which simultaneously considers six different types of behaviors as well as user’s demand on the news diversity. We have three mainsteps. First, we build an interaction behavior graph for multi-level and multi-category data. Second, we apply DeepWalk on the behavior graph to obtain entity semantics, then build a graph-based convolutional neural network called G-CNN to learn news representations, and an attention-based LSTM to learn behavior sequence representations. Third, we introduce core and coritivity features for the behavior graph, which measure the concentration degree of user’s interests. These features affect the trade-off between accuracy and diversity of our personalized recommendation system. Taking these features into account, our system finally achieves recommending news to different users at their different levels of concentration degrees. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Concentration feature; Interaction behavior graph; Interactive news recommendation","Convolutional neural networks; Economic and social effects; Graphic methods; Recommender systems; Semantics; Behavior graphs; Behavior sequences; Concentration degree; Heterogeneous graph; Interaction behavior; Learning mechanism; News recommendation; Personalized recommendation systems; Long short-term memory",Article,Scopus,2-s2.0-85107324174
"Lu X., Deng Y., Sun T., Gao Y., Feng J., Sun X., Sutcliffe R.","57222812260;57224223531;57224214029;57204727222;57189841077;56341695200;8878266300;","MKPM: Multi keyword-pair matching for natural language sentences",2022,"Applied Intelligence","52","2",,"1878","1892",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107287596&doi=10.1007%2fs10489-021-02306-5&partnerID=40&md5=f2b89963feb60545f07137e890f433cc","Sentence matching is widely used in various natural language tasks, such as natural language inference, paraphrase identification and question answering. For these tasks, we need to understand the logical and semantic relationship between two sentences. Most current methods use all information within a sentence to build a model and hence determine its relationship to another sentence. However, the information contained in some sentences may cause redundancy or introduce noise, impeding the performance of the model. Therefore, we propose a sentence matching method based on multi keyword-pair matching (MKPM), which uses keyword pairs in two sentences to represent the semantic relationship between them, avoiding the interference of redundancy and noise. Specifically, we first propose a sentence-pair-based attention mechanism sp-attention to select the most important word pair from the two sentences as a keyword pair, and then propose a Bi-task architecture to model the semantic information of these keyword pairs. The Bi-task architecture is as follows: 1. In order to understand the semantic relationship at the word level between two sentences, we design a word-pair task (WP-Task), which uses these keyword pairs to complete sentence matching independently. 2. We design a sentence-pair task (SP-Task) to understand the sentence level semantic relationship between the two sentences by sentence denoising. Through the integration of the two tasks, our model can understand sentences more accurately from the two granularities of word and sentence. Experimental results show that our model can achieve state-of-the-art performance in several tasks. Our source code is publicly available1. © 2021, The Author(s).","Bi-task architecture; Multi keyword-pair; Sentence matching","Redundancy; Semantics; Attention mechanisms; Natural languages; Paraphrase identifications; Question Answering; Semantic information; Semantic relationships; State-of-the-art performance; Task architectures; Natural language processing systems",Article,Scopus,2-s2.0-85107287596
"Qian Y., Li X., Zhang Q., Zhang J.","56446116500;57224205980;57224899084;36667959700;","SPP-CPI: Predicting Compound-Protein Interactions Based on Neural Networks",2022,"IEEE/ACM Transactions on Computational Biology and Bioinformatics","19","1",,"40","47",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107172894&doi=10.1109%2fTCBB.2021.3084397&partnerID=40&md5=80229542b82b628231706ee703157593","Identifying interactions between compound and protein is a substantial part of the drug discovery process. Accurate prediction of interaction relationships can greatly reduce the time of drug development. The uniqueness of our method lies in three aspects:1) it represents a compound with a distance matrix. A distance matrix can capture the structural information, compared with the SMILES string. On the other hand, a distance matrix does not require complex data preprocessing for the molecular structure as the molecular graph representation, and is easier to obtain; 2) it uses SPP(Spatial pyramid pooling)-net to extract compound features, which has been successfully applied in image classification; and 3) it extracts protein features through the natural language processing method (doc2vec) to obtain sequence semantic information. We evaluated our method on three benchmark datasets - human, C.elegans, and DUDE - and the experimental results demonstrate that our proposed model presents competitive performance against state-of-the-art predictors. We also carried out drug-drug interaction (DDI) experiments to verify the strong potential of distance matrix as molecular characteristics. The source code and datasets are available at https://github.com/lxlsu/SPP_CPI. © 2004-2012 IEEE.","doc2vec; Interaction prediction; residual block; SPP-net","Benchmarking; Classification (of information); Natural language processing systems; Neural networks; Proteins; Semantics; Competitive performance; Drug discovery process; Drug-drug interactions; Interaction relationship; Molecular characteristics; NAtural language processing; Semantic information; Structural information; Drug interactions; cyclopropane derivative; cyclopropapyrroloindole; indole derivative; protein; drug development; human; software; Cyclopropanes; Drug Discovery; Humans; Indoles; Neural Networks, Computer; Proteins; Software",Article,Scopus,2-s2.0-85107172894
"Yang S., Feng D., Liu Y., Li D.","57188971330;55984485000;57263989800;57204110845;","Distant context aware text generation from abstract meaning representation",2022,"Applied Intelligence","52","2",,"1672","1685",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106458486&doi=10.1007%2fs10489-021-02431-1&partnerID=40&md5=f3132a63d6608be2d8fd1eb9f0f818cd","Text generation from abstract meaning representation is a fundamental task in natural language generation. An interesting challenge is that distant context could influence the surface realization for each node. In the previous encoder-decoder based approaches, graph neural networks have been commonly used to encode abstract meaning representation graphs and exhibited superior performance over the sequence and tree encoders. However, most of them cannot stack numerous layers, thus being too shallow to capture distant context. In this paper, we propose solutions from three aspects. Firstly, we introduce a Transformer based graph encoder to embed abstract meaning representation graphs. This encoder can stack more layers to encode larger context, while without performance degrading. Secondly, we expand the receptive field of each node, i.e. building direct connections between node pairs, to capture the information of its distant neighbors. We also exploit relative position embedding to make the model aware of the original hierarchy of graphs. Thirdly, we encode the linearized version of abstract meaning representation with the pre-trained language model to get the sequence encoding and incorporate it into graph encoding to enrich features. We conduct experiments on LDC2015E86 and LDC2017T10. Experimental results demonstrate that our method outperforms previous strong baselines. Especially, we investigate the performance of our model on large graphs, finding a larger performance gain. Our best model achieves 31.99 of BLEU and 37.02 of METEOR on LDC2015E86, 34.21 of BLEU, and 39.26 of METEOR on LDC2017T10, which are new states of the art. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Abstract meaning representation; Graph encoder; Receptive field; Text generation","Encoding (symbols); Graphic methods; Natural language processing systems; Neural networks; Signal encoding; Encoder-decoder; Graph neural networks; Natural language generation; Performance Gain; Receptive fields; Relative positions; Sequence encoding; Text generations; Trees (mathematics)",Article,Scopus,2-s2.0-85106458486
"Mishra S.K., Saini N., Saha S., Bhattacharyya P.","57212412046;57206032797;17435835500;7101803108;","Scientific document summarization in multi-objective clustering framework",2022,"Applied Intelligence","52","2",,"1520","1543",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106408615&doi=10.1007%2fs10489-021-02376-5&partnerID=40&md5=27926105ed24f59da9e33188c1b12e8f","The exponential growth in the number of scientific articles has made it difficult for the researchers to keep themselves updated with the new developments. Scientific document summarization solves this problem by providing a summary of essential contributions. In this paper, we have presented a novel method of scientific document summarization using a multi-objective differential evolution technique. Here, firstly distinct important sentences are extracted by using citation contextualization. These sentences are further clustered using the concept of multi-objective clustering. Two objective functions, PBM index, and XB index, measuring the compactness and separation of sentence clusters, are simultaneously optimized utilizing the search capability of multi-objective differential evolution. We have conducted our experiments on CL-SciSumm 2016, CL-SciSumm 2017, CL-SciSumm 2018, and CL-SciSumm 2019 datasets. Obtained results of CL-SciSumm 2016 and CL-SciSumm 2017 are compared with the state-of-the-art methods. Evaluation results demonstrate that our method outperforms others in terms of ROUGE-2, ROUGE-3, and ROUGE-SU4 scores. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Clustering; Multi-objective optimization; Text summarization; Word mover’s distance","Evolutionary algorithms; Optimization; Exponential growth; Multi-objective clustering; Multi-objective differential evolutions; Objective functions; Scientific articles; Scientific documents; Search capabilities; State-of-the-art methods; Natural language processing systems",Article,Scopus,2-s2.0-85106408615
"Tóth Á., Suta A., Szauter F.","57215031756;57223709185;6505559342;","Interrelation between the climate-related sustainability and the financial reporting disclosures of the European automotive industry",2022,"Clean Technologies and Environmental Policy","24","1",,"437","445",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106032227&doi=10.1007%2fs10098-021-02108-w&partnerID=40&md5=054f858ee0c7a7ea061d2f97a221bd02","The financial reports of the automotive companies' are measured in a standardized manner; therefore, they are transparent and comparable to each other, but this is not valid for the sustainability reports and it is not possible to compare their sustainability performances. Standard-setting organizations are currently searching for better reporting procedures. This study aims to investigate the connection between sustainability and financial reports for the most dominant European car manufacturers. It reviews the traceability of the sustainability elements back to the financial statements, which helps transparency, comparability, and impact measurement of the disclosed items and issues. This investigation allowed us to additionally review whether these companies are targeting to disclose the most harmful pollution impacts, or only focus to disclose the required obligatory items. Given the financial and sustainability reports magnitude manual testing would not provide complete and proper coverage, therefore we utilized an automated and AI-assisted content analysis with natural language processing. In this new review method, the sustainable elements of the textual reports were automatically retrieved following the 5-stage model of Landrum & Ohsowski (2018). The study highlights the lack of true sustainability information content of reports and the potential discrepancies and connections between the financial and the sustainability reports. Findings concluded that sustainability disclosures at the reviewed companies from several aspects could be improved and quantified, traced back to the financial disclosures, and to be comparable to each other if they apply a similar review method. Graphic abstract: [Figure not available: see fulltext.]. © 2021, The Author(s).","Automotive companies; Carbon emission; Financial reporting; Global reporting initiative; IFRS; Sustainability reporting","Automobile manufacture; Finance; Natural language processing systems; Automotive companies; Financial statements; NAtural language processing; Standard setting organization; Sustainability disclosures; Sustainability informations; Sustainability performance; Sustainability report; Sustainable development",Article,Scopus,2-s2.0-85106032227
"Hui B., Zhang L., Zhou X., Wen X., Nian Y.","16506742000;56404489100;9841116600;57223629161;57217101703;","Personalized recommendation system based on knowledge embedding and historical behavior",2022,"Applied Intelligence","52","1",,"954","966",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105926740&doi=10.1007%2fs10489-021-02363-w&partnerID=40&md5=a94eec8d4d7c3c30335f2361b4016f90","Collaborative filtering (CF) usually suffers from limited performance in recommendation systems due to the sparsity of user–item interactions and cold start problems. To address these issues, auxiliary information from knowledge graphs, such as social networks and item properties, is typically used to boost performance. The current recommended algorithms based on knowledge graphs fail to utilize rich semantic associations. In this paper, we regard knowledge graphs as heterogeneous networks to add auxiliary information, propose a recommendation system with unified embeddings of behavior and knowledge features, and mine user preferences from their historical behavior and knowledge graphs to provide more accurate and diverse recommendations to the users. Our proposed ReBKC shows a significant improvement on three datasets compared to state-of-the-art methods. These results verify the effectiveness of learning short-term and long-term user preferences from their historical behavior and by integrating knowledge graphs to deeply identify user preferences. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Collaborative filtering; Historical behavior; Knowledge graph; Recommendation system","Collaborative filtering; Embeddings; Graph algorithms; Graphic methods; Heterogeneous networks; Knowledge representation; Semantics; Auxiliary information; Cold start problems; Knowledge embedding; Knowledge graphs; Personalized recommendation systems; Semantic associations; Short term; State-of-the-art methods; Recommender systems",Article,Scopus,2-s2.0-85105926740
"Feng Y., Sun X., Diao W., Li J., Gao X., Fu K.","57210817570;34875643000;56816620400;57219025113;57208287563;7202283802;","Continual Learning with Structured Inheritance for Semantic Segmentation in Aerial Imagery",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105864798&doi=10.1109%2fTGRS.2021.3076664&partnerID=40&md5=16bc37d541ab22f191ebc2374d960192","With the rapid update and iteration of current aerial image data, the continual learning scenarios and catastrophic forgetting problem attracted increased attention, especially in the semantic segmentation task. However, the existing methods mainly focus on the class continual learning in a single task and are not satisfactory when extended to multiple tasks. In this article, we consider more realistic and complicated settings, namely task continual learning. We revisit the characteristics of semantic segmentation and knowledge distillation (KD) strategy, then propose a general and effective framework, named structured inheritance, to learn new tasks while retaining high performance on old tasks. Specifically, we present two structure-preserving penalties: pixel affinity structure loss and representation consistency structure loss. The former breaks the isolation of pixels and retains the pixel interactive information learned by the old tasks. At the same time, the latter protects high-frequency stationary information between sequence semantic segmentation tasks. Our approach does not need to add extra parameters nor does it need to access the data stream of the old tasks. Therefore, it can be applied in practical applications with strict computational burden, memory cost, and storage budget. Extensive continual learning experiments on four semantic segmentation datasets of Vaihingen, Potsdam, DeepGlobe, and Gaofen challenge semantic segmentation dataset (GCSS) prove the effectiveness of our proposed framework, which outperforms the current state-of-the-art methods and even exceeds the theoretical upper-bound performance of multitask learning. The code and models will be made publicly available. © 1980-2012 IEEE.","Continual learning; deep convolution neural network; remote sensing; semantic segmentation","Aerial photography; Antennas; Budget control; Data streams; Digital storage; Distillation; Image segmentation; Iterative methods; Pixels; Semantics; Aerial image data; Catastrophic forgetting problem; Computational burden; Continual learning; Interactive informations; Semantic segmentation; State-of-the-art methods; Structure-preserving; Learning systems; aerial photograph; computer simulation; experimental study; parameterization; performance assessment; pixel; segmentation",Article,Scopus,2-s2.0-85105864798
"Do P., Phan T.H.V.","55120354700;57218366609;","Developing a BERT based triple classification model using knowledge graph embedding for question answering system",2022,"Applied Intelligence","52","1",,"636","651",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105500609&doi=10.1007%2fs10489-021-02460-w&partnerID=40&md5=44758cf260943b05fd2d6f240419e5e2","The current BERT-based question answering systems use a question and a contextual text to find the answer. This causes the systems to return wrong answers or nothing if the text contains irrelevant contents with the input question. Besides, the systems haven’t answered yes-no and aggregate questions yet. Besides that, the systems only concentrate on the contents of text regardless of the relationship between entities in the corpus. This systems cannot validate the answer. In this paper, we presented a solution to solve these issues by using the BERT model and the knowledge graph to enhance a question answering system. We combined content-based and linked-based information for knowledge graph representation learning and classified triples into one of three classes such as base class, derived class, or non-existent class. We then used the BERT model to build two classifiers: BERT-based text classification for content information and BERT-based triple classification for link information. The former was able to make a contextual embedding vector for representing triples that were used to classify into the three above classes. The latter generated all path instances from all meta paths of a large heterogeneous information network by running the Motif Search method of Apache Spark on a distributed environment. After creating the path instances, we produced triples from these path instances. We made content-based information by converting triples into natural language text with labels and considered them as a text classification problem. Our proposed solution outperformed other embedding methods with an average accuracy of 92.34% on benchmark datasets and the Motif Finding algorithm with an average executive time improvement of 37% on the distributed environment. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","BERT based triple classification model; Knowledge graph embedding; Meta-path; Motif finding","Embeddings; Information services; Knowledge representation; Natural language processing systems; Text processing; Classification models; Content-based information; Distributed environments; Heterogeneous information; Motif finding algorithms; Natural language text; Question answering systems; Text classification; Classification (of information)",Article,Scopus,2-s2.0-85105500609
"Zhao R., Shi Z., Zou Z.","57225978410;23398841900;56073977200;","High-Resolution Remote Sensing Image Captioning Based on Structured Attention",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104250823&doi=10.1109%2fTGRS.2021.3070383&partnerID=40&md5=4c5e38ca698b9d7656acbbd01c1eb407","Automatically generating language descriptions of remote sensing images has become an emerging research hot spot in the remote sensing field. Attention-based captioning, as a representative group of recent deep learning-based captioning methods, shares the advantage of generating the words while highlighting corresponding object locations in the image. Standard attention-based methods generate captions based on coarse-grained and unstructured attention units, which fails to exploit structured spatial relations of semantic contents in remote sensing images. Although the structure characteristic makes remote sensing images widely divergent to natural images and poses a greater challenge for the remote sensing image captioning task, the key of most remote sensing captioning methods is usually borrowed from the computer vision community without considering the domain knowledge behind. To overcome this problem, a fine-grained, structured attention-based method is proposed to utilize the structural characteristics of semantic contents in high-resolution remote sensing images. Our method learns better descriptions and can generate pixelwise segmentation masks of semantic contents. The segmentation can be jointly trained with the captioning in a unified framework without requiring any pixelwise annotations. Evaluations are conducted on three remote sensing image captioning benchmark data sets with detailed ablation studies and parameter analysis. Compared with the state-of-the-art methods, our method achieves higher captioning accuracy and can generate high-resolution and meaningful segmentation masks of semantic contents at the same time. © 1980-2012 IEEE.","Image captioning; Image segmentation; Remote sensing image; Structured attention","Deep learning; Image segmentation; Semantics; High resolution remote sensing images; Language description; Remote sensing images; Segmentation masks; State-of-the-art methods; Structural characteristics; Structure characteristic; Vision communities; Remote sensing; ablation; computer vision; image analysis; language; remote sensing; segmentation; spectral resolution",Article,Scopus,2-s2.0-85104250823
"Hu Z.-Z., Leng S., Lin J.-R., Li S.-W., Xiao Y.-Q.","36147587900;57216968702;56703744700;55523768500;57209294238;","Knowledge Extraction and Discovery Based on BIM: A Critical Review and Future Directions",2022,"Archives of Computational Methods in Engineering","29","1",,"335","356",,3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104156752&doi=10.1007%2fs11831-021-09576-9&partnerID=40&md5=681ddfc3acfd78eda40b8098faaf3a3d","In the past, knowledge in the fields of Architecture, Engineering and Construction (AEC) industries mainly come from experiences and are documented in hard copies or specific electronic databases. In order to make use of this knowledge, a lot of studies have focused on retrieving and storing this knowledge in a systematic and accessible way. The Building Information Modeling (BIM) technology proves to be a valuable media in extracting data because it provides physical and functional digital models for all the facilities within the life-cycle of the project. Therefore, the combination of the knowledge science with BIM shows great potential in constructing the knowledge map in the field of the AEC industry. Based on literature reviews, this article summarizes the latest achievements in the fields of knowledge science and BIM, in the aspects of (1) knowledge description, (2) knowledge discovery, (3) knowledge storage and management, (4) knowledge inference and (5) knowledge application, to show the state-of-arts and suggests the future directions in the application of knowledge science and BIM technology in the fields of AEC industries. The review indicates that BIM is capable of providing information for knowledge extraction and discovery, by adopting semantic network, knowledge graph and some other related methods. It also illustrates that the knowledge is helpful in the design, construction, operation and maintenance periods of the AEC industry, but now it is only at the beginning stage. © 2021, The Author(s).",,"Architectural design; Digital storage; Electronics industry; Extraction; Knowledge management; Knowledge representation; Life cycle; Semantics; Architecture , engineering and construction industries; Building Information Model - BIM; Electronic database; Knowledge application; Knowledge extraction; Knowledge science; Literature reviews; Operation and maintenance; Data mining",Article,Scopus,2-s2.0-85104156752
"Guan J., Liu J., Feng P., Wang W.","57202816841;57218452985;56517583500;56100717600;","Multiscale Deep Neural Network with Two-Stage Loss for SAR Target Recognition with Small Training Set",2022,"IEEE Geoscience and Remote Sensing Letters","19",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103274597&doi=10.1109%2fLGRS.2021.3064578&partnerID=40&md5=22b0d4f4934ba97339dd30005a9bf53f","Deep learning models have been used recently for target recognition from synthetic aperture radar (SAR) images. However, the performance of these models tends to deteriorate when only a small number of training samples are available due to the problem of overfitting. To address this problem, we propose a two-stage multiscale densely connected convolutional neural networks (TMDC-CNNs). In the proposed TMDC-CNNs, the overfitting issue is addressed with a novel multiscale densely connected network architecture and a two-stage loss function, which integrated the cosine similarity with the prevailing softmax cross-entropy loss. Experiments were conducted on the MSTAR data set, and the results show that our model offers significant recognition accuracy improvements as compared with other state-of-the-art methods, with severely limited training data. The source codes are available at https://github.com/Stubsx/TMDC-CNNs. © 2004-2012 IEEE.","Deep learning; limited data; synthetic aperture radar (SAR); target recognition","Convolutional neural networks; Deep learning; Deep neural networks; Natural language processing systems; Network architecture; Synthetic aperture radar; Cosine similarity; Densely connected networks; Limited training data; Recognition accuracy; State-of-the-art methods; Synthetic aperture radar (SAR) images; Target recognition; Training sample; Radar target recognition; artificial neural network; data set; satellite data; synthetic aperture radar",Article,Scopus,2-s2.0-85103274597
"Li R., Zheng S., Duan C., Su J., Zhang C.","57061176600;8265381500;57215424832;57219790529;56605657100;","Multistage Attention ResU-Net for Semantic Segmentation of Fine-Resolution Remote Sensing Images",2022,"IEEE Geoscience and Remote Sensing Letters","19",,,"","",,6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103205930&doi=10.1109%2fLGRS.2021.3063381&partnerID=40&md5=896ec5dff72ae98794ce1fe834c5bd39","The attention mechanism can refine the extracted feature maps and boost the classification performance of the deep network, which has become an essential technique in computer vision and natural language processing. However, the memory and computational costs of the dot-product attention mechanism increase quadratically with the spatiotemporal size of the input. Such growth hinders the usage of attention mechanisms considerably in application scenarios with large-scale inputs. In this letter, we propose a linear attention mechanism (LAM) to address this issue, which is approximately equivalent to dot-product attention with computational efficiency. Such a design makes the incorporation between attention mechanisms and deep networks much more flexible and versatile. Based on the proposed LAM, we refactor the skip connections in the raw U-Net and design a multistage attention ResU-Net (MAResU-Net) for semantic segmentation from fine-resolution remote sensing images. Experiments conducted on the Vaihingen data set demonstrated the effectiveness and efficiency of our MAResU-Net. Our code is available at <uri>https://github.com/lironui/MAResU-Net</uri>. 1558-0571 © 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.","Complexity theory; Decoding; Feature extraction; Image segmentation; Remote sensing; Semantics; Task analysis","Computational efficiency; Efficiency; Image segmentation; Natural language processing systems; Semantics; Application scenario; Attention mechanisms; Classification performance; Computational costs; Effectiveness and efficiencies; NAtural language processing; Remote sensing images; Semantic segmentation; Remote sensing; computer vision; data set; image analysis; image classification; remote sensing; semantic standardization; spatial resolution; Vaihingen an der Enz",Article,Scopus,2-s2.0-85103205930
"Zhao K., Liu Y., Hao S., Lu S., Liu H., Zhou L.","36610992900;57192251754;55554767400;57219876876;57221580919;55995752700;","Bounding Boxes Are All We Need: Street View Image Classification via Context Encoding of Detected Buildings",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103176876&doi=10.1109%2fTGRS.2021.3064316&partnerID=40&md5=f6978f55776a333e840b7fee61b3d8b4","Street view image classification aiming at the urban land use analysis is difficult because the class labels (e.g., commercial area) are concepts with higher abstract levels compared to the ones of general visual tasks (e.g., persons and cars). Therefore, classification models using only visual features often fail to achieve satisfactory performance. In this article, a novel approach based on a 'bottom-up and top-down' framework is proposed. Instead of using visual features of the whole image directly as common image-level models based on convolutional neural networks (CNNs) do, the proposed framework first obtains low-level semantic, namely, the bounding boxes of buildings in street view images through a bottom-up object discovery process. Their contextual information, such as the co-occurrence patterns of building classes and their layout, is then encoded into metadata by the proposed algorithm 'Context encOding of Detected buildINGs' (CODING). Finally, these metadata (low-level semantic encoded with context information) are abstracted to high-level semantic, namely, the land use label of the street view image through a top-down semantic aggregation process implemented by a recurrent neural network (RNN). In addition, in order to effectively discover low-level semantic as the bridge between visual features and higher abstract concepts, we made a dual-labeled data set named 'Building dEtection And Urban funcTional-zone portraYing' (BEAUTY) of 19070 street view images and 38857 buildings based on the existing BIC_GSV. The data set can be used not only for street view image classification but also for multiclass building detection. Experiments on 'BEAUTY' show that the proposed approach achieves a 12.65% performance improvement on macroprecision and 12% on macrorecall over image-level CNN-based models. Our code and data set are available at https://github.com/kyle-one/Context-Encoding-of-Detected-Buildings/. © 1980-2012 IEEE.","Building detection; context encoding; recurrent neural network (RNN); street view images classification; urban functional zone; urban land use classification","Buildings; Convolutional neural networks; Encoding (symbols); Image classification; Image enhancement; Land use; Metadata; Recurrent neural networks; Semantics; Signal encoding; Vision; Bottom-up and top-down; Classification models; Co-occurrence pattern; Context information; Contextual information; High level semantics; Recurrent neural network (RNN); Semantic aggregation; Classification (of information); algorithm; artificial neural network; building; data set; image classification; land use; metadata",Article,Scopus,2-s2.0-85103176876
"Lin D., Lin J., Zhao L., Wang Z.J., Chen Z.","57192194176;56726848100;57199018520;11241026700;56020772800;","Multilabel Aerial Image Classification with a Concept Attention Graph Neural Network",2022,"IEEE Transactions on Geoscience and Remote Sensing","60",,,"","",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102311411&doi=10.1109%2fTGRS.2020.3041461&partnerID=40&md5=6160a0a49f3ca6989c2f3825b0d61cd2","Compared with natural images, aerial images collected by satellite sensors/aerial cameras can provide a much larger field of view and often contain multiple objects of interest (multiple labels). There are certain limitations of existing multilabel aerial image classification methods. First, label correlations were often ignored in previous MAIC work, and thus, multilabel classifiers failed to be self-adapted. Second, existing multilabeled data sets for aerial images only cover limited images with fixed labels. Therefore, the underlying semantic correlations of labels cannot be fully included, while such correlation information is implicitly used as common knowledge by human beings. To tackle these concerns, we propose a novel multilabel classification method for aerial images. Our contributions are twofold. First, as the first attempt, label correlations are inferred from both the specific data set and ConceptNet (a popular knowledge graph for common sense). Second, based on graph neural network (GNN), we propose a novel end-to-end aerial image classification model, named the multiple label concept graph (ML-CG). ML-CG builds a concept graph to describe the semantic correlations from both the label set and the ConceptNet. We also incorporate both semantic attention and label attention in the GNN to better extract meaningful information of image labels. Compared with state-of-the-art methods, the effectiveness of the proposed method is demonstrated on both the commonly used UCM data set and a recently proposed DFC15 data set with high image resolution. © 1980-2012 IEEE.","Aerial images; graph neural network; multilabel image classification","Antennas; Classification (of information); Image resolution; Knowledge representation; Neural networks; Semantics; Classification methods; Classification models; Common knowledge; Graph neural networks; Label correlations; Multi-label classifications; Satellite sensors; State-of-the-art methods; Image classification; aerial photography; artificial neural network; data set; field of view; image classification; image resolution",Article,Scopus,2-s2.0-85102311411
"Tan H., Liu X., Yin B., Li X.","57209272265;36910875600;8616230700;57218467896;","Cross-Modal Semantic Matching Generative Adversarial Networks for Text-to-Image Synthesis",2022,"IEEE Transactions on Multimedia","24",,,"832","845",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101784148&doi=10.1109%2fTMM.2021.3060291&partnerID=40&md5=787528329591043b8fe2523bb8e9301d","Synthesizing photo-realistic images based on text descriptions is a challenging image generation problem. Although many recent approaches have significantly advanced the performance of text-to-image generation, to guarantee semantic matchings between the text description and synthesized image remains very challenging. In this paper, we propose a new model, Cross-modal Semantic Matching Generative Adversarial Networks (CSM-GAN), to improve the semantic consistency between text description and synthesized image for a fine-grained text-to-image generation. Two new modules are proposed in CSM-GAN: Text Encoder Module (TEM) and Textual-Visual Semantic Matching Module (TVSMM). TVSMM is aimed at making the distance of the pairs of synthesized image and its corresponding text description closer, in global semantic embedding space, than those of mismatched pairs. This improves the semantic consistency and consequently, the generalizability of CSM-GAN. In TEM, we introduce Text Convolutional Neural Networks (Text_CNNs) to capture and highlight local visual features in textual descriptions. Thorough experiments on two public benchmark datasets demonstrated the superiority of CSM-GAN over other representative state-of-the-art methods. © 1999-2012 IEEE.","Cross-modal semantic matching; generative adversarial network (GAN); text-CNNs; text-to-image synthesis","Convolutional neural networks; Semantic Web; Semantics; Adversarial networks; Benchmark datasets; Photorealistic images; Semantic consistency; Semantic embedding; State-of-the-art methods; Synthesized images; Textual description; Image enhancement",Article,Scopus,2-s2.0-85101784148
"Ben H., Pan Y., Li Y., Yao T., Hong R., Wang M., Mei T.","57222170674;55926067900;57191905508;36456529400;14010456800;57199061949;7005396035;","Unpaired Image Captioning with semantic-Constrained Self-Learning",2022,"IEEE Transactions on Multimedia","24",,,"904","916",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101742212&doi=10.1109%2fTMM.2021.3060948&partnerID=40&md5=1b25b3e67076f9fdcabe1bec5adb4531","Image captioning has been an emerging and fast-developing research topic. Nevertheless, most existing works heavily rely on large amounts of image-sentence pairs and therefore hinder the practical applications of captioning in the wild. In this paper, we present a novel Semantic-Constrained Self-learning (SCS) framework that explores an iterative self-learning strategy to learn an image captioner with only unpaired image and text data. Technically, SCS consists of two stages, i.e., pseudo pair generation and captioner re-training, iteratively producing ""pseudo""image-sentence pairs via a pre-trained captioner and re-training the captioner with the pseudo pairs, respectively. Particularly, both stages are guided by the recognized objects in the image, that act as semantic constraint to strengthen the semantic alignment between the input image and the output sentence. We leverage a semantic-constrained beam search for pseudo pair generation to regularize the decoding process with the recognized objects via forcing the inclusion/exclusion of the recognized/irrelevant objects in output sentence. For captioner re-training, a self-supervised triplet loss is utilized to preserve the relative semantic similarity ordering among generated sentences with regard to the input image triplets. Moreover, an object inclusion reward and an adversarial reward are adopted to encourage the inclusion of the predicted objects in the output sentence and pursue the generation of more realistic sentences during self-critical training, respectively. Experiments conducted on both dependent and independent unpaired data validate the superiority of SCS. More remarkably, we obtain the best published CIDEr score to-date of 74.7\% on COCO Karpathy test split for unpaired image captioning. © 1999-2012 IEEE.","Encoder-decoder networks; image captioning; self-supervised learning","Erbium compounds; Natural language processing systems; Constrained beams; Decoding process; Image captioning; Natural languages; Research topics; Semantic alignments; Semantic constraints; Semantic similarity; Semantics",Article,Scopus,2-s2.0-85101742212
"Araque O., Iglesias C.A.","56002164800;56357213400;","An Ensemble Method for Radicalization and Hate Speech Detection Online Empowered by Sentic Computing",2022,"Cognitive Computation","14","1",,"48","61",,2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101092143&doi=10.1007%2fs12559-021-09845-6&partnerID=40&md5=0582386c7bed32c6cabd9b33dd2e901c","The dramatic growth of the Web has motivated researchers to extract knowledge from enormous repositories and to exploit the knowledge in myriad applications. In this study, we focus on natural language processing (NLP) and, more concretely, the emerging field of affective computing to explore the automation of understanding human emotions from texts. This paper continues previous efforts to utilize and adapt affective techniques into different areas to gain new insights. This paper proposes two novel feature extraction methods that use the previous sentic computing resources AffectiveSpace and SenticNet. These methods are efficient approaches for extracting affect-aware representations from text. In addition, this paper presents a machine learning framework using an ensemble of different features to improve the overall classification performance. Following the description of this approach, we also study the effects of known feature extraction methods such as TF-IDF and SIMilarity-based sentiment projectiON (SIMON). We perform a thorough evaluation of the proposed features across five different datasets that cover radicalization and hate speech detection tasks. To compare the different approaches fairly, we conducted a statistical test that ranks the studied methods. The obtained results indicate that combining affect-aware features with the studied textual representations effectively improves performance. We also propose a criterion considering both classification performance and computational complexity to select among the different methods. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.","Affective computing; Hate speech detection; Machine learning; Natural language processing; Radicalization detection; Sentic computing","Extraction; Natural language processing systems; Speech recognition; Turing machines; Affective Computing; Classification performance; Feature extraction methods; Myriad applications; NAtural language processing; Sentic Computing; Speech detection; Textual representation; Feature extraction",Article,Scopus,2-s2.0-85101092143
"Linnik A., Bastiaanse R., Stede M., Khudyakova M.","56985757000;7003821546;6701450094;57034149900;","Linguistic mechanisms of coherence in aphasic and non-aphasic discourse",2022,"Aphasiology","36","2",,"123","146",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101053623&doi=10.1080%2f02687038.2020.1852527&partnerID=40&md5=b3d1d26a6f915cd8a15c1ec9b6364c89","Background: Coherence is the quality that distinguishes discourse from a random collection of sentences. People with aphasia have been reported to produce less-coherent discourse than non-language-impaired speakers. It is largely unclear how coherence is established in natural language and what leads to its impairment in aphasia. Aims: This paper presents a cross-methodological investigation on coherence in the discourse of Russian native speakers with and without aphasia. The purpose of this study was to examine the connection between language impairments in aphasia and different aspects of discourse coherence in order to determine the linguistic mechanisms that could be involved in establishing and maintaining it. Methods & Procedures: Coherence was operationalised as a combination of four aspects: informativeness, clarity, connectedness, and understandability. Twenty participants were asked to retell the content of a short movie. The retellings were annotated using Rhetorical Structure Theory (RST), a formalistic framework for discourse-structure analysis. Next, they were evaluated for coherence on a four-point scale by trained raters. The ratings were compared between groups. A classification analysis was performed to determine whether the ratings could be predicted based on the macrolinguistic variables collected from the RST annotations and several microlinguistic variables previously linked to coherence. Result: Retellings produced by speakers with aphasia received lower ratings than those of control participants on all aspects of coherence. The results indicate that different combinations of microlinguistic and discourse-structure variables play a role in establishing each of the coherence aspects. Conclusions: Our results provided supporting evidence on coherence impairment in aphasia. Perception of a discourse as more or less coherent was associated with both microlinguistic and macrolinguistic variables, with different combinations of variables relevant for each of the aspects. Furthermore, we found that discourse structure plays an important role, especially for understandability. We speculate that pragmatic knowledge shared by interlocutors may boost the coherence of aphasic discourse. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","Aphasia; coherence; discourse production; discourse structure; rhetorical structure theory","adult; aged; aphasia; Article; clinical article; coherence; controlled study; discourse analysis; disease severity; DNA transcription; female; human; interpersonal communication; language disability; linguistic mechanism; male; mental disease; multinuclear cell; neurorehabilitation; perception; phonetics; schizophrenia; semantics; structure analysis; theoretical study",Article,Scopus,2-s2.0-85101053623
"Wang H., Tang P., Li Q., Cheng M.","12783633400;57192544370;57202465109;57221977040;","Emotion Expression With Fact Transfer for Video Description",2022,"IEEE Transactions on Multimedia","24",,,"715","727",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100864641&doi=10.1109%2fTMM.2021.3058555&partnerID=40&md5=577fca0d95376619f0a8d94142c4e506","Translating a video into natural language is a fundamental but challenging task in visual understanding, since there is a great gap between visual content and linguistic sentence. More attention has been paid to this research field and a number of state-of-the-art results are achieved in recent years. However, the emotions in videos are usually overlooked, leading to the generated description sentences being boring and colorless. In this work, we construct a new dataset for video description with emotion expression, which consists of two parts: a re-annotated subset of the MSVD dataset with emotion embedded and another subset annotated with long sentences and rich emotions based on a video emotion recognition dataset. A fact transfer based framework is designed, which incorporates a fact stream and an emotion stream to generate sentences with emotion expression for video description. In addition, we propose a novel approach for sentence evaluation by balancing facts and emotions. A group of experiments are conducted, and the experimental results demonstrate the effectiveness of the proposed methods, including the idea of dataset construction for video description with emotion expression, model training and testing, and the emotion evaluation metric. The project page (including the code and dataset) can be found in https://mic.tongji.edu.cn/ce/70/c9778a183920/page.htm. © 1999-2012 IEEE.","Convolutional neural network; emotion; fact transfer; long short-term memory; video description","Natural language processing systems; Visual languages; Emotion expression; Emotion recognition; Evaluation metrics; Model training; Natural languages; Number of state; Research fields; Visual content; Computer hardware description languages",Article,Scopus,2-s2.0-85100864641
"Latif S., Tarner H., Beck F.","57204297580;57221051233;25824730200;","Talking Realities: Audio Guides in Virtual Reality Visualizations",2022,"IEEE Computer Graphics and Applications","42","1",,"73","83",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100836573&doi=10.1109%2fMCG.2021.3058129&partnerID=40&md5=e2bd14a4b2ae9156580f43d0da0207de","Building upon the ideas of storytelling and explorable explanations, we introduce Talking Realities, a concept for producing data-driven interactive narratives in virtual reality. It combines an audio narrative with an immersive visualization to communicate analysis results. The narrative is automatically produced using template-based natural language generation and adapts to data and user interactions. The synchronized animation of visual elements in accordance with the audio connects the two representations. In addition, we discuss various modes of explanation ranging from fully guided tours to free exploration of the data. We demonstrate the applicability of our concept by developing a virtual reality visualization for air traffic data. Furthermore, generalizability is exhibited by sketching mock-ups for two more application scenarios in the context of information and scientific visualization. © 2021 IEEE.","Data analysis; Data visualization; Speech synthesis; Synchronization; Two dimensional displays; Virtual reality; Visualization","Natural language processing systems; Virtual reality; Visualization; Application scenario; Immersive visualization; Information and scientific visualization; Interactive narrative; Natural language generation; Reality visualization; User interaction; Visual elements; Data visualization; article; human; human experiment; virtual reality",Article,Scopus,2-s2.0-85100836573
"Zheng Z., Yu Z., Zheng H., Yang Y., Shen H.T.","57202612793;36999020600;24922273300;57222954946;7404523209;","One-Shot Image-to-Image Translation via Part-Global Learning with a Multi-Adversarial Framework",2022,"IEEE Transactions on Multimedia","24",,,"480","491",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100456082&doi=10.1109%2fTMM.2021.3053775&partnerID=40&md5=b7627e78847e1e2a302adb4aeb339022","It is well known that humans can learn and recognize objects effectively from several limited image samples. However, learning from just a few images is still a tremendous challenge for existing main-stream deep neural networks. Inspired by analogical reasoning in the human mind, a feasible strategy is to 'translate' the abundant images of a rich source domain to enrich the relevant yet different target domain with insufficient image data. To achieve this goal, we propose a novel, effective multi-adversarial framework (MA) based on part-global learning, which accomplishes the one-shot cross-domain image-to-image translation. In specific, we first devise a part-global adversarial training scheme to provide an efficient way for feature extraction and prevent discriminators from being overfitted. Then, a multi-adversarial mechanism is employed to enhance the image-to-image translation ability to unearth the high-level semantic representation. Moreover, a balanced adversarial loss function is presented, which aims to balance the training data and stabilize the training process. Extensive experiments demonstrate that the proposed approach can obtain impressive results on various datasets between two extremely imbalanced image domains and outperform state-of-the-art methods on one-shot image-to-image translation. Our code will be released with this paper at https://github.com/zhengziqiang/OST. © 2021 IEEE.","generative adversarial networks; image-to-image translation; One-shot; unpaired cross-domain translation","Deep learning; Deep neural networks; Semantics; Translation (languages); Analogical reasoning; Global learning; High level semantics; Image translation; Loss functions; State-of-the-art methods; Training process; Training schemes; Image enhancement",Article,Scopus,2-s2.0-85100456082
"Du Y., Li T., Pathan M.S., Teklehaimanot H.K., Yang Z.","57221699951;56226319700;57192688833;57221702942;57193159652;","An Effective Sarcasm Detection Approach Based on Sentimental Context and Individual Expression Habits",2022,"Cognitive Computation","14","1",,"78","90",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099869700&doi=10.1007%2fs12559-021-09832-x&partnerID=40&md5=b515a1236582984575ceafb7652c40aa","Sarcasm is common in social media, and people use it to express their opinions with stronger emotions indirectly. Although it belongs to a branch of sentiment analysis, traditional sentiment analysis methods cannot identify the rhetoric of irony as it requires a significant amount of background knowledge. Existing sarcasm detection approaches mainly focus on analyzing the text content of sarcasm using various natural language processing techniques. It is argued herein that the essential issue for detecting sarcasm is examining its context, including sentiments of texts that reply to the target text and user’s expression habit. A dual-channel convolutional neural network is proposed that analyzes not only the semantics of the target text, but also its sentimental context. In addition, SenticNet is used to add common sense to the long short-term memory (LSTM) model. The attention mechanism is then applied to take the user’s expression habits into account. A series of experiments were carried out on several public datasets, the results of which show that the proposed approach can significantly improve the performance of sarcasm detection tasks. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.","Attention mechanism; Convolutional neural network; Expression habit; Sarcasm detection; Sentimental context","Convolutional neural networks; Semantics; Sentiment analysis; Attention mechanisms; Back-ground knowledge; Detection approach; Detection tasks; Dual channel; NAtural language processing; Social media; Text content; Long short-term memory",Article,Scopus,2-s2.0-85099869700
"Kamal A., Abulaish M.","57206729848;6505934038;","CAT-BiGRU: Convolution and Attention with Bi-Directional Gated Recurrent Unit for Self-Deprecating Sarcasm Detection",2022,"Cognitive Computation","14","1",,"91","109",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099753266&doi=10.1007%2fs12559-021-09821-0&partnerID=40&md5=0dab35d7e3b09ced52acc733fb3bb34d","Sarcasm detection has been a well-studied problem for the computational linguistic researchers. However, research related to different categories of sarcasm has still not gained much attention. Self-Deprecating Sarcasm (SDS) is a special category of sarcasm in which users apply sarcasm over themselves, and it is extensively used in social media platforms, mainly as an advertising tool for the brand endorsement, product campaign, and digital marketing with an aim to increase the sales volume. In this paper, we present a deep learning approach for detecting SDS on Twitter. We propose a novel Convolution and Attention with Bi-directional Gated Recurrent Unit (CAT-BiGRU) model, which consists of an input, embedding, convolutional, Bi-directional Gated Recurrent Unit (BiGRU), and two attention layers. The convolutional layer extracts SDS-based syntactic and semantic features from the embedding layer, BiGRU layer retrieves contextual information from the extracted features in both preceding and succeeding directions, and attention layers are used to retrieve SDS-based comprehensive context representation from the input texts. Finally, sigmoid function is employed to classify the input texts as a self-deprecating or non-self-deprecating sarcasm. Experiments are conducted over seven Twitter datasets to evaluate the proposed (CAT-BiGRU) model using standard evaluation metrics. The experimental results are impressive and significantly better than many neural network-based baselines and state-of-the-art methods. In this paper, we have highlighted biologically inspired and psychologically motivated basis of the proposed approach to examine its affective capabilities with respect to SenticNet. The efficacy of the proposed model is evaluated on two SenticNet-based sentic computing resources—Amazon word embedding and AffectiveSpace. Based on the experimental results, we conclude that deep learning-based approaches have potential to detect SDS in social media texts accurately. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.","Attention mechanism; BiGRU; CNN; Deep learning; Self-deprecating sarcasm; Sentic computing","Biomimetics; Convolution; Embeddings; Marketing; Semantics; Social networking (online); Biologically inspired; Context representation; Contextual information; Digital marketing; Learning-based approach; Social media platforms; Standard evaluations; State-of-the-art methods; Recurrent neural networks",Article,Scopus,2-s2.0-85099753266
"Liu C., Mao Z., Zhang T., Liu A.-A., Wang B., Zhang Y.","57203837593;29467706600;55729040600;23480048200;57161232700;57215882960;","Focus Your Attention: A Focal Attention for Multimodal Learning",2022,"IEEE Transactions on Multimedia","24",,,"103","115",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098765834&doi=10.1109%2fTMM.2020.3046855&partnerID=40&md5=ea48f13c255478eb17da2e8093a829ad","The key point in multimodal learning is to learn semantic alignment that finds the correspondence between sub-elements of instances from different modality data. Attention mechanism has shown its power in semantic alignment learning as it enables to densely associate sub-elements across different modalities. However, for each sub-element, existing attention aligns it with all the sub-elements from another modality, while most of them have no correspondence with it, i.e. irrelevant sub-elements. The irrelevant sub-elements will distract the semantic alignment if they are also attended. In this paper, we propose a novel focal attention mechanism to learn more accurate semantic alignment. The focal attention sparsely attends to a subset of sub-elements, which are identified as relevant ones according to their posterior probabilities given each sub-element from another modality. Based on the observation that relevant sub-elements mostly describe the same semantic, the posterior probability can precisely distinguish relevant and irrelevant ones by taking interactions within the same modality into consideration, such that relevant sub-elements get higher and closer posterior probabilities, while irrelevant ones get lower probabilities. Such a design learns better semantic alignment by preventing the interference of irrelevant sub-elements, and it facilitates subsequent multimodal tasks that demand semantic alignment. To validate the effectiveness of the focal attention, we conduct extensive experiments on image-text matching and text-to-image generation, and we propose a bidirectional and stacked version of focal attention for them, respectively. Experimental results on benchmarks show that the focal attention can significantly and consistently outperform state-of-the-arts. © 2021 IEEE.","Focal attention; multimodal learning","Alignment; Probability; Attention mechanisms; Image generations; Lower probabilities; Multi-modal learning; Posterior probability; Semantic alignments; State of the art; Sub-element; Semantics",Article,Scopus,2-s2.0-85098765834
"Najafipour S., Hosseini S., Hua W., Kangavari M.R., Zhou X.","57217070811;56154444900;55183678300;14039114000;8837009900;","SoulMate: Short-Text Author Linking through Multi-Aspect Temporal-Textual Embedding",2022,"IEEE Transactions on Knowledge and Data Engineering","34","1",,"448","461",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097274499&doi=10.1109%2fTKDE.2020.2982148&partnerID=40&md5=83331dd47e0a318025f579c2a147e246","Linking authors of short-text contents has important usages in many applications, including Named Entity Recognition (NER) and human community detection. However, certain challenges lie ahead. First, the input short-text contents are noisy, ambiguous, and do not follow the grammatical rules. Second, traditional text mining methods fail to effectively extract concepts through words and phrases. Third, the textual contents are temporally skewed, which can affect the semantic understanding by multiple time facets. Finally, using knowledge-bases can make the results biased to the content of the external database and deviate the meaning from the input short text corpus. To overcome these challenges, we devise a neural network-based temporal-textual framework that generates the subgraphs with highly correlated authors from short-text contents. Our approach, on the one hand, computes the relevance score (edge weight) between the authors through considering a portmanteau of contents and concepts, and on the other hand, employs a stack-wise graph cutting algorithm to extract the communities of the related authors. Experimental results show that compared to other knowledge-centered competitors, our multi-aspect vector space model can achieve a higher performance in linking short-text authors. In addition, given the author linking task, the more comprehensive the dataset is, the higher the significance of the extracted concepts will be. © 1989-2012 IEEE.","Author linking; Semantic understanding; Short text inference; Temporally multifaceted; Word2Vec","Character recognition; Graph algorithms; Graphic methods; Natural language processing systems; Semantics; Vector spaces; Cutting algorithm; External database; Highly-correlated; Human communities; Knowledge basis; Named entity recognition; Semantic understanding; Vector space models; Text mining",Article,Scopus,2-s2.0-85097274499
"Chakrawarti R.K., Bansal J., Bansal P.","55547333500;57211783498;55517000200;","Machine translation model for effective translation of Hindi poetries into English",2022,"Journal of Experimental and Theoretical Artificial Intelligence","34","1",,"95","109",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096585591&doi=10.1080%2f0952813X.2020.1836033&partnerID=40&md5=0fc13ad32dd3a0580bb9ec85207a2de3","The Word Sense Disambiguation (WSD) is a process of disambiguating the sense of the text according to its context. Machine translation is one of the challenging task since it requires effective representation of the text to capture semantic relation between Hindi lyrics in English normal language behaviour. This paper focuses on WSD methods to deal with dialects that convert Hindi lyrics to English in its syntactic structure of the words. WSD is a phenomenon for disambiguating the text so that machine would be capable to deduce correct sense of individual given words. WSD is critical for solving natural language tasks such as Machine Translation (MT) and speech processing. The distinguishing proof of significant words in Hindi as the language is not as simple as that of dialects in English. The interpretations of sonnets through the machines are exceptionally essential and deliberate about mind-blowing events. The interpretation of English ballads into other local dialects can turn out to be very straightforward, however, vice-versa is troublesome. This is due to the assortment of structures, classes, and feelings of the local dialects. Various endeavours have been connected far and wide towards the programmed interpretation of ballads from local dialects into English. In this paper, we propose a half breed MT (HBMT) procedure driven by the standard based MT together with measurements based on statistical machine translation (SMT) and rule-based machine translation (RBMT) for WSD in natural script Hindi in English Lyrics. This proposed method improves the semantic and syntactic accuracy of a machine interpretation framework. Finally, the proposed approach result is compared with the machine translation methods such a Google and Microsoft Bing Babylonian and HMT translators provided achieves a better outcome compared to the existing standards. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","machine translation; rule-based method; statistical-based method; tree-based algorithm; Word sense disambiguation","Computational linguistics; Computer aided language translation; Natural language processing systems; Semantics; Speech processing; Syntactics; Turing machines; Effective translation; Machine interpretation; Machine translation methods; Machine translation models; Machine translations; Rule-based machine translations; Statistical machine translation; Word-sense disambiguation; Speech transmission",Article,Scopus,2-s2.0-85096585591
"Kavé G., Sapir-Yogev S., Zamsh O., Waintraub N.","8107938700;57212133990;57219961492;57217829169;","Explaining vocabulary knowledge in adulthood through comparison with knowledge of math concepts",2022,"Aging, Neuropsychology, and Cognition","29","1",,"34","47",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096292121&doi=10.1080%2f13825585.2020.1846675&partnerID=40&md5=8680aaee2c894dbeb367279871c26182","Previous research has assumed that all types of semantic knowledge are similarly affected by aging. We investigate whether knowledge of vocabulary and math concepts show comparable lifetime change. A sample of 252 participants aged 17–91 completed two multiple-choice tasks that examined knowledge of infrequent word meanings and knowledge of basic math concepts. Up to age 64, vocabulary scores improved, whereas math scores remained stable. After that age, vocabulary scores remained stable, while math scores declined. We suggest that the fact that the learning and use of infrequent vocabulary are incidental, incremental, and contextual contributes to maintenance of word knowledge into old age. In contrast, learning of basic math concepts occurs relatively early in life in an intentional manner, and both learning and use of these concepts involve constrained contexts. Thus, the nature of the acquisition and use of semantic knowledge across the lifespan affects its fate in old age. © 2022 Informa UK Limited, trading as Taylor & Francis Group.","cognitive aging; Language; long-term retention; semantics; vocabulary","adolescent; adult; adulthood; aged; article; cognitive aging; female; human; human experiment; human tissue; language; learning; lifespan; major clinical study; male; semantics; vocabulary",Article,Scopus,2-s2.0-85096292121
"Saha T., Saha S., Bhattacharyya P.","57205201968;17435835500;7101803108;","Towards Sentiment-Aware Multi-Modal Dialogue Policy Learning",2022,"Cognitive Computation","14","1",,"246","260",,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095713695&doi=10.1007%2fs12559-020-09769-7&partnerID=40&md5=7a4c539d4cf086842d5feae04970c378","Creation of task-oriented dialog/virtual agent (VA) capable of managing complex domain-specific user queries pertaining to multiple intents is difficult since the agent must deal with several subtasks simultaneously. Most end-to-end dialogue systems, however, only provide user semantics as inputs from texts into the learning process and neglect other useful user behaviour and information from other modalities such as images. This stresses the benefit of incorporating multi-modal inputs for eliciting user preference in the task. Also, sentiment of the user plays a significant role in achieving maximum user/customer satisfaction during the conversation. Thus, it is also important to incorporate users’ sentiments during policy learning, especially when serving user’s composite goals. For the creation of multi-modal VA aided with sentiment for conversations encompassing multi-intents, this paper introduces a new dataset, named Vis-SentiVA: Visual and Sentiment aided VA created from open-accessed conversational dataset. We present a hierarchical reinforcement learning (HRL) typically options-based VA to learn policies for serving multi-intent dialogues. Multi-modal information (texts and images) extraction to identify user’s preference is incorporated in the learning framework. A combination of task-based and sentiment-based rewards is integrated in the hierarchical value functions for the VA to be user adaptive. Empirically, we show that all these aspects induced together in the learning framework play a vital role in acquiring higher dialogue task success and increased user contentment in the process of creating composite-natured VAs. This is the first effort in integrating sentiment-aware rewards in the multi-modal HRL framework. The paper highlights that it is indeed essential to include other modes of information extraction such as images and behavioural cues of the user such as sentiment to secure greater user contentment. This also helps in improving success of composite-natured VAs serving task-oriented dialogues. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Hierarchical reinforcement learning; Multi-intent; Multi-modal; Policy learning; Sentiment; Task-oriented","Image processing; Learning systems; Reinforcement learning; Semantics; Speech processing; Complex domains; Dialogue systems; Hierarchical reinforcement learning; Learning frameworks; Learning process; Multi-modal information; Policy learning; Value functions; Behavioral research",Article,Scopus,2-s2.0-85095713695
"Sur A., Birge J.R.","57217572801;7005600806;","Asymptotic behavior of solutions: An application to stochastic NLP",2022,"Mathematical Programming","191","1",,"281","306",,1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089512321&doi=10.1007%2fs10107-020-01554-6&partnerID=40&md5=2b12caf0865c2f1a80a9877bdeb715e0","In this article we study the consistency of optimal and stationary (KKT) points of a stochastic non-linear optimization problem involving expectation functionals, when the underlying probability distribution associated with the random variable is weakly approximated by a sequence of random probability measures. The optimization model includes constraints with expectation functionals those are not captured in direct application of the previous results on optimality conditions exist in the literature. We first study the consistency of stationary points of a general NLP problem with convex and locally Lipschitz data and then apply those results to the stochastic NLP problem and stochastic minimax problem. Moreover, we derive an exponential bound for such approximations using a large deviation principle. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature and Mathematical Optimization Society.","Approximation in optimization; Consistency; Large deviation principle; Optimal points; Sanov’s theorem; Stationary points; Stochastic minimax program; Stochastic non-linear optimization problem","Natural language processing systems; Nonlinear programming; Probability distributions; Asymptotic behavior of solutions; Exponential bounds; Large deviation principle; Non-linear optimization problems; Optimality conditions; Optimization modeling; Probability measures; Stationary points; Stochastic systems",Article,Scopus,2-s2.0-85089512321
